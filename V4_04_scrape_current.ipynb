{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SMU Course Scraping Using Selenium for new classes**\n",
    "\n",
    "<div style=\"background-color:#FFD700; padding:15px; border-radius:5px; border: 2px solid #FF4500;\">\n",
    "    \n",
    "  <h1 style=\"color:#8B0000;\">‚ö†Ô∏èüö® SCRAPE THIS DATA AT YOUR OWN RISK üö®‚ö†Ô∏è</h1>\n",
    "  \n",
    "  <p><strong>üìå If you need the data, please contact me directly.</strong> Only available for **existing students**.</p>\n",
    "\n",
    "  <h3>üîó üì© How to Get the Data?</h3>\n",
    "  <p>üì® <strong>Reach out to me for access</strong> instead of scraping manually.</p>\n",
    "  <p>Visit <a href='https://www.afterclass.io/'>AfterClass</a> to use the data for planning</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Objective**\n",
    "This script is designed to scrape SMU course details from the BOSS system using Selenium. The process involves:\n",
    "1. Logging into the system manually to bypass authentication.\n",
    "2. Iteratively scraping class details for specified academic years and terms.\n",
    "3. Writing the scraped data to structured CSV files.\n",
    "\n",
    "The data is then ingested into [AfterClass.io](https://www.afterclass.io/) to serve students.\n",
    "\n",
    "### **Script Structure**\n",
    "1. **Setup**: Import libraries and initialize Selenium WebDriver.\n",
    "2. **Login**: Wait for manual login and authentication.\n",
    "3. **Scraping Logic**:\n",
    "    - `scrape_class_details`: Scrapes course details for a specific class number, academic year, and term.\n",
    "    - `main`: Manages the scraping process for multiple academic years and terms.\n",
    "4. **Execution**: Log in and start scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PGGSSENCMODE'] = 'disable'\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import win32com.client as win32\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from pathlib import Path\n",
    "from thefuzz import fuzz\n",
    "import uuid\n",
    "import logging\n",
    "import psycopg2\n",
    "from typing import List, Optional, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "import webbrowser\n",
    "import json\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the academic term range you want to scrape or process.\n",
    "# For a single term, set both START and END to the same value.\n",
    "START_AY_TERM = '2025-26_T1'\n",
    "END_AY_TERM = '2025-26_T1'\n",
    "ACAD_TERM_ID = 'AY202526T1'\n",
    "\n",
    "# Define the specific bidding round and window you want to target.\n",
    "# Set to None to let the script auto-detect the current phase based on the schedule.\n",
    "TARGET_ROUND = None   # e.g., '1A', '2', etc.\n",
    "TARGET_WINDOW = None  # e.g., 1, 2, 3, etc.\n",
    "\n",
    "# Central bidding schedule for each academic term.\n",
    "# The script will use this to determine the correct folder names and bidding phases.\n",
    "# Format: (results_datetime, \"Full Bidding Window Name\", \"Folder_Suffix\")\n",
    "BIDDING_SCHEDULES = {\n",
    "    '2025-26_T1': [\n",
    "        (datetime(2025, 7, 9, 14, 0), \"Round 1 Window 1\", \"R1W1\"),\n",
    "        (datetime(2025, 7, 11, 14, 0), \"Round 1A Window 1\", \"R1AW1\"),\n",
    "        (datetime(2025, 7, 14, 14, 0), \"Round 1A Window 2\", \"R1AW2\"),\n",
    "        (datetime(2025, 7, 16, 14, 0), \"Round 1A Window 3\", \"R1AW3\"),\n",
    "        (datetime(2025, 7, 18, 14, 0), \"Round 1B Window 1\", \"R1BW1\"),\n",
    "        (datetime(2025, 7, 21, 14, 0), \"Round 1B Window 2\", \"R1BW2\"),\n",
    "        (datetime(2025, 7, 30, 14, 0), \"Incoming Exchange Rnd 1C Win 1\", \"R1CW1\"),\n",
    "        (datetime(2025, 7, 31, 14, 0), \"Incoming Exchange Rnd 1C Win 2\", \"R1CW2\"),\n",
    "        (datetime(2025, 8, 1, 14, 0), \"Incoming Exchange Rnd 1C Win 3\", \"R1CW3\"),\n",
    "        (datetime(2025, 8, 11, 14, 0), \"Incoming Freshmen Rnd 1 Win 1\", \"R1FW1\"),\n",
    "        (datetime(2025, 8, 12, 14, 0), \"Incoming Freshmen Rnd 1 Win 2\", \"R1FW2\"),\n",
    "        (datetime(2025, 8, 13, 14, 0), \"Incoming Freshmen Rnd 1 Win 3\", \"R1FW3\"),\n",
    "        (datetime(2025, 8, 14, 14, 0), \"Incoming Freshmen Rnd 1 Win 4\", \"R1FW4\"),\n",
    "        (datetime(2025, 8, 20, 14, 0), \"Round 2 Window 1\", \"R2W1\"),\n",
    "        (datetime(2025, 8, 22, 14, 0), \"Round 2 Window 2\", \"R2W2\"),\n",
    "        (datetime(2025, 8, 25, 14, 0), \"Round 2 Window 3\", \"R2W3\"),\n",
    "        (datetime(2025, 8, 27, 14, 0), \"Round 2A Window 1\", \"R2AW1\"),\n",
    "        (datetime(2025, 8, 29, 14, 0), \"Round 2A Window 2\", \"R2AW2\"),\n",
    "        (datetime(2025, 9, 1, 14, 0), \"Round 2A Window 3\", \"R2AW3\"),\n",
    "    ]\n",
    "    # You can add schedules for other terms here, e.g., '2025-26_T2': [...]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Scrape all BOSS data**\n",
    "\n",
    "### **BOSS Class Scraper Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `BOSSClassScraper` class automates the extraction of class timing data from SMU's BOSS system. It systematically scrapes class details across multiple academic terms and saves them as HTML files for further processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Automated Web Scraping**: Navigates through BOSS class detail pages using Selenium WebDriver\n",
    "- **Flexible Term Range**: Dynamically derives academic years from input parameters (e.g., '2025-26_T1' to '2028-29_T2') rather than hardcoded lists\n",
    "- **Smart Pagination**: Scans class numbers from 1000-5000 with intelligent termination after 300 consecutive empty records\n",
    "- **Data Organization**: Saves HTML files in structured directories by academic term (`script_input/classTimingsFull/`)\n",
    "- **Incremental CSV Updates**: Appends only new valid files to the existing CSV index, avoiding duplicates\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, standard libraries (`os`, `time`, `csv`, `re`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- Network access to SMU's BOSS system\n",
    "\n",
    "**User Requirements:**\n",
    "- **Manual Authentication**: User must manually log in and complete Microsoft Authenticator process when prompted\n",
    "- **SMU Credentials**: Valid access to BOSS system\n",
    "- **Directory Structure**: Code creates `script_input/classTimingsFull/` for HTML files and `script_input/scraped_filepaths.csv` for the file index\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "# The scraper now uses the global configuration variables defined in cell 1.1\n",
    "scraper = BOSSClassScraper()\n",
    "\n",
    "# The start and end terms are read from START_AY_TERM and END_AY_TERM\n",
    "success = scraper.run_full_scraping_process(START_AY_TERM, END_AY_TERM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "class BOSSClassScraper:\n",
    "    \"\"\"\n",
    "    A class to scrape class details from BOSS (SMU's online class registration system).\n",
    "    It performs a full scan for the first bidding window of a term and targeted\n",
    "    re-scrapes for subsequent windows based on previously found classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BOSS Class Scraper with configuration parameters.\n",
    "        \"\"\"\n",
    "        self.term_code_map = {'T1': '10', 'T2': '20', 'T3A': '31', 'T3B': '32'}\n",
    "        self.all_terms = ['T1', 'T2', 'T3A', 'T3B']\n",
    "        self.driver = None\n",
    "        self.min_class_number = 1000\n",
    "        self.max_class_number = 5000\n",
    "        self.consecutive_empty_threshold = 300\n",
    "        \n",
    "        # Use the global bidding schedule\n",
    "        self.bidding_schedule = BIDDING_SCHEDULES\n",
    "\n",
    "    def _get_bidding_round_info_for_term(self, ay_term, now):\n",
    "        \"\"\"\n",
    "        Determines the bidding round folder name for a given academic term based on the current time.\n",
    "        \"\"\"\n",
    "        # Get the schedule for the specific academic term\n",
    "        schedule = self.bidding_schedule.get(ay_term)\n",
    "        if not schedule:\n",
    "            return None\n",
    "\n",
    "        # Find the correct window from the schedule\n",
    "        for results_date, _, folder_suffix in schedule:\n",
    "            if now < results_date:\n",
    "                return f\"{ay_term}_{folder_suffix}\"\n",
    "        return None\n",
    "\n",
    "    def wait_for_manual_login(self):\n",
    "        \"\"\"Wait for manual login and Microsoft Authenticator process completion.\"\"\"\n",
    "        print(\"Please log in manually and complete the Microsoft Authenticator process.\")\n",
    "        print(\"Waiting for BOSS dashboard to load...\")\n",
    "        wait = WebDriverWait(self.driver, 120)\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"Label_UserName\")))\n",
    "            username = self.driver.find_element(By.ID, \"Label_UserName\").text\n",
    "            print(f\"Login successful! Logged in as {username}\")\n",
    "        except TimeoutException:\n",
    "            raise Exception(\"Login failed or timed out.\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    def scrape_and_save_html(self, start_ay_term=START_AY_TERM, end_ay_term=END_AY_TERM, base_dir='script_input/classTimingsFull'):\n",
    "        \"\"\"\n",
    "        Scrapes class details, always performing a full scan from 1000-5000.\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        start_year = int(start_ay_term[:4])\n",
    "        end_year = int(end_ay_term[:4])\n",
    "        all_academic_years = [f\"{year}-{(year + 1) % 100:02d}\" for year in range(start_year, end_year + 1)]\n",
    "        all_ay_terms = [f\"{ay}_{term}\" for ay in all_academic_years for term in self.all_terms]\n",
    "        \n",
    "        try:\n",
    "            start_idx = all_ay_terms.index(start_ay_term)\n",
    "            end_idx = all_ay_terms.index(end_ay_term)\n",
    "        except ValueError:\n",
    "            print(\"Invalid start or end term provided.\")\n",
    "            return\n",
    "            \n",
    "        ay_terms_to_scrape = all_ay_terms[start_idx:end_idx+1]\n",
    "        \n",
    "        for ay_term in ay_terms_to_scrape:\n",
    "            print(f\"\\nProcessing Academic Term: {ay_term}\")\n",
    "            \n",
    "            round_window_folder_name = self._get_bidding_round_info_for_term(ay_term, now)\n",
    "            if not round_window_folder_name:\n",
    "                print(f\"Not in a bidding window for {ay_term} at this time. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            current_round_path = os.path.join(base_dir, ay_term, round_window_folder_name)\n",
    "            os.makedirs(current_round_path, exist_ok=True)\n",
    "            \n",
    "            ay, term = ay_term.split('_')\n",
    "            ay_short, term_code = ay[2:4], self.term_code_map.get(term, '10')\n",
    "\n",
    "            # Always perform full scan regardless of previous rounds\n",
    "            print(f\"Performing full scan for {ay_term}.\")\n",
    "            consecutive_empty = 0\n",
    "            for class_num in range(self.min_class_number, self.max_class_number + 1):\n",
    "                was_scraped = self._scrape_single_class(current_round_path, ay_short, term_code, class_num)\n",
    "                if was_scraped is None: # Error occurred, stop this scan\n",
    "                    break\n",
    "                if not was_scraped: # Page had no record\n",
    "                    consecutive_empty += 1\n",
    "                    if consecutive_empty >= self.consecutive_empty_threshold:\n",
    "                        print(f\"Stopping scan after {consecutive_empty} consecutive empty records.\")\n",
    "                        break\n",
    "                else: # Successful scrape\n",
    "                    consecutive_empty = 0\n",
    "        print(\"\\nScraping process completed.\")\n",
    "    \n",
    "    def _scrape_single_class(self, target_path, ay_short, term_code, class_num):\n",
    "        \"\"\"\n",
    "        Scrapes a single class number and saves the HTML, always overwriting existing files.\n",
    "        Returns True if data was found, False if \"No record found\", None on error.\n",
    "        \"\"\"\n",
    "        filename = f\"SelectedAcadTerm={ay_short}{term_code}&SelectedClassNumber={class_num:04}.html\"\n",
    "        filepath = os.path.join(target_path, filename)\n",
    "\n",
    "        # Remove the existing file check - always scrape\n",
    "        url = f\"https://boss.intranet.smu.edu.sg/ClassDetails.aspx?SelectedClassNumber={class_num:04}&SelectedAcadTerm={ay_short}{term_code}&SelectedAcadCareer=UGRD\"\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            # Robust wait for either content or an error message\n",
    "            WebDriverWait(self.driver, 15).until(EC.any_of(\n",
    "                EC.visibility_of_element_located((By.ID, \"RadGrid_MeetingInfo_ctl00\")),\n",
    "                EC.presence_of_element_located((By.ID, \"lblErrorDetails\"))\n",
    "            ))\n",
    "            \n",
    "            page_source = self.driver.page_source\n",
    "            if \"No record found\" in page_source:\n",
    "                print(f\"No record for class {class_num}\")\n",
    "                return False\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(page_source)\n",
    "            print(f\"Saved {filepath}\")\n",
    "            time.sleep(1) # Small delay to be polite\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "            time.sleep(5)\n",
    "            return None # Indicate an error occurred\n",
    "\n",
    "    def generate_scraped_filepaths_csv(self, base_dir='script_input/classTimingsFull', output_csv='script_input/scraped_filepaths.csv'):\n",
    "        \"\"\"Generates/appends to a CSV file with paths to all valid HTML files.\"\"\"\n",
    "        existing_filepaths = set()\n",
    "        if os.path.exists(output_csv):\n",
    "            try:\n",
    "                with open(output_csv, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "                    reader = csv.reader(csvfile)\n",
    "                    next(reader)\n",
    "                    for row in reader:\n",
    "                        if row: existing_filepaths.add(row[0])\n",
    "            except (IOError, StopIteration) as e:\n",
    "                print(f\"Could not read existing CSV, will overwrite: {e}\")\n",
    "\n",
    "        new_filepaths = []\n",
    "        for root, _, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.html'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    if filepath not in existing_filepaths:\n",
    "                        new_filepaths.append(filepath)\n",
    "        \n",
    "        if not new_filepaths:\n",
    "            print(\"No new valid HTML files found to add to the CSV.\")\n",
    "            return\n",
    "\n",
    "        mode = 'a' if existing_filepaths else 'w'\n",
    "        with open(output_csv, mode, newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if mode == 'w':\n",
    "                writer.writerow(['Filepath'])\n",
    "            for path in new_filepaths:\n",
    "                writer.writerow([path])\n",
    "        \n",
    "        print(f\"CSV updated. Total valid files now: {len(existing_filepaths) + len(new_filepaths)}\")\n",
    "\n",
    "    def run_full_scraping_process(self, start_ay_term=START_AY_TERM, end_ay_term=END_AY_TERM):\n",
    "        \"\"\"Run the complete scraping process for a specified term range.\"\"\"\n",
    "        try:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            self.driver.get(\"https://boss.intranet.smu.edu.sg/\")\n",
    "            self.wait_for_manual_login()\n",
    "            \n",
    "            self.scrape_and_save_html(start_ay_term, end_ay_term)\n",
    "            self.generate_scraped_filepaths_csv()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping process: {str(e)}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "            print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "scraper = BOSSClassScraper()\n",
    "success = scraper.run_full_scraping_process(START_AY_TERM, END_AY_TERM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrape Overall Bidding Results**\n",
    "\n",
    "### **ScrapeOverallResults Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `ScrapeOverallResults` class is designed to scrape the main \"Overall Results\" page from the BOSS system. This page provides a summary of bidding data for all courses in a specific round, including median/min bids and vacancy information. It is the primary source for historical and current bidding statistics.\n",
    "\n",
    "The scraper operates in two main modes, controlled by the global configuration variables:\n",
    "\n",
    "- **Automatic Phase Detection**: If `TARGET_ROUND` and `TARGET_WINDOW` are set to `None`, the scraper uses the current system time to check against the `BIDDING_SCHEDULES`. It automatically determines the most recently concluded bidding phase and scrapes its results. This is the default and recommended mode for running during the bidding period.\n",
    "\n",
    "- **Manual Targeting**: If `TARGET_ROUND` and `TARGET_WINDOW` are set to specific values (e.g., `'1A'`, `2`), the scraper will ignore the current time and target that exact round and window for the academic term defined in `START_AY_TERM`.\n",
    "\n",
    "**Key Features:**\n",
    "- **Automated or Manual Mode**: Can either auto-detect the correct bidding phase or be manually aimed at a specific round/window.\n",
    "- **Robust Form Interaction**: Reliably navigates the BOSS interface, selecting the correct term, round, and window from dropdown menus.\n",
    "- **Full Data Extraction**: Scrapes all pages of the results table, setting the page size to 50 for efficiency.\n",
    "- **Structured Output**: Saves the final, cleaned data into a single Excel file named after the academic term (e.g., `2025-26 T1.xlsx`) in the `script_input/overallBossResults/` directory.\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- All packages from the main setup cell.\n",
    "- Network access to SMU's BOSS system.\n",
    "\n",
    "**User Requirements:**\n",
    "- **Manual Authentication**: User must manually log in and complete the Microsoft Authenticator process.\n",
    "- **Global Configuration**: Relies on `START_AY_TERM`, `TARGET_ROUND`, `TARGET_WINDOW`, and `BIDDING_SCHEDULES` defined in the configuration cell.\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "# The scraper reads its targets from the global configuration variables.\n",
    "# This example will use the values set in START_AY_TERM, TARGET_ROUND, and TARGET_WINDOW.\n",
    "scraper = ScrapeOverallResults(headless=False, delay=5)\n",
    "scraper.run(\n",
    "    term=START_AY_TERM.replace('_', ' '), # Converts '2025-26_T1' to '2025-26 T1'\n",
    "    bid_round=TARGET_ROUND,\n",
    "    bid_window=TARGET_WINDOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeOverallResults:\n",
    "    \"\"\"\n",
    "    BOSS Overall Results Scraper using Selenium\n",
    "    \n",
    "    This class scrapes course bidding results from the BOSS system with proper\n",
    "    authentication, form interaction, and data extraction capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=False, delay=5):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            headless (bool): Run browser in headless mode\n",
    "            delay (int): Delay between requests in seconds\n",
    "        \"\"\"\n",
    "        self.driver = None\n",
    "        self.delay = delay\n",
    "        self.headless = headless\n",
    "        self.base_url = \"https://boss.intranet.smu.edu.sg/OverallResults.aspx\"\n",
    "        \n",
    "        # Column mapping and ordering as specified\n",
    "        self.desired_columns = [\n",
    "            'Term', 'Session', 'Bidding Window', 'Course Code', 'Description',\n",
    "            'Section', 'Vacancy', 'Opening Vacancy', 'Before Process Vacancy',\n",
    "            'D.I.C.E', 'After Process Vacancy', 'Enrolled Students',\n",
    "            'Median Bid', 'Min Bid', 'Instructor', 'School/Department'\n",
    "        ]\n",
    "        \n",
    "        # Use the global bidding schedule, extracting only the date and name\n",
    "        # Assuming we are targeting the start term for this scraper.\n",
    "        self.boss_schedule = []\n",
    "        if START_AY_TERM in BIDDING_SCHEDULES:\n",
    "            schedule_for_term = BIDDING_SCHEDULES[START_AY_TERM]\n",
    "            # Keep only the datetime and full name for this class's logic\n",
    "            self.boss_schedule = [(dt, name) for dt, name, suffix in schedule_for_term]\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def _determine_current_bidding_phase(self):\n",
    "        \"\"\"\n",
    "        Determine the current bidding phase based on current time\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (round, window) or (None, None) if no active phase\n",
    "        \"\"\"\n",
    "        current_time = datetime.now()\n",
    "        self.logger.info(f\"Current time: {current_time}\")\n",
    "        \n",
    "        # Find the most recent bidding phase that has started\n",
    "        active_phase = None\n",
    "        for schedule_time, phase_name in self.boss_schedule:\n",
    "            if current_time >= schedule_time:\n",
    "                active_phase = (schedule_time, phase_name)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if active_phase is None:\n",
    "            self.logger.warning(\"No active bidding phase found - before first scheduled phase\")\n",
    "            return None, None\n",
    "        \n",
    "        schedule_time, phase_name = active_phase\n",
    "        self.logger.info(f\"Current bidding phase: {phase_name} (started at {schedule_time})\")\n",
    "        \n",
    "        # Parse the phase name to extract round and window\n",
    "        # Handle different formats:\n",
    "        # \"Round 1 Window 1\" -> (\"1\", \"1\")\n",
    "        # \"Round 1A Window 2\" -> (\"1A\", \"2\")\n",
    "        # \"Round 2A Window 3\" -> (\"2A\", \"3\")\n",
    "        # \"Incoming Exchange Rnd 1C Win 1\" -> (\"1C\", \"1\")\n",
    "        # \"Incoming Freshmen Rnd 1 Win 1\" -> (\"1\", \"1\")\n",
    "        \n",
    "        try:\n",
    "            # Remove prefixes and normalize\n",
    "            normalized = phase_name.replace(\"Incoming Exchange \", \"\").replace(\"Incoming Freshmen \", \"\")\n",
    "            normalized = normalized.replace(\"Rnd \", \"Round \").replace(\"Win \", \"Window \")\n",
    "            \n",
    "            # Extract round and window using regex\n",
    "            import re\n",
    "            match = re.search(r'Round\\s+(\\d+[A-Z]*)\\s+Window\\s+(\\d+)', normalized)\n",
    "            \n",
    "            if match:\n",
    "                round_value = match.group(1)\n",
    "                window_value = match.group(2)\n",
    "                \n",
    "                self.logger.info(f\"Parsed phase: Round {round_value}, Window {window_value}\")\n",
    "                return round_value, window_value\n",
    "            else:\n",
    "                self.logger.warning(f\"Could not parse phase name: {phase_name}\")\n",
    "                return None, None\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing phase name '{phase_name}': {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Setup Chrome WebDriver with appropriate options\"\"\"\n",
    "        chrome_options = Options()\n",
    "        \n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            self.logger.info(\"Chrome WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize WebDriver: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def wait_for_manual_login(self):\n",
    "        \"\"\"\n",
    "        Wait for manual login and Microsoft Authenticator process completion.\n",
    "        \"\"\"\n",
    "        print(\"Please log in manually and complete the Microsoft Authenticator process.\")\n",
    "        print(\"Waiting for BOSS dashboard to load...\")\n",
    "        \n",
    "        wait = WebDriverWait(self.driver, 120)\n",
    "        \n",
    "        try:\n",
    "            # Wait for login success indicators\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"Label_UserName\")))\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(text(),'Sign out')]\")))\n",
    "            \n",
    "            username = self.driver.find_element(By.ID, \"Label_UserName\").text\n",
    "            print(f\"Login successful! Logged in as {username}\")\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Login failed or timed out. Could not detect login elements.\")\n",
    "            raise Exception(\"Login failed\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    def _navigate_to_overall_results(self):\n",
    "        \"\"\"Navigate to the Overall Results page\"\"\"\n",
    "        try:\n",
    "            self.driver.get(self.base_url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            wait = WebDriverWait(self.driver, 30)\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"rcboCourseCareer\")))\n",
    "            \n",
    "            self.logger.info(\"Successfully navigated to Overall Results page\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to navigate to Overall Results page: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _select_course_career(self, career=\"Undergraduate\"):\n",
    "        \"\"\"Select course career (default: Undergraduate)\"\"\"\n",
    "        try:\n",
    "            # The dropdown is already set to Undergraduate by default\n",
    "            career_input = self.driver.find_element(By.ID, \"rcboCourseCareer_Input\")\n",
    "            current_value = career_input.get_attribute(\"value\")\n",
    "            \n",
    "            if current_value != career:\n",
    "                # If we need to change it, click the dropdown arrow\n",
    "                dropdown_arrow = self.driver.find_element(By.ID, \"rcboCourseCareer_Arrow\")\n",
    "                dropdown_arrow.click()\n",
    "                time.sleep(1)\n",
    "                \n",
    "                # Select the desired option\n",
    "                option = self.driver.find_element(By.XPATH, f\"//li[@class='rcbItem' and text()='{career}']\")\n",
    "                option.click()\n",
    "                time.sleep(1)\n",
    "            \n",
    "            self.logger.info(f\"Course career set to: {career}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to select course career: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _select_term(self, term):\n",
    "        \"\"\"\n",
    "        Selects a term ONLY if it's not already the selected term.\n",
    "        \n",
    "        Args:\n",
    "            term (str): The full-text term to select (e.g., '2025-26 Term 1').\n",
    "        \"\"\"\n",
    "        try:\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            \n",
    "            # 1. First, check the currently displayed value in the term input box.\n",
    "            current_term_input = self.driver.find_element(By.ID, \"rcboTerm_Input\")\n",
    "            current_term_value = current_term_input.get_attribute(\"value\").strip()\n",
    "            \n",
    "            # 2. Compare with the desired term. If they match, do nothing.\n",
    "            if current_term_value == term:\n",
    "                self.logger.info(f\"Term '{term}' is already selected. Skipping interaction.\")\n",
    "                return\n",
    "\n",
    "            # 3. If the term needs to be changed, proceed with the selection logic.\n",
    "            self.logger.info(f\"Current term is '{current_term_value}', changing to '{term}'.\")\n",
    "            term_arrow = wait.until(EC.element_to_be_clickable((By.ID, \"rcboTerm_Arrow\")))\n",
    "            self.driver.execute_script(\"arguments[0].click();\", term_arrow)\n",
    "            \n",
    "            dropdown_div = wait.until(EC.visibility_of_element_located((By.ID, \"rcboTerm_DropDown\")))\n",
    "            \n",
    "            selected_checkboxes = dropdown_div.find_elements(By.XPATH, \".//input[@type='checkbox' and @checked='checked']\")\n",
    "            for checkbox in selected_checkboxes:\n",
    "                self.driver.execute_script(\"arguments[0].checked = false;\", checkbox)\n",
    "            \n",
    "            term_checkbox_xpath = f\"//div[@id='rcboTerm_DropDown']//label[contains(., '{term}')]/input[@type='checkbox']\"\n",
    "            term_checkbox = wait.until(EC.presence_of_element_located((By.XPATH, term_checkbox_xpath)))\n",
    "\n",
    "            self.driver.execute_script(\"arguments[0].click();\", term_checkbox)\n",
    "            \n",
    "            self.driver.find_element(By.TAG_NAME, \"body\").click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            self.logger.info(f\"Term selected: {term}\")\n",
    "            \n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            self.logger.error(f\"Failed to select term '{term}'. The element could not be found or timed out.\")\n",
    "            # Make sure to import these exceptions at the top of your script:\n",
    "            # from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "            with open(\"error_page_source.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(self.driver.page_source)\n",
    "            self.logger.info(\"Page HTML at the time of error saved to 'error_page_source.html'.\")\n",
    "            raise e\n",
    "    \n",
    "    def _select_bid_round(self, round_value=None):\n",
    "        \"\"\"\n",
    "        Select bid round\n",
    "        \n",
    "        Args:\n",
    "            round_value (str): Round to select (e.g., '1', '1A', '2', '1F')\n",
    "                            If None, leave as default (empty)\n",
    "                            Note: '1F' is automatically mapped to '1' for dropdown selection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if round_value is None:\n",
    "                self.logger.info(\"Bid round left as default (empty)\")\n",
    "                return\n",
    "            \n",
    "            # Map round values that don't exist in dropdown to valid options\n",
    "            original_round_value = round_value\n",
    "            if round_value == '1F':  # Freshmen Round 1 maps to Round 1\n",
    "                round_value = '1'\n",
    "                self.logger.info(f\"Mapped round '{original_round_value}' to '{round_value}' for dropdown selection\")\n",
    "            \n",
    "            # Click the round dropdown arrow\n",
    "            round_arrow = self.driver.find_element(By.ID, \"rcboBidRound_Arrow\")\n",
    "            round_arrow.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Select the round option\n",
    "            round_option = self.driver.find_element(\n",
    "                By.XPATH, \n",
    "                f\"//div[@id='rcboBidRound_DropDown']//li[@class='rcbItem' and text()='{round_value}']\"\n",
    "            )\n",
    "            round_option.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            self.logger.info(f\"Bid round selected: {round_value} (original: {original_round_value})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to select bid round '{original_round_value}' (mapped to '{round_value}'): {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _select_bid_window(self, window_value=None):\n",
    "        \"\"\"\n",
    "        Select bid window\n",
    "        \n",
    "        Args:\n",
    "            window_value (str): Window to select (e.g., '1', '2', '3')\n",
    "                               If None, leave as default (empty)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if window_value is None:\n",
    "                self.logger.info(\"Bid window left as default (empty)\")\n",
    "                return\n",
    "            \n",
    "            # Click the window dropdown arrow\n",
    "            window_arrow = self.driver.find_element(By.ID, \"rcboBidWindow_Arrow\")\n",
    "            window_arrow.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Select the window option\n",
    "            window_option = self.driver.find_element(\n",
    "                By.XPATH, \n",
    "                f\"//div[@id='rcboBidWindow_DropDown']//li[@class='rcbItem' and text()='{window_value}']\"\n",
    "            )\n",
    "            window_option.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            self.logger.info(f\"Bid window selected: {window_value}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to select bid window '{window_value}': {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _click_search(self):\n",
    "        \"\"\"Click the search button to submit the form\"\"\"\n",
    "        try:\n",
    "            search_button = self.driver.find_element(By.ID, \"RadButton_Search_input\")\n",
    "            search_button.click()\n",
    "            \n",
    "            # Wait for results to load\n",
    "            wait = WebDriverWait(self.driver, 30)\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"RadGrid_OverallResults_ctl00\")))\n",
    "            \n",
    "            self.logger.info(\"Search completed successfully\")\n",
    "            time.sleep(3)  # Give extra time for data to load\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to click search or load results: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _set_page_size_to_50(self):\n",
    "        \"\"\"Set the page size dropdown to 50 records per page\"\"\"\n",
    "        try:\n",
    "            # Click the page size dropdown arrow\n",
    "            page_size_arrow = self.driver.find_element(\n",
    "                By.ID, \"RadGrid_OverallResults_ctl00_ctl03_ctl01_PageSizeComboBox_Arrow\"\n",
    "            )\n",
    "            page_size_arrow.click()\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Select 50 from the dropdown\n",
    "            option_50 = self.driver.find_element(\n",
    "                By.XPATH, \n",
    "                \"//div[@id='RadGrid_OverallResults_ctl00_ctl03_ctl01_PageSizeComboBox_DropDown']//li[text()='50']\"\n",
    "            )\n",
    "            option_50.click()\n",
    "            \n",
    "            # Wait for page to reload with new page size\n",
    "            time.sleep(5)  # Extended wait for page reload\n",
    "            \n",
    "            self.logger.info(\"Page size set to 50 records per page\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to set page size to 50: {str(e)}\")\n",
    "            # Continue anyway, might work with default page size\n",
    "\n",
    "    def _sort_by_bidding_window(self):\n",
    "        \"\"\"Sort the results by bidding window to get Incoming Freshmen first\"\"\"\n",
    "        try:\n",
    "            # Click the Bidding Window header to sort\n",
    "            sort_link = self.driver.find_element(\n",
    "                By.XPATH, \n",
    "                \"//a[contains(@onclick, 'EVENT_TYPE_DESCRIPTION') and contains(text(), 'Bidding Window')]\"\n",
    "            )\n",
    "            sort_link.click()\n",
    "            \n",
    "            # Wait for table to reload after sorting\n",
    "            time.sleep(3)\n",
    "            wait = WebDriverWait(self.driver, 15)\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"RadGrid_OverallResults_ctl00\")))\n",
    "            \n",
    "            self.logger.info(\"Successfully sorted by Bidding Window\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to sort by bidding window: {str(e)}\")\n",
    "            # Continue without sorting rather than failing completely\n",
    "\n",
    "    def _extract_table_data(self, stop_on_bidding_window_change=False, last_bidding_window=None):\n",
    "        \"\"\"\n",
    "        Extract data from the current page table with improved robustness\n",
    "        \n",
    "        Args:\n",
    "            stop_on_bidding_window_change (bool): Whether to stop when bidding window changes\n",
    "            last_bidding_window (str): The last bidding window seen (for change detection)\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (page_data, should_stop, current_bidding_window)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Wait for table to be fully loaded\n",
    "            wait = WebDriverWait(self.driver, 15)\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"RadGrid_OverallResults_ctl00\")))\n",
    "            \n",
    "            # Additional wait for data to populate\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Find the main table\n",
    "            table = self.driver.find_element(By.ID, \"RadGrid_OverallResults_ctl00\")\n",
    "            \n",
    "            # Get all rows in the table\n",
    "            all_rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            self.logger.info(f\"Found {len(all_rows)} total rows in table\")\n",
    "            \n",
    "            # Filter for data rows only (rgRow and rgAltRow classes)\n",
    "            data_rows = []\n",
    "            for row in all_rows:\n",
    "                try:\n",
    "                    row_class = row.get_attribute(\"class\") or \"\"\n",
    "                    if \"rgRow\" in row_class or \"rgAltRow\" in row_class:\n",
    "                        data_rows.append(row)\n",
    "                except StaleElementReferenceException:\n",
    "                    continue\n",
    "            \n",
    "            self.logger.info(f\"Found {len(data_rows)} data rows\")\n",
    "            \n",
    "            if len(data_rows) == 0:\n",
    "                self.logger.warning(\"No data rows found - checking table structure\")\n",
    "                self._debug_table_content()\n",
    "                return []\n",
    "                        \n",
    "            page_data = []\n",
    "            current_bidding_window = None\n",
    "            should_stop = False\n",
    "\n",
    "            for i, row in enumerate(data_rows):\n",
    "                try:\n",
    "                    # Get all cells in the row\n",
    "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    \n",
    "                    if len(cells) < 16:\n",
    "                        self.logger.warning(f\"Row {i} has only {len(cells)} cells, expected 16\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract current row's bidding window for change detection\n",
    "                    row_bidding_window = cells[2].text.strip()\n",
    "                    if current_bidding_window is None:\n",
    "                        current_bidding_window = row_bidding_window\n",
    "                    \n",
    "                    # Check for bidding window change if requested\n",
    "                    if stop_on_bidding_window_change and last_bidding_window:\n",
    "                        if (last_bidding_window.startswith(\"Incoming Freshmen\") and \n",
    "                            not row_bidding_window.startswith(\"Incoming Freshmen\")):\n",
    "                            self.logger.info(f\"Bidding window changed from '{last_bidding_window}' to '{row_bidding_window}' - stopping extraction\")\n",
    "                            should_stop = True\n",
    "                            break\n",
    "                    \n",
    "                    # Extract section text from cell 5 (which contains a link)\n",
    "                    section_cell = cells[5]\n",
    "                    section_text = \"\"\n",
    "                    try:\n",
    "                        # Try to get link text first\n",
    "                        link = section_cell.find_element(By.TAG_NAME, \"a\")\n",
    "                        section_text = link.get_attribute(\"title\") or link.text.strip()\n",
    "                    except NoSuchElementException:\n",
    "                        # If no link, get cell text directly\n",
    "                        section_text = section_cell.text.strip()\n",
    "                    \n",
    "                    # Create record with proper column mapping\n",
    "                    record = {\n",
    "                        'Term': cells[0].text.strip(),\n",
    "                        'Session': cells[1].text.strip(), \n",
    "                        'Bidding Window': cells[2].text.strip(),\n",
    "                        'Course Code': cells[3].text.strip(),\n",
    "                        'Description': cells[4].text.strip(),\n",
    "                        'Section': section_text,\n",
    "                        'Median Bid': cells[6].text.strip(),\n",
    "                        'Min Bid': cells[7].text.strip(),\n",
    "                        'Vacancy': cells[8].text.strip(),\n",
    "                        'Opening Vacancy': cells[9].text.strip(),\n",
    "                        'Before Process Vacancy': cells[10].text.strip(),\n",
    "                        'After Process Vacancy': cells[11].text.strip(),\n",
    "                        'D.I.C.E': cells[12].text.strip(),\n",
    "                        'Enrolled Students': cells[13].text.strip(),\n",
    "                        'Instructor': cells[14].text.strip(),\n",
    "                        'School/Department': cells[15].text.strip()\n",
    "                    }\n",
    "                    \n",
    "                    # Clean up data\n",
    "                    cleaned_record = {}\n",
    "                    for key, value in record.items():\n",
    "                        # Remove extra whitespace and handle special characters\n",
    "                        cleaned_value = re.sub(r'\\s+', ' ', str(value)).strip()\n",
    "                        cleaned_value = cleaned_value.replace('\\u00a0', ' ')  # Remove &nbsp;\n",
    "                        cleaned_value = cleaned_value.replace('&nbsp;', ' ')\n",
    "                        \n",
    "                        # Handle empty values\n",
    "                        if cleaned_value == '' or cleaned_value == ' ':\n",
    "                            cleaned_value = '-'\n",
    "                        \n",
    "                        # Convert '-' to '0' for Median Bid and Min Bid fields\n",
    "                        if key in ['Median Bid', 'Min Bid'] and cleaned_value == '-':\n",
    "                            cleaned_value = '0'\n",
    "                        \n",
    "                        cleaned_record[key] = cleaned_value\n",
    "                    \n",
    "                    # Only add record if it has a valid course code\n",
    "                    if cleaned_record['Course Code'] and cleaned_record['Course Code'] != '-':\n",
    "                        page_data.append(cleaned_record)\n",
    "                        \n",
    "                        # Log first record for verification\n",
    "                        if len(page_data) == 1:\n",
    "                            self.logger.info(f\"Sample record: {cleaned_record['Course Code']} - {cleaned_record['Description'][:30]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error processing row {i}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            self.logger.info(f\"Successfully extracted {len(page_data)} valid records\")\n",
    "            return page_data, should_stop, current_bidding_window\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to extract table data: {str(e)}\")\n",
    "            self._debug_table_content()\n",
    "            return []\n",
    "    \n",
    "    def _debug_table_content(self):\n",
    "        \"\"\"Debug method to inspect table content when extraction fails\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"=== TABLE DEBUG INFO ===\")\n",
    "            \n",
    "            # Check if main table exists\n",
    "            try:\n",
    "                table = self.driver.find_element(By.ID, \"RadGrid_OverallResults_ctl00\")\n",
    "                self.logger.info(\"‚úì Main table found\")\n",
    "            except NoSuchElementException:\n",
    "                self.logger.error(\"‚úó Main table NOT found\")\n",
    "                return\n",
    "            \n",
    "            # Check all rows\n",
    "            all_rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            self.logger.info(f\"Total rows in table: {len(all_rows)}\")\n",
    "            \n",
    "            # Analyze first few rows\n",
    "            for i, row in enumerate(all_rows[:5]):\n",
    "                try:\n",
    "                    row_class = row.get_attribute(\"class\") or \"no-class\"\n",
    "                    cells = row.find_elements(By.TAG_NAME, \"td\") + row.find_elements(By.TAG_NAME, \"th\")\n",
    "                    cell_count = len(cells)\n",
    "                    \n",
    "                    first_cell_text = \"\"\n",
    "                    if cells:\n",
    "                        first_cell_text = cells[0].text.strip()[:50]\n",
    "                    \n",
    "                    is_data_row = \"rgRow\" in row_class or \"rgAltRow\" in row_class\n",
    "                    \n",
    "                    self.logger.info(f\"Row {i}: class='{row_class}', cells={cell_count}, data_row={is_data_row}\")\n",
    "                    self.logger.info(f\"  First cell: '{first_cell_text}'\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error analyzing row {i}: {str(e)}\")\n",
    "            \n",
    "            # Check for error messages in the page\n",
    "            error_elements = self.driver.find_elements(By.CLASS_NAME, \"error\")\n",
    "            if error_elements:\n",
    "                for error in error_elements:\n",
    "                    if error.text.strip():\n",
    "                        self.logger.warning(f\"Error message found: {error.text}\")\n",
    "            \n",
    "            # Check if there's a \"no data\" message\n",
    "            no_data_elements = self.driver.find_elements(By.XPATH, \"//*[contains(text(), 'No record') or contains(text(), 'no data') or contains(text(), 'No data')]\")\n",
    "            if no_data_elements:\n",
    "                for element in no_data_elements:\n",
    "                    self.logger.warning(f\"No data message: {element.text}\")\n",
    "            \n",
    "            self.logger.info(\"=== END TABLE DEBUG ===\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Debug method failed: {str(e)}\")\n",
    "    \n",
    "    def _has_next_page(self):\n",
    "        \"\"\"Check if there is a next page available using flexible selectors\"\"\"\n",
    "        try:\n",
    "            # Method 1: Look for Next Page button by title attribute (most reliable)\n",
    "            next_buttons = self.driver.find_elements(\n",
    "                By.XPATH, \"//input[@title='Next Page' and contains(@name, 'RadGrid_OverallResults')]\"\n",
    "            )\n",
    "            \n",
    "            if next_buttons:\n",
    "                next_button = next_buttons[0]\n",
    "                is_enabled = next_button.is_enabled()\n",
    "                button_class = next_button.get_attribute(\"class\") or \"\"\n",
    "                is_not_disabled = \"disabled\" not in button_class.lower()\n",
    "                \n",
    "                self.logger.debug(f\"Next button found: enabled={is_enabled}, class='{button_class}'\")\n",
    "                return is_enabled and is_not_disabled\n",
    "            \n",
    "            # Method 2: Look for Next Page button by class\n",
    "            next_buttons_by_class = self.driver.find_elements(By.CLASS_NAME, \"rgPageNext\")\n",
    "            if next_buttons_by_class:\n",
    "                next_button = next_buttons_by_class[0]\n",
    "                is_enabled = next_button.is_enabled()\n",
    "                button_class = next_button.get_attribute(\"class\") or \"\"\n",
    "                is_not_disabled = \"disabled\" not in button_class.lower()\n",
    "                \n",
    "                self.logger.debug(f\"Next button (by class) found: enabled={is_enabled}, class='{button_class}'\")\n",
    "                return is_enabled and is_not_disabled\n",
    "            \n",
    "            # Method 3: Check pagination info to see if we're on the last page\n",
    "            try:\n",
    "                # Look for pagination info like \"1129 items in 23 pages\"\n",
    "                info_elements = self.driver.find_elements(By.CLASS_NAME, \"rgInfoPart\")\n",
    "                if info_elements:\n",
    "                    info_text = info_elements[0].text\n",
    "                    self.logger.debug(f\"Pagination info: {info_text}\")\n",
    "                    \n",
    "                    # Extract current page and total pages\n",
    "                    import re\n",
    "                    match = re.search(r'(\\d+)\\s+items\\s+in\\s+(\\d+)\\s+pages', info_text)\n",
    "                    if match:\n",
    "                        total_items = int(match.group(1))\n",
    "                        total_pages = int(match.group(2))\n",
    "                        \n",
    "                        # Find current page by looking for rgCurrentPage\n",
    "                        current_page_elements = self.driver.find_elements(By.CLASS_NAME, \"rgCurrentPage\")\n",
    "                        if current_page_elements:\n",
    "                            current_page_text = current_page_elements[0].text.strip()\n",
    "                            if current_page_text.isdigit():\n",
    "                                current_page = int(current_page_text)\n",
    "                                has_next = current_page < total_pages\n",
    "                                \n",
    "                                self.logger.info(f\"Pagination: Page {current_page} of {total_pages} (has_next: {has_next})\")\n",
    "                                return has_next\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Error checking pagination info: {str(e)}\")\n",
    "            \n",
    "            self.logger.warning(\"No Next Page button found using any method\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking for next page: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _click_next_page(self):\n",
    "        \"\"\"Click the next page button using flexible selectors\"\"\"\n",
    "        try:\n",
    "            next_button = None\n",
    "            \n",
    "            # Method 1: Find by title attribute\n",
    "            next_buttons = self.driver.find_elements(\n",
    "                By.XPATH, \"//input[@title='Next Page' and contains(@name, 'RadGrid_OverallResults')]\"\n",
    "            )\n",
    "            \n",
    "            if next_buttons:\n",
    "                next_button = next_buttons[0]\n",
    "                self.logger.debug(\"Found Next button by title attribute\")\n",
    "            else:\n",
    "                # Method 2: Find by class\n",
    "                next_buttons_by_class = self.driver.find_elements(By.CLASS_NAME, \"rgPageNext\")\n",
    "                if next_buttons_by_class:\n",
    "                    next_button = next_buttons_by_class[0]\n",
    "                    self.logger.debug(\"Found Next button by class\")\n",
    "            \n",
    "            if next_button and next_button.is_enabled():\n",
    "                # Log button details for debugging\n",
    "                button_name = next_button.get_attribute(\"name\")\n",
    "                self.logger.debug(f\"Clicking Next button: {button_name}\")\n",
    "                \n",
    "                next_button.click()\n",
    "                \n",
    "                # Wait for page to load\n",
    "                time.sleep(self.delay)\n",
    "                \n",
    "                # Wait for table to be updated with longer timeout\n",
    "                wait = WebDriverWait(self.driver, 15)\n",
    "                wait.until(EC.presence_of_element_located((By.ID, \"RadGrid_OverallResults_ctl00\")))\n",
    "                \n",
    "                # Additional wait for content to fully load\n",
    "                time.sleep(2)\n",
    "                \n",
    "                self.logger.info(\"Successfully navigated to next page\")\n",
    "                return True\n",
    "            else:\n",
    "                if next_button:\n",
    "                    self.logger.info(\"Next button found but is disabled\")\n",
    "                else:\n",
    "                    self.logger.info(\"No Next button found\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to click next page: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def _generate_filename(self, term):\n",
    "        \"\"\"\n",
    "        Generate filename based on term\n",
    "        \n",
    "        Args:\n",
    "            term (str): Term like '2025-26 Term 1'\n",
    "            \n",
    "        Returns:\n",
    "            str: Filename like '2025-26_T1.xlsx'\n",
    "        \"\"\"\n",
    "        # Convert term format\n",
    "        term_map = {\n",
    "            'Term 1': 'T1',\n",
    "            'Term 2': 'T2', \n",
    "            'Term 3A': 'T3A',\n",
    "            'Term 3B': 'T3B'\n",
    "        }\n",
    "        \n",
    "        filename = term\n",
    "        for full_term, short_term in term_map.items():\n",
    "            if full_term in term:\n",
    "                filename = term.replace(full_term, short_term)\n",
    "                break\n",
    "        \n",
    "        return filename + '.xlsx'\n",
    "    \n",
    "    def _save_to_excel(self, data, filename):\n",
    "        \"\"\"\n",
    "        Save data to Excel file, concatenating if file exists\n",
    "        \n",
    "        Args:\n",
    "            data (list): List of dictionaries containing the data\n",
    "            filename (str): Excel filename to save to\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not data:\n",
    "                self.logger.warning(\"No data to save\")\n",
    "                return\n",
    "            \n",
    "            # Create DataFrame with desired column order\n",
    "            new_df = pd.DataFrame(data)\n",
    "            new_df = new_df[self.desired_columns]\n",
    "            \n",
    "            # Check if file exists\n",
    "            if os.path.exists(filename):\n",
    "                self.logger.info(f\"File {filename} exists, concatenating data...\")\n",
    "                \n",
    "                try:\n",
    "                    # Read existing data\n",
    "                    existing_df = pd.read_excel(filename, engine='openpyxl')\n",
    "                    \n",
    "                    # Concatenate and remove duplicates\n",
    "                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "                    combined_df = combined_df.drop_duplicates().reset_index(drop=True)\n",
    "                    \n",
    "                    self.logger.info(f\"Combined {len(existing_df)} existing + {len(new_df)} new = {len(combined_df)} total records\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error reading existing file, creating new: {str(e)}\")\n",
    "                    combined_df = new_df\n",
    "            else:\n",
    "                combined_df = new_df\n",
    "            \n",
    "            # Save to Excel\n",
    "            combined_df.to_excel(filename, index=False, engine='openpyxl')\n",
    "            \n",
    "            self.logger.info(f\"Data saved to {filename}\")\n",
    "            self.logger.info(f\"Total records in file: {len(combined_df)}\")\n",
    "            \n",
    "            return len(combined_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save data to Excel: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _get_current_page_info(self):\n",
    "        \"\"\"Get current page number and total pages\"\"\"\n",
    "        try:\n",
    "            # Method 1: From pagination info text\n",
    "            info_elements = self.driver.find_elements(By.CLASS_NAME, \"rgInfoPart\")\n",
    "            if info_elements:\n",
    "                info_text = info_elements[0].text\n",
    "                # Extract total pages from text like \"1129 items in 23 pages\"\n",
    "                import re\n",
    "                match = re.search(r'(\\d+)\\s+items\\s+in\\s+(\\d+)\\s+pages', info_text)\n",
    "                if match:\n",
    "                    total_items = int(match.group(1))\n",
    "                    total_pages = int(match.group(2))\n",
    "                    \n",
    "                    # Get current page from rgCurrentPage element\n",
    "                    current_page_elements = self.driver.find_elements(By.CLASS_NAME, \"rgCurrentPage\")\n",
    "                    if current_page_elements:\n",
    "                        current_page_text = current_page_elements[0].text.strip()\n",
    "                        if current_page_text.isdigit():\n",
    "                            current_page = int(current_page_text)\n",
    "                            return current_page, total_pages, total_items\n",
    "            \n",
    "            return None, None, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Error getting page info: {str(e)}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    def scrape_term_data(self, term, bid_round=None, bid_window=None, output_dir=\"./\"):\n",
    "        \"\"\"\n",
    "        Scrape data for a specific term with improved pagination handling\n",
    "        \n",
    "        Args:\n",
    "            term (str): Term to scrape (e.g., '2025-26 Term 1')\n",
    "            bid_round (str): Specific bid round to filter by\n",
    "            bid_window (str): Specific bid window to filter by\n",
    "            output_dir (str): Directory to save the Excel file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Setup driver if not already done\n",
    "            if self.driver is None:\n",
    "                self._setup_driver()\n",
    "            \n",
    "            # Navigate to page\n",
    "            self._navigate_to_overall_results()\n",
    "            \n",
    "            # Fill form\n",
    "            self._select_course_career(\"Undergraduate\")\n",
    "            self._select_term(term)\n",
    "            self._select_bid_round(bid_round)\n",
    "            self._select_bid_window(bid_window)\n",
    "            \n",
    "            # Submit search\n",
    "            self._click_search()\n",
    "            \n",
    "            # Set page size to 50\n",
    "            self._set_page_size_to_50()\n",
    "\n",
    "            # Sort by bidding window to get Incoming Freshmen first\n",
    "            self._sort_by_bidding_window()\n",
    "            \n",
    "            # Get initial page information\n",
    "            current_page, total_pages, total_items = self._get_current_page_info()\n",
    "            if total_pages:\n",
    "                self.logger.info(f\"Starting scrape: {total_items} total items across {total_pages} pages\")\n",
    "            \n",
    "            # Collect all data from all pages\n",
    "            all_data = []\n",
    "            page_num = 1\n",
    "            max_pages = 200  # Increased safety limit\n",
    "            last_bidding_window = None\n",
    "            should_stop_scraping = False\n",
    "\n",
    "            while page_num <= max_pages and not should_stop_scraping:\n",
    "                # Get current page info for verification\n",
    "                current_page, total_pages, total_items = self._get_current_page_info()\n",
    "                \n",
    "                if current_page and total_pages:\n",
    "                    self.logger.info(f\"Scraping page {current_page} of {total_pages} (iteration {page_num})...\")\n",
    "                else:\n",
    "                    self.logger.info(f\"Scraping page {page_num}...\")\n",
    "                \n",
    "                # Extract data from current page with early termination support\n",
    "                page_data, should_stop, current_bidding_window = self._extract_table_data(\n",
    "                    stop_on_bidding_window_change=True, \n",
    "                    last_bidding_window=last_bidding_window\n",
    "                )\n",
    "                \n",
    "                # Update tracking variables\n",
    "                if current_bidding_window:\n",
    "                    last_bidding_window = current_bidding_window\n",
    "                if should_stop:\n",
    "                    should_stop_scraping = True\n",
    "                \n",
    "                if page_data:\n",
    "                    all_data.extend(page_data)\n",
    "                    self.logger.info(f\"Page {page_num}: Found {len(page_data)} records\")\n",
    "                    if should_stop_scraping:\n",
    "                        self.logger.info(\"Early termination triggered due to bidding window change\")\n",
    "                        break\n",
    "                else:\n",
    "                    self.logger.warning(f\"Page {page_num}: No data found\")\n",
    "                    \n",
    "                    # If first page has no data, something is wrong\n",
    "                    if page_num == 1:\n",
    "                        self.logger.error(\"No data on first page - check search criteria or page structure\")\n",
    "                        break\n",
    "                \n",
    "                # Check if we've reached the last page using multiple methods\n",
    "                if current_page and total_pages and current_page >= total_pages:\n",
    "                    self.logger.info(f\"Reached last page ({current_page}/{total_pages})\")\n",
    "                    break\n",
    "                \n",
    "                # Check if there's a next page using our improved method\n",
    "                if self._has_next_page():\n",
    "                    # Store current page for verification\n",
    "                    old_page = current_page\n",
    "                    \n",
    "                    if self._click_next_page():\n",
    "                        # Verify we actually moved to next page\n",
    "                        time.sleep(1)  # Brief wait\n",
    "                        new_current_page, _, _ = self._get_current_page_info()\n",
    "                        \n",
    "                        if new_current_page and old_page and new_current_page <= old_page:\n",
    "                            self.logger.warning(f\"Page didn't advance (was {old_page}, now {new_current_page})\")\n",
    "                            break\n",
    "                        \n",
    "                        page_num += 1\n",
    "                        time.sleep(self.delay)  # Rate limiting\n",
    "                    else:\n",
    "                        self.logger.info(\"Failed to navigate to next page, stopping\")\n",
    "                        break\n",
    "                else:\n",
    "                    self.logger.info(\"No more pages available\")\n",
    "                    break\n",
    "            \n",
    "            # Generate filename and save data\n",
    "            if all_data:\n",
    "                filename = self._generate_filename(term)\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                \n",
    "                total_records = self._save_to_excel(all_data, filepath)\n",
    "                \n",
    "                self.logger.info(f\"Scraping completed for {term}\")\n",
    "                self.logger.info(f\"Records collected this session: {len(all_data)}\")\n",
    "                self.logger.info(f\"Total records in file: {total_records}\")\n",
    "                \n",
    "                # Final verification\n",
    "                if current_page and total_pages:\n",
    "                    expected_total = total_items if total_items else \"unknown\"\n",
    "                    self.logger.info(f\"Expected ~{expected_total} total records from {total_pages} pages\")\n",
    "            else:\n",
    "                self.logger.error(\"No data collected for any page\")\n",
    "            \n",
    "            return all_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to scrape term data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def scrape_multiple_terms(self, terms_config, output_dir=\"./\"):\n",
    "        \"\"\"\n",
    "        Scrape data for multiple terms\n",
    "        \n",
    "        Args:\n",
    "            terms_config (list): List of dictionaries with term configurations\n",
    "                                Example: [\n",
    "                                    {'term': '2025-26 Term 1', 'round': '1', 'window': '1'},\n",
    "                                    {'term': '2024-25 Term 2', 'round': None, 'window': None}\n",
    "                                ]\n",
    "            output_dir (str): Directory to save Excel files\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create output directory\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Setup driver\n",
    "            self._setup_driver()\n",
    "            \n",
    "            # Navigate to login page and wait for manual login\n",
    "            self.driver.get(\"https://boss.intranet.smu.edu.sg/\")\n",
    "            self.wait_for_manual_login()\n",
    "            \n",
    "            # Process each term configuration\n",
    "            for i, config in enumerate(terms_config):\n",
    "                try:\n",
    "                    self.logger.info(f\"Processing term {i+1}/{len(terms_config)}: {config['term']}\")\n",
    "                    \n",
    "                    data = self.scrape_term_data(\n",
    "                        term=config['term'],\n",
    "                        bid_round=config.get('round'),\n",
    "                        bid_window=config.get('window'),\n",
    "                        output_dir=output_dir\n",
    "                    )\n",
    "                    \n",
    "                    self.logger.info(f\"Completed {config['term']}: {len(data)} records\")\n",
    "                    \n",
    "                    # Delay between terms\n",
    "                    if i < len(terms_config) - 1:\n",
    "                        self.logger.info(f\"Waiting {self.delay} seconds before next term...\")\n",
    "                        time.sleep(self.delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to process term {config['term']}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            self.logger.info(\"All terms processing completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to scrape multiple terms: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def _transform_term_format(self, short_term):\n",
    "        \"\"\"\n",
    "        Converts a short-form term into the website's full-text format.\n",
    "        Example: '2025-26_T1' -> '2025-26 Term 1'\n",
    "        \n",
    "        Args:\n",
    "            short_term (str): The term in short format (e.g., 'YYYY-YY_TX').\n",
    "            \n",
    "        Returns:\n",
    "            str: The term in the website's format.\n",
    "        \"\"\"\n",
    "        # Mapping from short-form to the website's text.\n",
    "        term_map = {\n",
    "            'T1': 'Term 1',\n",
    "            'T2': 'Term 2',\n",
    "            'T3A': 'Term 3A',\n",
    "            'T3B': 'Term 3B'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Split the string into the year part and the term part (e.g., '2025-26' and 'T1')\n",
    "            year_part, term_part = short_term.split('_')\n",
    "            \n",
    "            # Look up the full term name from our map.\n",
    "            full_term_name = term_map.get(term_part)\n",
    "            \n",
    "            if full_term_name:\n",
    "                # Combine them back into the final format.\n",
    "                return f\"{year_part} {full_term_name}\"\n",
    "            else:\n",
    "                # If the term part is not in our map, raise an error.\n",
    "                raise ValueError(f\"Unknown term suffix: '{term_part}'\")\n",
    "                \n",
    "        except (ValueError, IndexError) as e:\n",
    "            self.logger.error(f\"Invalid term format: '{short_term}'. Expected format like '2025-26_T1'.\")\n",
    "            raise e\n",
    "    \n",
    "    def run(self, term, bid_round=None, bid_window=None, output_dir=\"./script_input/overallBossResults\", auto_detect_phase=True):\n",
    "        \"\"\"\n",
    "        Runs the scraper for a single term, handling term format transformation internally.\n",
    "        \n",
    "        Args:\n",
    "            term (str): Term to scrape in short format (e.g., '2025-26_T1').\n",
    "            ... (other args are the same) ...\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, transform the short-form term into the website-friendly format.\n",
    "            website_term = self._transform_term_format(term)\n",
    "            \n",
    "            # Auto-detect current bidding phase if enabled and no explicit round/window provided\n",
    "            if auto_detect_phase and (bid_round is None or bid_window is None):\n",
    "                detected_round, detected_window = self._determine_current_bidding_phase()\n",
    "                if detected_round and detected_window:\n",
    "                    if bid_round is None: bid_round = detected_round\n",
    "                    if bid_window is None: bid_window = detected_window\n",
    "                    self.logger.info(f\"Auto-detected bidding phase: Round {bid_round}, Window {bid_window}\")\n",
    "                else:\n",
    "                    self.logger.info(\"Could not auto-detect a current bidding phase. Scraping with default filters.\")\n",
    "            \n",
    "            round_str = f\"Round {bid_round}\" if bid_round else \"All Rounds\"\n",
    "            window_str = f\"Window {bid_window}\" if bid_window else \"All Windows\"\n",
    "            self.logger.info(f\"Scraping {website_term} - {round_str}, {window_str}\")\n",
    "            \n",
    "            self._setup_driver()\n",
    "            self.driver.get(\"https://boss.intranet.smu.edu.sg/\")\n",
    "            self.wait_for_manual_login()\n",
    "            \n",
    "            # Scrape the term data using the correctly formatted term.\n",
    "            data = self.scrape_term_data(\n",
    "                term=website_term,\n",
    "                bid_round=bid_round,\n",
    "                bid_window=bid_window,\n",
    "                output_dir=output_dir\n",
    "            )\n",
    "            \n",
    "            print(f\"Scraping completed! Collected {len(data)} records for {website_term}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the scraping process: {str(e)}\")\n",
    "            self.logger.error(f\"Error during scraping: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.logger.info(\"WebDriver closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use the run() method with the new configuration variables\n",
    "scraper = ScrapeOverallResults(headless=False, delay=5)\n",
    "scraper.run(\n",
    "    term=START_AY_TERM,\n",
    "    bid_round=TARGET_ROUND,\n",
    "    bid_window=TARGET_WINDOW,\n",
    "    auto_detect_phase=True  # Ensure auto-detection is enabled.\n",
    ")\n",
    "\n",
    "# # Option 2: Use the run() method with automatic detection disabled\n",
    "# scraper = ScrapeOverallResults(headless=False, delay=5)\n",
    "# scraper.run(auto_detect_phase=False)  # Will scrape all rounds and windows\n",
    "\n",
    "# # Option 3: Use the run() method with custom parameters (overrides auto-detection)\n",
    "# scraper = ScrapeOverallResults(headless=False, delay=5)\n",
    "# scraper.run(\n",
    "#     term='2024-25 Term 2',\n",
    "#     bid_round='2', \n",
    "#     bid_window='2',\n",
    "#     output_dir=\"./my_output_folder\",\n",
    "#     auto_detect_phase=False  # Disable auto-detection since we're specifying manually\n",
    "# )\n",
    "\n",
    "# # Option 4: Mix auto-detection with manual override\n",
    "# scraper = ScrapeOverallResults(headless=False, delay=5)\n",
    "# scraper.run(\n",
    "#     term='2025-26 Term 1',\n",
    "#     bid_round='1A',  # Override auto-detected round\n",
    "#     # bid_window will be auto-detected\n",
    "#     auto_detect_phase=True\n",
    "# )\n",
    "\n",
    "# # Option 5: Use the scrape_multiple_terms method for multiple terms\n",
    "# scraper = ScrapeOverallResults(headless=False, delay=5)\n",
    "# terms_config = [\n",
    "#     {'term': '2025-26 Term 1', 'round': '1', 'window': '1'},\n",
    "#     {'term': '2024-25 Term 2', 'round': '2', 'window': '2'},\n",
    "#     {'term': '2024-25 Term 1', 'round': None, 'window': None}  # None means use default/empty\n",
    "# ]\n",
    "# scraper.scrape_multiple_terms(terms_config, output_dir=\"./multiple_terms_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Extract Data from HTML Files**\n",
    "\n",
    "### **HTML Data Extractor Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `HTMLDataExtractor` class processes previously scraped HTML files from SMU's BOSS system and extracts structured data into Excel format. It systematically parses course information, class timings, academic terms, and exam schedules from local HTML files without requiring network access or authentication.\n",
    "\n",
    "**Key Features:**\n",
    "- **Local File Processing**: Uses Selenium WebDriver to parse local HTML files without network connectivity requirements\n",
    "- **Comprehensive Data Extraction**: Extracts course details, academic terms, class timings, exam schedules, grading information, and professor names\n",
    "- **Test-First Approach**: Includes `run_test()` function to validate extraction logic on a small sample before processing all files\n",
    "- **Structured Output**: Organizes extracted data into two Excel sheets - standalone records (one per HTML file) and multiple records (class/exam timings)\n",
    "- **Error Tracking**: Captures and logs parsing errors in a separate sheet for debugging and quality assurance\n",
    "- **Flexible Data Parsing**: Handles multiple academic term naming conventions and date formats used across different years\n",
    "- **Record Linking**: Uses record keys to maintain relationships between standalone and multiple data records\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, `pandas`, `openpyxl`, standard libraries (`os`, `re`, `datetime`, `pathlib`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- No network access required (processes local files only)\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Scraped HTML Files**: Previously downloaded HTML files from BOSS system stored locally\n",
    "- **File Path CSV**: `script_input/scraped_filepaths.csv` containing paths to valid HTML files\n",
    "- **Directory Structure**: HTML files organized in the expected folder structure (typically `script_input/classTimingsFull/`)\n",
    "\n",
    "**Output Structure:**\n",
    "- **Excel File**: `script_input/raw_data.xlsx` (or custom path) with multiple sheets:\n",
    "  - `standalone`: One record per HTML file with course and class information\n",
    "  - `multiple`: Multiple records for class timings and exam schedules\n",
    "  - `errors`: Parsing errors and problematic files for debugging\n",
    "\n",
    "**Data Extraction Capabilities:**\n",
    "- **Course Information**: Course codes, names, descriptions, credit units, course areas, enrollment requirements\n",
    "- **Academic Terms**: Term IDs, academic years, start/end dates, BOSS IDs\n",
    "- **Class Details**: Sections, grading basis, course outline URLs, professor names\n",
    "- **Timing Data**: Class schedules, exam dates, venues, day-of-week information\n",
    "- **Cross-References**: Maintains linking keys between related records across sheets\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "# Initialize extractor\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Test with sample files first (recommended)\n",
    "test_success = extractor.run_test(test_count=10)\n",
    "\n",
    "if test_success:\n",
    "    # Run full extraction\n",
    "    extractor.run()\n",
    "    \n",
    "# Or run directly without testing\n",
    "extractor.run(\n",
    "    scraped_filepaths_csv='script_input/scraped_filepaths.csv',\n",
    "    output_path='script_input/raw_data.xlsx'\n",
    ")\n",
    "```\n",
    "\n",
    "The class provides a crucial intermediate step between raw HTML scraping and database insertion, creating clean, structured data that can be further processed for database integration or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLDataExtractor:\n",
    "    \"\"\"\n",
    "    Extract raw data from scraped HTML files and save to Excel format using Selenium\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        self.driver = None\n",
    "        \n",
    "    def setup_selenium_driver(self):\n",
    "        \"\"\"Set up Selenium WebDriver for local file access\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--headless')  # Run in headless mode for efficiency\n",
    "            options.add_argument('--disable-gpu')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            print(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Selenium WebDriver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def safe_find_element_text(self, by, value):\n",
    "        \"\"\"Safely find element and return its text with proper encoding handling\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            if element:\n",
    "                raw_text = element.text.strip()\n",
    "                return self.clean_text_encoding(raw_text)\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def safe_find_element_attribute(self, by, value, attribute):\n",
    "        \"\"\"Safely find element and return its attribute with proper encoding handling\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            if element:\n",
    "                raw_attr = element.get_attribute(attribute)\n",
    "                return self.clean_text_encoding(raw_attr) if raw_attr else None\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def convert_date_to_timestamp(self, date_str):\n",
    "        \"\"\"Convert DD-Mmm-YYYY to database timestamp format\"\"\"\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "            return date_obj.strftime('%Y-%m-%d 00:00:00.000 +0800')\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def parse_acad_term(self, term_text, filepath=None):\n",
    "        \"\"\"Parse academic term text and return structured data with folder path fallback\"\"\"\n",
    "        try:\n",
    "            # Clean the term text first\n",
    "            if term_text:\n",
    "                term_text = self.clean_text_encoding(term_text)\n",
    "            \n",
    "            # Pattern like \"2021-22 Term 2\" or \"2021-22 Session 1\"\n",
    "            pattern = r'(\\d{4})-(\\d{2})\\s+(.*)'\n",
    "            match = re.search(pattern, term_text) if term_text else None\n",
    "            \n",
    "            if not match:\n",
    "                return None, None, None, None\n",
    "            \n",
    "            start_year = int(match.group(1))\n",
    "            end_year_short = int(match.group(2))\n",
    "            term_desc = match.group(3).lower()\n",
    "            \n",
    "            # Convert 2-digit year to 4-digit\n",
    "            if end_year_short < 50:\n",
    "                end_year = 2000 + end_year_short\n",
    "            else:\n",
    "                end_year = 1900 + end_year_short\n",
    "            \n",
    "            # Determine term code from text\n",
    "            term_code = None\n",
    "            if 'term 1' in term_desc or 'session 1' in term_desc or 'august term' in term_desc:\n",
    "                term_code = 'T1'\n",
    "            elif 'term 2' in term_desc or 'session 2' in term_desc or 'january term' in term_desc:\n",
    "                term_code = 'T2'\n",
    "            elif 'term 3a' in term_desc:\n",
    "                term_code = 'T3A'\n",
    "            elif 'term 3b' in term_desc:\n",
    "                term_code = 'T3B'\n",
    "            elif 'term 3' in term_desc:\n",
    "                # Generic T3 - need to check folder path for A/B\n",
    "                term_code = 'T3'\n",
    "            \n",
    "            # If term_code is incomplete or missing, use folder path as fallback\n",
    "            if not term_code or term_code == 'T3':\n",
    "                folder_term = self.extract_term_from_folder_path(filepath) if filepath else None\n",
    "                if folder_term:\n",
    "                    # If we have folder term, use it\n",
    "                    if term_code == 'T3' and folder_term in ['T3A', 'T3B']:\n",
    "                        term_code = folder_term\n",
    "                    elif not term_code:\n",
    "                        term_code = folder_term\n",
    "            \n",
    "            # If still no term code, return None\n",
    "            if not term_code:\n",
    "                return start_year, end_year, None, None\n",
    "            \n",
    "            acad_term_id = f\"AY{start_year}{end_year_short:02d}{term_code}\"\n",
    "            \n",
    "            return start_year, end_year, term_code, acad_term_id\n",
    "        except Exception as e:\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def parse_course_and_section(self, header_text):\n",
    "        \"\"\"Parse course code and section from header text with encoding fixes\"\"\"\n",
    "        try:\n",
    "            if not header_text:\n",
    "                return None, None\n",
    "            \n",
    "            # Clean the text first\n",
    "            clean_text = self.clean_text_encoding(header_text)\n",
    "            clean_text = re.sub(r'<[^>]+>', '', clean_text)\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text.strip())\n",
    "            \n",
    "            # Try multiple regex patterns\n",
    "            patterns = [\n",
    "                r'([A-Z0-9_-]+)\\s+‚Äî\\s+(.+)',  # Standard format with em-dash\n",
    "                r'([A-Z0-9_-]+)\\s+-\\s+(.+)',  # Standard format with hyphen\n",
    "                r'([A-Z]+)\\s+(\\d+[A-Z0-9_]*)\\s+‚Äî\\s+(.+)',  # Split format with em-dash\n",
    "                r'([A-Z]+)\\s+(\\d+[A-Z0-9_]*)\\s+-\\s+(.+)',  # Split format with hyphen\n",
    "                r'([A-Z0-9_\\s-]+?)\\s+‚Äî\\s+([^‚Äî]+)',  # Flexible format with em-dash\n",
    "                r'([A-Z0-9_\\s-]+?)\\s+-\\s+([^-]+)',  # Flexible format with hyphen\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.match(pattern, clean_text)\n",
    "                if match:\n",
    "                    if len(match.groups()) == 2:\n",
    "                        # Standard format: course_code - section\n",
    "                        course_section = match.group(1).strip()\n",
    "                        section_name = match.group(2).strip()\n",
    "                        \n",
    "                        # Extract section from the end of course_section if it's there\n",
    "                        section_match = re.search(r'^(.+?)\\s+(SG\\d+|G\\d+|\\d+)$', course_section)\n",
    "                        if section_match:\n",
    "                            course_code = section_match.group(1)\n",
    "                            section = section_match.group(2)\n",
    "                        else:\n",
    "                            course_code = course_section\n",
    "                            # Try to extract section from section_name\n",
    "                            section_extract = re.search(r'(SG\\d+|G\\d+|\\d+)', section_name)\n",
    "                            section = section_extract.group(1) if section_extract else None\n",
    "                    else:\n",
    "                        # Split format: course_prefix course_number - section_name\n",
    "                        course_code = f\"{match.group(1)}{match.group(2)}\"\n",
    "                        section_name = match.group(3).strip()\n",
    "                        section_extract = re.search(r'(SG\\d+|G\\d+|\\d+)', section_name)\n",
    "                        section = section_extract.group(1) if section_extract else None\n",
    "                    \n",
    "                    return course_code.strip() if course_code else None, section\n",
    "            \n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def parse_date_range(self, date_text):\n",
    "        \"\"\"Parse date range text and return start and end timestamps\"\"\"\n",
    "        try:\n",
    "            # Example: \"10-Jan-2022 to 01-May-2022\"\n",
    "            pattern = r'(\\d{1,2}-\\w{3}-\\d{4})\\s+to\\s+(\\d{1,2}-\\w{3}-\\d{4})'\n",
    "            match = re.search(pattern, date_text)\n",
    "            \n",
    "            if not match:\n",
    "                return None, None\n",
    "            \n",
    "            start_date = self.convert_date_to_timestamp(match.group(1))\n",
    "            end_date = self.convert_date_to_timestamp(match.group(2))\n",
    "            \n",
    "            return start_date, end_date\n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_course_areas_list(self):\n",
    "        \"\"\"Extract course areas with encoding fixes\"\"\"\n",
    "        try:\n",
    "            course_areas_element = self.driver.find_element(By.ID, 'lblCourseAreas')\n",
    "            if not course_areas_element:\n",
    "                return None\n",
    "            \n",
    "            # Get innerHTML to handle HTML content\n",
    "            course_areas_html = course_areas_element.get_attribute('innerHTML')\n",
    "            if course_areas_html:\n",
    "                # Clean encoding first\n",
    "                course_areas_html = self.clean_text_encoding(course_areas_html)\n",
    "                \n",
    "                # Extract list items\n",
    "                areas_list = re.findall(r'<li[^>]*>([^<]+)</li>', course_areas_html)\n",
    "                if areas_list:\n",
    "                    # Clean each area and join\n",
    "                    cleaned_areas = [self.clean_text_encoding(area.strip()) for area in areas_list]\n",
    "                    return ', '.join(cleaned_areas)\n",
    "                else:\n",
    "                    # Fallback to text content\n",
    "                    text_content = course_areas_element.text.strip()\n",
    "                    return self.clean_text_encoding(text_content)\n",
    "            else:\n",
    "                # Fallback to text content\n",
    "                text_content = course_areas_element.text.strip()\n",
    "                return self.clean_text_encoding(text_content)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def extract_course_outline_url(self):\n",
    "        \"\"\"Extract course outline URL from HTML using Selenium\"\"\"\n",
    "        try:\n",
    "            onclick_attr = self.safe_find_element_attribute(By.ID, 'imgCourseOutline', 'onclick')\n",
    "            if onclick_attr:\n",
    "                url_match = re.search(r\"window\\.open\\('([^']+)'\", onclick_attr)\n",
    "                if url_match:\n",
    "                    return url_match.group(1)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def extract_boss_ids_from_filepath(self, filepath):\n",
    "        \"\"\"Extract BOSS IDs from filepath\"\"\"\n",
    "        try:\n",
    "            filename = os.path.basename(filepath)\n",
    "            acad_term_match = re.search(r'SelectedAcadTerm=(\\d+)', filename)\n",
    "            class_match = re.search(r'SelectedClassNumber=(\\d+)', filename)\n",
    "            \n",
    "            acad_term_boss_id = int(acad_term_match.group(1)) if acad_term_match else None\n",
    "            class_boss_id = int(class_match.group(1)) if class_match else None\n",
    "            \n",
    "            return acad_term_boss_id, class_boss_id\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_meeting_information(self, record_key):\n",
    "        \"\"\"Extract class timing and exam timing information using Selenium\"\"\"\n",
    "        try:\n",
    "            meeting_table = self.driver.find_element(By.ID, 'RadGrid_MeetingInfo_ctl00')\n",
    "            tbody = meeting_table.find_element(By.TAG_NAME, 'tbody')\n",
    "            rows = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if len(cells) < 7:\n",
    "                    continue\n",
    "                \n",
    "                meeting_type = cells[0].text.strip()\n",
    "                start_date_text = cells[1].text.strip()\n",
    "                end_date_text = cells[2].text.strip()\n",
    "                day_of_week = cells[3].text.strip()\n",
    "                start_time = cells[4].text.strip()\n",
    "                end_time = cells[5].text.strip()\n",
    "                venue = cells[6].text.strip() if len(cells) > 6 else \"\"\n",
    "                professor_name = cells[7].text.strip() if len(cells) > 7 else \"\"\n",
    "                \n",
    "                # Assume CLASS if meeting_type is empty\n",
    "                if not meeting_type:\n",
    "                    meeting_type = 'CLASS'\n",
    "                \n",
    "                if meeting_type == 'CLASS':\n",
    "                    # Convert dates to timestamp format\n",
    "                    start_date = self.convert_date_to_timestamp(start_date_text)\n",
    "                    end_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    timing_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'CLASS',\n",
    "                        'start_date': start_date,\n",
    "                        'end_date': end_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(timing_record)\n",
    "                \n",
    "                elif meeting_type == 'EXAM':\n",
    "                    # For exams, use the second date (end_date_text) as the exam date\n",
    "                    exam_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    exam_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'EXAM',\n",
    "                        'date': exam_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(exam_record)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'record_key': record_key,\n",
    "                'error': f'Error extracting meeting information: {str(e)}',\n",
    "                'type': 'parse_error'\n",
    "            })\n",
    "    \n",
    "    def process_html_file(self, filepath):\n",
    "        \"\"\"Process a single HTML file and extract all data using Selenium\"\"\"\n",
    "        try:\n",
    "            # Load HTML file\n",
    "            html_file = Path(filepath).resolve()\n",
    "            file_url = html_file.as_uri()\n",
    "            self.driver.get(file_url)\n",
    "            \n",
    "            # Create unique record key\n",
    "            record_key = f\"{os.path.basename(filepath)}\"\n",
    "            \n",
    "            # Extract basic information\n",
    "            class_header_text = self.safe_find_element_text(By.ID, 'lblClassInfoHeader')\n",
    "            if not class_header_text:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': 'Missing class header',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            course_code, section = self.parse_course_and_section(class_header_text)\n",
    "            \n",
    "            # Extract academic term\n",
    "            term_text = self.safe_find_element_text(By.ID, 'lblClassInfoSubHeader')\n",
    "            acad_year_start, acad_year_end, term, acad_term_id = self.parse_acad_term(term_text, filepath) if term_text else (None, None, None, None)\n",
    "            \n",
    "            # Extract course information\n",
    "            course_name = self.safe_find_element_text(By.ID, 'lblClassSection')\n",
    "            course_description = self.safe_find_element_text(By.ID, 'lblCourseDescription')\n",
    "            credit_units_text = self.safe_find_element_text(By.ID, 'lblUnits')\n",
    "            course_areas = self.extract_course_areas_list()\n",
    "            enrolment_requirements = self.safe_find_element_text(By.ID, 'lblEnrolmentRequirements')\n",
    "            \n",
    "            # Process credit units\n",
    "            try:\n",
    "                credit_units = float(credit_units_text) if credit_units_text else None\n",
    "            except (ValueError, TypeError):\n",
    "                credit_units = None\n",
    "            \n",
    "            # Extract grading basis\n",
    "            grading_text = self.safe_find_element_text(By.ID, 'lblGradingBasis')\n",
    "            grading_basis = None\n",
    "            if grading_text:\n",
    "                if grading_text.lower() == 'graded':\n",
    "                    grading_basis = 'Graded'\n",
    "                elif grading_text.lower() in ['pass/fail', 'pass fail']:\n",
    "                    grading_basis = 'Pass/Fail'\n",
    "                else:\n",
    "                    grading_basis = 'NA'\n",
    "            \n",
    "            # Extract course outline URL\n",
    "            course_outline_url = self.extract_course_outline_url()\n",
    "            \n",
    "            # Extract dates\n",
    "            period_text = self.safe_find_element_text(By.ID, 'lblDates')\n",
    "            start_dt, end_dt = self.parse_date_range(period_text) if period_text else (None, None)\n",
    "            \n",
    "            # Extract BOSS IDs\n",
    "            acad_term_boss_id, class_boss_id = self.extract_boss_ids_from_filepath(filepath)\n",
    "            \n",
    "            # Extract bidding information\n",
    "            total, current_enrolled, reserved, available = self.extract_bidding_info()\n",
    "            \n",
    "            # Get extraction date and determine bidding window from folder path\n",
    "            extraction_date = datetime.now()\n",
    "            bidding_window = self.determine_bidding_window_from_filepath(filepath)\n",
    "            \n",
    "            # Create standalone record\n",
    "            standalone_record = {\n",
    "                'record_key': record_key,\n",
    "                'filepath': filepath,\n",
    "                'course_code': course_code,\n",
    "                'section': section,\n",
    "                'course_name': course_name,\n",
    "                'course_description': course_description,\n",
    "                'credit_units': credit_units,\n",
    "                'course_area': course_areas,\n",
    "                'enrolment_requirements': enrolment_requirements,\n",
    "                'acad_term_id': acad_term_id,\n",
    "                'acad_year_start': acad_year_start,\n",
    "                'acad_year_end': acad_year_end,\n",
    "                'term': term,\n",
    "                'start_dt': start_dt,\n",
    "                'end_dt': end_dt,\n",
    "                'grading_basis': grading_basis,\n",
    "                'course_outline_url': course_outline_url,\n",
    "                'acad_term_boss_id': acad_term_boss_id,\n",
    "                'class_boss_id': class_boss_id,\n",
    "                'term_text': term_text,\n",
    "                'period_text': period_text,\n",
    "                'total': total,\n",
    "                'current_enrolled': current_enrolled,\n",
    "                'reserved': reserved,\n",
    "                'available': available,\n",
    "                'date_extracted': extraction_date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'bidding_window': bidding_window\n",
    "            }\n",
    "            \n",
    "            self.standalone_data.append(standalone_record)\n",
    "            \n",
    "            # Extract meeting information\n",
    "            self.extract_meeting_information(record_key)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'filepath': filepath,\n",
    "                'error': str(e),\n",
    "                'type': 'processing_error'\n",
    "            })\n",
    "            return False\n",
    "\n",
    "    def determine_bidding_window_from_filepath(self, filepath):\n",
    "        \"\"\"Determine bidding window from the file path structure\"\"\"\n",
    "        try:\n",
    "            # Extract folder name from path\n",
    "            # e.g., script_input/classTimingsFull/2025-26_T1/2025-26_T1_R1W1/file.html\n",
    "            folder_path = os.path.dirname(filepath)\n",
    "            folder_name = os.path.basename(folder_path)\n",
    "            \n",
    "            return self.extract_bidding_window_from_folder(folder_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error determining bidding window from filepath: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_test(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv', test_count=10):\n",
    "        \"\"\"Randomly test the extraction on a subset of files\"\"\"\n",
    "        try:\n",
    "            print(f\"Starting test run with {test_count} randomly selected files...\")\n",
    "\n",
    "            # Reset data containers\n",
    "            self.standalone_data = []\n",
    "            self.multiple_data = []\n",
    "            self.errors = []\n",
    "\n",
    "            # Set up Selenium driver\n",
    "            self.setup_selenium_driver()\n",
    "\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "\n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            all_filepaths = df[filepath_column].dropna().tolist()\n",
    "\n",
    "            if len(all_filepaths) == 0:\n",
    "                raise ValueError(\"No valid filepaths found in CSV\")\n",
    "\n",
    "            # Randomly sample filepaths\n",
    "            sample_size = min(test_count, len(all_filepaths))\n",
    "            sampled_filepaths = random.sample(all_filepaths, sample_size)\n",
    "\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "\n",
    "            for i, filepath in enumerate(sampled_filepaths, start=1):\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"Processing test file {i}/{sample_size}: {os.path.basename(filepath)}\")\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "\n",
    "            print(f\"\\nTest run complete: {successful_files}/{processed_files} files successful\")\n",
    "            print(f\"Standalone records extracted: {len(self.standalone_data)}\")\n",
    "            print(f\"Multiple records extracted: {len(self.multiple_data)}\")\n",
    "            if self.errors:\n",
    "                print(f\"Errors encountered: {len(self.errors)}\")\n",
    "                for error in self.errors[:3]:  # Show only the first 3 errors\n",
    "                    print(f\"  - {error['type']}: {error['error']}\")\n",
    "\n",
    "            # Save test results\n",
    "            test_output_path = 'script_input/test_raw_data.xlsx'\n",
    "            self.save_to_excel(test_output_path)\n",
    "\n",
    "            return successful_files > 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in test run: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Test selenium driver closed\")\n",
    "    \n",
    "    def process_all_files(self, base_dir='script_input/classTimingsFull'):\n",
    "        \"\"\"Process only files from the latest round folder that haven't been processed yet\"\"\"\n",
    "        try:\n",
    "            # Find the current academic term (e.g., 2025-26_T1)\n",
    "            current_term = self.get_current_academic_term()\n",
    "            if not current_term:\n",
    "                print(\"Could not determine current academic term\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Current academic term: {current_term}\")\n",
    "            print(f\"Base directory: {base_dir}\")\n",
    "            \n",
    "            term_path = os.path.join(base_dir, current_term)\n",
    "            print(f\"Term path: {term_path}\")\n",
    "            \n",
    "            if not os.path.exists(term_path):\n",
    "                print(f\"Academic term folder not found: {term_path}\")\n",
    "                return\n",
    "            \n",
    "            # Find the latest round folder\n",
    "            latest_round_folder = self.find_latest_round_folder(term_path)\n",
    "            if not latest_round_folder:\n",
    "                print(f\"No round folders found in {term_path}\")\n",
    "                return\n",
    "            \n",
    "            latest_round_path = os.path.join(term_path, latest_round_folder)\n",
    "            bidding_window = self.extract_bidding_window_from_folder(latest_round_folder)\n",
    "            \n",
    "            print(f\"Processing latest round: {latest_round_folder}\")\n",
    "            print(f\"Latest round path: {latest_round_path}\")\n",
    "            print(f\"Bidding window: {bidding_window}\")\n",
    "            \n",
    "            # Get all HTML files from the latest round folder\n",
    "            html_files = []\n",
    "            for filename in os.listdir(latest_round_path):\n",
    "                if filename.endswith('.html'):\n",
    "                    filepath = os.path.join(latest_round_path, filename)\n",
    "                    html_files.append(filepath)\n",
    "            \n",
    "            if not html_files:\n",
    "                print(f\"No HTML files found in {latest_round_path}\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Found {len(html_files)} HTML files in latest round\")\n",
    "            \n",
    "            # Load existing data to check what's already processed\n",
    "            existing_standalone, _, _ = self.load_existing_data()\n",
    "            \n",
    "            # Filter files that haven't been processed for this bidding window\n",
    "            files_to_process = []\n",
    "            for filepath in html_files:\n",
    "                record_key = os.path.basename(filepath)\n",
    "                \n",
    "                # Check if this file has already been processed for this bidding window\n",
    "                if existing_standalone.empty:\n",
    "                    files_to_process.append(filepath)\n",
    "                else:\n",
    "                    # Extract course info from filename to check against existing data\n",
    "                    acad_term_boss_id, class_boss_id = self.extract_boss_ids_from_filepath(filepath)\n",
    "                    \n",
    "                    # Check if record exists for this bidding window\n",
    "                    mask = (existing_standalone['acad_term_boss_id'] == acad_term_boss_id) & \\\n",
    "                        (existing_standalone['class_boss_id'] == class_boss_id) & \\\n",
    "                        (existing_standalone['bidding_window'] == bidding_window)\n",
    "                    \n",
    "                    if not mask.any():\n",
    "                        files_to_process.append(filepath)\n",
    "            \n",
    "            if not files_to_process:\n",
    "                print(f\"All files from {latest_round_folder} have already been processed\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Processing {len(files_to_process)} new files from {latest_round_folder}\")\n",
    "            \n",
    "            # Process only the new files\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "            \n",
    "            for filepath in files_to_process:\n",
    "                if os.path.exists(filepath):\n",
    "                    # print(f\"Processing: {os.path.basename(filepath)}\")\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                    \n",
    "                    if processed_files % 100 == 0:\n",
    "                        print(f\"Processed {processed_files}/{len(files_to_process)} files\")\n",
    "            \n",
    "            print(f\"Processing complete: {successful_files}/{processed_files} files successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_all_files: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_to_excel(self, output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Save extracted data to Excel file, appending new records only\"\"\"\n",
    "        try:\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Load existing data\n",
    "            existing_standalone, existing_multiple, existing_errors = self.load_existing_data(output_path)\n",
    "            \n",
    "            # Filter out duplicate standalone records\n",
    "            new_standalone_records = []\n",
    "            skipped_standalone = 0\n",
    "            \n",
    "            for record in self.standalone_data:\n",
    "                if not self.check_record_exists(existing_standalone, record):\n",
    "                    new_standalone_records.append(record)\n",
    "                else:\n",
    "                    skipped_standalone += 1\n",
    "            \n",
    "            # Handle multiple records - replace if changes detected per record_key\n",
    "            records_to_update = set()\n",
    "            updated_multiple = 0\n",
    "            \n",
    "            # Group new records by record_key\n",
    "            new_records_by_key = {}\n",
    "            for record in self.multiple_data:\n",
    "                key = record['record_key']\n",
    "                if key not in new_records_by_key:\n",
    "                    new_records_by_key[key] = []\n",
    "                new_records_by_key[key].append(record)\n",
    "            \n",
    "            # Check each record_key for changes\n",
    "            for record_key, new_records in new_records_by_key.items():\n",
    "                if existing_multiple.empty:\n",
    "                    records_to_update.add(record_key)\n",
    "                else:\n",
    "                    # Get existing records for this record_key\n",
    "                    existing_records = existing_multiple[existing_multiple['record_key'] == record_key]\n",
    "                    \n",
    "                    # Check if there are any changes\n",
    "                    changes_detected = False\n",
    "                    \n",
    "                    # Compare counts first\n",
    "                    if len(existing_records) != len(new_records):\n",
    "                        changes_detected = True\n",
    "                    else:\n",
    "                        # Compare each record\n",
    "                        for new_record in new_records:\n",
    "                            # Find matching existing record\n",
    "                            existing_match = existing_records[existing_records['type'] == new_record['type']]\n",
    "                            \n",
    "                            if existing_match.empty:\n",
    "                                changes_detected = True\n",
    "                                break\n",
    "                            \n",
    "                            # Compare relevant fields based on type\n",
    "                            if new_record['type'] == 'CLASS':\n",
    "                                compare_fields = ['start_date', 'end_date', 'day_of_week', 'start_time', 'end_time', 'venue', 'professor_name']\n",
    "                            elif new_record['type'] == 'EXAM':\n",
    "                                compare_fields = ['date', 'day_of_week', 'start_time', 'end_time', 'venue', 'professor_name']\n",
    "                            else:\n",
    "                                continue\n",
    "                            \n",
    "                            for field in compare_fields:\n",
    "                                if new_record.get(field) != existing_match.iloc[0].get(field):\n",
    "                                    changes_detected = True\n",
    "                                    break\n",
    "                            \n",
    "                            if changes_detected:\n",
    "                                break\n",
    "                    \n",
    "                    if changes_detected:\n",
    "                        records_to_update.add(record_key)\n",
    "                        updated_multiple += 1\n",
    "            \n",
    "            # Build final multiple records list\n",
    "            if existing_multiple.empty:\n",
    "                final_multiple_records = self.multiple_data\n",
    "            else:\n",
    "                # Keep existing records that are not being updated\n",
    "                final_multiple_records = []\n",
    "                for _, row in existing_multiple.iterrows():\n",
    "                    if row['record_key'] not in records_to_update:\n",
    "                        final_multiple_records.append(row.to_dict())\n",
    "                \n",
    "                # Add new records for updated record_keys\n",
    "                for record_key in records_to_update:\n",
    "                    final_multiple_records.extend(new_records_by_key[record_key])\n",
    "            \n",
    "            new_multiple_df = pd.DataFrame(final_multiple_records)\n",
    "            \n",
    "            # Create DataFrames for new records\n",
    "            new_standalone_df = pd.DataFrame(new_standalone_records)\n",
    "            new_errors_df = pd.DataFrame(self.errors)\n",
    "            \n",
    "            # Combine with existing data\n",
    "            if not existing_standalone.empty and not new_standalone_df.empty:\n",
    "                combined_standalone = pd.concat([existing_standalone, new_standalone_df], ignore_index=True)\n",
    "            elif not new_standalone_df.empty:\n",
    "                combined_standalone = new_standalone_df\n",
    "            else:\n",
    "                combined_standalone = existing_standalone\n",
    "            \n",
    "            # Multiple records are already handled above\n",
    "            combined_multiple = new_multiple_df\n",
    "            \n",
    "            if not existing_errors.empty and not new_errors_df.empty:\n",
    "                combined_errors = pd.concat([existing_errors, new_errors_df], ignore_index=True)\n",
    "            elif not new_errors_df.empty:\n",
    "                combined_errors = new_errors_df\n",
    "            else:\n",
    "                combined_errors = existing_errors\n",
    "            \n",
    "            # Save to Excel with multiple sheets\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                        combined_standalone.to_excel(writer, sheet_name='standalone', index=False)\n",
    "                        combined_multiple.to_excel(writer, sheet_name='multiple', index=False)\n",
    "                        \n",
    "                        if not combined_errors.empty:\n",
    "                            combined_errors.to_excel(writer, sheet_name='errors', index=False)\n",
    "                    break\n",
    "                except PermissionError:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        print(f\"Excel file is locked. Retrying in 2 seconds... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                        time.sleep(2)\n",
    "                    else:\n",
    "                        print(f\"Failed to save Excel file after {max_retries} attempts. Please close the file and try again.\")\n",
    "                        raise\n",
    "            \n",
    "            print(f\"Data saved to {output_path}\")\n",
    "            print(f\"New standalone records added: {len(new_standalone_records)}\")\n",
    "            print(f\"Skipped duplicate standalone records: {skipped_standalone}\")\n",
    "            print(f\"Total standalone records: {len(combined_standalone)}\")\n",
    "            print(f\"Updated multiple record keys: {updated_multiple}\")\n",
    "            print(f\"Total multiple records: {len(combined_multiple)}\")\n",
    "            if self.errors:\n",
    "                print(f\"New errors: {len(self.errors)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Excel: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self, output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Run the complete extraction process\"\"\"\n",
    "        print(\"Starting HTML data extraction...\")\n",
    "        \n",
    "        # Reset data containers\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        \n",
    "        # Set up Selenium driver\n",
    "        self.setup_selenium_driver()\n",
    "        \n",
    "        try:\n",
    "            # Process files from latest round folder only\n",
    "            self.process_all_files()  # Use default base_dir parameter\n",
    "            \n",
    "            # Save to Excel\n",
    "            self.save_to_excel(output_path)\n",
    "            \n",
    "            print(\"HTML data extraction completed!\")\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Selenium driver closed\")\n",
    "\n",
    "    def clean_text_encoding(self, text):\n",
    "        \"\"\"Clean text to fix encoding issues like √¢‚Ç¨\" -> ‚Äî\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        # Common encoding fixes - ORDER MATTERS! Process longer patterns first\n",
    "        encoding_fixes = [\n",
    "            ('√¢‚Ç¨\"', '‚Äî'),   # em-dash\n",
    "            ('√¢‚Ç¨‚Ñ¢', \"'\"),   # right single quotation mark\n",
    "            ('√¢‚Ç¨≈ì', '\"'),   # left double quotation mark\n",
    "            ('√¢‚Ç¨¬¶', '‚Ä¶'),   # horizontal ellipsis\n",
    "            ('√¢‚Ç¨¬¢', '‚Ä¢'),   # bullet\n",
    "            ('√¢‚Ç¨‚Äπ', ''),    # zero-width space\n",
    "            ('√¢‚Ç¨‚Äö', ' '),   # en space\n",
    "            ('√¢‚Ç¨∆í', ' '),   # em space\n",
    "            ('√¢‚Ç¨‚Ä∞', ' '),   # thin space\n",
    "            ('√¢‚Ç¨', '\"'),    # right double quotation mark (shorter pattern, process last)\n",
    "            ('√Ç', ''),      # non-breaking space artifacts\n",
    "        ]\n",
    "        \n",
    "        cleaned_text = text\n",
    "        # Process in order to avoid substring conflicts\n",
    "        for bad, good in encoding_fixes:\n",
    "            cleaned_text = cleaned_text.replace(bad, good)\n",
    "        \n",
    "        # Remove any remaining problematic characters\n",
    "        cleaned_text = re.sub(r'√¢‚Ç¨[^\\w]', '', cleaned_text)\n",
    "        \n",
    "        return cleaned_text.strip()\n",
    "    \n",
    "    def extract_term_from_folder_path(self, filepath):\n",
    "        \"\"\"Extract term from folder path as fallback\n",
    "        E.g., script_input\\\\classTimingsFull\\\\2023-24_T3A -> T3A\"\"\"\n",
    "        try:\n",
    "            # Get the folder path\n",
    "            folder_path = os.path.dirname(filepath)\n",
    "            folder_name = os.path.basename(folder_path)\n",
    "            \n",
    "            # Look for term pattern in folder name\n",
    "            # Pattern: YYYY-YY_TXX or YYYY-YY_TXXA\n",
    "            term_match = re.search(r'(\\d{4}-\\d{2})_T(\\w+)', folder_name)\n",
    "            if term_match:\n",
    "                return f\"T{term_match.group(2)}\"\n",
    "            \n",
    "            # Fallback: look for any T followed by alphanumeric\n",
    "            term_fallback = re.search(r'T(\\w+)', folder_name)\n",
    "            if term_fallback:\n",
    "                return f\"T{term_fallback.group(1)}\"\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def extract_bidding_info(self):\n",
    "        \"\"\"Extract current bidding information from HTML elements\"\"\"\n",
    "        try:\n",
    "            # Extract Total\n",
    "            total = self.safe_find_element_text(By.ID, 'lblClassCapacity')\n",
    "            total = int(total) if total and total.isdigit() else None\n",
    "            \n",
    "            # Extract Current Enrolled\n",
    "            current_enrolled = self.safe_find_element_text(By.ID, 'lblEnrolmentTotal')\n",
    "            current_enrolled = int(current_enrolled) if current_enrolled and current_enrolled.isdigit() else None\n",
    "            \n",
    "            # Extract Reserved (for incoming students)\n",
    "            reserved = self.safe_find_element_text(By.ID, 'lblReserved')\n",
    "            reserved = int(reserved) if reserved and reserved.isdigit() else None\n",
    "            \n",
    "            # Extract Available Seats\n",
    "            available = self.safe_find_element_text(By.ID, 'lblAvailableSeats')\n",
    "            available = int(available) if available and available.isdigit() else None\n",
    "            \n",
    "            return total, current_enrolled, reserved, available\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, None, None, None\n",
    "\n",
    "    def load_existing_data(self, output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Load existing data from Excel file if it exists\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(output_path):\n",
    "                existing_standalone = pd.read_excel(output_path, sheet_name='standalone')\n",
    "                existing_multiple = pd.read_excel(output_path, sheet_name='multiple')\n",
    "                \n",
    "                # Handle case where errors sheet might not exist\n",
    "                try:\n",
    "                    existing_errors = pd.read_excel(output_path, sheet_name='errors')\n",
    "                except:\n",
    "                    existing_errors = pd.DataFrame()\n",
    "                \n",
    "                return existing_standalone, existing_multiple, existing_errors\n",
    "            else:\n",
    "                return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing data: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "    def check_record_exists(self, existing_df, new_record):\n",
    "        \"\"\"Check if a record already exists based on key fields\"\"\"\n",
    "        if existing_df.empty:\n",
    "            return False\n",
    "        \n",
    "        # Define key fields that make a record unique\n",
    "        key_fields = ['acad_term_id', 'course_code', 'section', 'bidding_window']\n",
    "        \n",
    "        # Check if all key fields exist in both dataframes\n",
    "        for field in key_fields:\n",
    "            if field not in existing_df.columns:\n",
    "                return False\n",
    "            if new_record.get(field) is None:\n",
    "                return False\n",
    "        \n",
    "        # Create a mask to check for matching records\n",
    "        mask = True\n",
    "        for field in key_fields:\n",
    "            mask = mask & (existing_df[field] == new_record[field])\n",
    "        \n",
    "        return mask.any()\n",
    "    \n",
    "    def get_current_academic_term(self):\n",
    "        \"\"\"\n",
    "        Determines the current academic term (e.g., 2025-26_T1) by mapping the\n",
    "        current date to the academic calendar terms.\n",
    "\n",
    "        This logic is based on the SMU Academic Calendar for 2025-26, where bidding\n",
    "        for a term can start before the official term commencement date. The function\n",
    "        approximates term boundaries based on the calendar.\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        month = now.month\n",
    "\n",
    "        # Determine the starting year of the academic year.\n",
    "        # The academic year 'YYYY-(YY+1)' is assumed to start with T1 bidding in July of 'YYYY'.\n",
    "        # So, from July onwards, we are in the 'YYYY' to 'YYYY+1' academic year.\n",
    "        # Before July, we are in the latter half of the 'YYYY-1' to 'YYYY' academic year.\n",
    "        if month >= 7:\n",
    "            acad_year_start = now.year\n",
    "        else:\n",
    "            acad_year_start = now.year - 1\n",
    "        \n",
    "        acad_year_end_short = (acad_year_start + 1) % 100\n",
    "        \n",
    "        # Define the approximate start dates for each term's activities (including bidding)\n",
    "        # for the determined academic year. These dates are based on the 2025-26 calendar.\n",
    "        # \n",
    "        t1_start = datetime(acad_year_start, 7, 1) # Bidding for T1 starts in July\n",
    "        t2_start = datetime(acad_year_start + 1, 1, 1) # T2 starts in January\n",
    "        t3a_start = datetime(acad_year_start + 1, 5, 11) # T3A starts May 11\n",
    "        t3b_start = datetime(acad_year_start + 1, 6, 29) # T3B starts June 29\n",
    "        \n",
    "        # Determine the term by checking the current date against the start dates\n",
    "        # in reverse chronological order.\n",
    "        term_code = None\n",
    "        if now >= t3b_start:\n",
    "            term_code = 'T3B'\n",
    "        elif now >= t3a_start:\n",
    "            term_code = 'T3A'\n",
    "        elif now >= t2_start:\n",
    "            term_code = 'T2'\n",
    "        elif now >= t1_start:\n",
    "            term_code = 'T1'\n",
    "\n",
    "        if term_code:\n",
    "            return f\"{acad_year_start}-{acad_year_end_short:02d}_{term_code}\"\n",
    "        else:\n",
    "            # This is a fallback for dates that might fall outside the defined ranges,\n",
    "            # though the logic should cover all dates within a valid academic year.\n",
    "            print(f\"Warning: Could not determine the academic term for the date {now}. Review term start dates.\")\n",
    "            return None\n",
    "\n",
    "    def find_latest_round_folder(self, term_path):\n",
    "        \"\"\"Find the latest round folder in the academic term directory\"\"\"\n",
    "        try:\n",
    "            # Get all subdirectories\n",
    "            subdirs = [d for d in os.listdir(term_path) if os.path.isdir(os.path.join(term_path, d))]\n",
    "            \n",
    "            # Filter for round folders (should contain R and W)\n",
    "            round_folders = [d for d in subdirs if 'R' in d and 'W' in d]\n",
    "            \n",
    "            if not round_folders:\n",
    "                return None\n",
    "            \n",
    "            # Sort by folder name (this should work for the naming convention)\n",
    "            # R1W1, R1AW1, R1AW2, etc.\n",
    "            round_folders.sort(key=lambda x: (\n",
    "                int(x.split('R')[1].split('W')[0].replace('A', '').replace('B', '').replace('C', '').replace('F', '')),\n",
    "                x.count('A') + x.count('B') * 2 + x.count('C') * 3 + x.count('F') * 4,\n",
    "                int(x.split('W')[1])\n",
    "            ))\n",
    "            \n",
    "            return round_folders[-1]  # Return the latest one\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding latest round folder: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_bidding_window_from_folder(self, folder_name):\n",
    "        \"\"\"Extract bidding window from folder name (e.g., 2025-26_T1_R1W1 -> Round 1 Window 1)\"\"\"\n",
    "        try:\n",
    "            # Extract the round and window part (e.g., R1W1, R1AW2, etc.)\n",
    "            round_part = folder_name.split('_')[-1]  # Get the last part after underscore\n",
    "            \n",
    "            # Map folder codes to full names\n",
    "            folder_to_window = {\n",
    "                'R1W1': 'Round 1 Window 1',\n",
    "                'R1AW1': 'Round 1A Window 1',\n",
    "                'R1AW2': 'Round 1A Window 2',\n",
    "                'R1AW3': 'Round 1A Window 3',\n",
    "                'R1BW1': 'Round 1B Window 1',\n",
    "                'R1BW2': 'Round 1B Window 2',\n",
    "                'R1CW1': 'Incoming Exchange Rnd 1C Win 1',\n",
    "                'R1CW2': 'Incoming Exchange Rnd 1C Win 2',\n",
    "                'R1CW3': 'Incoming Exchange Rnd 1C Win 3',\n",
    "                'R1FW1': 'Incoming Freshmen Rnd 1 Win 1',\n",
    "                'R1FW2': 'Incoming Freshmen Rnd 1 Win 2',\n",
    "                'R1FW3': 'Incoming Freshmen Rnd 1 Win 3',\n",
    "                'R1FW4': 'Incoming Freshmen Rnd 1 Win 4',\n",
    "                'R2W1': 'Round 2 Window 1',\n",
    "                'R2W2': 'Round 2 Window 2',\n",
    "                'R2W3': 'Round 2 Window 3',\n",
    "                'R2AW1': 'Round 2A Window 1',\n",
    "                'R2AW2': 'Round 2A Window 2',\n",
    "                'R2AW3': 'Round 2A Window 3',\n",
    "            }\n",
    "            \n",
    "            return folder_to_window.get(round_part, round_part)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bidding window from folder: {e}\")\n",
    "            return folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Run the extraction process\n",
    "extractor.run(output_path='script_input/raw_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **4. Process Raw Data into Database Tables**\n",
    "\n",
    "### **What This Code Does**\n",
    "The `TableBuilder` class processes structured data from the HTML extractor and transforms it into database-ready CSV files for SMU's class management system. It handles complex data relationships, professor name normalization, duplicate detection, and creates all necessary tables for courses, classes, professors, timing schedules, bidding data, and faculty assignments while maintaining referential integrity across all database tables.\n",
    "\n",
    "**Key Features:**\n",
    "- **Three-Phase Processing**: Phase 1 (professors/courses with automated faculty mapping), Phase 2 (classes/timings), Phase 3 (BOSS bidding results from raw_data.xlsx)\n",
    "- **Enhanced Professor Schema**: Uses `boss_aliases` as JSON array strings for CSV output supporting multiple name variations per professor\n",
    "- **Intelligent Professor Matching**: Advanced name normalization with email resolution via Outlook integration, comprehensive duplicate detection, and improved NaN handling from raw_data.xlsx\n",
    "- **Automated Faculty Mapping**: Uses course code patterns from existing database courses to automatically assign new courses to SMU's schools and centers\n",
    "- **Comprehensive Data Pipeline**: Processes professors, courses, academic terms, classes, class timings, exam schedules, bid windows, class availability, and bid results with proper foreign key relationships\n",
    "- **Database Cache Integration**: Loads existing data from PostgreSQL to avoid duplicates and maintain consistency with enhanced validation for clean data files\n",
    "- **Manual Review Workflow**: Outputs verification files for human review and correction before final processing\n",
    "- **BOSS Bidding Integration**: Complete processing of bidding data directly from raw_data.xlsx with fields like total capacity, current enrollment, reserved seats, available seats, extraction timestamps, and bidding windows\n",
    "- **Asian Name Handling**: Specialized normalization for Asian, Western, and mixed naming conventions with hardcoded multi-instructor handling\n",
    "- **Data Integrity Validation**: Comprehensive validation system that checks referential integrity across all generated CSV files with detailed error reporting\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Raw Data Excel**: `script_input/raw_data.xlsx` from HTML extractor with `standalone` and `multiple` sheets containing bidding data fields\n",
    "- **Database Configuration**: `.env` file with PostgreSQL connection parameters\n",
    "- **Professor Lookup**: `script_input/professor_lookup.csv` for existing professor mappings (optional)\n",
    "\n",
    "**Output Structure:**\n",
    "- **Verification Files** (`script_output/verify/`): `new_professors.csv`, `new_courses.csv`, `new_faculties.csv`\n",
    "- **Database Insert Files** (`script_output/`): All table CSV files including classes, timings, exams, bid windows, class availability, and bid results\n",
    "- **Update Files**: `update_courses.csv`, `update_professor.csv` for existing record modifications\n",
    "- **Validation Reports**: Data integrity validation with error/warning reports and comprehensive statistics\n",
    "- **Processing Logs**: Detailed BOSS processing logs with timestamps and failure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import traceback\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TableBuilder:\n",
    "    \"\"\"Comprehensive table builder for university class management system\"\"\"\n",
    "    \n",
    "    def __init__(self, input_file: str = 'script_input/raw_data.xlsx'):\n",
    "        \"\"\"Initialize TableBuilder with database configuration and caching\"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_base = 'script_output'\n",
    "        self.verify_dir = os.path.join(self.output_base, 'verify')\n",
    "        self.cache_dir = 'db_cache'\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(self.output_base, exist_ok=True)\n",
    "        os.makedirs(self.verify_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        self.db_config = {\n",
    "            'host': os.getenv('DB_HOST'),\n",
    "            'database': os.getenv('DB_NAME'),\n",
    "            'user': os.getenv('DB_USER'),\n",
    "            'password': os.getenv('DB_PASSWORD'),\n",
    "            'port': int(os.getenv('DB_PORT', 5432)),\n",
    "            'gssencmode': 'disable'\n",
    "        }\n",
    "        \n",
    "        # Database connection\n",
    "        self.connection = None\n",
    "        \n",
    "        # Data storage\n",
    "        self.standalone_data = None\n",
    "        self.multiple_data = None\n",
    "        \n",
    "        # Caches\n",
    "        self.professors_cache = {}  # name -> professor data\n",
    "        self.courses_cache = {}     # code -> course data\n",
    "        self.acad_term_cache = {}   # id -> acad_term data\n",
    "        self.faculties_cache = {}   # id -> faculty data\n",
    "        self.faculty_acronym_to_id = {}  # acronym -> faculty_id mapping\n",
    "        self.professor_lookup = {}  # scraped_name -> database mapping\n",
    "        \n",
    "        self.processed_timing_keys = set()\n",
    "        self.processed_exam_class_ids = set()\n",
    "\n",
    "        # Output data collectors\n",
    "        self.new_professors = []\n",
    "        self.new_courses = []\n",
    "        self.update_courses = []\n",
    "        self.new_acad_terms = []\n",
    "        self.new_classes = []\n",
    "        self.new_class_timings = []\n",
    "        self.new_class_exam_timings = []\n",
    "        self.update_professors = []  # For boss_name updates\n",
    "        self.update_bid_result = []  # For bid result updates\n",
    "        \n",
    "        # Class ID mapping for timing tables\n",
    "        self.class_id_mapping = {}  # record_key -> class_id\n",
    "        \n",
    "        # Courses requiring faculty assignment\n",
    "        self.courses_needing_faculty = []\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'professors_created': 0,\n",
    "            'professors_updated': 0,\n",
    "            'courses_created': 0,\n",
    "            'courses_updated': 0,\n",
    "            'classes_created': 0,\n",
    "            'timings_created': 0,\n",
    "            'exams_created': 0,\n",
    "            'courses_needing_faculty': 0\n",
    "        }\n",
    "        \n",
    "        # Use the global bidding schedule, assuming the start term is the target\n",
    "        self.bidding_schedule = BIDDING_SCHEDULES.get(START_AY_TERM, [])\n",
    "        \n",
    "        # Asian surnames database for name normalization\n",
    "        self.asian_surnames = {\n",
    "            # Top 100+ common surnames covering Mainland China, Taiwan, HK, and Singapore/Malaysia\n",
    "            'chinese': [\n",
    "                'WANG', 'LI', 'ZHANG', 'LIU', 'CHEN', 'YANG', 'HUANG', 'ZHAO', 'WU', 'ZHOU', 'XU', 'SUN', 'MA', 'ZHU', 'HU', 'GUO', 'HE', 'LIN', 'GAO', 'LUO', \n",
    "                'CHENG', 'LIANG', 'XIE', 'SONG', 'TANG', 'HAN', 'FENG', 'DENG', 'CAO', 'PENG', 'YUAN', 'SU', 'JIANG', 'JIA', 'LU', 'WEI', 'XIAO', 'YU', 'QIAN', \n",
    "                'PAN', 'YAO', 'TAN', 'DU', 'YE', 'TIAN', 'SHI', 'BAI', 'QIN', 'XUE', 'YAN', 'DAI', 'MO', 'CHANG', 'WAN', 'GU', 'ZENG', 'LUO', 'FAN', 'JIN',\n",
    "                'ONG', 'LIM', 'LEE', 'TEO', 'NG', 'GOH', 'CHUA', 'CHAN', 'KOH', 'ANG', 'YEO', 'SIM', 'CHIA', 'CHONG', 'LAM', 'CHEW', 'TOH', 'LOW', 'SEAH',\n",
    "                'PEK', 'KWEK', 'QUEK', 'LOH', 'AW', 'CHYE', 'LOK'\n",
    "            ],\n",
    "            # Top ~30 Korean surnames\n",
    "            'korean': [\n",
    "                'KIM', 'LEE', 'PARK', 'CHOI', 'JEONG', 'KANG', 'CHO', 'YOON', 'JANG', 'LIM', 'HAN', 'OH', 'SEO', 'KWON', 'HWANG', 'SONG', 'JUNG', 'HONG', \n",
    "                'AHN', 'GO', 'MOON', 'SON', 'BAE', 'BAEK', 'HEO', 'NAM'\n",
    "            ],\n",
    "            # Top ~20 Vietnamese surnames\n",
    "            'vietnamese': [\n",
    "                'NGUYEN', 'TRAN', 'LE', 'PHAM', 'HOANG', 'PHAN', 'VU', 'VO', 'DANG', 'BUI', 'DO', 'HO', 'NGO', 'DUONG', 'LY'\n",
    "            ],\n",
    "            # Top ~30 Indian surnames from various regions\n",
    "            'indian': [\n",
    "                'SHARMA', 'SINGH', 'KUMAR', 'GUPTA', 'PATEL', 'KHAN', 'REDDY', 'YADAV', 'DAS', 'JAIN', 'RAO', 'MEHTA', 'CHOPRA', 'KAPOOR', 'MALHOTRA',\n",
    "                'AGGARWAL', 'JOSHI', 'MISHRA', 'TRIPATHI', 'PANDEY', 'NAIR', 'MENON', 'PILLAI', 'IYER', 'MUKHERJEE', 'BANERJEE', 'CHATTERJEE'\n",
    "            ],\n",
    "            # Top ~20 Japanese surnames\n",
    "            'japanese': [\n",
    "                'SATO', 'SUZUKI', 'TAKAHASHI', 'TANAKA', 'WATANABE', 'ITO', 'YAMAMOTO', 'NAKAMURA', 'KOBAYASHI', 'SAITO', 'KATO', 'YOSHIDA', 'YAMADA'\n",
    "            ]\n",
    "        }\n",
    "        self.all_asian_surnames = set()\n",
    "        for surnames in self.asian_surnames.values():\n",
    "            self.all_asian_surnames.update(surnames)\n",
    "        \n",
    "        # Western given names\n",
    "        self.western_given_names = {\n",
    "            'AARON', 'ADAM', 'ADRIAN', 'ALAN', 'ALBERT', 'ALEX', 'ALEXANDER', 'ALFRED', 'ALVIN', 'AMANDA', 'AMY', 'ANDREA', 'ANDREW', 'ANGELA', 'ANNA', 'ANTHONY', 'ARTHUR', 'AUDREY',\n",
    "            'BEN', 'BENJAMIN', 'BERNARD', 'BETTY', 'BILLY', 'BOB', 'BOWEN', 'BRANDON', 'BRENDA', 'BRIAN', 'BRYAN', 'BRUCE',\n",
    "            'CARL', 'CAROL', 'CATHERINE', 'CHARLES', 'CHRIS', 'CHRISTIAN', 'CHRISTINA', 'CHRISTINE', 'CHRISTOPHER', 'COLIN', 'CRAIG', 'CRYS',\n",
    "            'DANIEL', 'DANNY', 'DARREN', 'DAVID', 'DEBORAH', 'DENISE', 'DENNIS', 'DEREK', 'DIANA', 'DONALD', 'DOUGLAS',\n",
    "            'EDWARD', 'EDWIN', 'ELAINE', 'ELIZABETH', 'EMILY', 'ERIC', 'EUGENE', 'EVELYN',\n",
    "            'FELIX', 'FRANCIS', 'FRANK',\n",
    "            'GABRIEL', 'GARY', 'GEOFFREY', 'GEORGE', 'GERALD', 'GLORIA', 'GORDON', 'GRACE', 'GRAHAM', 'GREGORY',\n",
    "            'HANNAH', 'HARRY', 'HELEN', 'HENRY', 'HOWARD',\n",
    "            'IAN', 'IVAN',\n",
    "            'JACK', 'JACOB', 'JAMES', 'JANE', 'JANET', 'JASON', 'JEAN', 'JEFFREY', 'JENNIFER', 'JEREMY', 'JERRY', 'JESSICA', 'JIM', 'JOAN', 'JOE', 'JOHN', 'JONATHAN', 'JOSEPH', 'JOSHUA', 'JOYCE', 'JUDY', 'JULIA', 'JULIE', 'JUSTIN',\n",
    "            'KAREN', 'KATHERINE', 'KATHY', 'KEITH', 'KELLY', 'KELVIN', 'KENNETH', 'KEVIN', 'KIMBERLY',\n",
    "            'LARRY', 'LAURA', 'LAWRENCE', 'LEO', 'LEONARD', 'LINDA', 'LISA',\n",
    "            'MARGARET', 'MARIA', 'MARK', 'MARTIN', 'MARY', 'MATTHEW', 'MEGAN', 'MELISSA', 'MICHAEL', 'MICHELLE', 'MIKE',\n",
    "            'NANCY', 'NATHAN', 'NEHA', 'NICHOLAS', 'NICOLE',\n",
    "            'OLIVER', 'OLIVIA',\n",
    "            'PAMELA', 'PATRICIA', 'PATRICK', 'PAUL', 'PETER', 'PHILIP',\n",
    "            'RACHEL', 'RAYMOND', 'REBECCA', 'RICHARD', 'ROBERT', 'ROGER', 'RONALD', 'ROY', 'RUSSELL', 'RYAN',\n",
    "            'SAM', 'SAMUEL', 'SANDRA', 'SARAH', 'SCOTT', 'SEAN', 'SHARON', 'SOPHIA', 'STANLEY', 'STEPHANIE', 'STEPHEN', 'STEVEN', 'SUSAN',\n",
    "            'TERENCE', 'TERRY', 'THERESA', 'THOMAS', 'TIMOTHY', 'TONY',\n",
    "            'VALERIE', 'VICTOR', 'VINCENT', 'VIRGINIA',\n",
    "            'WALTER', 'WAYNE', 'WENDY', 'WILLIAM', 'WILLIE'\n",
    "        }\n",
    "\n",
    "        # Keywords for patronymic names (Malay, Indian, etc.)\n",
    "        self.patronymic_keywords = {'BIN', 'BINTE', 'S/O', 'D/O'}\n",
    "\n",
    "        # European surname particles\n",
    "        self.surname_particles = {'DE', 'DI', 'DA', 'VAN', 'VON', 'LA', 'LE', 'DEL', 'DELLA'}       \n",
    "\n",
    "        # Bid results data collectors\n",
    "        self.boss_log_file = os.path.join(self.output_base, 'boss_result_log.txt')\n",
    "        self.new_bid_windows = []\n",
    "        self.new_class_availability = []\n",
    "        self.new_bid_result = []\n",
    "        self.failed_mappings = []\n",
    "        self.bid_window_cache = {}\n",
    "        self.bid_window_id_counter = 1\n",
    "\n",
    "        # Professor lookup from CSV\n",
    "        self.professor_lookup = {}\n",
    "        \n",
    "        # Load professor lookup if available\n",
    "        self.load_professor_lookup_csv()\n",
    "\n",
    "        # LLM Configuration\n",
    "        logger.info(\"üîß Initializing LLM configuration...\")\n",
    "        self.llm_model_name = \"gemini-2.5-flash\"\n",
    "        self.llm_batch_size = 50\n",
    "        self.llm_prompt = \"\"\"\n",
    "        You are an expert in academic name structures from around the world.\n",
    "        You will be given a JSON list of professor names.\n",
    "        Your task is to identify the primary surname for each name.\n",
    "        You MUST return a single JSON array of strings, where each string is the identified surname.\n",
    "        The order of surnames in your response must exactly match the order of the full names in the input list.\n",
    "        Provide ONLY the JSON array in your response.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pre-configure the model if the API key exists\n",
    "        self.llm_model = None\n",
    "        if os.getenv('GEMINI_API_KEY'):\n",
    "            try:\n",
    "                genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "                optimized_config = genai.GenerationConfig(\n",
    "                    response_mime_type=\"application/json\"\n",
    "                )\n",
    "                self.llm_model = genai.GenerativeModel(\n",
    "                    self.llm_model_name,\n",
    "                    generation_config=optimized_config\n",
    "                )\n",
    "                logger.info(f\"‚úÖ Gemini model '{self.llm_model_name}' configured successfully.\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Could not pre-configure Gemini model: {e}\")\n",
    "        else:\n",
    "            logger.warning(\"‚ö†Ô∏è GEMINI_API_KEY not found. LLM normalization will be skipped.\")\n",
    "\n",
    "    def connect_database(self):\n",
    "        \"\"\"Connect to PostgreSQL database\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(**self.db_config)\n",
    "            logger.info(\"‚úÖ Database connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Database connection failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_or_cache_data(self):\n",
    "        \"\"\"Load data from cache or database\"\"\"\n",
    "        # Try loading from cache first\n",
    "        if self._load_from_cache():\n",
    "            logger.info(\"‚úÖ Loaded data from cache\")\n",
    "            return True\n",
    "        \n",
    "        # Connect to database and download\n",
    "        if not self.connect_database():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self._download_and_cache_data()\n",
    "            logger.info(\"‚úÖ Downloaded and cached data from database\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to download data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _download_and_cache_data(self):\n",
    "        \"\"\"Download data from all required database tables and cache them locally.\"\"\"\n",
    "        try:\n",
    "            # Define all tables to be cached\n",
    "            tables_to_cache = [\n",
    "                \"professors\", \"courses\", \"acad_term\", \"faculties\", \n",
    "                \"classes\", \"class_timing\", \"class_exam_timing\", \n",
    "                \"class_availability\", \"bid_window\", \"bid_result\", \"bid_prediction\"\n",
    "            ]\n",
    "            \n",
    "            for table_name in tables_to_cache:\n",
    "                logger.info(f\"‚¨áÔ∏è Caching table: {table_name}\")\n",
    "                query = f\"SELECT * FROM {table_name}\"\n",
    "                df = pd.read_sql_query(query, self.connection)\n",
    "                df.to_pickle(os.path.join(self.cache_dir, f'{table_name}_cache.pkl'))\n",
    "                \n",
    "            logger.info(\"‚úÖ Downloaded all tables from database and cached locally\")\n",
    "            \n",
    "            # Load the newly cached data into memory\n",
    "            self._load_from_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to download and cache data for table '{table_name}': {e}\")\n",
    "            # Add traceback for detailed debugging\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _load_from_cache(self) -> bool:\n",
    "        \"\"\"\n",
    "        Load cached data from files, with robust validation of the professor lookup against the database cache.\n",
    "        Professor validation only runs during Phase 1.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cache_files = {\n",
    "                'professors': os.path.join(self.cache_dir, 'professors_cache.pkl'),\n",
    "                'courses': os.path.join(self.cache_dir, 'courses_cache.pkl'),\n",
    "                'acad_terms': os.path.join(self.cache_dir, 'acad_term_cache.pkl'),\n",
    "                'faculties': os.path.join(self.cache_dir, 'faculties_cache.pkl'),\n",
    "                'bid_result': os.path.join(self.cache_dir, 'bid_result_cache.pkl'),\n",
    "                'bid_window': os.path.join(self.cache_dir, 'bid_window_cache.pkl'),\n",
    "                'class_availability': os.path.join(self.cache_dir, 'class_availability_cache.pkl'),\n",
    "                'class_exam_timing': os.path.join(self.cache_dir, 'class_exam_timing_cache.pkl'),\n",
    "                'class_timing': os.path.join(self.cache_dir, 'class_timing_cache.pkl'),\n",
    "                'classes': os.path.join(self.cache_dir, 'classes_cache.pkl')\n",
    "            }\n",
    "            \n",
    "            if not all(os.path.exists(f) for f in cache_files.values()):\n",
    "                logger.warning(\"‚ö†Ô∏è Not all cache files found. Need to download from database.\")\n",
    "                return False\n",
    "\n",
    "            # Load professors data first\n",
    "            professors_df = pd.read_pickle(cache_files['professors'])\n",
    "            \n",
    "            # Check if this is Phase 1 by looking at the call stack or phase indicator\n",
    "            is_phase1 = (hasattr(self, '_phase2_mode') and not self._phase2_mode) or not hasattr(self, '_phase2_mode')\n",
    "            \n",
    "            if is_phase1:\n",
    "                # --- Professor Lookup Synchronization (Phase 1 only) ---\n",
    "                logger.info(\"üîÑ Phase 1: Synchronizing professor lookup with database cache...\")\n",
    "                \n",
    "                database_professors = {}\n",
    "                all_database_aliases = {}\n",
    "\n",
    "                for _, row in professors_df.iterrows():\n",
    "                    professor_data = row.to_dict()\n",
    "                    professor_id = str(row.get('id'))\n",
    "                    professor_name = str(row.get('name', '')).strip()\n",
    "                    \n",
    "                    database_professors[professor_id] = professor_data\n",
    "                    \n",
    "                    # Add the professor's actual name as an alias\n",
    "                    if professor_name:\n",
    "                        all_database_aliases[professor_name.upper()] = professor_id\n",
    "\n",
    "                    # Handle boss_aliases - robust parsing\n",
    "                    aliases_list = self._parse_boss_aliases(row.get('boss_aliases'))\n",
    "                    for alias in aliases_list:\n",
    "                        if alias and str(alias).strip():\n",
    "                            all_database_aliases[str(alias).upper()] = professor_id\n",
    "\n",
    "                logger.info(f\"üìö Loaded {len(database_professors)} professors from cache\")\n",
    "                logger.info(f\"üìö Found {len(all_database_aliases)} total aliases (including names)\")\n",
    "\n",
    "                # Load and validate professor_lookup.csv\n",
    "                lookup_file = 'script_input/professor_lookup.csv'\n",
    "                validated_professor_lookup = {}\n",
    "                csv_entries_removed = 0\n",
    "                csv_entries_corrected = 0\n",
    "                csv_entries_added = 0\n",
    "\n",
    "                if os.path.exists(lookup_file):\n",
    "                    try:\n",
    "                        lookup_df = pd.read_csv(lookup_file)\n",
    "                        for _, row in lookup_df.iterrows():\n",
    "                            boss_name = str(row.get('boss_name', '')).strip()\n",
    "                            afterclass_name = str(row.get('afterclass_name', '')).strip()\n",
    "                            database_id = str(row.get('database_id', '')).strip()\n",
    "                            \n",
    "                            if not boss_name or not database_id: continue\n",
    "                                \n",
    "                            boss_name_key = boss_name.upper()\n",
    "\n",
    "                            # CRITICAL: Validate database_id exists in database\n",
    "                            if database_id not in database_professors:\n",
    "                                logger.warning(f\"‚ùå Invalid database_id in lookup: '{boss_name}' references non-existent ID {database_id}. Removing.\")\n",
    "                                csv_entries_removed += 1\n",
    "                                continue\n",
    "\n",
    "                            db_professor = database_professors[database_id]\n",
    "                            db_name = str(db_professor.get('name', '')).strip()\n",
    "                            \n",
    "                            # Correct afterclass_name if it differs from database\n",
    "                            if afterclass_name != db_name:\n",
    "                                logger.warning(f\"‚úèÔ∏è Correcting lookup entry for '{boss_name}': Name mismatch (CSV: '{afterclass_name}' vs DB: '{db_name}'). Using DB name.\")\n",
    "                                afterclass_name = db_name\n",
    "                                csv_entries_corrected += 1\n",
    "                            \n",
    "                            validated_professor_lookup[boss_name_key] = {\n",
    "                                'database_id': database_id,\n",
    "                                'boss_name': boss_name,\n",
    "                                'afterclass_name': afterclass_name\n",
    "                            }\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"‚ùå Error reading professor_lookup.csv: {e}\")\n",
    "                else:\n",
    "                    logger.info(\"üìã professor_lookup.csv not found. Creating from database.\")\n",
    "\n",
    "                # Add missing database aliases to lookup (bidirectional sync)\n",
    "                for alias_key, professor_id in all_database_aliases.items():\n",
    "                    if alias_key not in validated_professor_lookup:\n",
    "                        db_professor = database_professors[professor_id]\n",
    "                        db_name = str(db_professor.get('name', '')).strip()\n",
    "                        \n",
    "                        validated_professor_lookup[alias_key] = {\n",
    "                            'database_id': str(professor_id),\n",
    "                            'boss_name': alias_key,\n",
    "                            'afterclass_name': db_name\n",
    "                        }\n",
    "                        csv_entries_added += 1\n",
    "                        # Only log for non-name aliases to reduce noise\n",
    "                        if alias_key != db_name.upper():\n",
    "                            logger.info(f\"‚ûï Added missing DB alias to lookup: '{alias_key}' -> '{db_name}' (ID: {professor_id})\")\n",
    "\n",
    "                self.professor_lookup = validated_professor_lookup\n",
    "                \n",
    "                logger.info(\"‚úÖ Phase 1 Professor lookup synchronization complete:\")\n",
    "                logger.info(f\"  - Entries validated: {len(validated_professor_lookup)}\")\n",
    "                logger.info(f\"  - Corrected entries: {csv_entries_corrected}\")\n",
    "                logger.info(f\"  - Added DB entries: {csv_entries_added}\")\n",
    "                logger.info(f\"  - Removed invalid entries: {csv_entries_removed}\")\n",
    "\n",
    "                # Save corrected lookup back to file\n",
    "                corrected_lookup_data = sorted(list(self.professor_lookup.values()), key=lambda x: x['boss_name'])\n",
    "                for item in corrected_lookup_data:\n",
    "                    item['method'] = 'validated'\n",
    "                \n",
    "                corrected_df = pd.DataFrame(corrected_lookup_data)\n",
    "                corrected_df.to_csv(lookup_file, index=False, columns=['boss_name', 'afterclass_name', 'database_id', 'method'])\n",
    "                logger.info(f\"üíæ Updated '{lookup_file}' with synchronized data.\")\n",
    "\n",
    "                # Build professors_cache for lookups\n",
    "                self.professors_cache = {}\n",
    "                for lookup_data in self.professor_lookup.values():\n",
    "                    db_id = lookup_data['database_id']\n",
    "                    boss_name_key = lookup_data['boss_name'].upper()\n",
    "                    if db_id in database_professors:\n",
    "                        self.professors_cache[boss_name_key] = database_professors[db_id]\n",
    "            else:\n",
    "                # Phase 2/3: Simple loading without validation\n",
    "                logger.info(\"üîÑ Phase 2/3: Loading professor data without validation...\")\n",
    "                self.professors_cache = {}\n",
    "                for _, row in professors_df.iterrows(): \n",
    "                    # Simple loading for non-Phase 1\n",
    "                    professor_data = row.to_dict()\n",
    "                    professor_name = str(row.get('name', '')).strip().upper()\n",
    "                    if professor_name:\n",
    "                        self.professors_cache[professor_name] = professor_data\n",
    "\n",
    "            # --- Load Remaining Caches (all phases) ---\n",
    "            courses_df = pd.read_pickle(cache_files['courses'])\n",
    "            for _, row in courses_df.iterrows(): self.courses_cache[row['code']] = row.to_dict()\n",
    "            \n",
    "            acad_terms_df = pd.read_pickle(cache_files['acad_terms'])\n",
    "            for _, row in acad_terms_df.iterrows(): self.acad_term_cache[row['id']] = row.to_dict()\n",
    "                \n",
    "            faculties_df = pd.read_pickle(cache_files['faculties'])\n",
    "            for _, row in faculties_df.iterrows():\n",
    "                self.faculties_cache[row['id']] = row.to_dict()\n",
    "                self.faculty_acronym_to_id[row['acronym'].upper()] = row['id']\n",
    "                \n",
    "            bid_window_df = pd.read_pickle(cache_files['bid_window'])\n",
    "            if not bid_window_df.empty:\n",
    "                self.bid_window_id_counter = bid_window_df['id'].max() + 1\n",
    "                for _, row in bid_window_df.iterrows():\n",
    "                    self.bid_window_cache[(row['acad_term_id'], row['round'], row['window'])] = row['id']\n",
    "            else:\n",
    "                self.bid_window_id_counter = 1\n",
    "                self.bid_window_cache = {}\n",
    "\n",
    "            logger.info(\"‚úÖ All cache files loaded successfully.\")\n",
    "            if is_phase1:\n",
    "                logger.info(f\"  - Professor lookup entries: {len(self.professor_lookup)} entries\")\n",
    "            logger.info(f\"  - Professors cache: {len(self.professors_cache)} entries\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Cache loading error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def load_raw_data(self):\n",
    "        \"\"\"\n",
    "        Load raw data WITHOUT applying global filtering.\n",
    "        Each processing function will apply its own appropriate filtering.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"üìÇ Loading raw data from {self.input_file}\")\n",
    "            \n",
    "            full_standalone_df = pd.read_excel(self.input_file, sheet_name='standalone')\n",
    "            full_multiple_df = pd.read_excel(self.input_file, sheet_name='multiple')\n",
    "            \n",
    "            logger.info(f\"‚úÖ Loaded {len(full_standalone_df)} total standalone and {len(full_multiple_df)} total multiple records.\")\n",
    "            \n",
    "            # === REMOVED GLOBAL FILTERING ===\n",
    "            # Store the full data without filtering - each processing function will filter as needed\n",
    "            self.standalone_data = full_standalone_df\n",
    "            self.multiple_data = full_multiple_df\n",
    "            \n",
    "            # Log available bidding windows for debugging\n",
    "            if 'bidding_window' in full_standalone_df.columns:\n",
    "                available_windows = full_standalone_df['bidding_window'].dropna().unique()\n",
    "                logger.info(f\"üìä Available bidding windows in data: {sorted(available_windows)}\")\n",
    "            \n",
    "            # Log available academic terms for debugging  \n",
    "            if 'acad_term_id' in full_standalone_df.columns:\n",
    "                available_terms = full_standalone_df['acad_term_id'].dropna().unique()\n",
    "                logger.info(f\"üìä Available academic terms in data: {sorted(available_terms)}\")\n",
    "\n",
    "            # Create optimized lookup for the multiple_data\n",
    "            self.multiple_lookup = defaultdict(list)\n",
    "            for _, row in self.multiple_data.iterrows():\n",
    "                key = row.get('record_key')\n",
    "                if pd.notna(key):\n",
    "                    self.multiple_lookup[key].append(row)\n",
    "            \n",
    "            logger.info(f\"‚úÖ Created optimized lookup for {len(self.multiple_lookup)} record keys from unfiltered data.\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to load raw data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _normalize_professor_name_fallback(self, name: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        (Fallback Method) Normalizes professor names using a definitive, rule-based system.\n",
    "        \"\"\"\n",
    "        if name is None or pd.isna(name) or not str(name).strip():\n",
    "            return \"UNKNOWN\", \"Unknown\"\n",
    "\n",
    "        # --- Step 1: Aggressive Preprocessing ---\n",
    "        name_str = str(name).strip().replace(\"‚Äô\", \"'\")\n",
    "        name_str = re.sub(r'\\s*\\(.*\\)\\s*', ' ', name_str).strip()\n",
    "        # Remove all middle initials (e.g., \"S.\", \"H.\", \"H H\", \"S\") to standardize names\n",
    "        # This looks for standalone single letters, with or without a dot.\n",
    "        words = name_str.split()\n",
    "        words_no_initials = [word for word in words if not (len(word) == 1 and word.isalpha()) and not (len(word) == 2 and word.endswith('.'))]\n",
    "        name_str = ' '.join(words_no_initials)\n",
    "\n",
    "        boss_name = name_str.upper()\n",
    "\n",
    "        # --- Step 2: Handle High-Certainty Delimiters ---\n",
    "        if ',' in name_str:\n",
    "            parts = [p.strip() for p in name_str.split(',')]\n",
    "            words = ' '.join(parts).split()\n",
    "            surname_to_check = words[0].upper()\n",
    "            if len(parts) == 2:\n",
    "                words_after_comma = parts[1].split()\n",
    "                if words_after_comma and words_after_comma[0].upper() in self.all_asian_surnames:\n",
    "                    surname_to_check = words_after_comma[0].upper()\n",
    "                else:\n",
    "                    words_before_comma = parts[0].split()\n",
    "                    if words_before_comma and words_before_comma[0].upper() in self.all_asian_surnames:\n",
    "                        surname_to_check = words_before_comma[0].upper()\n",
    "            afterclass_parts = [word.capitalize() for word in words]\n",
    "            for i, word in enumerate(words):\n",
    "                if word.upper() == surname_to_check:\n",
    "                    afterclass_parts[i] = word.upper()\n",
    "            return boss_name, ' '.join(afterclass_parts)\n",
    "\n",
    "        words = name_str.split()\n",
    "        if not words: return boss_name, \"Unknown\"\n",
    "        if len(words) == 1: return boss_name, words[0].capitalize()\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if word.upper() in self.patronymic_keywords and i < len(words) - 1:\n",
    "                surname_index = i + 1\n",
    "                afterclass_parts = [w.capitalize() for w in words]\n",
    "                afterclass_parts[i] = word.lower()\n",
    "                afterclass_parts[surname_index] = words[surname_index].upper()\n",
    "                return boss_name, ' '.join(afterclass_parts)\n",
    "\n",
    "        # --- Step 3: Definitive Rule-Based Surname Identification ---\n",
    "        surname_index = -1\n",
    "        \n",
    "        # Rule 1 (Fixes \"Middle Surname\"): If the name starts with a Western/Indian given name,\n",
    "        # actively search for the first known Asian/Indian surname that follows.\n",
    "        if words[0].upper() in self.western_given_names:\n",
    "            for i in range(1, len(words)):\n",
    "                if words[i].upper() in self.all_asian_surnames:\n",
    "                    surname_index = i\n",
    "                    break\n",
    "        \n",
    "        # Rule 2 (Fixes \"Surname-First Western\"): If a name contains a Western given name but\n",
    "        # does NOT start with it, and the first word is not an Asian surname, assume the first word is the surname.\n",
    "        elif any(w.upper() in self.western_given_names for w in words) and words[0].upper() not in self.all_asian_surnames:\n",
    "            surname_index = 0\n",
    "\n",
    "        # Rule 3: If neither of the complex cases above apply, check if the name starts with a known Asian surname.\n",
    "        # This handles the most common SURNAME-first pattern.\n",
    "        elif words[0].upper() in self.all_asian_surnames:\n",
    "            surname_index = 0\n",
    "\n",
    "        # Rule 4 (Fallback): If no specific pattern has been matched, default to the last word.\n",
    "        if surname_index == -1:\n",
    "            surname_index = len(words) - 1\n",
    "            \n",
    "        # Post-processing for European particles\n",
    "        afterclass_parts = [word.capitalize() for word in words]\n",
    "        if surname_index > 0 and words[surname_index-1].upper() in self.surname_particles:\n",
    "             afterclass_parts[surname_index-1] = words[surname_index-1].upper()\n",
    "\n",
    "        afterclass_parts[surname_index] = words[surname_index].upper()\n",
    "        \n",
    "        return boss_name, ' '.join(afterclass_parts)\n",
    "    def resolve_professor_email(self, professor_name):\n",
    "        \"\"\"Resolve professor email using Outlook contacts\"\"\"\n",
    "        try:\n",
    "            # Initialize Outlook\n",
    "            outlook = win32.Dispatch(\"Outlook.Application\")\n",
    "            namespace = outlook.GetNamespace(\"MAPI\")\n",
    "            \n",
    "            # Try exact resolver first\n",
    "            recipient = namespace.CreateRecipient(professor_name)\n",
    "            if recipient.Resolve():\n",
    "                # Try to get SMTP address\n",
    "                address_entry = recipient.AddressEntry\n",
    "                \n",
    "                # Try Exchange user\n",
    "                try:\n",
    "                    exchange_user = address_entry.GetExchangeUser()\n",
    "                    if exchange_user and exchange_user.PrimarySmtpAddress:\n",
    "                        return exchange_user.PrimarySmtpAddress.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try Exchange distribution list\n",
    "                try:\n",
    "                    exchange_dl = address_entry.GetExchangeDistributionList()\n",
    "                    if exchange_dl and exchange_dl.PrimarySmtpAddress:\n",
    "                        return exchange_dl.PrimarySmtpAddress.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try PR_SMTP_ADDRESS property\n",
    "                try:\n",
    "                    property_accessor = address_entry.PropertyAccessor\n",
    "                    smtp_addr = property_accessor.GetProperty(\"http://schemas.microsoft.com/mapi/proptag/0x39FE001E\")\n",
    "                    if smtp_addr:\n",
    "                        return smtp_addr.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Fallback: regex search in Address field\n",
    "                try:\n",
    "                    address = getattr(address_entry, \"Address\", \"\") or \"\"\n",
    "                    match = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", address)\n",
    "                    if match:\n",
    "                        return match.group(0).lower()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # If exact resolve fails, try contacts search\n",
    "            contacts_folder = namespace.GetDefaultFolder(10)  # olFolderContacts\n",
    "            tokens = [t.lower() for t in professor_name.split() if t]\n",
    "            \n",
    "            for item in contacts_folder.Items:\n",
    "                try:\n",
    "                    full_name = (item.FullName or \"\").lower()\n",
    "                    if all(token in full_name for token in tokens):\n",
    "                        # Try the three standard email slots\n",
    "                        for field in (\"Email1Address\", \"Email2Address\", \"Email3Address\"):\n",
    "                            addr = getattr(item, field, \"\") or \"\"\n",
    "                            if addr and \"@\" in addr:\n",
    "                                return addr.lower()\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # If no email found, return default\n",
    "            return 'enquiry@smu.edu.sg'\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Email resolution failed for {professor_name}: {e}\")\n",
    "            return 'enquiry@smu.edu.sg'\n",
    "        \n",
    "    def process_professors(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the processing of professors: extraction, normalization, and creation.\n",
    "        \"\"\"\n",
    "        logger.info(\"üë• Processing professors...\")\n",
    "        \n",
    "        # Step 1: Extract unique names and their variations from the data source.\n",
    "        unique_professors, professor_variations = self._extract_unique_professors()\n",
    "\n",
    "        # Step 2: Filter out existing professors to find only new names.\n",
    "        new_professors_to_normalize = []\n",
    "        for prof_name in unique_professors:\n",
    "            # A professor is considered \"new\" if they are not in the primary lookup.\n",
    "            if prof_name.upper() not in self.professor_lookup:\n",
    "                new_professors_to_normalize.append(prof_name)\n",
    "\n",
    "        logger.info(f\"Found {len(unique_professors)} unique names. \"\n",
    "                    f\"Identified {len(new_professors_to_normalize)} as new and requiring normalization.\")\n",
    "        \n",
    "        # Step 2b: Normalize only the new names using the LLM-first, fallback-second approach.\n",
    "        normalized_map = self._normalize_professors_batch(new_professors_to_normalize)\n",
    "        \n",
    "        # Add a fallback for existing professors to ensure they are still processed later\n",
    "        for prof_name in unique_professors:\n",
    "            if prof_name not in normalized_map:\n",
    "                # If an existing professor wasn't normalized, add them to the map using the fallback\n",
    "                # to ensure they are processed correctly in the steps that follow.\n",
    "                normalized_map[prof_name] = self._normalize_professor_name_fallback(prof_name)\n",
    "\n",
    "        if not normalized_map:\n",
    "            logger.info(\"No professor names were normalized. Aborting professor processing.\")\n",
    "            return\n",
    "\n",
    "        # Step 3: Check cache, fuzzy match, and create new professor records.\n",
    "        email_to_professor = {}\n",
    "        for boss_name_key, prof_data in self.professors_cache.items():\n",
    "            if 'email' in prof_data and prof_data['email'] and prof_data['email'].lower() != 'enquiry@smu.edu.sg':\n",
    "                email_to_professor[prof_data['email'].lower()] = prof_data\n",
    "\n",
    "        fuzzy_matched_professors = []\n",
    "        \n",
    "        for prof_name in unique_professors:\n",
    "            try:\n",
    "                boss_name, afterclass_name = normalized_map[prof_name]\n",
    "                \n",
    "                # --- The logic below is IDENTICAL to your original script's Step 3 ---\n",
    "                if hasattr(self, 'professor_lookup') and prof_name.upper() in self.professor_lookup:\n",
    "                    continue\n",
    "                if hasattr(self, 'professor_lookup') and boss_name.upper() in self.professor_lookup:\n",
    "                    self.professor_lookup[prof_name.upper()] = self.professor_lookup[boss_name.upper()]\n",
    "                    continue\n",
    "                \n",
    "                if hasattr(self, 'professor_lookup'):\n",
    "                    found_partial_match = False\n",
    "                    for lookup_boss_name, lookup_data in self.professor_lookup.items():\n",
    "                        prof_words = set(prof_name.upper().split())\n",
    "                        lookup_words = set(lookup_boss_name.split())\n",
    "                        \n",
    "                        if prof_words.issubset(lookup_words) and len(prof_words) >= 2:\n",
    "                            self.professor_lookup[prof_name.upper()] = lookup_data\n",
    "                            found_partial_match = True\n",
    "                            break\n",
    "                    if found_partial_match:\n",
    "                        continue\n",
    "                \n",
    "                if boss_name in self.professors_cache:\n",
    "                    if not hasattr(self, 'professor_lookup'):\n",
    "                        self.professor_lookup = {}\n",
    "                    self.professor_lookup[prof_name.upper()] = {\n",
    "                        'database_id': self.professors_cache[boss_name]['id'],\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': self.professors_cache[boss_name].get('name', afterclass_name)\n",
    "                    }\n",
    "                    continue\n",
    "                \n",
    "                fuzzy_match_found = False\n",
    "                normalized_prof = ' '.join(str(prof_name).replace(',', ' ').split()).upper()\n",
    "                \n",
    "                for cached_name, cached_prof in self.professors_cache.items():\n",
    "                    if cached_name is None:\n",
    "                        continue\n",
    "                    cached_normalized = ' '.join(str(cached_name).replace(',', ' ').split()).upper()\n",
    "                    if normalized_prof == cached_normalized:\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = {\n",
    "                            'database_id': cached_prof['id'],\n",
    "                            'boss_name': cached_prof.get('boss_name', cached_prof['name'].upper()),\n",
    "                            'afterclass_name': cached_prof.get('name', afterclass_name)\n",
    "                        }\n",
    "                        fuzzy_match_found = True\n",
    "                        break\n",
    "                if fuzzy_match_found:\n",
    "                    continue\n",
    "                \n",
    "                # This is the block that was previously a placeholder\n",
    "                for new_prof in self.new_professors:\n",
    "                    if 'boss_aliases' in new_prof:\n",
    "                        try:\n",
    "                            boss_aliases = json.loads(new_prof['boss_aliases'])\n",
    "                            if isinstance(boss_aliases, list) and boss_aliases:\n",
    "                                new_normalized = ' '.join(boss_aliases[0].replace(',', ' ').split()).upper()\n",
    "                            else:\n",
    "                                new_normalized = ' '.join(new_prof.get('afterclass_name', '').replace(',', ' ').split()).upper()\n",
    "                        except (json.JSONDecodeError, TypeError):\n",
    "                            new_normalized = ' '.join(new_prof.get('afterclass_name', '').replace(',', ' ').split()).upper()\n",
    "                    else:\n",
    "                        new_normalized = ' '.join(new_prof.get('afterclass_name', '').replace(',', ' ').split()).upper()\n",
    "\n",
    "                    if normalized_prof == new_normalized:\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = {\n",
    "                            'database_id': new_prof['id'],\n",
    "                            'boss_name': boss_name,\n",
    "                            'afterclass_name': new_prof['afterclass_name']\n",
    "                        }\n",
    "                        fuzzy_match_found = True\n",
    "                        break\n",
    "                \n",
    "                if fuzzy_match_found:\n",
    "                    continue\n",
    "                \n",
    "                if hasattr(self, 'professor_lookup'):\n",
    "                    best_fuzzy_match = None\n",
    "                    best_fuzzy_score = 0\n",
    "                    FUZZY_MATCH_THRESHOLD = 90\n",
    "                    for lookup_boss_name, lookup_data in self.professor_lookup.items():                        \n",
    "                        score = self._calculate_fuzzy_score(prof_name, lookup_boss_name)\n",
    "                        if score > best_fuzzy_score:\n",
    "                            best_fuzzy_match = lookup_data\n",
    "                            best_fuzzy_score = score\n",
    "                    \n",
    "                    if best_fuzzy_match and best_fuzzy_score >= FUZZY_MATCH_THRESHOLD:\n",
    "                        fuzzy_matched_professors.append({\n",
    "                            'boss_aliases': f'[\"{prof_name.upper()}\"]',\n",
    "                            'afterclass_name': best_fuzzy_match.get('afterclass_name', prof_name),\n",
    "                            'database_id': best_fuzzy_match['database_id'],\n",
    "                            'method': 'fuzzy_match',\n",
    "                            'confidence_score': f\"{best_fuzzy_score:.2f}\"\n",
    "                        })\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = best_fuzzy_match\n",
    "                        continue\n",
    "                \n",
    "                resolved_email = self.resolve_professor_email(afterclass_name)\n",
    "                \n",
    "                if (resolved_email and \n",
    "                    resolved_email.lower() != 'enquiry@smu.edu.sg' and \n",
    "                    resolved_email.lower() in email_to_professor):\n",
    "                    existing_prof = email_to_professor[resolved_email.lower()]\n",
    "                    if not hasattr(self, 'professor_lookup'):\n",
    "                        self.professor_lookup = {}\n",
    "                    self.professor_lookup[prof_name.upper()] = {\n",
    "                        'database_id': existing_prof['id'],\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': existing_prof.get('name', afterclass_name)\n",
    "                    }\n",
    "                    continue\n",
    "                \n",
    "                self._create_new_professor(prof_name, professor_variations, email_to_professor)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error processing professor '{prof_name}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        if fuzzy_matched_professors:\n",
    "            fuzzy_df = pd.DataFrame(fuzzy_matched_professors)\n",
    "            fuzzy_path = os.path.join(self.verify_dir, 'fuzzy_matched_professors.csv')\n",
    "            fuzzy_df.to_csv(fuzzy_path, index=False)\n",
    "            logger.info(f\"üîç Saved {len(fuzzy_matched_professors)} fuzzy matched professors for validation\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created {self.stats['professors_created']} new professors\")\n",
    "\n",
    "    def _calculate_fuzzy_score(self, new_name: str, known_alias: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a fuzzy match score using a hybrid strategy that prioritizes\n",
    "        ordered matches and handles permutations.\n",
    "        \"\"\"\n",
    "        if not new_name or not known_alias:\n",
    "            return 0.0\n",
    "        \n",
    "        # Clean and normalize names for consistent comparison\n",
    "        new_name_clean = ' '.join(str(new_name).upper().replace(',', ' ').split())\n",
    "        known_alias_clean = ' '.join(str(known_alias).upper().replace(',', ' ').split())\n",
    "        \n",
    "        # --- Layer 1: High-Precision Substring Check ---\n",
    "        # This is the most important check. It handles short forms like \n",
    "        # 'WARREN B. CHIK' being perfectly contained within 'KAM WAI WARREN BARTHOLOMEW CHIK'.\n",
    "        # This check is fast and respects word order.\n",
    "        if new_name_clean in known_alias_clean or known_alias_clean in new_name_clean:\n",
    "            # We return a very high score, but not 100, to indicate a strong partial match.\n",
    "            return 95\n",
    "\n",
    "        # --- Layer 2: Hybrid Fuzzy Logic ---\n",
    "        # If it's not a direct substring, we use two different fuzzy algorithms.\n",
    "        \n",
    "        # a) Partial Ratio: Good for ordered, partial matches.\n",
    "        # This respects word order. A jumbled name will get a LOW score here.\n",
    "        # e.g., 'KAM WAI CHIK' vs 'KAM WAI WARREN CHIK' will score high.\n",
    "        partial_score = fuzz.partial_ratio(new_name_clean, known_alias_clean)\n",
    "        \n",
    "        # b) Token Set Ratio: Good for permutations and names with extra/missing words.\n",
    "        # This handles cases like 'RACHEL TAN YEN JUN' vs 'RACHEL TAN'.\n",
    "        token_set_score = fuzz.token_set_ratio(new_name_clean, known_alias_clean)\n",
    "        \n",
    "        # We take the best score from the two fuzzy methods. This gives us the\n",
    "        # flexibility to catch both ordered variations and unordered permutations.\n",
    "        return max(partial_score, token_set_score)\n",
    "\n",
    "    def process_courses(self):\n",
    "        \"\"\"\n",
    "        Processes courses from the standalone sheet. It correctly identifies if a course\n",
    "        is new or if an existing course needs to be updated.\n",
    "        \"\"\"\n",
    "        logger.info(\"üìö Processing courses with robust CREATE vs. UPDATE logic...\")\n",
    "        \n",
    "        # Use a set to only process each unique course code once from the input file.\n",
    "        processed_course_codes_in_run = set()\n",
    "\n",
    "        for idx, row in self.standalone_data.iterrows():\n",
    "            course_code = row.get('course_code')\n",
    "            if pd.isna(course_code) or course_code in processed_course_codes_in_run:\n",
    "                continue\n",
    "            \n",
    "            processed_course_codes_in_run.add(course_code)\n",
    "\n",
    "            # Check if the course already exists in our database cache\n",
    "            if course_code in self.courses_cache:\n",
    "                # --- UPDATE LOGIC ---\n",
    "                existing_course = self.courses_cache[course_code]\n",
    "                update_record = {'id': existing_course['id'], 'code': course_code}\n",
    "                \n",
    "                # Define fields to check for potential updates\n",
    "                field_mapping = {\n",
    "                    'name': 'course_name', 'description': 'course_description',\n",
    "                    'credit_units': 'credit_units', 'course_area': 'course_area',\n",
    "                    'enrolment_requirements': 'enrolment_requirements'\n",
    "                }\n",
    "\n",
    "                if self.needs_update(existing_course, row, field_mapping):\n",
    "                    # self.needs_update will populate the update_record\n",
    "                    for db_field, raw_field in field_mapping.items():\n",
    "                        new_value = row.get(raw_field)\n",
    "                        if pd.notna(new_value) and str(new_value) != str(existing_course.get(db_field)):\n",
    "                            update_record[db_field] = new_value\n",
    "                    \n",
    "                    self.update_courses.append(update_record)\n",
    "                    self.stats['courses_updated'] += 1\n",
    "            else:\n",
    "                # --- CREATE LOGIC ---\n",
    "                course_id = str(uuid.uuid4())\n",
    "                new_course = {\n",
    "                    'id': course_id, 'code': course_code,\n",
    "                    'name': row.get('course_name', 'Unknown Course'),\n",
    "                    'description': row.get('course_description', 'No description available'),\n",
    "                    'credit_units': float(row.get('credit_units', 1.0)) if pd.notna(row.get('credit_units')) else 1.0,\n",
    "                    'belong_to_university': 1, 'belong_to_faculty': None,\n",
    "                    'course_area': row.get('course_area'),\n",
    "                    'enrolment_requirements': row.get('enrolment_requirements')\n",
    "                }\n",
    "                self.new_courses.append(new_course)\n",
    "                self.courses_cache[course_code] = new_course  # Add to cache for this run\n",
    "                self.stats['courses_created'] += 1\n",
    "        \n",
    "        logger.info(f\"‚úÖ Course processing complete. New: {self.stats['courses_created']}, Updated: {self.stats['courses_updated']}.\")\n",
    "\n",
    "    def assign_course_faculties_interactive(self):\n",
    "        \"\"\"Interactive faculty assignment with option to create new faculties\"\"\"\n",
    "        if not self.courses_needing_faculty:\n",
    "            logger.info(\"‚úÖ No courses need faculty assignment\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"üéì Starting interactive faculty assignment for {len(self.courses_needing_faculty)} courses\")\n",
    "        \n",
    "        # Get current max faculty ID for incrementing\n",
    "        max_faculty_id = max(self.faculties_cache.keys()) if self.faculties_cache else 0\n",
    "        \n",
    "        faculty_assignments = []\n",
    "        \n",
    "        for course_info in self.courses_needing_faculty:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üéì FACULTY ASSIGNMENT NEEDED\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Course Code: {course_info['course_code']}\")\n",
    "            print(f\"Course Name: {course_info['course_name']}\")\n",
    "            \n",
    "            # Get the last filepath for this course from multiple sheet\n",
    "            driver = None\n",
    "            course_code = course_info['course_code']\n",
    "            last_filepath = self.get_last_filepath_by_course(course_code)\n",
    "            \n",
    "            if last_filepath:\n",
    "                print(f\"\\nOpening scraped HTML file: {last_filepath}\")\n",
    "                \n",
    "                try:\n",
    "                    # Setup Chrome options\n",
    "                    chrome_options = Options()\n",
    "                    chrome_options.add_argument(\"--new-window\")\n",
    "                    chrome_options.add_argument(\"--start-maximized\")\n",
    "                    \n",
    "                    # Initialize driver\n",
    "                    driver = webdriver.Chrome(options=chrome_options)\n",
    "                    \n",
    "                    # Open the HTML file\n",
    "                    abs_path = os.path.abspath(last_filepath)\n",
    "                    from pathlib import Path\n",
    "                    file_path = Path(abs_path)\n",
    "                    \n",
    "                    if file_path.exists():\n",
    "                        # Use pathlib's as_uri() method for proper file:// URL\n",
    "                        file_url = file_path.as_uri()\n",
    "                        driver.get(file_url)\n",
    "                        print(\"‚úÖ Scraped HTML file opened in browser\")\n",
    "                        print(\"üìã Review the course content to determine the correct faculty\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è HTML file not found: {abs_path}\")\n",
    "                        print(\"üìã Proceeding without file preview\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Could not open HTML file: {e}\")\n",
    "                    print(\"üìã Proceeding without file preview\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No scraped HTML file found for course {course_code}\")\n",
    "                print(\"üìã Proceeding without file preview\")\n",
    "            \n",
    "            # Show existing faculties\n",
    "            print(\"\\nExisting Faculty Options:\")\n",
    "            faculty_list = sorted(self.faculties_cache.values(), key=lambda x: x['id'])\n",
    "            for faculty in faculty_list:\n",
    "                print(f\"{faculty['id']}. {faculty['name']} ({faculty['acronym']})\")\n",
    "            \n",
    "            print(f\"\\n0. Skip (will need manual review)\")\n",
    "            print(f\"99. Create new faculty\")\n",
    "            \n",
    "            while True:\n",
    "                choice = input(f\"\\nEnter faculty number (0-{max(f['id'] for f in faculty_list)}, 99): \").strip()\n",
    "                \n",
    "                if choice == '0':\n",
    "                    faculty_id = None\n",
    "                    break\n",
    "                elif choice == '99':\n",
    "                    # Create new faculty\n",
    "                    print(\"\\nüìù Creating new faculty:\")\n",
    "                    faculty_name = input(\"Enter faculty name: \").strip()\n",
    "                    faculty_acronym = input(\"Enter faculty acronym (e.g., SCIS): \").strip().upper()\n",
    "                    faculty_url = input(\"Enter faculty website URL (or press Enter for default): \").strip()\n",
    "                    \n",
    "                    if not faculty_url:\n",
    "                        faculty_url = f\"https://smu.edu.sg/{faculty_acronym.lower()}\"\n",
    "                    \n",
    "                    # Increment faculty ID\n",
    "                    max_faculty_id += 1\n",
    "                    new_faculty = {\n",
    "                        'id': max_faculty_id,\n",
    "                        'name': faculty_name,\n",
    "                        'acronym': faculty_acronym,\n",
    "                        'site_url': faculty_url,\n",
    "                        'belong_to_university': 1,  # SMU\n",
    "                        'created_at': datetime.now().isoformat(),\n",
    "                        'updated_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Add to cache\n",
    "                    self.faculties_cache[max_faculty_id] = new_faculty\n",
    "                    self.faculty_acronym_to_id[faculty_acronym] = max_faculty_id\n",
    "                    \n",
    "                    # Save to new_faculties list\n",
    "                    if not hasattr(self, 'new_faculties'):\n",
    "                        self.new_faculties = []\n",
    "                    self.new_faculties.append(new_faculty)\n",
    "                    \n",
    "                    faculty_id = max_faculty_id\n",
    "                    print(f\"‚úÖ Created new faculty: {faculty_name} (ID: {faculty_id})\")\n",
    "                    break\n",
    "                else:\n",
    "                    try:\n",
    "                        faculty_id = int(choice)\n",
    "                        if faculty_id in [f['id'] for f in faculty_list]:\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Invalid choice. Please enter a valid faculty ID.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid input. Please enter a number.\")\n",
    "            \n",
    "            # Close browser after selection\n",
    "            if driver:\n",
    "                try:\n",
    "                    print(\"\\nüîÑ Closing browser...\")\n",
    "                    driver.quit()\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error closing browser: {e}\")\n",
    "            \n",
    "            # Store assignment\n",
    "            faculty_assignments.append({\n",
    "                'course_id': course_info['course_id'],\n",
    "                'course_code': course_info['course_code'],\n",
    "                'faculty_id': faculty_id\n",
    "            })\n",
    "        \n",
    "        # Apply assignments\n",
    "        for assignment in faculty_assignments:\n",
    "            if assignment['faculty_id'] is not None:\n",
    "                # Update new_courses\n",
    "                for course in self.new_courses:\n",
    "                    if course['id'] == assignment['course_id']:\n",
    "                        course['belong_to_faculty'] = assignment['faculty_id']\n",
    "                        break\n",
    "                \n",
    "                # Update cache\n",
    "                if assignment['course_code'] in self.courses_cache:\n",
    "                    self.courses_cache[assignment['course_code']]['belong_to_faculty'] = assignment['faculty_id']\n",
    "        \n",
    "        # Save outputs\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Updated new_courses.csv with faculty assignments\")\n",
    "        \n",
    "        if hasattr(self, 'new_faculties') and self.new_faculties:\n",
    "            df = pd.DataFrame(self.new_faculties)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_faculties.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_faculties)} new faculties\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Faculty assignment completed\")\n",
    "\n",
    "    # Also add this as an alias to the existing method name\n",
    "    def assign_course_faculties(self):\n",
    "        \"\"\"Alias for assign_course_faculties_interactive\"\"\"\n",
    "        return self.assign_course_faculties_interactive()\n",
    "\n",
    "    def process_acad_terms(self):\n",
    "        \"\"\"Process academic terms from standalone sheet\"\"\"\n",
    "        logger.info(\"üìÖ Processing academic terms...\")\n",
    "        \n",
    "        # Group by (acad_year_start, acad_year_end, term)\n",
    "        term_groups = defaultdict(list)\n",
    "        \n",
    "        for _, row in self.standalone_data.iterrows():\n",
    "            # Try to extract from row data first\n",
    "            year_start = row.get('acad_year_start')\n",
    "            year_end = row.get('acad_year_end')\n",
    "            term = row.get('term')\n",
    "            \n",
    "            # If any are missing, try to extract from source file path if available\n",
    "            if pd.isna(year_start) or pd.isna(year_end) or pd.isna(term):\n",
    "                if 'source_file' in row and pd.notna(row['source_file']):\n",
    "                    fallback_term_id = self.extract_acad_term_from_path(row['source_file'])\n",
    "                    if fallback_term_id:\n",
    "                        # Parse the fallback\n",
    "                        match = re.match(r'AY(\\d{4})(\\d{2})T(\\w+)', fallback_term_id)\n",
    "                        if match:\n",
    "                            year_start = int(match.group(1)) if pd.isna(year_start) else year_start\n",
    "                            year_end = int(match.group(2)) if pd.isna(year_end) else year_end\n",
    "                            term = f\"T{match.group(3)}\" if pd.isna(term) else term\n",
    "            \n",
    "            key = (year_start, year_end, term)\n",
    "            if all(pd.notna(v) for v in key):\n",
    "                term_groups[key].append(row)\n",
    "        \n",
    "        # Rest of the function remains the same...\n",
    "        for (year_start, year_end, term), rows in term_groups.items():\n",
    "            # Generate acad_term_id (keep T for ID)\n",
    "            acad_term_id = f\"AY{int(year_start)}{int(year_end) % 100:02d}{term}\"\n",
    "            \n",
    "            # Check if already exists\n",
    "            if acad_term_id in self.acad_term_cache:\n",
    "                continue\n",
    "            \n",
    "            # Find most common period_text and dates\n",
    "            period_counter = Counter()\n",
    "            date_info = {}\n",
    "            \n",
    "            for row in rows:\n",
    "                period_text = row.get('period_text', '')\n",
    "                if pd.notna(period_text):\n",
    "                    period_counter[period_text] += 1\n",
    "                    if period_text not in date_info:\n",
    "                        date_info[period_text] = {\n",
    "                            'start_dt': row.get('start_dt'),\n",
    "                            'end_dt': row.get('end_dt')\n",
    "                        }\n",
    "            \n",
    "            # Get most common period\n",
    "            if period_counter:\n",
    "                most_common_period = period_counter.most_common(1)[0][0]\n",
    "                dates = date_info[most_common_period]\n",
    "            else:\n",
    "                dates = {'start_dt': None, 'end_dt': None}\n",
    "            \n",
    "            # Get boss_id from first row\n",
    "            boss_id = rows[0].get('acad_term_boss_id')\n",
    "            \n",
    "            # Remove T prefix from term field for database storage\n",
    "            clean_term = str(term)[1:] if str(term).startswith('T') else str(term)\n",
    "            \n",
    "            new_term = {\n",
    "                'id': acad_term_id,\n",
    "                'acad_year_start': int(year_start),\n",
    "                'acad_year_end': int(year_end),\n",
    "                'term': clean_term,  # Store without T prefix\n",
    "                'boss_id': int(boss_id) if pd.notna(boss_id) else None,\n",
    "                'start_dt': dates['start_dt'],\n",
    "                'end_dt': dates['end_dt']\n",
    "            }\n",
    "            \n",
    "            self.new_acad_terms.append(new_term)\n",
    "            self.acad_term_cache[acad_term_id] = new_term\n",
    "            \n",
    "            logger.info(f\"‚úÖ Created academic term: {acad_term_id} (term: {clean_term})\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created {len(self.new_acad_terms)} new academic terms\")\n",
    "\n",
    "    def process_classes(self, use_db_cache_for_classes=True):\n",
    "        \"\"\"\n",
    "        Process classes from the standalone sheet. For single professor classes,\n",
    "        uniqueness is determined by course_id + section + acad_term_id. For multi-professor classes, uniqueness includes professor_id.\n",
    "        \"\"\"\n",
    "        logger.info(\"üè´ Processing classes with robust CREATE vs. UPDATE logic...\")\n",
    "        \n",
    "        # Use sets to track processed classes and prevent duplicate operations\n",
    "        processed_class_keys = set()\n",
    "        processed_update_class_ids = set() # FIX: Prevents duplicate TBA updates\n",
    "\n",
    "        # Initialize update_classes if it doesn't exist\n",
    "        if not hasattr(self, 'update_classes'):\n",
    "            self.update_classes = []\n",
    "\n",
    "        # Build a lookup of existing classes from the cache for quick checks\n",
    "        self.existing_class_lookup = {}\n",
    "        if use_db_cache_for_classes:\n",
    "            self.load_existing_classes_cache()\n",
    "            if self.existing_classes_cache:\n",
    "                for c in self.existing_classes_cache:\n",
    "                    # For existing classes, use acad_term_id + boss_id + professor_id as the key\n",
    "                    acad_term_id = c.get('acad_term_id')\n",
    "                    class_boss_id = c.get('boss_id')\n",
    "                    professor_id = c.get('professor_id')\n",
    "\n",
    "                    # The primary key parts must exist to create a valid entry\n",
    "                    if acad_term_id and class_boss_id is not None:\n",
    "                        key = (acad_term_id, class_boss_id, professor_id)\n",
    "                        self.existing_class_lookup[key] = c\n",
    "\n",
    "        for idx, row in self.standalone_data.iterrows():\n",
    "            try:\n",
    "                acad_term_id = row.get('acad_term_id')\n",
    "                class_boss_id = row.get('class_boss_id')\n",
    "                course_code = row.get('course_code')\n",
    "                section = str(row.get('section'))\n",
    "\n",
    "                if pd.isna(acad_term_id) or pd.isna(class_boss_id):\n",
    "                    continue\n",
    "                \n",
    "                course_id = self.get_course_id(course_code)\n",
    "                if not course_id:\n",
    "                    continue\n",
    "                \n",
    "                record_key = row.get('record_key')\n",
    "                \n",
    "                # Get all professors for this class\n",
    "                professor_mappings = self._find_professors_for_class(record_key) if record_key else []\n",
    "                \n",
    "                # Handle the specific case where a TBA class gets a professor assigned.\n",
    "                class_rows_in_multiple = [r for r in self.multiple_lookup.get(record_key, []) if r.get('type') == 'CLASS']\n",
    "                \n",
    "                # Condition: Exactly one professor is assigned now, and there's only one CLASS row for it.\n",
    "                if len(professor_mappings) == 1 and len(class_rows_in_multiple) == 1:\n",
    "                    new_prof_id = professor_mappings[0][0]\n",
    "                    \n",
    "                    # Search for a matching TBA record in the cache (where professor_id is null).\n",
    "                    tba_class_to_update = None\n",
    "                    if hasattr(self, 'existing_classes_cache') and self.existing_classes_cache:\n",
    "                        for existing_class in self.existing_classes_cache:\n",
    "                            if (existing_class.get('course_id') == course_id and\n",
    "                                str(existing_class.get('section')) == section and\n",
    "                                existing_class.get('acad_term_id') == acad_term_id and\n",
    "                                pd.isna(existing_class.get('professor_id'))):\n",
    "                                tba_class_to_update = existing_class\n",
    "                                break\n",
    "                    \n",
    "                    # If we found a TBA record and haven't updated it yet, perform an UPDATE.\n",
    "                    if tba_class_to_update and tba_class_to_update['id'] not in processed_update_class_ids:\n",
    "                        logger.info(f\"üîÑ Converting TBA class {course_code}-{section} to assigned.\")\n",
    "                        \n",
    "                        # Add to set to prevent this specific update from repeating.\n",
    "                        processed_update_class_ids.add(tba_class_to_update['id'])\n",
    "\n",
    "                        # Create the update record with the new professor.\n",
    "                        update_record = {\n",
    "                            'id': tba_class_to_update['id'],\n",
    "                            'professor_id': new_prof_id\n",
    "                        }\n",
    "                        self.update_classes.append(update_record)\n",
    "                        \n",
    "                        # Map the record_key to the now-updated class ID for timing processing.\n",
    "                        if record_key:\n",
    "                            if record_key not in self.class_id_mapping:\n",
    "                                self.class_id_mapping[record_key] = []\n",
    "                            if tba_class_to_update['id'] not in self.class_id_mapping[record_key]:\n",
    "                                self.class_id_mapping[record_key].append(tba_class_to_update['id'])\n",
    "                                \n",
    "                        # Update the in-memory cache to prevent a duplicate record from being created.\n",
    "                        # This tells the rest of the function that this class now exists with an assigned professor.\n",
    "                        new_assigned_key = (acad_term_id, class_boss_id, new_prof_id)\n",
    "                        old_tba_key = (acad_term_id, class_boss_id, None)\n",
    "\n",
    "                        updated_class_record = tba_class_to_update.copy()\n",
    "                        updated_class_record['professor_id'] = new_prof_id\n",
    "                        \n",
    "                        self.existing_class_lookup[new_assigned_key] = updated_class_record\n",
    "                        \n",
    "                        # Clean up the old TBA entry from the in-memory cache\n",
    "                        if old_tba_key in self.existing_class_lookup:\n",
    "                            del self.existing_class_lookup[old_tba_key]\n",
    "\n",
    "                        logger.info(f\"üß† In-memory cache updated for {course_code}-{section} to prevent duplicate creation.\")\n",
    "\n",
    "                        # Mark the new professor-class combination as processed to prevent the default logic\n",
    "                        # from creating a duplicate class later in the loop.\n",
    "                        class_key_for_processing = (acad_term_id, class_boss_id, new_prof_id)\n",
    "                        processed_class_keys.add(class_key_for_processing)\n",
    "                \n",
    "                # If no professors found, create one class with professor_id = None\n",
    "                if not professor_mappings:\n",
    "                    professor_mappings = [(None, '')]\n",
    "                \n",
    "                # Check if this is a multi-professor class\n",
    "                is_multi_professor = len(professor_mappings) > 1\n",
    "                warn_inaccuracy = is_multi_professor\n",
    "                \n",
    "                # Process each professor\n",
    "                for prof_id, prof_name in professor_mappings:\n",
    "                    # Create unique key using boss_id\n",
    "                    class_key = (acad_term_id, class_boss_id, prof_id)\n",
    "                    \n",
    "                    # Skip if we've already processed this exact class\n",
    "                    if class_key in processed_class_keys:\n",
    "                        continue\n",
    "                        \n",
    "                    processed_class_keys.add(class_key)\n",
    "                    \n",
    "                    # Check if this exact class exists (including professor_id match)\n",
    "                    existing_class = self.existing_class_lookup.get(class_key)\n",
    "                    \n",
    "                    if existing_class:\n",
    "                        # --- UPDATE LOGIC ---\n",
    "                        update_record = {'id': existing_class['id']}\n",
    "                        needs_update = False\n",
    "                        \n",
    "                        fields_to_check = {\n",
    "                            'grading_basis': row.get('grading_basis'),\n",
    "                            'course_outline_url': row.get('course_outline_url'),\n",
    "                            'boss_id': int(row.get('class_boss_id')) if pd.notna(row.get('class_boss_id')) else None,\n",
    "                            'warn_inaccuracy': warn_inaccuracy\n",
    "                        }\n",
    "                        \n",
    "                        for field, new_value in fields_to_check.items():\n",
    "                            old_value = existing_class.get(field)\n",
    "\n",
    "                            # FIX: Safely handle array-like objects and pandas/numpy types\n",
    "                            if hasattr(old_value, '__iter__') and not isinstance(old_value, str):\n",
    "                                try:\n",
    "                                    if hasattr(old_value, 'item'):  # numpy array\n",
    "                                        old_value = old_value.item()\n",
    "                                    elif hasattr(old_value, '__len__') and len(old_value) > 0:\n",
    "                                        old_value = old_value[0]\n",
    "                                    else:\n",
    "                                        old_value = None\n",
    "                                except:\n",
    "                                    old_value = None\n",
    "                            \n",
    "                            # Convert new_value if it's also an array\n",
    "                            if hasattr(new_value, '__iter__') and not isinstance(new_value, str):\n",
    "                                try:\n",
    "                                    if hasattr(new_value, 'item'):  # numpy array\n",
    "                                        new_value = new_value.item()\n",
    "                                    elif hasattr(new_value, '__len__') and len(new_value) > 0:\n",
    "                                        new_value = new_value[0]\n",
    "                                    else:\n",
    "                                        new_value = None\n",
    "                                except:\n",
    "                                    new_value = None\n",
    "                            \n",
    "                            # Safe comparison\n",
    "                            try:\n",
    "                                # Handle None/NaN values first\n",
    "                                if pd.isna(new_value) and pd.isna(old_value):\n",
    "                                    continue\n",
    "                                elif pd.isna(new_value) or pd.isna(old_value):\n",
    "                                    if pd.notna(new_value):\n",
    "                                        update_record[field] = new_value\n",
    "                                        needs_update = True\n",
    "                                    continue\n",
    "                                \n",
    "                                # Convert to strings for comparison\n",
    "                                new_val_str = str(new_value).strip()\n",
    "                                old_val_str = str(old_value).strip()\n",
    "                                \n",
    "                                if new_val_str != old_val_str:\n",
    "                                    update_record[field] = new_value\n",
    "                                    needs_update = True\n",
    "                                    \n",
    "                            except Exception as e:\n",
    "                                # If any comparison fails, check if we have a new value to update\n",
    "                                if pd.notna(new_value):\n",
    "                                    update_record[field] = new_value\n",
    "                                    needs_update = True\n",
    "                        \n",
    "                        if needs_update:\n",
    "                            self.update_classes.append(update_record)\n",
    "                        \n",
    "                        # Map record_key to existing class ID for timings\n",
    "                        if record_key:\n",
    "                            if record_key not in self.class_id_mapping:\n",
    "                                self.class_id_mapping[record_key] = []\n",
    "                            if existing_class['id'] not in self.class_id_mapping[record_key]:\n",
    "                                self.class_id_mapping[record_key].append(existing_class['id'])\n",
    "                    else:\n",
    "                        # --- CREATE LOGIC ---\n",
    "                        # Check if we already created this class in new_classes\n",
    "                        already_created = False\n",
    "                        for new_class in self.new_classes:\n",
    "                            if (new_class['acad_term_id'] == acad_term_id and\n",
    "                                str(new_class.get('boss_id')) == str(class_boss_id) and\n",
    "                                new_class.get('professor_id') == prof_id):\n",
    "                                already_created = True\n",
    "                                # Map record_key to this class ID\n",
    "                                if record_key:\n",
    "                                    if record_key not in self.class_id_mapping:\n",
    "                                        self.class_id_mapping[record_key] = []\n",
    "                                    if new_class['id'] not in self.class_id_mapping[record_key]:\n",
    "                                        self.class_id_mapping[record_key].append(new_class['id'])\n",
    "                                break\n",
    "                        \n",
    "                        if not already_created:\n",
    "                            class_id = str(uuid.uuid4())\n",
    "                            new_class = {\n",
    "                                'id': class_id,\n",
    "                                'section': section,\n",
    "                                'course_id': course_id,\n",
    "                                'professor_id': prof_id,\n",
    "                                'acad_term_id': acad_term_id,\n",
    "                                'created_at': datetime.now().isoformat(),\n",
    "                                'updated_at': datetime.now().isoformat(),\n",
    "                                'grading_basis': row.get('grading_basis'),\n",
    "                                'course_outline_url': row.get('course_outline_url'),\n",
    "                                'boss_id': int(row.get('class_boss_id')) if pd.notna(row.get('class_boss_id')) else None,\n",
    "                                'raw_professor_name': prof_name,\n",
    "                                'warn_inaccuracy': warn_inaccuracy\n",
    "                            }\n",
    "                            \n",
    "                            self.new_classes.append(new_class)\n",
    "                            self.stats['classes_created'] += 1\n",
    "                            \n",
    "                            # Also add to existing_class_lookup to prevent duplicates in same run\n",
    "                            self.existing_class_lookup[class_key] = new_class\n",
    "                            \n",
    "                            # Map record_key to new class ID\n",
    "                            if record_key:\n",
    "                                if record_key not in self.class_id_mapping:\n",
    "                                    self.class_id_mapping[record_key] = []\n",
    "                                self.class_id_mapping[record_key].append(class_id)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Exception processing class row {idx}: {e}\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Class processing complete. New: {self.stats['classes_created']}, Updates: {len(self.update_classes)}.\")\n",
    "        return True\n",
    "        \n",
    "    def _find_professors_for_class(self, record_key: str) -> List[tuple]:\n",
    "        \"\"\"Find professor IDs for a class and return list of (professor_id, original_name) tuples\n",
    "        Deduplicates by professor_id to avoid creating multiple class records for same professor\"\"\"\n",
    "        if not record_key or pd.isna(record_key):\n",
    "            return []\n",
    "        \n",
    "        rows = self.multiple_lookup.get(record_key, [])\n",
    "        professor_mappings = []\n",
    "        seen_professor_ids = set()  # Track unique professor IDs\n",
    "        \n",
    "        # Ensure professor lookup is loaded\n",
    "        if not hasattr(self, 'professor_lookup_loaded'):\n",
    "            self.load_professor_lookup_csv()\n",
    "        \n",
    "        for row in rows:\n",
    "            prof_name_raw = row.get('professor_name')\n",
    "            \n",
    "            # FIXED: Better handling of NaN values from raw_data.xlsx\n",
    "            if prof_name_raw is None or pd.isna(prof_name_raw):\n",
    "                continue\n",
    "            \n",
    "            # Convert to string and strip - handles float NaN properly\n",
    "            original_prof_name = str(prof_name_raw).strip()\n",
    "            \n",
    "            # Skip empty strings and 'nan' strings\n",
    "            if not original_prof_name or original_prof_name.lower() == 'nan':\n",
    "                continue\n",
    "            \n",
    "            # Split the professor names intelligently\n",
    "            split_professors = self._split_professor_names(original_prof_name)\n",
    "            \n",
    "            # Process each split professor\n",
    "            for prof_name in split_professors:\n",
    "                if prof_name and prof_name.strip():  # Additional check for empty strings\n",
    "                    prof_id = self._lookup_professor_with_fallback(prof_name.strip())\n",
    "                    if prof_id and prof_id not in seen_professor_ids:\n",
    "                        professor_mappings.append((prof_id, prof_name.strip()))\n",
    "                        seen_professor_ids.add(prof_id)\n",
    "        \n",
    "        return professor_mappings\n",
    "\n",
    "    def _split_professor_names(self, prof_name: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Intelligently splits a string of professor names using a greedy, longest-match-first\n",
    "        approach, which eliminates the need for hardcoded combinations.\n",
    "        \"\"\"\n",
    "        if prof_name is None or pd.isna(prof_name) or not str(prof_name).strip():\n",
    "            return []\n",
    "\n",
    "        prof_name_str = str(prof_name).strip()\n",
    "        \n",
    "        # First, check if the entire string is already a known professor.\n",
    "        # This handles names that include commas, like \"LEE, MICHELLE PUI YEE\".\n",
    "        if prof_name_str.upper() in self.professor_lookup:\n",
    "            return [prof_name_str]\n",
    "            \n",
    "        # If there are no commas, it can only be one professor.\n",
    "        if ',' not in prof_name_str:\n",
    "            return [prof_name_str]\n",
    "\n",
    "        parts = [p.strip() for p in prof_name_str.split(',') if p.strip()]\n",
    "        \n",
    "        found_professors = []\n",
    "        i = 0\n",
    "        while i < len(parts):\n",
    "            # Start from the longest possible combination of remaining parts and work backwards.\n",
    "            match_found = False\n",
    "            for j in range(len(parts), i, -1):\n",
    "                # Create a candidate name by joining the parts\n",
    "                candidate = ', '.join(parts[i:j])\n",
    "                \n",
    "                # Check if this longest possible candidate is a known professor\n",
    "                if candidate.upper() in self.professor_lookup:\n",
    "                    found_professors.append(candidate)\n",
    "                    i = j  # Move the pointer past the parts we just consumed\n",
    "                    match_found = True\n",
    "                    break # Exit the inner loop and continue with the next part\n",
    "            \n",
    "            # If the inner loop finished without finding any match for the part(s)\n",
    "            if not match_found:\n",
    "                # This part is an unknown entity.\n",
    "                # Per your logic, we can try to append it to the previously found professor.\n",
    "                # This handles cases like \"WONG LI DE, BRIAN\" where \"BRIAN\" is unknown.\n",
    "                unknown_part = parts[i]\n",
    "                if found_professors and len(unknown_part.split()) == 1:\n",
    "                    # Append the unknown single-word part to the last known professor\n",
    "                    found_professors[-1] = f\"{found_professors[-1]}, {unknown_part}\"\n",
    "                    logger.info(f\"‚úÖ Combined unknown single word '{unknown_part}' with previous professor -> '{found_professors[-1]}'\")\n",
    "                else:\n",
    "                    # Otherwise, treat it as its own (potentially new) professor\n",
    "                    found_professors.append(unknown_part)\n",
    "                \n",
    "                i += 1 # Move to the next part\n",
    "                \n",
    "        return found_professors\n",
    "\n",
    "    def _lookup_professor_with_fallback(self, prof_name: str) -> Optional[str]:\n",
    "        \"\"\"Enhanced professor lookup with improved partial word matching and no phantom professor creation.\"\"\"\n",
    "        \n",
    "        if prof_name is None or pd.isna(prof_name):\n",
    "            return None\n",
    "        \n",
    "        prof_name = str(prof_name).strip()\n",
    "        if not prof_name or prof_name.lower() == 'nan':\n",
    "            return None\n",
    "        \n",
    "        # Strategy 1 & 2: Direct and variation-based lookup (unchanged).\n",
    "        normalized_name = prof_name.upper()\n",
    "        if hasattr(self, 'professor_lookup'):\n",
    "            if normalized_name in self.professor_lookup:\n",
    "                return self.professor_lookup[normalized_name]['database_id']\n",
    "            \n",
    "            variations = [\n",
    "                prof_name.strip().upper(),\n",
    "                prof_name.replace(',', '').strip().upper(),\n",
    "                ' '.join(prof_name.replace(',', ' ').split()).upper()\n",
    "            ]\n",
    "            for variation in variations:\n",
    "                if variation in self.professor_lookup:\n",
    "                    return self.professor_lookup[variation]['database_id']\n",
    "        \n",
    "        # Strategy 3: Search boss_aliases in professors_cache using the new robust parser.\n",
    "        search_name_normalized = normalized_name\n",
    "        for prof_data in self.professors_cache.values():\n",
    "            aliases_list = self._parse_boss_aliases(prof_data.get('boss_aliases'))\n",
    "            \n",
    "            for alias in aliases_list:\n",
    "                alias_normalized = alias.strip().upper()\n",
    "                \n",
    "                if alias_normalized == search_name_normalized:\n",
    "                    logger.info(f\"‚úÖ Found exact match in boss_aliases: {prof_name} ‚Üí {alias} (ID: {prof_data.get('id')})\")\n",
    "                    if not hasattr(self, 'professor_lookup'): self.professor_lookup = {}\n",
    "                    self.professor_lookup[search_name_normalized] = {\n",
    "                        'database_id': str(prof_data.get('id')),\n",
    "                        'boss_name': alias_normalized,\n",
    "                        'afterclass_name': prof_data.get('name', prof_name)\n",
    "                    }\n",
    "                    return str(prof_data.get('id'))\n",
    "\n",
    "        # Strategy 4: Enhanced partial word matching for cases like \"DENNIS LIM\" ‚Üí \"LIM CHONG BOON DENNIS\"\n",
    "        search_words = set(normalized_name.replace(',', ' ').split())\n",
    "        if len(search_words) >= 2:  # Only try partial matching for multi-word names\n",
    "            best_match = None\n",
    "            best_score = 0\n",
    "            \n",
    "            for prof_data in self.professors_cache.values():\n",
    "                # Check against afterclass_name (database name)\n",
    "                db_name = prof_data.get('name', '').upper()\n",
    "                db_words = set(db_name.replace(',', ' ').split())\n",
    "                \n",
    "                # Check if all search words are found in database name\n",
    "                if search_words.issubset(db_words):\n",
    "                    # Calculate match score (percentage of db_words that match search_words)\n",
    "                    match_score = len(search_words) / len(db_words) if db_words else 0\n",
    "                    \n",
    "                    if match_score > best_score and match_score >= 0.5:  # At least 50% match\n",
    "                        best_match = prof_data\n",
    "                        best_score = match_score\n",
    "                \n",
    "                # Also check against boss_aliases\n",
    "                aliases_list = self._parse_boss_aliases(prof_data.get('boss_aliases'))\n",
    "                for alias in aliases_list:\n",
    "                    alias_words = set(alias.upper().replace(',', ' ').split())\n",
    "                    if search_words.issubset(alias_words):\n",
    "                        match_score = len(search_words) / len(alias_words) if alias_words else 0\n",
    "                        if match_score > best_score and match_score >= 0.5:\n",
    "                            best_match = prof_data\n",
    "                            best_score = match_score\n",
    "            \n",
    "            if best_match and best_score >= 0.5:\n",
    "                logger.info(f\"üîç Partial word match found: '{prof_name}' ‚Üí '{best_match.get('name')}' (score: {best_score:.2f})\")\n",
    "                \n",
    "                # Add to lookup and save to partial matches tracking\n",
    "                if not hasattr(self, 'professor_lookup'): self.professor_lookup = {}\n",
    "                self.professor_lookup[normalized_name] = {\n",
    "                    'database_id': str(best_match.get('id')),\n",
    "                    'boss_name': normalized_name,\n",
    "                    'afterclass_name': best_match.get('name', prof_name)\n",
    "                }\n",
    "                \n",
    "                # Track partial matches for review\n",
    "                if not hasattr(self, 'partial_matches'):\n",
    "                    self.partial_matches = []\n",
    "                self.partial_matches.append({\n",
    "                    'boss_name': prof_name,\n",
    "                    'afterclass_name': best_match.get('name'),\n",
    "                    'database_id': str(best_match.get('id')),\n",
    "                    'method': 'partial_match',\n",
    "                    'match_score': f\"{best_score:.2f}\"\n",
    "                })\n",
    "                \n",
    "                return str(best_match.get('id'))\n",
    "        \n",
    "        # Strategy 5: Exact fuzzy matching (unchanged)\n",
    "        if hasattr(self, 'professor_lookup'):\n",
    "            for lookup_name in self.professor_lookup.keys():\n",
    "                if self._names_match_fuzzy_exact(normalized_name, lookup_name):\n",
    "                    return self.professor_lookup[lookup_name]['database_id']\n",
    "        \n",
    "        if normalized_name in self.professors_cache:\n",
    "            return self.professors_cache[normalized_name]['id']\n",
    "        \n",
    "        # Strategy 6: DO NOT create new professor - log as unmatched instead\n",
    "        logger.warning(f\"‚ö†Ô∏è Professor not found, will create new: {prof_name}\")\n",
    "        \n",
    "        # Create new professor (only when absolutely necessary)\n",
    "        return self._create_new_professor(prof_name)\n",
    "        \n",
    "    def _names_match_fuzzy_exact(self, name1: str, name2: str) -> bool:\n",
    "        \"\"\"Exact fuzzy matching for names - only matches if completely identical after normalization\"\"\"\n",
    "        \n",
    "        # Handle None and non-string values\n",
    "        if name1 is None or name2 is None:\n",
    "            return False\n",
    "        \n",
    "        # Ensure both names are strings\n",
    "        name1 = str(name1) if name1 is not None else \"\"\n",
    "        name2 = str(name2) if name2 is not None else \"\"\n",
    "        \n",
    "        # Remove common variations and normalize\n",
    "        clean1 = ' '.join(name1.replace(',', ' ').replace('.', ' ').split()).upper()\n",
    "        clean2 = ' '.join(name2.replace(',', ' ').replace('.', ' ').split()).upper()\n",
    "        \n",
    "        # Only return True if they are exactly the same after cleaning\n",
    "        return clean1 == clean2\n",
    "\n",
    "    def load_professor_lookup_csv(self):\n",
    "        \"\"\"Load professor lookup CSV once and cache it properly\"\"\"\n",
    "        # Check if already loaded to prevent repeated loading\n",
    "        if hasattr(self, 'professor_lookup_loaded') and self.professor_lookup_loaded:\n",
    "            return\n",
    "        \n",
    "        lookup_file = 'script_input/professor_lookup.csv'\n",
    "        \n",
    "        if not os.path.exists(lookup_file):\n",
    "            logger.warning(\"üìã professor_lookup.csv not found - will use database cache only\")\n",
    "            self.professor_lookup_loaded = True\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            lookup_df = pd.read_csv(lookup_file)\n",
    "            \n",
    "            # Validate required columns exist\n",
    "            required_cols = ['boss_name', 'afterclass_name', 'database_id', 'method']\n",
    "            missing_cols = [col for col in required_cols if col not in lookup_df.columns]\n",
    "            if missing_cols:\n",
    "                logger.error(f\"‚ùå professor_lookup.csv missing required columns: {missing_cols}\")\n",
    "                self.professor_lookup_loaded = True\n",
    "                return\n",
    "            \n",
    "            # Clear existing lookup and load fresh data\n",
    "            self.professor_lookup = {}\n",
    "            loaded_count = 0\n",
    "            \n",
    "            for _, row in lookup_df.iterrows():\n",
    "                boss_name = row.get('boss_name')\n",
    "                afterclass_name = row.get('afterclass_name')\n",
    "                database_id = row.get('database_id')\n",
    "                \n",
    "                # Skip rows with critical missing values\n",
    "                if pd.isna(boss_name) or pd.isna(database_id):\n",
    "                    continue\n",
    "                    \n",
    "                # Use boss_name as the primary key for lookup (as you specified)\n",
    "                boss_name_key = str(boss_name).strip().upper()\n",
    "                self.professor_lookup[boss_name_key] = {\n",
    "                    'database_id': str(database_id),\n",
    "                    'boss_name': str(boss_name),\n",
    "                    'afterclass_name': str(afterclass_name) if not pd.isna(afterclass_name) else str(boss_name)\n",
    "                }\n",
    "                loaded_count += 1\n",
    "            \n",
    "            logger.info(f\"‚úÖ Loaded {loaded_count} entries from professor_lookup.csv\")\n",
    "            self.professor_lookup_loaded = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error loading professor_lookup.csv: {e}\")\n",
    "            logger.info(\"üìã Continuing with database cache only\")\n",
    "            self.professor_lookup_loaded = True\n",
    "\n",
    "    def _create_new_professor(self, prof_name: str, professor_variations: dict = None, email_to_professor: dict = None) -> str:\n",
    "        \"\"\"\n",
    "        Create a new professor record, ensure proper tracking, and handle both primary and fallback alias creation.\n",
    "        \"\"\"\n",
    "        boss_name, afterclass_name = self._normalize_professor_name_fallback(prof_name)\n",
    "        \n",
    "        # Check if already created in this session to prevent duplicates\n",
    "        for new_prof in self.new_professors:\n",
    "            aliases_val = new_prof.get('boss_aliases', '[]')\n",
    "            try:\n",
    "                import json\n",
    "                alias_list = json.loads(aliases_val) if isinstance(aliases_val, str) else aliases_val\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                alias_list = []\n",
    "\n",
    "            if boss_name in alias_list or afterclass_name == new_prof.get('name', ''):\n",
    "                # This professor was already created in this run, just return its ID.\n",
    "                return new_prof.get('id')\n",
    "        \n",
    "        # --- Unified Creation Logic ---\n",
    "        professor_id = str(uuid.uuid4())\n",
    "        slug = re.sub(r'[^a-zA-Z0-9]+', '-', afterclass_name.lower()).strip('-')\n",
    "        resolved_email = self.resolve_professor_email(afterclass_name)\n",
    "\n",
    "        # --- Conditional Alias Creation ---\n",
    "        boss_aliases_set = set()\n",
    "        boss_aliases_set.add(boss_name)\n",
    "        \n",
    "        # SCENARIO A: Use sophisticated alias creation if professor_variations dictionary is provided\n",
    "        if professor_variations:\n",
    "            professor_specific_variations = professor_variations.get(prof_name, set())\n",
    "            for variation in professor_specific_variations:\n",
    "                if variation and variation.strip():\n",
    "                    variation_boss_name, _ = self._normalize_professor_name_fallback(variation.strip())\n",
    "                    boss_aliases_set.add(variation_boss_name)\n",
    "        # SCENARIO B: Fallback to simple alias creation if not provided\n",
    "        else:\n",
    "            if boss_name != prof_name.upper():\n",
    "                boss_aliases_set.add(prof_name.upper())\n",
    "\n",
    "        boss_aliases_list = sorted(list(boss_aliases_set))\n",
    "        import json\n",
    "        boss_aliases_json = json.dumps(boss_aliases_list)\n",
    "        \n",
    "        # --- Create and Register the New Professor ---\n",
    "        new_prof = {\n",
    "            'id': professor_id,\n",
    "            'name': afterclass_name,\n",
    "            'email': resolved_email,\n",
    "            'slug': slug,\n",
    "            'photo_url': 'https://smu.edu.sg',\n",
    "            'profile_url': 'https://smu.edu.sg',\n",
    "            'belong_to_university': 1,\n",
    "            'boss_aliases': boss_aliases_json,\n",
    "            'original_scraped_name': prof_name\n",
    "        }\n",
    "        \n",
    "        self.new_professors.append(new_prof)\n",
    "        self.stats['professors_created'] += 1\n",
    "        \n",
    "        # Update lookup tables\n",
    "        if not hasattr(self, 'professor_lookup'):\n",
    "            self.professor_lookup = {}\n",
    "        \n",
    "        lookup_entry = {\n",
    "            'database_id': professor_id,\n",
    "            'boss_name': boss_name,\n",
    "            'afterclass_name': afterclass_name\n",
    "        }\n",
    "        # Map the original name and all its aliases to the new ID\n",
    "        self.professor_lookup[prof_name.upper()] = lookup_entry\n",
    "        for alias in boss_aliases_list:\n",
    "            self.professor_lookup[alias.upper()] = lookup_entry\n",
    "\n",
    "        # Update the email duplicate checker dictionary if it was passed in\n",
    "        if email_to_professor is not None and resolved_email and resolved_email.lower() != 'enquiry@smu.edu.sg':\n",
    "            email_to_professor[resolved_email.lower()] = new_prof\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created professor: {afterclass_name} with email: {resolved_email}\")\n",
    "        logger.info(f\"   Boss aliases: {boss_aliases_list}\")\n",
    "        \n",
    "        return professor_id\n",
    "\n",
    "    def process_timings(self):\n",
    "        \"\"\"\n",
    "        MODIFICATION 2 (REVISED): Process class and exam timings with robust deduplication that handles TBA cases.\n",
    "        \"\"\"\n",
    "        logger.info(\"‚è∞ Processing class timings and exam timings with strict uniqueness checks...\")\n",
    "\n",
    "        # A tiny, local helper function to consistently handle missing values for key creation.\n",
    "        # This prevents the 'nan' vs 'None' string issue.\n",
    "        def _clean_key_val(v):\n",
    "            return '' if pd.isna(v) else str(v)\n",
    "\n",
    "        # Load existing timing keys from the database cache ONCE.\n",
    "        if not self.processed_timing_keys:\n",
    "            cache_file = os.path.join(self.cache_dir, 'class_timing_cache.pkl')\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    df = pd.read_pickle(cache_file)\n",
    "                    if not df.empty and 'class_id' in df.columns:\n",
    "                        for _, record in df.iterrows():\n",
    "                            # FIX: Use the robust key creation logic for cached data\n",
    "                            key = (\n",
    "                                record['class_id'],\n",
    "                                _clean_key_val(record.get('day_of_week')),\n",
    "                                _clean_key_val(record.get('start_time')),\n",
    "                                _clean_key_val(record.get('end_time')),\n",
    "                                _clean_key_val(record.get('venue'))\n",
    "                            )\n",
    "                            self.processed_timing_keys.add(key)\n",
    "                        logger.info(f\"‚úÖ Pre-loaded {len(self.processed_timing_keys)} existing class timing keys from cache.\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not preload class_timing_cache.pkl: {e}\")\n",
    "\n",
    "        # Load existing exam timing keys\n",
    "        if not self.processed_exam_class_ids:\n",
    "            exam_cache_file = os.path.join(self.cache_dir, 'class_exam_timing_cache.pkl')\n",
    "            if os.path.exists(exam_cache_file):\n",
    "                try:\n",
    "                    df = pd.read_pickle(exam_cache_file)\n",
    "                    if not df.empty and 'class_id' in df.columns:\n",
    "                        self.processed_exam_class_ids.update(df['class_id'].unique())\n",
    "                        logger.info(f\"‚úÖ Pre-loaded {len(self.processed_exam_class_ids)} existing exam class IDs from cache.\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not preload class_exam_timing_cache.pkl: {e}\")\n",
    "        \n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            record_key = row.get('record_key')\n",
    "            if record_key not in self.class_id_mapping:\n",
    "                continue\n",
    "            \n",
    "            class_ids = self.class_id_mapping.get(record_key, [])\n",
    "            timing_type = row.get('type', 'CLASS')\n",
    "            \n",
    "            for class_id in class_ids:\n",
    "                if timing_type == 'CLASS':\n",
    "                    # --- FIX: Use the same robust key generation for new data ---\n",
    "                    timing_key = (\n",
    "                        class_id,\n",
    "                        _clean_key_val(row.get('day_of_week')),\n",
    "                        _clean_key_val(row.get('start_time')),\n",
    "                        _clean_key_val(row.get('end_time')),\n",
    "                        _clean_key_val(row.get('venue'))\n",
    "                    )\n",
    "                    \n",
    "                    # Check if this exact timing record has already been processed\n",
    "                    if timing_key in self.processed_timing_keys:\n",
    "                        continue\n",
    "                    \n",
    "                    # Add to set *before* appending to prevent duplicates within the same run\n",
    "                    self.processed_timing_keys.add(timing_key)\n",
    "                    \n",
    "                    timing_record = {\n",
    "                        'class_id': class_id, 'start_date': row.get('start_date'),\n",
    "                        'end_date': row.get('end_date'), 'day_of_week': row.get('day_of_week'),\n",
    "                        'start_time': row.get('start_time'), 'end_time': row.get('end_time'),\n",
    "                        'venue': row.get('venue', '')\n",
    "                    }\n",
    "                    self.new_class_timings.append(timing_record)\n",
    "                    self.stats['timings_created'] += 1\n",
    "                \n",
    "                elif timing_type == 'EXAM':\n",
    "                    if class_id in self.processed_exam_class_ids:\n",
    "                        continue\n",
    "                    \n",
    "                    self.processed_exam_class_ids.add(class_id)\n",
    "                    \n",
    "                    exam_record = {\n",
    "                        'class_id': class_id, 'date': row.get('date'),\n",
    "                        'day_of_week': row.get('day_of_week'), \n",
    "                        'start_time': str(row.get('start_time')),\n",
    "                        'end_time': str(row.get('end_time')), \n",
    "                        'venue': row.get('venue')\n",
    "                    }\n",
    "                    self.new_class_exam_timings.append(exam_record)\n",
    "                    self.stats['exams_created'] += 1\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created {self.stats['timings_created']} new class timings (after deduplication).\")\n",
    "        logger.info(f\"‚úÖ Created {self.stats['exams_created']} new exam timings (after deduplication).\")\n",
    "\n",
    "    def save_outputs(self):\n",
    "        \"\"\"Save all generated CSV files, only creating files that have data.\"\"\"\n",
    "        logger.info(\"üíæ Saving output files...\")\n",
    "\n",
    "        def to_csv_if_not_empty(data_list, filename):\n",
    "            if data_list:\n",
    "                df = pd.DataFrame(data_list)\n",
    "                if not df.empty:\n",
    "                    path = os.path.join(self.output_base, filename)\n",
    "                    df.to_csv(path, index=False)\n",
    "                    logger.info(f\"‚úÖ Saved {len(df)} records to {filename}\")\n",
    "                    \n",
    "                    # DEBUG: For update_bid_result, show what we're saving\n",
    "                    if filename == 'update_bid_result.csv':\n",
    "                        logger.info(f\"üîç DEBUG: update_bid_result.csv columns: {list(df.columns)}\")\n",
    "                        # Show first few rows with median/min values\n",
    "                        rows_with_bid_data = df[(df['median'].notna()) | (df['min'].notna())]\n",
    "                        if not rows_with_bid_data.empty:\n",
    "                            logger.info(f\"üîç DEBUG: {len(rows_with_bid_data)} rows have median/min data\")\n",
    "                            logger.info(f\"üîç DEBUG: Sample data:\")\n",
    "                            for idx, row in rows_with_bid_data.head(3).iterrows():\n",
    "                                logger.info(f\"  bid_window_id={row['bid_window_id']}, class_id={row['class_id']}, median={row.get('median')}, min={row.get('min')}\")\n",
    "                        else:\n",
    "                            logger.warning(\"‚ö†Ô∏è DEBUG: No rows in update_bid_result have median/min values!\")\n",
    "\n",
    "        # The following line is the only change. It has been removed.\n",
    "        # to_csv_if_not_empty(self.new_professors, 'new_professors.csv') \n",
    "        \n",
    "        to_csv_if_not_empty(getattr(self, 'update_professors', []), 'update_professor.csv')\n",
    "        # We also no longer need to save new_courses here, as it's handled in Phase 1.\n",
    "        # to_csv_if_not_empty(self.new_courses, os.path.join('verify', 'new_courses.csv'))\n",
    "        to_csv_if_not_empty(self.update_courses, 'update_courses.csv')\n",
    "        to_csv_if_not_empty(getattr(self, 'update_classes', []), 'update_classes.csv')\n",
    "        to_csv_if_not_empty(self.new_acad_terms, 'new_acad_term.csv')\n",
    "        to_csv_if_not_empty(self.new_classes, 'new_classes.csv')\n",
    "        to_csv_if_not_empty(self.new_class_timings, 'new_class_timing.csv')\n",
    "        to_csv_if_not_empty(self.new_class_exam_timings, 'new_class_exam_timing.csv')\n",
    "        update_records = getattr(self, 'update_bid_result', [])\n",
    "        if update_records:\n",
    "            logger.info(f\"üìù Preparing to save {len(update_records)} bid result updates\")\n",
    "            # Show sample of records with actual bid data\n",
    "            records_with_bids = [r for r in update_records if r.get('median') is not None or r.get('min') is not None]\n",
    "            logger.info(f\"   - {len(records_with_bids)} records have median/min bid data\")\n",
    "            if records_with_bids:\n",
    "                sample = records_with_bids[0]\n",
    "                logger.info(f\"   - Sample: bid_window_id={sample.get('bid_window_id')}, median={sample.get('median')}, min={sample.get('min')}\")\n",
    "        to_csv_if_not_empty(update_records, 'update_bid_result.csv')\n",
    "\n",
    "        if self.courses_needing_faculty:\n",
    "            df = pd.DataFrame(self.courses_needing_faculty)\n",
    "            # Note: This path should probably also be in the 'verify' folder\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'courses_needing_faculty.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.courses_needing_faculty)} courses needing faculty assignment to the verify folder.\")\n",
    "            \n",
    "    def update_professor_lookup_from_corrected_csv(self):\n",
    "        \"\"\"Update professor lookup from manually corrected new_professors.csv\"\"\"\n",
    "        logger.info(\"üîÑ Updating professor lookup from corrected CSV...\")\n",
    "        \n",
    "        # Read corrected new_professors.csv\n",
    "        corrected_csv_path = os.path.join(self.verify_dir, 'new_professors.csv')\n",
    "        if not os.path.exists(corrected_csv_path):\n",
    "            logger.info(f\"üìù No corrected CSV found: {corrected_csv_path} - assuming all professors already exist\")\n",
    "            return True\n",
    "\n",
    "        corrected_df = pd.read_csv(corrected_csv_path)\n",
    "        if corrected_df.empty:\n",
    "            logger.info(f\"üìù Empty corrected CSV - no professors to update\")\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"üìñ Reading {len(corrected_df)} corrected professor records\")\n",
    "            \n",
    "            # Clear and rebuild the new_professors list with corrected data\n",
    "            self.new_professors = []\n",
    "            \n",
    "            # Update internal professor_lookup and rebuild new_professors\n",
    "            updated_count = 0\n",
    "            \n",
    "            # FIXED: Initialize professor_lookup if it doesn't exist\n",
    "            if not hasattr(self, 'professor_lookup'):\n",
    "                self.professor_lookup = {}\n",
    "            \n",
    "            for _, row in corrected_df.iterrows():\n",
    "                original_name = row.get('original_scraped_name', '')\n",
    "                corrected_afterclass_name = row.get('name', '')  # This is the corrected name\n",
    "                boss_aliases = row.get('boss_aliases', '')  # This should be JSON string\n",
    "                professor_id = row.get('id', '')\n",
    "                \n",
    "                # Parse boss_aliases JSON string\n",
    "                try:\n",
    "                    import json\n",
    "                    if isinstance(boss_aliases, str) and boss_aliases.strip():\n",
    "                        boss_aliases_list = json.loads(boss_aliases)\n",
    "                        if isinstance(boss_aliases_list, list) and boss_aliases_list:\n",
    "                            boss_name = boss_aliases_list[0]  # Use first boss alias\n",
    "                        else:\n",
    "                            boss_name = original_name.upper() if original_name else corrected_afterclass_name.upper()\n",
    "                    else:\n",
    "                        boss_name = original_name.upper() if original_name else corrected_afterclass_name.upper()\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # Fallback if JSON parsing fails\n",
    "                    boss_name = original_name.upper() if original_name else corrected_afterclass_name.upper()\n",
    "                \n",
    "                # Rebuild the professor record with corrected data\n",
    "                corrected_prof = {\n",
    "                    'id': professor_id,\n",
    "                    'name': corrected_afterclass_name,  # Use corrected name\n",
    "                    'email': row.get('email', 'enquiry@smu.edu.sg'),\n",
    "                    'slug': row.get('slug', ''),\n",
    "                    'photo_url': row.get('photo_url', 'https://smu.edu.sg'),\n",
    "                    'profile_url': row.get('profile_url', 'https://smu.edu.sg'),\n",
    "                    'belong_to_university': row.get('belong_to_university', 1),\n",
    "                    'boss_aliases': boss_aliases,  # Keep as JSON string\n",
    "                    'afterclass_name': corrected_afterclass_name,\n",
    "                    'original_scraped_name': original_name\n",
    "                }\n",
    "                \n",
    "                # Add to new_professors list\n",
    "                self.new_professors.append(corrected_prof)\n",
    "                \n",
    "                # FIXED: Update professor_lookup with ALL variations\n",
    "                if professor_id:\n",
    "                    lookup_entry = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': corrected_afterclass_name\n",
    "                    }\n",
    "                    \n",
    "                    # Add original scraped name to lookup\n",
    "                    if original_name:\n",
    "                        self.professor_lookup[original_name.upper()] = lookup_entry\n",
    "                        updated_count += 1\n",
    "                    \n",
    "                    # Add corrected afterclass name to lookup\n",
    "                    if corrected_afterclass_name:\n",
    "                        self.professor_lookup[corrected_afterclass_name.upper()] = lookup_entry\n",
    "                    \n",
    "                    # Add boss_name to lookup\n",
    "                    if boss_name:\n",
    "                        self.professor_lookup[boss_name.upper()] = lookup_entry\n",
    "                    \n",
    "                    # FIXED: Add all boss aliases to lookup\n",
    "                    try:\n",
    "                        if isinstance(boss_aliases, str) and boss_aliases.strip():\n",
    "                            boss_aliases_list = json.loads(boss_aliases)\n",
    "                            if isinstance(boss_aliases_list, list):\n",
    "                                for alias in boss_aliases_list:\n",
    "                                    if alias and str(alias).strip():\n",
    "                                        self.professor_lookup[str(alias).upper()] = lookup_entry\n",
    "                    except (json.JSONDecodeError, TypeError):\n",
    "                        pass  # Skip if JSON parsing fails\n",
    "            \n",
    "            # Save updated professor lookup to CSV\n",
    "            self._save_corrected_professor_lookup()\n",
    "            \n",
    "            logger.info(f\"‚úÖ Updated {updated_count} professor lookup entries\")\n",
    "            logger.info(f\"‚úÖ Rebuilt {len(self.new_professors)} professor records with corrections\")\n",
    "            logger.info(f\"‚úÖ Total lookup entries now: {len(self.professor_lookup)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to update professor lookup: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def update_professors_with_boss_names(self):\n",
    "        \"\"\"\n",
    "        Update professors with missing/additional boss_names by comparing professor_lookup.csv\n",
    "        with database boss_aliases and combining new variations from high-confidence fuzzy matches.\n",
    "        \"\"\"\n",
    "        logger.info(\"üë§ Updating professors with boss_names and detecting new variations...\")\n",
    "\n",
    "        # --- Step 1: Load high-confidence fuzzy matches from Phase 1 ---\n",
    "        fuzzy_path = os.path.join(self.verify_dir, 'fuzzy_matched_professors.csv')\n",
    "        new_aliases_by_id = defaultdict(list)\n",
    "\n",
    "        if os.path.exists(fuzzy_path):\n",
    "            try:\n",
    "                fuzzy_df = pd.read_csv(fuzzy_path)\n",
    "                high_confidence_matches = fuzzy_df[fuzzy_df['confidence_score'] >= 95]\n",
    "                logger.info(f\"üîç Found {len(high_confidence_matches)} high-confidence (>=95) fuzzy matches to process.\")\n",
    "\n",
    "                for _, row in high_confidence_matches.iterrows():\n",
    "                    database_id = str(row['database_id'])\n",
    "                    afterclass_name = row['afterclass_name']\n",
    "                    \n",
    "                    try:\n",
    "                        import json\n",
    "                        aliases_val = row.get('boss_aliases', '[]')\n",
    "                        new_aliases = json.loads(aliases_val) if isinstance(aliases_val, str) else []\n",
    "                        \n",
    "                        for alias in new_aliases:\n",
    "                            if alias and str(alias).strip():\n",
    "                                clean_alias = str(alias).strip()\n",
    "                                new_aliases_by_id[database_id].append(clean_alias)\n",
    "                                \n",
    "                                # Add to in-memory professor_lookup to be saved later\n",
    "                                if not hasattr(self, 'professor_lookup'):\n",
    "                                    self.professor_lookup = {}\n",
    "                                \n",
    "                                alias_key = clean_alias.upper()\n",
    "                                if alias_key not in self.professor_lookup:\n",
    "                                    self.professor_lookup[alias_key] = {\n",
    "                                        'database_id': database_id,\n",
    "                                        'boss_name': clean_alias,\n",
    "                                        'afterclass_name': afterclass_name,\n",
    "                                        'method': 'fuzzy_match' # Add method for tracking\n",
    "                                    }\n",
    "                                    logger.info(f\"‚ûï Adding fuzzy match to lookup: '{clean_alias}' -> '{afterclass_name}'\")\n",
    "\n",
    "                    except (json.JSONDecodeError, TypeError) as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è Could not parse boss_aliases from fuzzy_matched_professors.csv for row: {row.to_dict()}. Error: {e}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error processing fuzzy_matched_professors.csv: {e}\")\n",
    "\n",
    "        # --- Step 2: Load existing variations from professor_lookup.csv ---\n",
    "        lookup_file = 'script_input/professor_lookup.csv'\n",
    "        lookup_groups = defaultdict(list)\n",
    "        if os.path.exists(lookup_file):\n",
    "            try:\n",
    "                lookup_df = pd.read_csv(lookup_file)\n",
    "                for _, row in lookup_df.iterrows():\n",
    "                    database_id = row.get('database_id')\n",
    "                    boss_name = row.get('boss_name')\n",
    "                    if pd.notna(database_id) and pd.notna(boss_name):\n",
    "                        lookup_groups[str(database_id)].append(str(boss_name).strip())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error loading professor_lookup.csv: {e}\")\n",
    "\n",
    "        # --- Step 3: Iterate through professors and combine all alias sources ---\n",
    "        updated_professor_ids = set() \n",
    "        self.update_professors = []\n",
    "        new_variations_found = []\n",
    "        import json\n",
    "\n",
    "        for prof_key, prof_data in self.professors_cache.items():\n",
    "            professor_id = str(prof_data.get('id'))\n",
    "            if professor_id in updated_professor_ids:\n",
    "                continue\n",
    "\n",
    "            # Get all sources of aliases as sets for easy combination\n",
    "            current_boss_aliases = set(self._parse_boss_aliases(prof_data.get('boss_aliases')))\n",
    "            lookup_variations = set(lookup_groups.get(professor_id, []))\n",
    "            fuzzy_variations = set(new_aliases_by_id.get(professor_id, []))\n",
    "\n",
    "            # Combine all unique variations using set union\n",
    "            final_aliases_raw = current_boss_aliases.union(lookup_variations).union(fuzzy_variations)\n",
    "\n",
    "            # Normalize both sets for a stable comparison, preventing repeated updates\n",
    "            current_aliases_normalized = {name.replace(\"‚Äô\", \"'\") for name in current_boss_aliases}\n",
    "            final_aliases_normalized = {name.replace(\"‚Äô\", \"'\") for name in final_aliases_raw}\n",
    "\n",
    "            # Check for changes using the normalized sets\n",
    "            if final_aliases_normalized != current_aliases_normalized:\n",
    "                # Save the raw, original names to preserve the smart quote from the source\n",
    "                unique_boss_names = sorted(list(final_aliases_raw))\n",
    "                # Use ensure_ascii=False to prevent encoding '‚Äô' to '\\u2019'\n",
    "                boss_aliases_json = json.dumps(unique_boss_names, ensure_ascii=False)\n",
    "\n",
    "                self.update_professors.append({\n",
    "                    'id': professor_id,\n",
    "                    'boss_aliases': boss_aliases_json,\n",
    "                })\n",
    "                \n",
    "                # For logging, find the newly added variations\n",
    "                newly_added = final_aliases_raw - current_boss_aliases\n",
    "                if newly_added:\n",
    "                    logger.info(f\"‚úÖ Adding {len(newly_added)} new variations for professor {professor_id}: {sorted(list(newly_added))}\")\n",
    "                    new_variations_found.append({\n",
    "                        'professor_id': professor_id,\n",
    "                        'professor_name': prof_data.get('name', 'Unknown'),\n",
    "                        'existing_aliases': sorted(list(current_boss_aliases)),\n",
    "                        'new_variations': sorted(list(newly_added)),\n",
    "                        'final_aliases': unique_boss_names\n",
    "                    })\n",
    "                \n",
    "                updated_professor_ids.add(professor_id)\n",
    "\n",
    "        # --- Step 4: Save all outputs ---\n",
    "        # Save partial matches if any were found\n",
    "        if hasattr(self, 'partial_matches') and self.partial_matches:\n",
    "            partial_df = pd.DataFrame(self.partial_matches)\n",
    "            partial_path = os.path.join(self.verify_dir, 'partial_matches.csv')\n",
    "            partial_df.to_csv(partial_path, index=False)\n",
    "            logger.info(f\"üîç Saved {len(self.partial_matches)} partial matches to partial_matches.csv\")\n",
    "\n",
    "        # Save new variations summary\n",
    "        if new_variations_found:\n",
    "            report_data = [{'professor_id': item.get('professor_id'),'professor_name': item.get('professor_name'), 'existing_aliases': '|'.join(item.get('existing_aliases', [])), 'new_variations': '|'.join(item.get('new_variations', [])),'final_aliases': '|'.join(item.get('final_aliases', []))} for item in new_variations_found]\n",
    "            variations_df = pd.DataFrame(report_data)\n",
    "            variations_path = os.path.join(self.verify_dir, 'new_variations_found.csv')\n",
    "            variations_df.to_csv(variations_path, index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"üÜï Saved {len(new_variations_found)} professors with new variations to new_variations_found.csv\")\n",
    "\n",
    "        # Save the update_professor.csv file\n",
    "        if self.update_professors:\n",
    "            df = pd.DataFrame(self.update_professors)\n",
    "            update_path = os.path.join(self.output_base, 'update_professor.csv')\n",
    "            df.to_csv(update_path, index=False, encoding='utf-8')\n",
    "            logger.info(f\"‚úÖ Saved {len(self.update_professors)} unique professor updates to update_professor.csv\")\n",
    "            self.stats['professors_updated'] = len(self.update_professors)\n",
    "        else:\n",
    "            logger.info(\"‚ÑπÔ∏è No professors need boss_name updates.\")\n",
    "            self.stats['professors_updated'] = 0\n",
    "\n",
    "        # --- Step 5: Persist the updated professor lookup table ---\n",
    "        self._save_corrected_professor_lookup()\n",
    "\n",
    "    def process_remaining_tables(self):\n",
    "        \"\"\"Process classes and timings after professor lookup is updated\"\"\"\n",
    "        logger.info(\"üè´ Processing remaining tables (classes, timings)...\")\n",
    "        \n",
    "        try:\n",
    "            # Clear any existing data from Phase 1 to avoid duplicates\n",
    "            self.new_classes = []\n",
    "            self.new_class_timings = []\n",
    "            self.new_class_exam_timings = []\n",
    "            self.class_id_mapping = {}\n",
    "            self.stats['classes_created'] = 0\n",
    "            self.stats['timings_created'] = 0\n",
    "            self.stats['exams_created'] = 0\n",
    "            \n",
    "            # Process classes (depends on updated professor lookup)\n",
    "            self.process_classes()\n",
    "            \n",
    "            # Process timings (depends on classes)\n",
    "            self.process_timings()\n",
    "            \n",
    "            logger.info(\"‚úÖ Remaining tables processed successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to process remaining tables: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_corrected_professor_lookup(self):\n",
    "        \"\"\"Save professor lookup preserving all input entries, adding new ones, and including partial matches\"\"\"\n",
    "        # Start with all existing entries from input professor_lookup.csv\n",
    "        all_lookup_data = {}\n",
    "        \n",
    "        # Step 1: Load ALL entries from input professor_lookup.csv (preserve existing)\n",
    "        input_lookup_file = 'script_input/professor_lookup.csv'\n",
    "        if os.path.exists(input_lookup_file):\n",
    "            try:\n",
    "                input_df = pd.read_csv(input_lookup_file)\n",
    "                for _, row in input_df.iterrows():\n",
    "                    boss_name = row.get('boss_name')\n",
    "                    afterclass_name = row.get('afterclass_name')\n",
    "                    database_id = row.get('database_id')\n",
    "                    method = row.get('method', 'exists')\n",
    "                    \n",
    "                    # Only require database_id to be present\n",
    "                    if pd.notna(database_id):\n",
    "                        if pd.isna(boss_name) or str(boss_name).strip() == '':\n",
    "                            if pd.notna(afterclass_name):\n",
    "                                lookup_key = f\"EMPTY_BOSS_{str(afterclass_name).upper().replace(' ', '_')}\"\n",
    "                                boss_name_value = \"\"\n",
    "                            else:\n",
    "                                lookup_key = f\"EMPTY_BOSS_{str(database_id)}\"\n",
    "                                boss_name_value = \"\"\n",
    "                        else:\n",
    "                            lookup_key = str(boss_name).upper()\n",
    "                            boss_name_value = str(boss_name)\n",
    "                        \n",
    "                        all_lookup_data[lookup_key] = {\n",
    "                            'boss_name': boss_name_value,\n",
    "                            'afterclass_name': str(afterclass_name) if pd.notna(afterclass_name) else \"\",\n",
    "                            'database_id': str(database_id),\n",
    "                            'method': str(method)\n",
    "                        }\n",
    "                \n",
    "                logger.info(f\"üìñ Loaded {len(all_lookup_data)} existing entries from input professor_lookup.csv\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Could not load input professor_lookup.csv: {e}\")\n",
    "        \n",
    "        # Step 2: Add/update with new entries from current processing\n",
    "        new_entries_count = 0\n",
    "        updated_entries_count = 0\n",
    "        \n",
    "        for scraped_name, data in self.professor_lookup.items():\n",
    "            boss_name = data.get('boss_name', scraped_name.upper())\n",
    "            afterclass_name = data.get('afterclass_name', scraped_name)\n",
    "            database_id = data['database_id']\n",
    "            \n",
    "            # Determine method: check if this is a newly created professor or partial match\n",
    "            method = 'exists'  # default\n",
    "            if any(prof['id'] == database_id for prof in self.new_professors):\n",
    "                method = 'created'\n",
    "            elif hasattr(self, 'partial_matches') and any(match['database_id'] == database_id and match['boss_name'] == scraped_name for match in self.partial_matches):\n",
    "                method = 'partial_match'\n",
    "            \n",
    "            boss_name_key = str(boss_name).upper()\n",
    "            \n",
    "            if boss_name_key in all_lookup_data:\n",
    "                # Update existing entry if method changed\n",
    "                if method in ['created', 'partial_match']:\n",
    "                    all_lookup_data[boss_name_key]['method'] = method\n",
    "                    updated_entries_count += 1\n",
    "            else:\n",
    "                # Add new entry\n",
    "                all_lookup_data[boss_name_key] = {\n",
    "                    'boss_name': str(boss_name),\n",
    "                    'afterclass_name': str(afterclass_name),\n",
    "                    'database_id': str(database_id),\n",
    "                    'method': method\n",
    "                }\n",
    "                new_entries_count += 1\n",
    "                logger.info(f\"   -> NEW LOOKUP: Adding '{boss_name}' for '{afterclass_name}' (ID: {database_id}, method: {method})\")\n",
    "        \n",
    "        # Step 3: Add partial matches that weren't already in professor_lookup\n",
    "        if hasattr(self, 'partial_matches'):\n",
    "            for match in self.partial_matches:\n",
    "                boss_name_key = match['boss_name'].upper()\n",
    "                if boss_name_key not in all_lookup_data:\n",
    "                    all_lookup_data[boss_name_key] = {\n",
    "                        'boss_name': match['boss_name'],\n",
    "                        'afterclass_name': match['afterclass_name'],\n",
    "                        'database_id': match['database_id'],\n",
    "                        'method': f\"partial_match_{match.get('match_score', '')}\"\n",
    "                    }\n",
    "                    new_entries_count += 1\n",
    "                    logger.info(f\"   -> PARTIAL MATCH: Adding '{match['boss_name']}' ‚Üí '{match['afterclass_name']}' (score: {match.get('match_score', 'N/A')})\")\n",
    "        \n",
    "        # Step 4: Convert to list and sort\n",
    "        lookup_data = list(all_lookup_data.values())\n",
    "        lookup_data.sort(key=lambda x: x['boss_name'] if x['boss_name'] else x['afterclass_name'])\n",
    "        \n",
    "        # Step 5: Save main lookup file\n",
    "        df = pd.DataFrame(lookup_data)\n",
    "        df.to_csv(input_lookup_file, index=False)\n",
    "        \n",
    "        # Step 6: Save separate tracking files for manual review\n",
    "        if hasattr(self, 'partial_matches') and self.partial_matches:\n",
    "            partial_df = pd.DataFrame(self.partial_matches)\n",
    "            partial_path = os.path.join(self.verify_dir, 'partial_matches_log.csv')\n",
    "            partial_df.to_csv(partial_path, index=False)\n",
    "            logger.info(f\"üîç Saved {len(self.partial_matches)} partial matches to partial_matches_log.csv\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Updated professor_lookup.csv:\")\n",
    "        logger.info(f\"   ‚Ä¢ Total entries: {len(lookup_data)}\")\n",
    "        logger.info(f\"   ‚Ä¢ New entries added: {new_entries_count}\")\n",
    "        logger.info(f\"   ‚Ä¢ Existing entries updated: {updated_entries_count}\")\n",
    "        \n",
    "        # Step 7: Log summary of different methods\n",
    "        method_counts = {}\n",
    "        for entry in lookup_data:\n",
    "            method = entry.get('method', 'unknown')\n",
    "            method_counts[method] = method_counts.get(method, 0) + 1\n",
    "        \n",
    "        logger.info(\"üìä Entries by method:\")\n",
    "        for method, count in sorted(method_counts.items()):\n",
    "            logger.info(f\"   ‚Ä¢ {method}: {count}\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚úÖ Professors created: {self.stats['professors_created']}\")\n",
    "        print(f\"‚úÖ Courses created: {self.stats['courses_created']}\")\n",
    "        print(f\"‚úÖ Courses updated: {self.stats['courses_updated']}\")\n",
    "        print(f\"‚ö†Ô∏è  Courses needing faculty: {self.stats['courses_needing_faculty']}\")\n",
    "        print(f\"‚úÖ Classes created: {self.stats['classes_created']}\")\n",
    "        print(f\"‚úÖ Class timings created: {self.stats['timings_created']}\")\n",
    "        print(f\"‚úÖ Exam timings created: {self.stats['exams_created']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìÅ OUTPUT FILES:\")\n",
    "        print(f\"   Verify folder: {self.verify_dir}/\")\n",
    "        print(f\"   - new_professors.csv ({self.stats['professors_created']} records)\")\n",
    "        print(f\"   - new_courses.csv ({self.stats['courses_created']} records)\")\n",
    "        print(f\"   Output folder: {self.output_base}/\")\n",
    "        print(f\"   - update_courses.csv ({self.stats['courses_updated']} records)\")\n",
    "        print(f\"   - new_acad_term.csv ({len(self.new_acad_terms)} records)\")\n",
    "        print(f\"   - new_classes.csv ({self.stats['classes_created']} records)\")\n",
    "        print(f\"   - new_class_timing.csv ({self.stats['timings_created']} records)\")\n",
    "        print(f\"   - new_class_exam_timing.csv ({self.stats['exams_created']} records)\")\n",
    "        print(f\"   - professor_lookup.csv (updated)\")\n",
    "        print(f\"   - courses_needing_faculty.csv ({self.stats['courses_needing_faculty']} records)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def run_phase1_professors_and_courses(self):\n",
    "        \"\"\"Phase 1: Process professors and courses with automated faculty mapping and cache checking\"\"\"\n",
    "        try:\n",
    "            logger.info(\"üöÄ Starting Phase 1: Professors and Courses with Cache Checking\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Load or cache database data.\n",
    "            # The new _load_from_cache now handles all professor lookup validation and synchronization internally.\n",
    "            if not self.load_or_cache_data_with_freshness_check():\n",
    "                logger.error(\"‚ùå Failed to load or validate database data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 2: Load the raw input data from Excel\n",
    "            if not self.load_raw_data():\n",
    "                logger.error(\"‚ùå Failed to load raw data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Process the data using the now-validated caches\n",
    "            logger.info(\"\\nüéì Running automated faculty mapping...\")\n",
    "            self.process_professors()\n",
    "            self.process_courses()\n",
    "            \n",
    "            try:\n",
    "                self.map_courses_to_faculties_from_boss()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Automated faculty mapping failed: {e}\")\n",
    "                logger.info(\"  Continuing with manual faculty assignment...\")\n",
    "            \n",
    "            self.process_acad_terms()\n",
    "            \n",
    "            # Step 4: Save phase 1 outputs\n",
    "            self._save_phase1_outputs()\n",
    "            \n",
    "            # Step 5: Print faculty mapping summary\n",
    "            if hasattr(self, 'courses_needing_faculty') and self.courses_needing_faculty:\n",
    "                logger.info(f\"\\nüìã Faculty Assignment Summary:\")\n",
    "                logger.info(f\"  ‚Ä¢ Automated mappings applied to {self.stats.get('courses_created', 0) - len(self.courses_needing_faculty)} courses\")\n",
    "                logger.info(f\"  ‚Ä¢ {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "                \n",
    "                if len(self.courses_needing_faculty) <= 10:\n",
    "                    logger.info(f\"  Courses needing manual review:\")\n",
    "                    for course_info in self.courses_needing_faculty:\n",
    "                        logger.info(f\"    - {course_info['course_code']}: {course_info['course_name']}\")\n",
    "            \n",
    "            logger.info(\"‚úÖ Phase 1 completed - Review files in verify/ folder\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Phase 1 failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def run_phase2_remaining_tables(self):\n",
    "        \"\"\"Phase 2: Process classes and timings after professor correction with cache checking\"\"\"\n",
    "        try:\n",
    "            logger.info(\"üöÄ Starting Phase 2: Classes and Timings with Cache Checking\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Set phase 2 mode to prevent overwriting corrected professors\n",
    "            self._phase2_mode = True\n",
    "            \n",
    "            # Ensure cache is fresh\n",
    "            if not self.load_or_cache_data_with_freshness_check():\n",
    "                logger.error(\"‚ùå Failed to load fresh database data\")\n",
    "                return False\n",
    "            \n",
    "            # Update professor lookup from corrected CSV\n",
    "            if not self.update_professor_lookup_from_corrected_csv():\n",
    "                logger.error(\"‚ùå Failed to update professor lookup\")\n",
    "                return False\n",
    "            \n",
    "            # Update professors with missing boss_names\n",
    "            self.update_professors_with_boss_names()\n",
    "            \n",
    "            # Process remaining tables with cache checking\n",
    "            if not self.process_remaining_tables():\n",
    "                logger.error(\"‚ùå Failed to process remaining tables\")\n",
    "                return False\n",
    "            \n",
    "            # Save all outputs\n",
    "            self.save_outputs()\n",
    "            \n",
    "            # Print summary\n",
    "            self.print_summary()\n",
    "            \n",
    "            logger.info(\"‚úÖ Phase 2 completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Phase 2 failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_phase1_outputs(self):\n",
    "        \"\"\"Save Phase 1 outputs (professors, courses, acad_terms)\"\"\"\n",
    "        # Save new professors (to verify folder for manual correction)\n",
    "        # Always create the file, even if empty\n",
    "        df = pd.DataFrame(self.new_professors) if self.new_professors else pd.DataFrame(columns=['id', 'name', 'boss_name', 'afterclass_name', 'original_scraped_name'])\n",
    "        df.to_csv(os.path.join(self.verify_dir, 'new_professors.csv'), index=False)\n",
    "        if self.new_professors:\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_professors)} new professors for review\")\n",
    "        else:\n",
    "            logger.info(f\"‚úÖ Created empty new_professors.csv (all professors already exist)\")\n",
    "        \n",
    "        # Save new courses (to verify folder)\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_courses)} new courses\")\n",
    "        \n",
    "        # Save course updates\n",
    "        if self.update_courses:\n",
    "            df = pd.DataFrame(self.update_courses)\n",
    "            df.to_csv(os.path.join(self.output_base, 'update_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.update_courses)} course updates\")\n",
    "        \n",
    "        # Save academic terms\n",
    "        if self.new_acad_terms:\n",
    "            df = pd.DataFrame(self.new_acad_terms)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_acad_term.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_acad_terms)} academic terms\")\n",
    "\n",
    "    def setup_boss_processing(self):\n",
    "        \"\"\"Initialize BOSS results processing with logging and caches\"\"\"\n",
    "        # Setup logging for BOSS processing\n",
    "        self.boss_log_file = os.path.join(self.output_base, 'boss_result_log.txt')\n",
    "        \n",
    "        # Create the log file and write header\n",
    "        try:\n",
    "            with open(self.boss_log_file, 'w') as f:\n",
    "                f.write(f\"BOSS Results Processing Log - {datetime.now().isoformat()}\\n\")\n",
    "                f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            print(f\"üìù Log file created: {self.boss_log_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not create log file {self.boss_log_file}: {e}\")\n",
    "            self.boss_log_file = None\n",
    "        \n",
    "        # Initialize existing classes cache\n",
    "        self.existing_classes_cache = []\n",
    "        \n",
    "        # Data storage for BOSS results\n",
    "        self.boss_data = []\n",
    "        self.failed_mappings = []\n",
    "        \n",
    "        # Output collectors\n",
    "        self.new_bid_windows = []\n",
    "        self.new_class_availability = []\n",
    "        self.new_bid_result = []\n",
    "        self.update_bid_result = []\n",
    "        \n",
    "        # Caches for deduplication\n",
    "        self.bid_window_cache = {}  # (acad_term_id, round, window) -> bid_window_id\n",
    "        \n",
    "        # PROPERLY INITIALIZE bid_window_id_counter from database cache\n",
    "        self.bid_window_id_counter = 1  # Default fallback\n",
    "        \n",
    "        # Load existing bid_window data and find max ID\n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, 'bid_window_cache.pkl')\n",
    "            if os.path.exists(cache_file):\n",
    "                bid_window_df = pd.read_pickle(cache_file)\n",
    "                if not bid_window_df.empty:\n",
    "                    max_id = bid_window_df['id'].max()\n",
    "                    self.bid_window_id_counter = max_id + 1\n",
    "                    \n",
    "                    # Build deduplication cache\n",
    "                    for _, row in bid_window_df.iterrows():\n",
    "                        window_key = (row['acad_term_id'], row['round'], row['window'])\n",
    "                        self.bid_window_cache[window_key] = row['id']\n",
    "                    \n",
    "                    self.log_boss_activity(f\"‚úÖ Loaded {len(bid_window_df)} existing bid windows, next ID will be {self.bid_window_id_counter}\")\n",
    "                else:\n",
    "                    self.log_boss_activity(\"‚ö†Ô∏è Bid window cache exists but is empty, starting from ID 1\")\n",
    "            else:\n",
    "                # Try to download from database if cache doesn't exist\n",
    "                if hasattr(self, 'connection') and self.connection:\n",
    "                    try:\n",
    "                        query = \"SELECT * FROM bid_window ORDER BY id\"\n",
    "                        bid_window_df = pd.read_sql_query(query, self.connection)\n",
    "                        if not bid_window_df.empty:\n",
    "                            # Save to cache for future use\n",
    "                            bid_window_df.to_pickle(cache_file)\n",
    "                            \n",
    "                            max_id = bid_window_df['id'].max()\n",
    "                            self.bid_window_id_counter = max_id + 1\n",
    "                            \n",
    "                            # Build deduplication cache\n",
    "                            for _, row in bid_window_df.iterrows():\n",
    "                                window_key = (row['acad_term_id'], row['round'], row['window'])\n",
    "                                self.bid_window_cache[window_key] = row['id']\n",
    "                            \n",
    "                            self.log_boss_activity(f\"‚úÖ Downloaded {len(bid_window_df)} bid windows from database, next ID will be {self.bid_window_id_counter}\")\n",
    "                        else:\n",
    "                            self.log_boss_activity(\"‚ö†Ô∏è Database bid_window table is empty, starting from ID 1\")\n",
    "                    except Exception as e:\n",
    "                        self.log_boss_activity(f\"‚ö†Ô∏è Could not download bid_window from database: {e}\")\n",
    "                else:\n",
    "                    self.log_boss_activity(\"‚ö†Ô∏è No bid window cache found and no database connection, starting from ID 1\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Error initializing bid_window counter: {e}\")\n",
    "            self.bid_window_id_counter = 1\n",
    "        \n",
    "        # Statistics\n",
    "        self.boss_stats = {\n",
    "            'files_processed': 0,\n",
    "            'total_rows': 0,\n",
    "            'bid_windows_created': 0,\n",
    "            'class_availability_created': 0,\n",
    "            'bid_results_created': 0,\n",
    "            'failed_mappings': 0\n",
    "        }\n",
    "        \n",
    "        print(\"üîÑ BOSS results processing setup completed\")\n",
    "\n",
    "    def log_boss_activity(self, message, print_to_stdout=True):\n",
    "        \"\"\"Log activity to both file and optionally stdout\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"[{timestamp}] {message}\\n\"\n",
    "        \n",
    "        # Only write to file if boss_log_file exists (after setup_boss_processing is called)\n",
    "        if hasattr(self, 'boss_log_file') and self.boss_log_file:\n",
    "            try:\n",
    "                with open(self.boss_log_file, 'a') as f:\n",
    "                    f.write(log_message)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Warning: Could not write to log file: {e}\")\n",
    "        \n",
    "        if print_to_stdout:\n",
    "            print(f\"üìù {message}\")\n",
    "\n",
    "    def parse_bidding_window(self, bidding_window_str):\n",
    "        \"\"\"Complete parser for bidding window string to extract round and window\n",
    "        \n",
    "        Examples:\n",
    "        \"Round 1 Window 1\" -> (\"1\", 1)\n",
    "        \"Round 1A Window 2\" -> (\"1A\", 2)\n",
    "        \"Round 2A Window 3\" -> (\"2A\", 3)\n",
    "        \"Incoming Exchange Rnd 1C Win 1\" -> (\"1C\", 1)\n",
    "        \"Incoming Freshmen Rnd 1 Win 4\" -> (\"1F\", 4)\n",
    "        \"\"\"\n",
    "        if not bidding_window_str or pd.isna(bidding_window_str):\n",
    "            return None, None\n",
    "        \n",
    "        # Clean the string\n",
    "        bidding_window_str = str(bidding_window_str).strip()\n",
    "        \n",
    "        # Pattern 1: Standard format \"Round X[A/B/C] Window Y\"\n",
    "        pattern1 = r'Round\\s+(\\w+)\\s+Window\\s+(\\d+)'\n",
    "        match1 = re.match(pattern1, bidding_window_str)\n",
    "        if match1:\n",
    "            round_str = match1.group(1)\n",
    "            window_num = int(match1.group(2))\n",
    "            return round_str, window_num\n",
    "        \n",
    "        # Pattern 2: Incoming Exchange format \"Incoming Exchange Rnd X[A/B/C] Win Y\"\n",
    "        # Map to same round but keep distinction if needed\n",
    "        pattern2 = r'Incoming\\s+Exchange\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)'\n",
    "        match2 = re.match(pattern2, bidding_window_str)\n",
    "        if match2:\n",
    "            round_str = match2.group(1)  # Keep original round (1C)\n",
    "            window_num = int(match2.group(2))\n",
    "            return round_str, window_num\n",
    "        \n",
    "        # Pattern 3: Incoming Freshmen format \"Incoming Freshmen Rnd X Win Y\"\n",
    "        # Map Round 1 -> Round 1F for distinction\n",
    "        pattern3 = r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)'\n",
    "        match3 = re.match(pattern3, bidding_window_str)\n",
    "        if match3:\n",
    "            original_round = match3.group(1)\n",
    "            window_num = int(match3.group(2))\n",
    "            # Map Incoming Freshmen Round 1 to Round 1F\n",
    "            if original_round == \"1\":\n",
    "                round_str = \"1F\"\n",
    "            else:\n",
    "                round_str = f\"{original_round}F\"  # For other rounds if they exist\n",
    "            return round_str, window_num\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    def load_boss_results(self):\n",
    "        \"\"\"Load BOSS results from raw_data.xlsx standalone sheet\"\"\"\n",
    "        self.log_boss_activity(\"üîç Loading BOSS results from raw_data.xlsx...\")\n",
    "        \n",
    "        # Use existing standalone_data that's already loaded\n",
    "        if not hasattr(self, 'standalone_data') or self.standalone_data is None:\n",
    "            self.log_boss_activity(\"‚ùå No standalone data loaded\")\n",
    "            return False\n",
    "        \n",
    "        # Filter rows that have bidding data using the correct column names\n",
    "        bidding_data = self.standalone_data[\n",
    "            self.standalone_data['bidding_window'].notna() & \n",
    "            self.standalone_data['total'].notna()\n",
    "        ].copy()\n",
    "        \n",
    "        if bidding_data.empty:\n",
    "            self.log_boss_activity(\"‚ùå No bidding data found in raw_data.xlsx\")\n",
    "            return False\n",
    "        \n",
    "        self.boss_data = bidding_data\n",
    "        self.boss_stats['total_rows'] = len(self.boss_data)\n",
    "        self.boss_stats['files_processed'] = 1\n",
    "        \n",
    "        self.log_boss_activity(f\"‚úÖ Loaded {self.boss_stats['total_rows']} bidding records from raw_data.xlsx\")\n",
    "        return True\n",
    "\n",
    "    def process_bid_windows(self):\n",
    "        \"\"\"Process and create bid_window entries from raw_data bidding_window column\"\"\"\n",
    "        self.log_boss_activity(\"ü™ü Processing bid windows from raw_data...\")\n",
    "        \n",
    "        if self.boss_data is None or len(self.boss_data) == 0:\n",
    "            self.log_boss_activity(\"‚ùå No BOSS data loaded\")\n",
    "            return False\n",
    "        \n",
    "        # Track all unique bid windows found in data\n",
    "        found_windows = defaultdict(set)  # acad_term_id -> set of (round, window) tuples\n",
    "        \n",
    "        # Discover all windows that exist in the data\n",
    "        for _, row in self.boss_data.iterrows():\n",
    "            acad_term_id = row.get('acad_term_id')\n",
    "            bidding_window_str = row.get('bidding_window')\n",
    "            \n",
    "            if pd.isna(acad_term_id) or pd.isna(bidding_window_str):\n",
    "                continue\n",
    "            \n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            \n",
    "            if acad_term_id and round_str and window_num:\n",
    "                found_windows[acad_term_id].add((round_str, window_num))\n",
    "        \n",
    "        # Use the counter that was set from existing data\n",
    "        bid_window_id = self.bid_window_id_counter\n",
    "        round_order = {'1': 1, '1A': 2, '1B': 3, '1C': 4, '1F': 5, '2': 6, '2A': 7}\n",
    "        \n",
    "        for acad_term_id in sorted(found_windows.keys()):\n",
    "            windows_for_term = found_windows[acad_term_id]\n",
    "            sorted_windows = sorted(windows_for_term, key=lambda x: (round_order.get(x[0], 99), x[1]))\n",
    "            \n",
    "            self.log_boss_activity(f\"üìÖ Processing {acad_term_id}: found {len(sorted_windows)} windows\")\n",
    "            \n",
    "            for round_str, window_num in sorted_windows:\n",
    "                window_key = (acad_term_id, round_str, window_num)\n",
    "                \n",
    "                # Skip if already exists in database\n",
    "                if window_key in self.bid_window_cache:\n",
    "                    self.log_boss_activity(f\"‚è≠Ô∏è Bid window already exists: {acad_term_id} Round {round_str} Window {window_num}\")\n",
    "                    continue\n",
    "                \n",
    "                new_bid_window = {\n",
    "                    'id': bid_window_id,\n",
    "                    'acad_term_id': acad_term_id,\n",
    "                    'round': round_str,\n",
    "                    'window': window_num\n",
    "                }\n",
    "                \n",
    "                self.new_bid_windows.append(new_bid_window)\n",
    "                self.bid_window_cache[window_key] = bid_window_id\n",
    "                self.boss_stats['bid_windows_created'] += 1\n",
    "                \n",
    "                self.log_boss_activity(f\"‚úÖ Created bid_window {bid_window_id}: {acad_term_id} Round {round_str} Window {window_num}\")\n",
    "                bid_window_id += 1\n",
    "        \n",
    "        self.bid_window_id_counter = bid_window_id\n",
    "        self.log_boss_activity(f\"‚úÖ Created {self.boss_stats['bid_windows_created']} bid windows\")\n",
    "        return True\n",
    "\n",
    "    def get_course_id(self, course_code):\n",
    "        \"\"\"Get course_id from course_code, checking multiple sources\"\"\"\n",
    "        # Check courses cache (from database)\n",
    "        if course_code in self.courses_cache:\n",
    "            return self.courses_cache[course_code]['id']\n",
    "        \n",
    "        # Check in new_courses (newly created)\n",
    "        for course in self.new_courses:\n",
    "            if course['code'] == course_code:\n",
    "                return course['id']\n",
    "        \n",
    "        # Check new_courses.csv file\n",
    "        try:\n",
    "            new_courses_path = os.path.join(self.output_base, 'new_courses.csv')\n",
    "            verify_courses_path = os.path.join(self.verify_dir, 'new_courses.csv')\n",
    "            \n",
    "            for path in [verify_courses_path, new_courses_path]:\n",
    "                if os.path.exists(path):\n",
    "                    df = pd.read_csv(path)\n",
    "                    matching_courses = df[df['code'] == course_code]\n",
    "                    if not matching_courses.empty:\n",
    "                        return matching_courses.iloc[0]['id']\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Error reading new_courses.csv: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def load_existing_classes_cache(self):\n",
    "        \"\"\"Load existing classes from database cache with proper full extraction\"\"\"\n",
    "        self.existing_classes_cache = []\n",
    "        \n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, 'classes_cache.pkl')\n",
    "            \n",
    "            # Try loading from cache file first\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    classes_df = pd.read_pickle(cache_file)\n",
    "                    if not classes_df.empty:\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        logger.info(f\"üìö Loaded {len(self.existing_classes_cache)} existing classes from cache\")\n",
    "                        return\n",
    "                    else:\n",
    "                        logger.info(\"‚ö†Ô∏è Cache file exists but is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Error reading cache file: {e}\")\n",
    "            \n",
    "            # If cache doesn't exist or is empty, try database with SELECT *\n",
    "            if self.connection:\n",
    "                try:\n",
    "                    query = \"SELECT * FROM classes\"\n",
    "                    classes_df = pd.read_sql_query(query, self.connection)\n",
    "                    if not classes_df.empty:\n",
    "                        # Save to cache for future use\n",
    "                        classes_df.to_pickle(cache_file)\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        logger.info(f\"üìö Downloaded and cached {len(self.existing_classes_cache)} existing classes\")\n",
    "                        return\n",
    "                    else:\n",
    "                        logger.warning(\"‚ö†Ô∏è Database classes table is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Error downloading classes from database: {e}\")\n",
    "            \n",
    "            # Final fallback\n",
    "            logger.warning(\"‚ö†Ô∏è All class loading methods failed - using empty cache\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.existing_classes_cache = []\n",
    "            logger.error(f\"‚ö†Ô∏è Critical error in load_existing_classes_cache: {e}\")\n",
    "\n",
    "    def process_class_availability(self):\n",
    "        \"\"\"\n",
    "        Process class availability data, preventing the creation of duplicate records\n",
    "        by checking against existing cache data first.\n",
    "        \"\"\"\n",
    "        self.log_boss_activity(\"üìä Processing class availability from raw_data...\")\n",
    "        \n",
    "        # === STEP 1: Determine Current Bidding Window ===\n",
    "        now = datetime.now()\n",
    "        current_window_name = None\n",
    "        \n",
    "        # Get the bidding schedule for the current term\n",
    "        bidding_schedule_for_term = BIDDING_SCHEDULES.get(START_AY_TERM, [])\n",
    "        \n",
    "        if bidding_schedule_for_term:\n",
    "            # Find the current window (first future window)\n",
    "            for i, (results_date, window_name, folder_suffix) in enumerate(bidding_schedule_for_term):\n",
    "                if now < results_date:\n",
    "                    current_window_name = window_name\n",
    "                    break\n",
    "            \n",
    "            # If no future window found, we're past all scheduled windows\n",
    "            if current_window_name is None and bidding_schedule_for_term:\n",
    "                # Use the last window as current\n",
    "                current_window_name = bidding_schedule_for_term[-1][1]\n",
    "        \n",
    "        logger.info(f\"üéØ Processing class availability for current window: '{current_window_name}'\")\n",
    "\n",
    "        # === STEP 2: Filter the data to only current window and current term records ===\n",
    "        if current_window_name and hasattr(self, 'standalone_data') and not self.standalone_data.empty:\n",
    "            if 'bidding_window' in self.standalone_data.columns:\n",
    "                original_count = len(self.standalone_data)\n",
    "                \n",
    "                # Filter by bidding window\n",
    "                current_window_data = self.standalone_data[\n",
    "                    self.standalone_data['bidding_window'] == current_window_name\n",
    "                ].copy()\n",
    "                \n",
    "                # Also filter by current academic term to prevent cross-term contamination\n",
    "                if 'acad_term_id' in current_window_data.columns:\n",
    "                    # Extract expected term from START_AY_TERM (e.g., '2025-26_T1' -> 'AY202526T1')\n",
    "                    expected_term_id = START_AY_TERM.replace('-', '').replace('_', '')\n",
    "                    expected_term_id = f\"AY{expected_term_id}\"\n",
    "                    \n",
    "                    before_term_filter = len(current_window_data)\n",
    "                    current_window_data = current_window_data[\n",
    "                        current_window_data['acad_term_id'] == expected_term_id\n",
    "                    ].copy()\n",
    "                    \n",
    "                    self.log_boss_activity(f\"üîΩ Filtered data: {original_count} ‚Üí {before_term_filter} (window) ‚Üí {len(current_window_data)} (window + term)\")\n",
    "                    self.log_boss_activity(f\"    Window filter: '{current_window_name}', Term filter: '{expected_term_id}'\")\n",
    "                else:\n",
    "                    self.log_boss_activity(f\"üîΩ Filtered data from {original_count} to {len(current_window_data)} records for current window: '{current_window_name}'\")\n",
    "            else:\n",
    "                self.log_boss_activity(\"‚ö†Ô∏è No 'bidding_window' column found - processing all data\")\n",
    "                current_window_data = self.standalone_data.copy()\n",
    "        else:\n",
    "            self.log_boss_activity(\"‚ö†Ô∏è Could not determine current window or no standalone data - processing all data\")\n",
    "            current_window_data = self.standalone_data.copy() if hasattr(self, 'standalone_data') else pd.DataFrame()\n",
    "        \n",
    "        # Load existing class availability data to prevent duplicates\n",
    "        existing_availability_keys = set()\n",
    "        cache_file = os.path.join(self.cache_dir, 'class_availability_cache.pkl')\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                existing_df = pd.read_pickle(cache_file)\n",
    "                if not existing_df.empty:\n",
    "                    for _, record in existing_df.iterrows():\n",
    "                        key = (record['class_id'], record['bid_window_id'])\n",
    "                        existing_availability_keys.add(key)\n",
    "                    self.log_boss_activity(f\"‚úÖ Pre-loaded {len(existing_availability_keys)} existing class availability keys from cache.\")\n",
    "            except Exception as e:\n",
    "                self.log_boss_activity(f\"‚ö†Ô∏è Could not pre-load class_availability_cache: {e}\")\n",
    "        \n",
    "        # ADDED: Track keys from current run to prevent duplicates within the same processing\n",
    "        current_run_keys = set()\n",
    "        for availability_record in self.new_class_availability:\n",
    "            key = (availability_record['class_id'], availability_record['bid_window_id'])\n",
    "            current_run_keys.add(key)\n",
    "\n",
    "        newly_created_count = 0\n",
    "        updated_count = 0\n",
    "        \n",
    "        # === STEP 3: Process only the filtered current window data ===\n",
    "        for _, row in current_window_data.iterrows():\n",
    "            course_code = row.get('course_code')\n",
    "            section = row.get('section')\n",
    "            acad_term_id = row.get('acad_term_id')\n",
    "            bidding_window_str = row.get('bidding_window')\n",
    "            \n",
    "            if pd.isna(course_code) or pd.isna(section) or pd.isna(acad_term_id) or pd.isna(bidding_window_str):\n",
    "                continue\n",
    "            \n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            if not all([round_str, window_num]):\n",
    "                continue\n",
    "            \n",
    "            class_boss_id = row.get('class_boss_id')\n",
    "            class_ids = self.find_all_class_ids(acad_term_id, class_boss_id)\n",
    "\n",
    "            if not class_ids:\n",
    "                failed_row = {\n",
    "                    'course_code': course_code, 'section': section, 'acad_term_id': acad_term_id,\n",
    "                    'bidding_window_str': bidding_window_str, 'reason': 'class_not_found'\n",
    "                }\n",
    "                self.failed_mappings.append(failed_row)\n",
    "                self.boss_stats['failed_mappings'] += 1\n",
    "                continue\n",
    "            \n",
    "            window_key = (acad_term_id, round_str, window_num)\n",
    "            bid_window_id = self.bid_window_cache.get(window_key)\n",
    "            if not bid_window_id:\n",
    "                self.log_boss_activity(f\"‚ö†Ô∏è No bid_window_id for {window_key}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract values safely\n",
    "            total_val = int(row.get('total')) if pd.notna(row.get('total')) else 0\n",
    "            current_enrolled_val = int(row.get('current_enrolled')) if pd.notna(row.get('current_enrolled')) else 0\n",
    "            reserved_val = int(row.get('reserved')) if pd.notna(row.get('reserved')) else 0\n",
    "            available_val = int(row.get('available')) if pd.notna(row.get('available')) else 0\n",
    "            \n",
    "            for class_id in class_ids:\n",
    "                # Check for existence in both existing data and current run\n",
    "                availability_key = (class_id, bid_window_id)\n",
    "                \n",
    "                # FIXED: Check both existing and current run keys\n",
    "                if availability_key in existing_availability_keys or availability_key in current_run_keys:\n",
    "                    # Check if update is needed\n",
    "                    if availability_key in existing_availability_keys:\n",
    "                        # Could implement update logic here if needed\n",
    "                        pass\n",
    "                    continue\n",
    "\n",
    "                # Create new record\n",
    "                availability_record = {\n",
    "                    'class_id': class_id,\n",
    "                    'bid_window_id': bid_window_id,\n",
    "                    'total': total_val,\n",
    "                    'current_enrolled': current_enrolled_val,\n",
    "                    'reserved': reserved_val,\n",
    "                    'available': available_val\n",
    "                }\n",
    "                \n",
    "                self.new_class_availability.append(availability_record)\n",
    "                current_run_keys.add(availability_key)\n",
    "                newly_created_count += 1\n",
    "        \n",
    "        self.boss_stats['class_availability_created'] = newly_created_count\n",
    "        self.log_boss_activity(f\"‚úÖ Class availability checks complete. Created {newly_created_count} new records, Updated {updated_count} records.\")\n",
    "        return True\n",
    "\n",
    "    def process_bid_results(self):\n",
    "        \"\"\"\n",
    "        Process bid data from raw_data.xlsx. Creates update records when median/min data exists,\n",
    "        and new records only when they don't exist yet.\n",
    "        \"\"\"\n",
    "        logger.info(\"üìà Processing bid results from raw_data...\")\n",
    "        \n",
    "        # Ensure the list for update records exists\n",
    "        if not hasattr(self, 'update_bid_result'):\n",
    "            self.update_bid_result = []\n",
    "\n",
    "        # === STEP 1: Determine Current and Previous Bidding Windows ===\n",
    "        now = datetime.now()\n",
    "        current_window_name = None\n",
    "        previous_window_name = None\n",
    "        \n",
    "        # Get the bidding schedule for the current term\n",
    "        bidding_schedule_for_term = BIDDING_SCHEDULES.get(START_AY_TERM, [])\n",
    "        \n",
    "        if bidding_schedule_for_term:\n",
    "            # Find the current window (first future window) and previous window\n",
    "            for i, (results_date, window_name, folder_suffix) in enumerate(bidding_schedule_for_term):\n",
    "                if now < results_date:\n",
    "                    current_window_name = window_name\n",
    "                    # Previous window is the one before current (if exists)\n",
    "                    if i > 0:\n",
    "                        previous_window_name = bidding_schedule_for_term[i-1][1]\n",
    "                    break\n",
    "            \n",
    "            # If no future window found, we're past all scheduled windows\n",
    "            if current_window_name is None and bidding_schedule_for_term:\n",
    "                # Use the last window as current, and second-to-last as previous\n",
    "                current_window_name = bidding_schedule_for_term[-1][1]\n",
    "                if len(bidding_schedule_for_term) > 1:\n",
    "                    previous_window_name = bidding_schedule_for_term[-2][1]\n",
    "        \n",
    "        logger.info(f\"üéØ Determined bidding windows - Current: '{current_window_name}', Previous: '{previous_window_name}'\")\n",
    "\n",
    "        # === STEP 2: Filter the data to only current window and current term records ===\n",
    "        # For new bid results, we only want data from the CURRENT window and CURRENT term\n",
    "        if current_window_name and hasattr(self, 'standalone_data') and not self.standalone_data.empty:\n",
    "            if 'bidding_window' in self.standalone_data.columns:\n",
    "                original_count = len(self.standalone_data)\n",
    "                \n",
    "                # Filter by bidding window\n",
    "                current_window_data = self.standalone_data[\n",
    "                    self.standalone_data['bidding_window'] == current_window_name\n",
    "                ].copy()\n",
    "                \n",
    "                # Also filter by current academic term to prevent cross-term contamination\n",
    "                if 'acad_term_id' in current_window_data.columns:\n",
    "                    # Extract expected term from START_AY_TERM (e.g., '2025-26_T1' -> 'AY202526T1')\n",
    "                    expected_term_id = START_AY_TERM.replace('-', '').replace('_', '')\n",
    "                    expected_term_id = f\"AY{expected_term_id}\"\n",
    "                    \n",
    "                    before_term_filter = len(current_window_data)\n",
    "                    current_window_data = current_window_data[\n",
    "                        current_window_data['acad_term_id'] == expected_term_id\n",
    "                    ].copy()\n",
    "                    \n",
    "                    logger.info(f\"üîΩ Filtered standalone_data: {original_count} ‚Üí {before_term_filter} (window) ‚Üí {len(current_window_data)} (window + term)\")\n",
    "                    logger.info(f\"    Window filter: '{current_window_name}', Term filter: '{expected_term_id}'\")\n",
    "                else:\n",
    "                    logger.info(f\"üîΩ Filtered standalone_data from {original_count} to {len(current_window_data)} records for current window: '{current_window_name}'\")\n",
    "            else:\n",
    "                logger.warning(\"‚ö†Ô∏è No 'bidding_window' column found - processing all data\")\n",
    "                current_window_data = self.standalone_data.copy()\n",
    "        else:\n",
    "            logger.warning(\"‚ö†Ô∏è Could not determine current window or no standalone data - processing all data\")\n",
    "            current_window_data = self.standalone_data.copy() if hasattr(self, 'standalone_data') else pd.DataFrame()\n",
    "\n",
    "        # Load existing bid_result data to check for duplicates\n",
    "        existing_bid_result_keys = set()\n",
    "        existing_bid_results = {}  # Store full records for update comparison\n",
    "        cache_file = os.path.join(self.cache_dir, 'bid_result_cache.pkl')\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                existing_df = pd.read_pickle(cache_file)\n",
    "                if not existing_df.empty:\n",
    "                    for _, record in existing_df.iterrows():\n",
    "                        key = (record['bid_window_id'], record['class_id'])\n",
    "                        existing_bid_result_keys.add(key)\n",
    "                        existing_bid_results[key] = record.to_dict()\n",
    "                    self.log_boss_activity(f\"‚úÖ Pre-loaded {len(existing_bid_result_keys)} existing bid result keys from cache.\")\n",
    "            except Exception as e:\n",
    "                self.log_boss_activity(f\"‚ö†Ô∏è Could not pre-load bid_result_cache: {e}\")\n",
    "\n",
    "        newly_created_count = 0\n",
    "        updated_count = 0\n",
    "        \n",
    "        # DEBUG: Check column names in the data\n",
    "        if not current_window_data.empty:\n",
    "            # Check for any rows with median/min data\n",
    "            median_cols = [col for col in current_window_data.columns if 'median' in col.lower() or 'bid' in col.lower()]\n",
    "            min_cols = [col for col in current_window_data.columns if 'min' in col.lower()]\n",
    "        \n",
    "        # === STEP 3: Process only the filtered current window data ===\n",
    "        for idx, row in current_window_data.iterrows():\n",
    "            try:\n",
    "                course_code = row.get('course_code')\n",
    "                section = row.get('section')\n",
    "                acad_term_id = row.get('acad_term_id')\n",
    "                class_boss_id = row.get('class_boss_id')\n",
    "                bidding_window_str = row.get('bidding_window')\n",
    "                \n",
    "                if pd.isna(acad_term_id) or pd.isna(class_boss_id):\n",
    "                    continue\n",
    "                \n",
    "                round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "                if not all([round_str, window_num]):\n",
    "                    continue\n",
    "                \n",
    "                class_ids = self.find_all_class_ids(acad_term_id, class_boss_id)\n",
    "                if not class_ids:\n",
    "                    continue\n",
    "                \n",
    "                window_key = (acad_term_id, round_str, window_num)\n",
    "                bid_window_id = self.bid_window_cache.get(window_key)\n",
    "                if not bid_window_id:\n",
    "                    continue\n",
    "\n",
    "                # FIXED: Check all possible column names for median and min\n",
    "                median_bid = None\n",
    "                min_bid = None\n",
    "                \n",
    "                # Try all possible column names for median\n",
    "                median_column_names = ['median', 'Median', 'Median Bid', 'median_bid', 'Median_Bid', 'MEDIAN']\n",
    "                for col_name in median_column_names:\n",
    "                    if col_name in row.index:\n",
    "                        val = row[col_name]\n",
    "                        if pd.notna(val):\n",
    "                            median_bid = val\n",
    "                            if idx < 5:  # Log first few for debugging\n",
    "                                self.log_boss_activity(f\"üîç DEBUG Row {idx}: Found median value {median_bid} in column '{col_name}'\")\n",
    "                            break\n",
    "                \n",
    "                # Try all possible column names for min\n",
    "                min_column_names = ['min', 'Min', 'Min Bid', 'min_bid', 'Min_Bid', 'MIN']\n",
    "                for col_name in min_column_names:\n",
    "                    if col_name in row.index:\n",
    "                        val = row[col_name]\n",
    "                        if pd.notna(val):\n",
    "                            min_bid = val\n",
    "                            if idx < 5:  # Log first few for debugging\n",
    "                                self.log_boss_activity(f\"üîç DEBUG Row {idx}: Found min value {min_bid} in column '{col_name}'\")\n",
    "                            break\n",
    "                \n",
    "                has_bid_data = pd.notna(median_bid) or pd.notna(min_bid)\n",
    "                \n",
    "                # DEBUG: Log if we have bid data\n",
    "                if has_bid_data and idx < 10:\n",
    "                    self.log_boss_activity(f\"üîç DEBUG: Row {idx} {course_code}-{section} has bid data: median={median_bid}, min={min_bid}\")\n",
    "\n",
    "                # Prepare data record\n",
    "                def safe_int(val): return int(val) if pd.notna(val) else None\n",
    "                def safe_float(val): return float(val) if pd.notna(val) else None\n",
    "                \n",
    "                total_val = safe_int(row.get('total'))\n",
    "                enrolled_val = safe_int(row.get('current_enrolled'))\n",
    "                \n",
    "                for class_id in class_ids:\n",
    "                    # Check if record exists\n",
    "                    bid_result_key = (bid_window_id, class_id)\n",
    "                    \n",
    "                    result_data = {\n",
    "                        'bid_window_id': bid_window_id, \n",
    "                        'class_id': class_id,\n",
    "                        'vacancy': total_val,\n",
    "                        'opening_vacancy': safe_int(row.get('opening_vacancy')),\n",
    "                        'before_process_vacancy': total_val - enrolled_val if total_val is not None and enrolled_val is not None else None,\n",
    "                        'dice': safe_int(row.get('d_i_c_e') or row.get('dice')),\n",
    "                        'after_process_vacancy': safe_int(row.get('after_process_vacancy')),\n",
    "                        'enrolled_students': enrolled_val,\n",
    "                        'median': safe_float(median_bid),\n",
    "                        'min': safe_float(min_bid)\n",
    "                    }\n",
    "\n",
    "                    if bid_result_key in existing_bid_result_keys:\n",
    "                        # Check if update is needed\n",
    "                        existing_record = existing_bid_results.get(bid_result_key, {})\n",
    "                        needs_update = False\n",
    "                        \n",
    "                        # Check if median or min values have changed\n",
    "                        if has_bid_data:\n",
    "                            if (pd.notna(median_bid) and safe_float(median_bid) != existing_record.get('median')):\n",
    "                                needs_update = True\n",
    "                            if (pd.notna(min_bid) and safe_float(min_bid) != existing_record.get('min')):\n",
    "                                needs_update = True\n",
    "                        \n",
    "                        # Also check other fields for updates\n",
    "                        for field in ['vacancy', 'opening_vacancy', 'before_process_vacancy', 'dice', \n",
    "                                    'after_process_vacancy', 'enrolled_students']:\n",
    "                            if result_data.get(field) is not None and result_data[field] != existing_record.get(field):\n",
    "                                needs_update = True\n",
    "                        \n",
    "                        if needs_update:\n",
    "                            self.update_bid_result.append(result_data)\n",
    "                            updated_count += 1\n",
    "                            if has_bid_data:\n",
    "                                self.log_boss_activity(f\"üìä Update bid_result: {course_code}-{section} with median={median_bid}, min={min_bid}\")\n",
    "                    else:\n",
    "                        # This is a NEW record\n",
    "                        self.new_bid_result.append(result_data)\n",
    "                        existing_bid_result_keys.add(bid_result_key)\n",
    "                        newly_created_count += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing bid result row for {row.get('course_code')}-{row.get('section')}: {e}\")\n",
    "\n",
    "        self.boss_stats['bid_results_created'] += newly_created_count\n",
    "        self.log_boss_activity(f\"‚úÖ Bid result checks complete. Created: {newly_created_count}, Updated: {updated_count}.\")\n",
    "        return True\n",
    "\n",
    "    def find_all_class_ids(self, acad_term_id, class_boss_id):\n",
    "        \"\"\"Finds all class_ids for a given acad_term_id and class_boss_id.\n",
    "        Returns ALL class records for multi-professor classes.\"\"\"\n",
    "        \n",
    "        if pd.isna(acad_term_id) or pd.isna(class_boss_id):\n",
    "            return []\n",
    "\n",
    "        found_class_ids = []\n",
    "\n",
    "        # Source 1: Check newly created classes in this run\n",
    "        if hasattr(self, 'new_classes') and self.new_classes:\n",
    "            for class_obj in self.new_classes:\n",
    "                if (class_obj.get('acad_term_id') == acad_term_id and \n",
    "                    str(class_obj.get('boss_id')) == str(class_boss_id)):\n",
    "                    found_class_ids.append(class_obj['id'])\n",
    "\n",
    "        # Source 2: Check classes that existed before this run (from cache)\n",
    "        if hasattr(self, 'existing_classes_cache') and self.existing_classes_cache:\n",
    "            for class_obj in self.existing_classes_cache:\n",
    "                if (class_obj.get('acad_term_id') == acad_term_id and \n",
    "                    str(class_obj.get('boss_id')) == str(class_boss_id)):\n",
    "                    found_class_ids.append(class_obj['id'])\n",
    "        \n",
    "        # Source 3: Check new_classes.csv file if cache is incomplete\n",
    "        try:\n",
    "            new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "            if os.path.exists(new_classes_path):\n",
    "                df = pd.read_csv(new_classes_path)\n",
    "                # This logic now correctly searches the CSV using boss_id\n",
    "                matching_classes = df[\n",
    "                    (df['acad_term_id'] == acad_term_id) & \n",
    "                    (df['boss_id'].astype(str) == str(class_boss_id))\n",
    "                ]\n",
    "                for _, row in matching_classes.iterrows():\n",
    "                    if row['id'] not in found_class_ids:\n",
    "                        found_class_ids.append(row['id'])\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Error reading new_classes.csv: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_class_ids = []\n",
    "        seen = set()\n",
    "        for class_id in found_class_ids:\n",
    "            if class_id not in seen:\n",
    "                unique_class_ids.append(class_id)\n",
    "                seen.add(class_id)\n",
    "        \n",
    "        # Debug logging for multi-professor classes\n",
    "        if len(unique_class_ids) > 1:\n",
    "            self.log_boss_activity(f\"üìö Found {len(unique_class_ids)} class records for boss_id {class_boss_id}: multi-professor class\")\n",
    "        \n",
    "        return unique_class_ids\n",
    "\n",
    "    def save_boss_outputs(self):\n",
    "        \"\"\"Save all BOSS-related output files\"\"\"\n",
    "        self.log_boss_activity(\"üíæ Saving BOSS output files...\")\n",
    "        \n",
    "        # Save bid windows\n",
    "        if self.new_bid_windows:\n",
    "            df = pd.DataFrame(self.new_bid_windows)\n",
    "            output_path = os.path.join(self.output_base, 'new_bid_window.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚úÖ Saved {len(self.new_bid_windows)} bid windows to new_bid_window.csv\")\n",
    "        \n",
    "        # Save class availability\n",
    "        if self.new_class_availability:\n",
    "            df = pd.DataFrame(self.new_class_availability)\n",
    "            output_path = os.path.join(self.output_base, 'new_class_availability.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚úÖ Saved {len(self.new_class_availability)} availability records to new_class_availability.csv\")\n",
    "        \n",
    "        # Save bid results\n",
    "        if self.new_bid_result:\n",
    "            df = pd.DataFrame(self.new_bid_result)\n",
    "            output_path = os.path.join(self.output_base, 'new_bid_result.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚úÖ Saved {len(self.new_bid_result)} bid results to new_bid_result.csv\")\n",
    "        \n",
    "        # Save failed mappings\n",
    "        if self.failed_mappings:\n",
    "            df = pd.DataFrame(self.failed_mappings)\n",
    "            output_path = os.path.join(self.output_base, 'failed_boss_results_mapping.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Saved {len(self.failed_mappings)} failed mappings to failed_boss_results_mapping.csv\")\n",
    "        \n",
    "        self.log_boss_activity(\"‚úÖ All BOSS output files saved successfully\")\n",
    "\n",
    "    def print_boss_summary(self):\n",
    "        \"\"\"Print BOSS processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä BOSS RESULTS PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"üìÇ Files processed: {self.boss_stats['files_processed']}\")\n",
    "        print(f\"üìÑ Total rows: {self.boss_stats['total_rows']}\")\n",
    "        print(f\"ü™ü Bid windows created: {self.boss_stats['bid_windows_created']}\")\n",
    "        print(f\"üìä Class availability records: {self.boss_stats['class_availability_created']}\")\n",
    "        print(f\"üìà Bid result records: {self.boss_stats['bid_results_created']}\")\n",
    "        print(f\"‚ùå Failed mappings: {self.boss_stats['failed_mappings']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìÅ OUTPUT FILES:\")\n",
    "        print(f\"   - new_bid_window.csv ({self.boss_stats['bid_windows_created']} records)\")\n",
    "        print(f\"   - new_class_availability.csv ({self.boss_stats['class_availability_created']} records)\")\n",
    "        print(f\"   - new_bid_result.csv ({self.boss_stats['bid_results_created']} records)\")\n",
    "        if self.boss_stats['failed_mappings'] > 0:\n",
    "            print(f\"   - failed_boss_results_mapping.csv ({self.boss_stats['failed_mappings']} records)\")\n",
    "        print(f\"   - boss_result_log.txt (processing log)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def run_phase3_boss_processing(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the entire bidding data processing workflow with a robust, linear order of operations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"üöÄ Starting Enhanced Phase 3: Final, Robust Workflow\")\n",
    "            print(\"============================================================\")\n",
    "\n",
    "            logger.info(\"üõ†Ô∏è Pre-Phase 3: Updating cache with newly created records from previous phases...\")\n",
    "            \n",
    "            # Update professors cache\n",
    "            new_profs_path = os.path.join(self.verify_dir, 'new_professors.csv')\n",
    "            prof_cache_path = os.path.join(self.cache_dir, 'professors_cache.pkl')\n",
    "            \n",
    "            if os.path.exists(new_profs_path) and os.path.exists(prof_cache_path):\n",
    "                new_profs_df = pd.read_csv(new_profs_path)\n",
    "                if not new_profs_df.empty:\n",
    "                    # Drop columns from new_profs_df that are not in the main professors table\n",
    "                    # This prevents errors if new_professors.csv has extra columns\n",
    "                    prof_cache_df = pd.read_pickle(prof_cache_path)\n",
    "                    new_profs_df = new_profs_df[[col for col in new_profs_df.columns if col in prof_cache_df.columns]]\n",
    "                    \n",
    "                    combined_profs_df = pd.concat([prof_cache_df, new_profs_df], ignore_index=True).drop_duplicates(subset=['id'])\n",
    "                    combined_profs_df.to_pickle(prof_cache_path)\n",
    "                    logger.info(f\"   ‚úÖ Updated professors_cache.pkl with {len(new_profs_df)} new records.\")\n",
    "\n",
    "            # Update classes cache (similar logic)\n",
    "            new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "            class_cache_path = os.path.join(self.cache_dir, 'classes_cache.pkl')\n",
    "\n",
    "            if os.path.exists(new_classes_path) and os.path.exists(class_cache_path):\n",
    "                new_classes_df = pd.read_csv(new_classes_path)\n",
    "                if not new_classes_df.empty:\n",
    "                    class_cache_df = pd.read_pickle(class_cache_path)\n",
    "                    combined_classes_df = pd.concat([class_cache_df, new_classes_df], ignore_index=True).drop_duplicates(subset=['id'])\n",
    "                    combined_classes_df.to_pickle(class_cache_path)\n",
    "                    logger.info(f\"   ‚úÖ Updated classes_cache.pkl with {len(new_classes_df)} new records.\")\n",
    "\n",
    "            self.setup_boss_processing()\n",
    "\n",
    "            # --- Step 1: Load ALL required data from cache and input files ---\n",
    "            self.log_boss_activity(\"üîÑ Loading all required data caches with freshness check and combination...\")\n",
    "            # Use the method that combines new data from CSVs before validation\n",
    "            if not self.load_or_cache_data_with_freshness_check(): return False\n",
    "            \n",
    "            self.log_boss_activity(\"üîÑ Loading all raw input files...\")\n",
    "            if not self.load_raw_data(): return False\n",
    "            self.overall_boss_results_df = self.load_overall_boss_results() # Load this once\n",
    "\n",
    "            # --- Step 2: Process base entities to establish a stable state ---\n",
    "            self.log_boss_activity(\"üîÑ Processing base entities (Courses, Classes, Timings)...\")\n",
    "            self.process_acad_terms()\n",
    "            self.process_professors()\n",
    "            self.process_courses() \n",
    "            self.process_classes()  # This now correctly handles create vs. update\n",
    "            self.process_timings()  # This now receives stable, non-duplicate class_ids\n",
    "\n",
    "            # --- Step 3: Sequential catch-up processing for all windows ---\n",
    "            self.log_boss_activity(\"üîÑ Starting sequential catch-up processing...\")\n",
    "\n",
    "            # Determine current live window and processing range\n",
    "            now = datetime.now()\n",
    "            current_window_index = None\n",
    "            bidding_schedule_for_term = BIDDING_SCHEDULES.get(START_AY_TERM, [])\n",
    "\n",
    "            if not bidding_schedule_for_term:\n",
    "                self.log_boss_activity(\"‚ùå No bidding schedule found for current term\")\n",
    "                return False\n",
    "\n",
    "            # Find current live window (first future window)\n",
    "            for i, (results_date, window_name, folder_suffix) in enumerate(bidding_schedule_for_term):\n",
    "                if now < results_date:\n",
    "                    current_window_index = i\n",
    "                    break\n",
    "\n",
    "            if current_window_index is None:\n",
    "                # Past all windows, use last window as current\n",
    "                current_window_index = len(bidding_schedule_for_term) - 1\n",
    "\n",
    "            current_window_name = bidding_schedule_for_term[current_window_index][1]\n",
    "            self.log_boss_activity(f\"üéØ Current live window: {current_window_name}\")\n",
    "\n",
    "            # Processing range: from start to current window (inclusive)\n",
    "            processing_range = bidding_schedule_for_term[:current_window_index + 1]\n",
    "            self.log_boss_activity(f\"üìÖ Processing {len(processing_range)} windows chronologically\")\n",
    "\n",
    "            # Load data sources once\n",
    "            if not self.load_raw_data():\n",
    "                self.log_boss_activity(\"‚ö†Ô∏è Could not load raw_data.xlsx\")\n",
    "                return False\n",
    "\n",
    "            self.overall_boss_results_df = self.load_overall_boss_results()\n",
    "            if self.overall_boss_results_df is None:\n",
    "                self.log_boss_activity(\"‚ö†Ô∏è Could not load overallBossResults.xlsx\")\n",
    "\n",
    "            # Load existing bid result keys for deduplication\n",
    "            existing_bid_result_keys = set()\n",
    "            cache_file = os.path.join(self.cache_dir, 'bid_result_cache.pkl')\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    existing_df = pd.read_pickle(cache_file)\n",
    "                    if not existing_df.empty:\n",
    "                        for _, record in existing_df.iterrows():\n",
    "                            key = (record['bid_window_id'], record['class_id'])\n",
    "                            existing_bid_result_keys.add(key)\n",
    "                        self.log_boss_activity(f\"‚úÖ Pre-loaded {len(existing_bid_result_keys)} existing bid result keys\")\n",
    "                except Exception as e:\n",
    "                    self.log_boss_activity(f\"‚ö†Ô∏è Could not pre-load bid_result_cache: {e}\")\n",
    "\n",
    "            # Sequential processing loop\n",
    "            for window_index, (results_date, window_name, folder_suffix) in enumerate(processing_range):\n",
    "                self.log_boss_activity(f\"üîÑ Processing window {window_index + 1}/{len(processing_range)}: {window_name}\")\n",
    "                \n",
    "                # Parse window name to get round and window number\n",
    "                round_str, window_num = self.parse_bidding_window(window_name)\n",
    "                if not round_str or not window_num:\n",
    "                    self.log_boss_activity(f\"‚ö†Ô∏è Could not parse window: {window_name}\")\n",
    "                    continue\n",
    "                \n",
    "                acad_term_id = ACAD_TERM_ID\n",
    "                window_key = (acad_term_id, round_str, window_num)\n",
    "                \n",
    "                # A. Find or create BidWindow record\n",
    "                bid_window_id = self.bid_window_cache.get(window_key)\n",
    "                if not bid_window_id:\n",
    "                    bid_window_id = self.bid_window_id_counter\n",
    "                    new_bid_window = {\n",
    "                        'id': bid_window_id,\n",
    "                        'acad_term_id': acad_term_id,\n",
    "                        'round': round_str,\n",
    "                        'window': window_num\n",
    "                    }\n",
    "                    self.new_bid_windows.append(new_bid_window)\n",
    "                    self.bid_window_cache[window_key] = bid_window_id\n",
    "                    self.bid_window_id_counter += 1\n",
    "                    self.boss_stats['bid_windows_created'] += 1\n",
    "                    self.log_boss_activity(f\"‚úÖ Created bid window {bid_window_id}: {window_name}\")\n",
    "                \n",
    "                # B. Process ClassAvailability (check if scrape data exists in raw_data.xlsx)\n",
    "                window_data_in_raw = self.standalone_data[\n",
    "                    self.standalone_data['bidding_window'] == window_name\n",
    "                ] if hasattr(self, 'standalone_data') and self.standalone_data is not None else pd.DataFrame()\n",
    "                \n",
    "                if not window_data_in_raw.empty:\n",
    "                    self.log_boss_activity(f\"üìä Processing ClassAvailability for {window_name} from raw_data.xlsx\")\n",
    "                    \n",
    "                    # Load existing availability keys for deduplication\n",
    "                    existing_availability_keys = set()\n",
    "                    avail_cache_file = os.path.join(self.cache_dir, 'class_availability_cache.pkl')\n",
    "                    if os.path.exists(avail_cache_file):\n",
    "                        try:\n",
    "                            avail_df = pd.read_pickle(avail_cache_file)\n",
    "                            if not avail_df.empty:\n",
    "                                for _, record in avail_df.iterrows():\n",
    "                                    key = (record['class_id'], record['bid_window_id'])\n",
    "                                    existing_availability_keys.add(key)\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                    \n",
    "                    for _, row in window_data_in_raw.iterrows():\n",
    "                        course_code = row.get('course_code')\n",
    "                        section = row.get('section')\n",
    "                        class_boss_id = row.get('class_boss_id')\n",
    "                        \n",
    "                        if pd.isna(course_code) or pd.isna(section) or pd.isna(class_boss_id):\n",
    "                            continue\n",
    "                        \n",
    "                        class_ids = self.find_all_class_ids(acad_term_id, class_boss_id)\n",
    "                        if not class_ids:\n",
    "                            continue\n",
    "                        \n",
    "                        # Extract availability values\n",
    "                        total_val = int(row.get('total')) if pd.notna(row.get('total')) else 0\n",
    "                        current_enrolled_val = int(row.get('current_enrolled')) if pd.notna(row.get('current_enrolled')) else 0\n",
    "                        reserved_val = int(row.get('reserved')) if pd.notna(row.get('reserved')) else 0\n",
    "                        available_val = int(row.get('available')) if pd.notna(row.get('available')) else 0\n",
    "                        \n",
    "                        for class_id in class_ids:\n",
    "                            availability_key = (class_id, bid_window_id)\n",
    "                            if availability_key not in existing_availability_keys:\n",
    "                                availability_record = {\n",
    "                                    'class_id': class_id,\n",
    "                                    'bid_window_id': bid_window_id,\n",
    "                                    'total': total_val,\n",
    "                                    'current_enrolled': current_enrolled_val,\n",
    "                                    'reserved': reserved_val,\n",
    "                                    'available': available_val\n",
    "                                }\n",
    "                                self.new_class_availability.append(availability_record)\n",
    "                                existing_availability_keys.add(availability_key)\n",
    "                                self.boss_stats['class_availability_created'] += 1\n",
    "                else:\n",
    "                    self.log_boss_activity(f\"‚è≠Ô∏è Skipping ClassAvailability for {window_name} (no scrape data)\")\n",
    "                \n",
    "                # C. Process BidResult based on window type\n",
    "                is_current_live = (window_index == current_window_index)\n",
    "                \n",
    "                if is_current_live:\n",
    "                    # Current live window: create placeholder BidResult from raw_data.xlsx\n",
    "                    self.log_boss_activity(f\"üìà Processing placeholder BidResult for current live window: {window_name}\")\n",
    "                    \n",
    "                    if not window_data_in_raw.empty:\n",
    "                        for _, row in window_data_in_raw.iterrows():\n",
    "                            course_code = row.get('course_code')\n",
    "                            section = row.get('section')\n",
    "                            class_boss_id = row.get('class_boss_id')\n",
    "                            \n",
    "                            if pd.isna(class_boss_id):\n",
    "                                continue\n",
    "                            \n",
    "                            class_ids = self.find_all_class_ids(acad_term_id, class_boss_id)\n",
    "                            if not class_ids:\n",
    "                                continue\n",
    "                            \n",
    "                            def safe_int(val): return int(val) if pd.notna(val) else None\n",
    "                            \n",
    "                            total_val = safe_int(row.get('total'))\n",
    "                            enrolled_val = safe_int(row.get('current_enrolled'))\n",
    "                            \n",
    "                            for class_id in class_ids:\n",
    "                                bid_result_key = (bid_window_id, class_id)\n",
    "                                if bid_result_key not in existing_bid_result_keys:\n",
    "                                    # Create placeholder record (median/min as None)\n",
    "                                    result_data = {\n",
    "                                        'bid_window_id': bid_window_id,\n",
    "                                        'class_id': class_id,\n",
    "                                        'vacancy': total_val,\n",
    "                                        'opening_vacancy': safe_int(row.get('opening_vacancy')),\n",
    "                                        'before_process_vacancy': total_val - enrolled_val if total_val is not None and enrolled_val is not None else None,\n",
    "                                        'dice': safe_int(row.get('d_i_c_e') or row.get('dice')),\n",
    "                                        'after_process_vacancy': safe_int(row.get('after_process_vacancy')),\n",
    "                                        'enrolled_students': enrolled_val,\n",
    "                                        'median': None,  # Placeholder\n",
    "                                        'min': None      # Placeholder\n",
    "                                    }\n",
    "                                    self.new_bid_result.append(result_data)\n",
    "                                    existing_bid_result_keys.add(bid_result_key)\n",
    "                                    self.boss_stats['bid_results_created'] += 1\n",
    "                else:\n",
    "                    # Historical window: process from overallBossResults.xlsx\n",
    "                    self.log_boss_activity(f\"üìà Processing historical BidResult for {window_name} from overallBossResults.xlsx\")\n",
    "                    \n",
    "                    if self.overall_boss_results_df is not None and not self.overall_boss_results_df.empty:\n",
    "                        # Filter overall results for this specific window\n",
    "                        overall_df = self.overall_boss_results_df.copy()\n",
    "                        \n",
    "                        # Parse bidding window column\n",
    "                        bidding_window_col = None\n",
    "                        for col in overall_df.columns:\n",
    "                            if 'bidding window' in col.lower() or 'bidding_window' in col.lower():\n",
    "                                bidding_window_col = col\n",
    "                                break\n",
    "                        \n",
    "                        if bidding_window_col:\n",
    "                            # Parse and filter for current window\n",
    "                            parsed_windows = overall_df[bidding_window_col].apply(self.parse_bidding_window)\n",
    "                            overall_df['round'] = parsed_windows.apply(lambda x: x[0] if isinstance(x, tuple) else None)\n",
    "                            overall_df['window'] = parsed_windows.apply(lambda x: x[1] if isinstance(x, tuple) else None)\n",
    "                            \n",
    "                            overall_df.dropna(subset=['round', 'window'], inplace=True)\n",
    "                            overall_df['round'] = overall_df['round'].astype(str)\n",
    "                            overall_df['window'] = pd.to_numeric(overall_df['window']).astype(int)\n",
    "                            \n",
    "                            window_filtered_df = overall_df[\n",
    "                                (overall_df['round'] == str(round_str)) &\n",
    "                                (overall_df['window'] == int(window_num))\n",
    "                            ]\n",
    "                            \n",
    "                            if not window_filtered_df.empty:\n",
    "                                for _, row in window_filtered_df.iterrows():\n",
    "                                    # Extract course and section info\n",
    "                                    course_code = self._get_column_value(row, ['Course Code', 'course_code', 'Course_Code'])\n",
    "                                    section = self._get_column_value(row, ['Section', 'section'])\n",
    "                                    \n",
    "                                    if pd.isna(course_code) or pd.isna(section):\n",
    "                                        continue\n",
    "                                    \n",
    "                                    # Find class_boss_id from raw data\n",
    "                                    class_boss_id = self._find_class_boss_id_from_course_section(course_code, section, acad_term_id)\n",
    "                                    if not class_boss_id:\n",
    "                                        continue\n",
    "                                    \n",
    "                                    class_ids = self.find_all_class_ids(acad_term_id, class_boss_id)\n",
    "                                    if not class_ids:\n",
    "                                        continue\n",
    "                                    \n",
    "                                    # Extract bid data\n",
    "                                    median_bid = None\n",
    "                                    min_bid = None\n",
    "                                    for col in row.index:\n",
    "                                        col_lower = str(col).lower()\n",
    "                                        if 'median' in col_lower and 'bid' in col_lower:\n",
    "                                            median_bid = row[col]\n",
    "                                        elif 'min' in col_lower and 'bid' in col_lower:\n",
    "                                            min_bid = row[col]\n",
    "                                    \n",
    "                                    def safe_int(val): return int(val) if pd.notna(val) else None\n",
    "                                    def safe_float(val): return float(val) if pd.notna(val) else None\n",
    "                                    \n",
    "                                    for class_id in class_ids:\n",
    "                                        result_data = {\n",
    "                                            'bid_window_id': bid_window_id,\n",
    "                                            'class_id': class_id,\n",
    "                                            'vacancy': safe_int(self._get_column_value(row, ['Vacancy', 'vacancy'])),\n",
    "                                            'opening_vacancy': safe_int(self._get_column_value(row, ['Opening Vacancy', 'opening_vacancy', 'Opening_Vacancy'])),\n",
    "                                            'before_process_vacancy': safe_int(self._get_column_value(row, ['Before Process Vacancy', 'before_process_vacancy', 'Before_Process_Vacancy'])),\n",
    "                                            'dice': safe_int(self._get_column_value(row, ['D.I.C.E', 'dice', 'd_i_c_e', 'DICE'])),\n",
    "                                            'after_process_vacancy': safe_int(self._get_column_value(row, ['After Process Vacancy', 'after_process_vacancy', 'After_Process_Vacancy'])),\n",
    "                                            'enrolled_students': safe_int(self._get_column_value(row, ['Enrolled Students', 'enrolled_students', 'Enrolled_Students'])),\n",
    "                                            'median': safe_float(median_bid),\n",
    "                                            'min': safe_float(min_bid)\n",
    "                                        }\n",
    "                                        \n",
    "                                        # Check if record exists (UPDATE vs CREATE)\n",
    "                                        bid_result_key = (bid_window_id, class_id)\n",
    "                                        if bid_result_key in existing_bid_result_keys:\n",
    "                                            self.update_bid_result.append(result_data)\n",
    "                                        else:\n",
    "                                            self.new_bid_result.append(result_data)\n",
    "                                            existing_bid_result_keys.add(bid_result_key)\n",
    "                                            self.boss_stats['bid_results_created'] += 1\n",
    "                            else:\n",
    "                                self.log_boss_activity(f\"‚ö†Ô∏è No data found in overallBossResults for {window_name}\")\n",
    "                        else:\n",
    "                            self.log_boss_activity(f\"‚ö†Ô∏è No bidding window column found in overallBossResults.xlsx\")\n",
    "\n",
    "            self.log_boss_activity(\"‚úÖ Sequential catch-up processing completed\")\n",
    "\n",
    "            # --- Step 5: Save all generated files ---\n",
    "            self.save_outputs()\n",
    "            self.save_boss_outputs()\n",
    "            \n",
    "            # --- Step 6: Final Summary ---\n",
    "            self.print_boss_summary()\n",
    "            \n",
    "            self.log_boss_activity(\"üìù ‚úÖ Enhanced Phase 3 completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Enhanced Phase 3 failed catastrophically: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def load_faculties_cache(self):\n",
    "        \"\"\"Load faculties from database cache for mapping\"\"\"\n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, 'faculties_cache.pkl')\n",
    "            \n",
    "            # Try loading from cache file first\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    faculties_df = pd.read_pickle(cache_file)\n",
    "                    if not faculties_df.empty:\n",
    "                        self.faculties_cache = {}\n",
    "                        self.faculty_acronym_to_id = {}\n",
    "                        \n",
    "                        for _, row in faculties_df.iterrows():\n",
    "                            faculty_id = row['id']\n",
    "                            acronym = row['acronym'].upper()\n",
    "                            \n",
    "                            self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                            self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                        \n",
    "                        logger.info(f\"üìö Loaded {len(self.faculties_cache)} faculties from cache\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(\"‚ö†Ô∏è Faculty cache file exists but is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Error reading faculty cache file: {e}\")\n",
    "            \n",
    "            # If cache doesn't exist or failed, try database\n",
    "            if self.connection:\n",
    "                try:\n",
    "                    query = \"SELECT * FROM faculties\"\n",
    "                    faculties_df = pd.read_sql_query(query, self.connection)\n",
    "                    if not faculties_df.empty:\n",
    "                        # Save to cache for future use\n",
    "                        faculties_df.to_pickle(cache_file)\n",
    "                        \n",
    "                        # Load into memory\n",
    "                        self.faculties_cache = {}\n",
    "                        self.faculty_acronym_to_id = {}\n",
    "                        \n",
    "                        for _, row in faculties_df.iterrows():\n",
    "                            faculty_id = row['id']\n",
    "                            acronym = row['acronym'].upper()\n",
    "                            \n",
    "                            self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                            self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                        \n",
    "                        logger.info(f\"üìö Downloaded and cached {len(self.faculties_cache)} faculties from database\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(\"‚ö†Ô∏è Database faculties table is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Error downloading faculties from database: {e}\")\n",
    "            \n",
    "            # Fallback: create basic mapping from known data\n",
    "            logger.warning(\"‚ö†Ô∏è Using fallback faculty mapping\")\n",
    "            self.faculties_cache = {}\n",
    "            self.faculty_acronym_to_id = {\n",
    "                'LKCSB': 1,   # Lee Kong Chian School of Business\n",
    "                'YPHSL': 2,   # Yong Pung How School of Law\n",
    "                'SOE': 3,     # School of Economics\n",
    "                'SCIS': 4,    # School of Computing and Information Systems\n",
    "                'SOSS': 5,    # School of Social Sciences\n",
    "                'SOA': 6,     # School of Accountancy\n",
    "                'CIS': 7,     # College of Integrative Studies\n",
    "                'CEC': 8,      # Center for English Communication\n",
    "                'C4SR': 9,      # Centre for Social Responsibility\n",
    "                'OCS': 10,      # Dato‚Äô Kho Hui Meng Career Centre\n",
    "            }\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Critical error in load_faculties_cache: {e}\")\n",
    "            return False\n",
    "\n",
    "    def map_courses_to_faculties_from_boss(self):\n",
    "        \"\"\"Map courses to faculties using course code prefix patterns from existing courses\"\"\"\n",
    "        logger.info(\"üéì Starting automated faculty mapping from course code patterns...\")\n",
    "        \n",
    "        # Load faculties cache first\n",
    "        if not self.load_faculties_cache():\n",
    "            logger.error(\"‚ùå Failed to load faculties cache\")\n",
    "            return False\n",
    "        \n",
    "        # Build prefix-to-faculty mapping from existing courses\n",
    "        prefix_faculty_mapping = defaultdict(set)  # prefix -> set of faculty_ids\n",
    "        \n",
    "        logger.info(\"üìã Analyzing existing course patterns...\")\n",
    "        \n",
    "        # Analyze existing courses in cache to build prefix patterns\n",
    "        for course_code, course_data in self.courses_cache.items():\n",
    "            faculty_id = course_data.get('belong_to_faculty')\n",
    "            if faculty_id:\n",
    "                # Extract prefix (characters before numbers)\n",
    "                # Handle patterns like \"COR-COMM567A\" -> \"COR-COMM\"\n",
    "                import re\n",
    "                prefix_match = re.match(r'^([A-Z-]+)', course_code.upper())\n",
    "                if prefix_match:\n",
    "                    prefix = prefix_match.group(1)\n",
    "                    prefix_faculty_mapping[prefix].add(faculty_id)\n",
    "        \n",
    "        logger.info(f\"üìä Found {len(prefix_faculty_mapping)} unique course prefixes in existing courses\")\n",
    "        \n",
    "        # Log the patterns found\n",
    "        for prefix, faculty_ids in prefix_faculty_mapping.items():\n",
    "            faculty_names = []\n",
    "            for fid in faculty_ids:\n",
    "                if fid in self.faculties_cache:\n",
    "                    faculty_names.append(self.faculties_cache[fid]['acronym'])\n",
    "            logger.info(f\"   {prefix}: {len(faculty_ids)} faculties ({', '.join(faculty_names)})\")\n",
    "        \n",
    "        # Apply automatic mapping to new courses\n",
    "        mapped_count = 0\n",
    "        course_faculty_mappings = {}\n",
    "        \n",
    "        for course_info in self.courses_needing_faculty[:]:  # Copy list to modify during iteration\n",
    "            course_code = course_info['course_code']\n",
    "            \n",
    "            # Extract prefix from new course code\n",
    "            import re\n",
    "            prefix_match = re.match(r'^([A-Z-]+)', course_code.upper())\n",
    "            if not prefix_match:\n",
    "                continue\n",
    "            \n",
    "            prefix = prefix_match.group(1)\n",
    "            \n",
    "            # Check if this prefix has exactly 1 unique faculty in existing courses\n",
    "            if prefix in prefix_faculty_mapping:\n",
    "                faculty_ids = prefix_faculty_mapping[prefix]\n",
    "                \n",
    "                if len(faculty_ids) == 1:\n",
    "                    # Only 1 faculty found - auto-assign\n",
    "                    faculty_id = list(faculty_ids)[0]\n",
    "                    course_faculty_mappings[course_code] = faculty_id\n",
    "                    mapped_count += 1\n",
    "                    \n",
    "                    faculty_name = self.faculties_cache[faculty_id]['acronym'] if faculty_id in self.faculties_cache else str(faculty_id)\n",
    "                    logger.info(f\"‚úÖ Auto-mapped {course_code}: prefix '{prefix}' ‚Üí {faculty_name}\")\n",
    "                else:\n",
    "                    # Multiple faculties found - leave for manual assignment\n",
    "                    faculty_names = [self.faculties_cache[fid]['acronym'] for fid in faculty_ids if fid in self.faculties_cache]\n",
    "                    logger.info(f\"‚ö†Ô∏è {course_code}: prefix '{prefix}' maps to {len(faculty_ids)} faculties ({', '.join(faculty_names)}) - manual review needed\")\n",
    "            else:\n",
    "                # No existing pattern found\n",
    "                logger.info(f\"üÜï {course_code}: new prefix '{prefix}' - manual review needed\")\n",
    "        \n",
    "        # Apply mappings to courses\n",
    "        if course_faculty_mappings:\n",
    "            self._apply_faculty_mappings_to_courses(course_faculty_mappings)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Pattern-based faculty mapping completed:\")\n",
    "        logger.info(f\"   ‚Ä¢ {mapped_count} courses auto-mapped based on prefix patterns\")\n",
    "        logger.info(f\"   ‚Ä¢ {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _apply_faculty_mappings_to_courses(self, course_faculty_mappings):\n",
    "        \"\"\"Apply faculty mappings to new courses and update courses needing faculty\"\"\"\n",
    "        logger.info(f\"üîÑ Applying faculty mappings to {len(course_faculty_mappings)} courses...\")\n",
    "        \n",
    "        mapped_count = 0\n",
    "        \n",
    "        # Update new_courses\n",
    "        for course in self.new_courses:\n",
    "            course_code = course['code']\n",
    "            if course_code in course_faculty_mappings:\n",
    "                course['belong_to_faculty'] = course_faculty_mappings[course_code]\n",
    "                mapped_count += 1\n",
    "        \n",
    "        # Update courses_cache\n",
    "        for course_code, faculty_id in course_faculty_mappings.items():\n",
    "            if course_code in self.courses_cache:\n",
    "                self.courses_cache[course_code]['belong_to_faculty'] = faculty_id\n",
    "        \n",
    "        # Remove mapped courses from courses_needing_faculty\n",
    "        original_needing_count = len(self.courses_needing_faculty)\n",
    "        self.courses_needing_faculty = [\n",
    "            course_info for course_info in self.courses_needing_faculty\n",
    "            if course_info['course_code'] not in course_faculty_mappings\n",
    "        ]\n",
    "        \n",
    "        removed_count = original_needing_count - len(self.courses_needing_faculty)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Applied faculty mappings:\")\n",
    "        logger.info(f\"   ‚Ä¢ {mapped_count} courses updated with faculty\")\n",
    "        logger.info(f\"   ‚Ä¢ {removed_count} courses removed from manual review queue\")\n",
    "        logger.info(f\"   ‚Ä¢ {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "\n",
    "    def extract_acad_term_from_path(self, file_path: str) -> Optional[str]:\n",
    "        r\"\"\"Extract acad_term_id from file path as fallback\n",
    "        Examples:\n",
    "        'script_input\\classTimingsFull\\2021-22_T1' -> 'AY202122T1'\n",
    "        'script_input\\classTimingsFull\\2022-23_T3A' -> 'AY202223T3A'\n",
    "        \"\"\"\n",
    "        # Extract the term folder name\n",
    "        path_parts = file_path.replace('/', '\\\\').split('\\\\')\n",
    "        \n",
    "        for part in path_parts:\n",
    "            # Look for pattern like \"2021-22_T1\"\n",
    "            match = re.match(r'(\\d{4})-(\\d{2})_T(\\w+)', part)\n",
    "            if match:\n",
    "                year_start = match.group(1)\n",
    "                year_end = match.group(2)\n",
    "                term = match.group(3)\n",
    "                return f\"AY{year_start}{year_end}T{term}\"\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def get_last_filepath_by_course(self, course_code):\n",
    "        \"\"\"Direct filepath lookup for course code - bypasses record_key linking\"\"\"\n",
    "        print(f\"üîç DEBUG: Looking for course {course_code} using direct method\")\n",
    "        \n",
    "        # Check if we have standalone data with filepath column\n",
    "        if hasattr(self, 'standalone_data') and self.standalone_data is not None:\n",
    "            if 'filepath' in self.standalone_data.columns:\n",
    "                print(f\"‚úÖ DEBUG: Found filepath column in standalone_data\")\n",
    "                \n",
    "                course_records = self.standalone_data[\n",
    "                    self.standalone_data['course_code'].str.upper() == course_code.upper()\n",
    "                ].copy()\n",
    "                \n",
    "                print(f\"üìä DEBUG: Found {len(course_records)} records for {course_code}\")\n",
    "                \n",
    "                if not course_records.empty:\n",
    "                    # Get the most recent record (last row)\n",
    "                    last_record = course_records.iloc[-1]\n",
    "                    filepath = last_record.get('filepath')\n",
    "                    \n",
    "                    print(f\"üìÅ DEBUG: Last record filepath: {filepath}\")\n",
    "                    \n",
    "                    if pd.notna(filepath):\n",
    "                        print(f\"‚úÖ Found filepath for {course_code}: {filepath}\")\n",
    "                        return filepath\n",
    "                    else:\n",
    "                        print(f\"‚ùå DEBUG: Filepath is NaN for {course_code}\")\n",
    "            else:\n",
    "                print(f\"‚ùå DEBUG: No 'filepath' column in standalone_data\")\n",
    "                print(f\"Available columns: {list(self.standalone_data.columns)}\")\n",
    "        \n",
    "        # Fallback: check multiple_data if standalone doesn't have filepath\n",
    "        if hasattr(self, 'multiple_data') and self.multiple_data is not None:\n",
    "            if 'filepath' in self.multiple_data.columns and 'course_code' in self.multiple_data.columns:\n",
    "                print(f\"‚úÖ DEBUG: Checking multiple_data as fallback\")\n",
    "                \n",
    "                course_records = self.multiple_data[\n",
    "                    self.multiple_data['course_code'].str.upper() == course_code.upper()\n",
    "                ].copy()\n",
    "                \n",
    "                if not course_records.empty:\n",
    "                    last_record = course_records.iloc[-1]\n",
    "                    filepath = last_record.get('filepath')\n",
    "                    \n",
    "                    if pd.notna(filepath):\n",
    "                        print(f\"‚úÖ Found filepath in multiple_data for {course_code}: {filepath}\")\n",
    "                        return filepath\n",
    "        \n",
    "        print(f\"‚ùå DEBUG: No filepath found for {course_code}\")\n",
    "        return None\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"Explicitly close database connection\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "            logger.info(\"üîí Database connection closed\")\n",
    "\n",
    "    def check_cache_freshness(self) -> bool:\n",
    "        \"\"\"\n",
    "        Implements granular, window-aware cache freshness logic.\n",
    "        The cache is STALE if its modification time is before the results date \n",
    "        of the previous bidding window.\n",
    "        \"\"\"\n",
    "        logger.info(\"üîç Checking cache freshness with window-aware logic...\")\n",
    "        now = datetime.now()\n",
    "\n",
    "        # a. Identify the current active bidding window from the schedule.\n",
    "        # The current round is the first one whose results_date is in the future.\n",
    "        current_window_index = -1\n",
    "        if not self.bidding_schedule:\n",
    "            logger.info(\"‚úÖ No bidding schedule found. Assuming cache is fresh.\")\n",
    "            return True\n",
    "\n",
    "        for i, (results_date, _, _) in enumerate(self.bidding_schedule):\n",
    "            if now < results_date:\n",
    "                current_window_index = i\n",
    "                break\n",
    "                \n",
    "        # b. If no future window is found, or we are before/in the first window,\n",
    "        # there is no \"previous window\" to check against. The cache is fresh.\n",
    "        if current_window_index <= 0:\n",
    "            logger.info(\"‚úÖ Not in an active bidding period or before the second round. Cache is considered fresh.\")\n",
    "            return True\n",
    "\n",
    "        # c. Get the results_date of the previous bidding window.\n",
    "        previous_window_info = self.bidding_schedule[current_window_index - 1]\n",
    "        previous_round_results_date = previous_window_info[0]\n",
    "        \n",
    "        logger.info(f\"‚ÑπÔ∏è Rule: Cache must be newer than the previous window's results date: {previous_round_results_date.strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "        # d. Check if the cache is fresh by comparing its modification time.\n",
    "        cache_files_to_check = [\n",
    "            os.path.join(self.cache_dir, 'professors_cache.pkl'),\n",
    "            os.path.join(self.cache_dir, 'courses_cache.pkl'),\n",
    "            os.path.join(self.cache_dir, 'classes_cache.pkl'),\n",
    "        ]\n",
    "        oldest_cache_time = None\n",
    "\n",
    "        for path in cache_files_to_check:\n",
    "            if not os.path.exists(path):\n",
    "                logger.warning(f\"‚ö†Ô∏è Critical cache file not found: {path}. A full download is required.\")\n",
    "                return False\n",
    "                \n",
    "            mod_time_dt = datetime.fromtimestamp(os.path.getmtime(path))\n",
    "            if oldest_cache_time is None or mod_time_dt < oldest_cache_time:\n",
    "                oldest_cache_time = mod_time_dt\n",
    "\n",
    "        if oldest_cache_time is None:\n",
    "            # This case should be prevented by the check above, but is included for safety.\n",
    "            logger.warning(\"‚ö†Ô∏è No cache files exist. A full download is required.\")\n",
    "            return False\n",
    "\n",
    "        logger.info(f\"‚ÑπÔ∏è Oldest relevant cache file was last modified on: {oldest_cache_time.strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "        # The cache is fresh if its oldest part was created on or after the previous window's results.\n",
    "        if oldest_cache_time >= previous_round_results_date:\n",
    "            logger.info(\"‚úÖ Cache is FRESH.\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(\"‚ùå Cache is STALE. A full download is required.\")\n",
    "            return False\n",
    "\n",
    "    def _get_bidding_round_info_for_term(self, ay_term, now):\n",
    "        \"\"\"Get current bidding round info for a term\"\"\"\n",
    "        if START_AY_TERM in ay_term:\n",
    "            for results_date, _, folder_suffix in self.bidding_schedule:\n",
    "                if now < results_date:\n",
    "                    return f\"{ay_term}_{folder_suffix}\"\n",
    "        return None\n",
    "\n",
    "    def load_or_cache_data_with_freshness_check(self):\n",
    "        \"\"\"\n",
    "        Load data with a freshness check, combine it with new CSV files,\n",
    "        and handle caching using robust, type-safe logic.\n",
    "        \"\"\"\n",
    "        # Part 1: Check cache freshness and download new data if necessary.\n",
    "        if not self.check_cache_freshness():\n",
    "            logger.info(\"üîÑ Cache is stale, downloading fresh data from the database...\")\n",
    "            if not self.connect_database():\n",
    "                return False\n",
    "            \n",
    "            try:\n",
    "                self._download_and_cache_data()\n",
    "                logger.info(\"‚úÖ Successfully downloaded fresh data from the database.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to download fresh data: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Part 2: Load the core data from the cache (either freshly downloaded or existing).\n",
    "        if not self.load_or_cache_data():\n",
    "            return False\n",
    "        \n",
    "        # Part 3: Combine the loaded cache with new data from local CSV files.\n",
    "        # This logic is integrated from the improved _combine_with_new_files method.\n",
    "        logger.info(\"üîÑ Combining database cache with new CSV files...\")\n",
    "        \n",
    "        # Combine new_classes.csv with existing_classes_cache\n",
    "        new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "        if os.path.exists(new_classes_path):\n",
    "            try:\n",
    "                new_classes_df = pd.read_csv(new_classes_path)\n",
    "                if not new_classes_df.empty:\n",
    "                    new_classes_list = new_classes_df.to_dict('records')\n",
    "                    \n",
    "                    # Safely initialize the cache if it doesn't exist to prevent errors.\n",
    "                    if not hasattr(self, 'existing_classes_cache'):\n",
    "                        self.existing_classes_cache = []\n",
    "                    \n",
    "                    # Add new classes, checking for duplicates with precise, multi-field logic.\n",
    "                    added_count = 0\n",
    "                    for new_class in new_classes_list:\n",
    "                        exists = False\n",
    "                        for existing_class in self.existing_classes_cache:\n",
    "                            # Precise check including course, section, term, and professor.\n",
    "                            if (existing_class['course_id'] == new_class['course_id'] and\n",
    "                                str(existing_class['section']) == str(new_class['section']) and\n",
    "                                existing_class['acad_term_id'] == new_class['acad_term_id'] and\n",
    "                                existing_class.get('professor_id') == new_class.get('professor_id')):\n",
    "                                exists = True\n",
    "                                break\n",
    "                        \n",
    "                        if not exists:\n",
    "                            self.existing_classes_cache.append(new_class)\n",
    "                            added_count += 1\n",
    "                    \n",
    "                    if added_count > 0:\n",
    "                        logger.info(f\"‚úÖ Added {added_count} new, unique classes to the existing cache.\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Could not combine new_classes.csv: {e}\")\n",
    "        \n",
    "        # Update bid_window_cache with new_bid_window.csv\n",
    "        new_bid_window_path = os.path.join(self.output_base, 'new_bid_window.csv')\n",
    "        if os.path.exists(new_bid_window_path):\n",
    "            try:\n",
    "                new_bid_window_df = pd.read_csv(new_bid_window_path)\n",
    "                if not new_bid_window_df.empty:\n",
    "                    added_count = 0\n",
    "                    for _, row in new_bid_window_df.iterrows():\n",
    "                        # Use explicit type casting for robust key creation.\n",
    "                        window_key = (row['acad_term_id'], str(row['round']), int(row['window']))\n",
    "                        if window_key not in self.bid_window_cache:\n",
    "                            self.bid_window_cache[window_key] = row['id']\n",
    "                            added_count += 1\n",
    "                    if added_count > 0:\n",
    "                        logger.info(f\"‚úÖ Added {added_count} new bid windows to the cache.\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Could not combine new_bid_window.csv: {e}\")\n",
    "        \n",
    "        # Update courses_cache with new_courses.csv from multiple locations\n",
    "        new_courses_paths = [\n",
    "            os.path.join(self.output_base, 'new_courses.csv'),\n",
    "            os.path.join(self.verify_dir, 'new_courses.csv')\n",
    "        ]\n",
    "        \n",
    "        for path in new_courses_paths:\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    new_courses_df = pd.read_csv(path)\n",
    "                    if not new_courses_df.empty:\n",
    "                        added_count = 0\n",
    "                        for _, row in new_courses_df.iterrows():\n",
    "                            if row['code'] not in self.courses_cache:\n",
    "                                self.courses_cache[row['code']] = row.to_dict()\n",
    "                                added_count += 1\n",
    "                        if added_count > 0:\n",
    "                            logger.info(f\"‚úÖ Added {added_count} new courses from {path}.\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Could not combine {path}: {e}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def check_record_exists_in_cache(self, table_name, record_data, key_fields):\n",
    "        \"\"\"Check if record exists in database cache and if it needs updates\"\"\"\n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, f'{table_name}_cache.pkl')\n",
    "            if not os.path.exists(cache_file):\n",
    "                return False, None\n",
    "            \n",
    "            df = pd.read_pickle(cache_file)\n",
    "            if df.empty:\n",
    "                return False, None\n",
    "            \n",
    "            # Build query mask\n",
    "            mask = True\n",
    "            for field in key_fields:\n",
    "                if field in df.columns and field in record_data:\n",
    "                    mask = mask & (df[field] == record_data[field])\n",
    "            \n",
    "            matching_records = df[mask]\n",
    "            if matching_records.empty:\n",
    "                return False, None\n",
    "            \n",
    "            # Return first match for update comparison\n",
    "            return True, matching_records.iloc[0].to_dict()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking cache for {table_name}: {e}\")\n",
    "            return False, None\n",
    "\n",
    "    def needs_update(self, existing_record, new_record_or_row, field_mapping_or_fields):\n",
    "        \"\"\"\n",
    "        Check if existing record needs updates based on field mapping or field list.\n",
    "        Handles both dictionary-to-dictionary and row-to-record comparisons.\n",
    "        \"\"\"\n",
    "        # Handle different parameter types\n",
    "        if isinstance(field_mapping_or_fields, dict):\n",
    "            # field_mapping case: {db_field: raw_field}\n",
    "            field_mapping = field_mapping_or_fields\n",
    "            compare_fields = []\n",
    "            for db_field, raw_field in field_mapping.items():\n",
    "                old_value = existing_record.get(db_field)\n",
    "                new_value = new_record_or_row.get(raw_field)\n",
    "                \n",
    "                # Type-specific comparison\n",
    "                if db_field == 'credit_units':\n",
    "                    new_value = float(new_value) if pd.notna(new_value) else None\n",
    "                    old_value = float(old_value) if pd.notna(old_value) else None\n",
    "                else:\n",
    "                    if pd.isna(new_value):\n",
    "                        new_value = None\n",
    "                    else:\n",
    "                        new_value = str(new_value).strip()\n",
    "                    \n",
    "                    if pd.isna(old_value):\n",
    "                        old_value = None\n",
    "                    else:\n",
    "                        old_value = str(old_value).strip() if old_value is not None else None\n",
    "                \n",
    "                # Check for actual change\n",
    "                if new_value != old_value:\n",
    "                    # Don't overwrite existing data with empty data\n",
    "                    if new_value is None or new_value == '':\n",
    "                        if old_value is not None and old_value != '':\n",
    "                            continue\n",
    "                    return True\n",
    "            return False\n",
    "        else:\n",
    "            # field list case: direct field comparison\n",
    "            compare_fields = field_mapping_or_fields\n",
    "            for field in compare_fields:\n",
    "                existing_value = existing_record.get(field)\n",
    "                new_value = new_record_or_row.get(field)\n",
    "                \n",
    "                # Handle different data types\n",
    "                if pd.isna(existing_value) and pd.isna(new_value):\n",
    "                    continue\n",
    "                if pd.isna(existing_value) or pd.isna(new_value):\n",
    "                    return True\n",
    "                \n",
    "                # FIXED: Proper string comparison for course_outline_url and other fields\n",
    "                if str(existing_value).strip() != str(new_value).strip():\n",
    "                    return True\n",
    "            \n",
    "            return False\n",
    "\n",
    "    def load_overall_boss_results(self):\n",
    "        \"\"\"Load overall BOSS results from script_input/overallBossResults/\"\"\"\n",
    "        logger.info(\"üìä Loading overall BOSS results...\")\n",
    "        \n",
    "        now = datetime.now()\n",
    "        current_round_info = self._get_bidding_round_info_for_term(START_AY_TERM, now)\n",
    "        \n",
    "        if not current_round_info:\n",
    "            logger.info(\"‚è≠Ô∏è Not in active bidding period - skipping overall results\")\n",
    "            return None\n",
    "        \n",
    "        # Determine which overall results file to load based on current round\n",
    "        overall_results_dir = 'script_input/overallBossResults'\n",
    "        if not os.path.exists(overall_results_dir):\n",
    "            logger.warning(f\"‚ö†Ô∏è Overall results directory not found: {overall_results_dir}\")\n",
    "            return None\n",
    "        \n",
    "        # Look for the appropriate Excel file\n",
    "        results_file = os.path.join(overall_results_dir, f\"{START_AY_TERM}.xlsx\")\n",
    "        if not os.path.exists(results_file):\n",
    "            logger.warning(f\"‚ö†Ô∏è Overall results file not found: {results_file}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Load the Excel file\n",
    "            df = pd.read_excel(results_file)\n",
    "            logger.info(f\"‚úÖ Loaded {len(df)} overall BOSS results from {results_file}\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error loading overall results: {e}\")\n",
    "            return None\n",
    "\n",
    "    def determine_previous_round_for_overall_results(self, current_round_info):\n",
    "        \"\"\"Determine which round's results to create based on current round\"\"\"\n",
    "        if not current_round_info:\n",
    "            return None, None\n",
    "        \n",
    "        # Extract current round suffix\n",
    "        current_suffix = current_round_info.split('_')[-1]\n",
    "        \n",
    "        # Mapping of current round to previous round results\n",
    "        round_mapping = {\n",
    "            'R1AW1': ('1', 1),     # When in R1A W1, create R1 W1 results\n",
    "            'R1AW2': ('1A', 1),    # When in R1A W2, create R1A W1 results\n",
    "            'R1AW3': ('1A', 2),    # When in R1A W3, create R1A W2 results\n",
    "            'R1BW1': ('1A', 3),    # When in R1B W1, create R1A W3 results\n",
    "            'R1BW2': ('1B', 1),    # When in R1B W2, create R1B W1 results\n",
    "            'R1CW1': ('1B', 2),    # When in R1C W1, create R1B W2 results\n",
    "            'R1CW2': ('1C', 1),    # When in R1C W2, create R1C W1 results\n",
    "            'R1CW3': ('1C', 2),    # When in R1C W3, create R1C W2 results\n",
    "            'R1FW1': ('1C', 3),    # When in R1F W1, create R1C W3 results\n",
    "            'R1FW2': ('1F', 1),    # When in R1F W2, create R1F W1 results\n",
    "            'R1FW3': ('1F', 2),    # When in R1F W3, create R1F W2 results\n",
    "            'R1FW4': ('1F', 3),    # When in R1F W4, create R1F W3 results\n",
    "            'R2W1': ('1F', 4),     # When in R2 W1, create R1F W4 results\n",
    "            'R2W2': ('2', 1),      # When in R2 W2, create R2 W1 results\n",
    "            'R2W3': ('2', 2),      # When in R2 W3, create R2 W2 results\n",
    "            'R2AW1': ('2', 3),     # When in R2A W1, create R2 W3 results\n",
    "            'R2AW2': ('2A', 1),    # When in R2A W2, create R2A W1 results\n",
    "            'R2AW3': ('2A', 2),    # When in R2A W3, create R2A W2 results\n",
    "        }\n",
    "        \n",
    "        return round_mapping.get(current_suffix, (None, None))\n",
    "\n",
    "    def process_overall_boss_results(self):\n",
    "        \"\"\"\n",
    "        Process overall BOSS results from the dedicated Excel file.\n",
    "        - It parses the 'Bidding Window' column to extract round and window numbers.\n",
    "        - For existing bid_result records, it creates an UPDATE record with the latest median/min values.\n",
    "        - If a record doesn't exist, it creates a NEW record as a fallback.\n",
    "        - It correctly logs any failed class lookups.\n",
    "        \"\"\"\n",
    "        logger.info(\"üìà Processing overall BOSS results (for updates)...\")\n",
    "        \n",
    "        overall_df = self.load_overall_boss_results()\n",
    "        if overall_df is None or overall_df.empty:\n",
    "            logger.warning(\"‚ö†Ô∏è No overall BOSS results file found or file is empty. Skipping.\")\n",
    "            return True\n",
    "\n",
    "        # --- FIX for 'Bidding Window' column ---\n",
    "        # Standardize column names for robustness BUT preserve the actual column names for data access\n",
    "        standardized_columns = {}\n",
    "        for col in overall_df.columns:\n",
    "            standardized = str(col).lower().replace(' ', '_').replace('.', '')\n",
    "            standardized_columns[standardized] = col\n",
    "        \n",
    "        # Check for the bidding window column\n",
    "        bidding_window_col = None\n",
    "        for std_name, orig_name in standardized_columns.items():\n",
    "            if 'bidding_window' in std_name or 'bidding window' in orig_name.lower():\n",
    "                bidding_window_col = orig_name\n",
    "                break\n",
    "        \n",
    "        if not bidding_window_col:\n",
    "            logger.error(\"‚ùå 'overallBossResults' file is missing the 'Bidding Window' column. Cannot process.\")\n",
    "            return False\n",
    "\n",
    "        # Apply the parsing function to create 'round' and 'window' columns from the 'bidding_window' string\n",
    "        try:\n",
    "            parsed_windows = overall_df[bidding_window_col].apply(self.parse_bidding_window)\n",
    "            overall_df['round'] = parsed_windows.apply(lambda x: x[0] if isinstance(x, tuple) else None)\n",
    "            overall_df['window'] = parsed_windows.apply(lambda x: x[1] if isinstance(x, tuple) else None)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to parse 'bidding_window' column: {e}\")\n",
    "            return False\n",
    "\n",
    "        # Drop rows where parsing might have failed (e.g., unexpected format)\n",
    "        overall_df.dropna(subset=['round', 'window'], inplace=True)\n",
    "        if overall_df.empty:\n",
    "            logger.warning(\"‚ö†Ô∏è Could not parse any valid round/window from the 'Bidding Window' column. Skipping.\")\n",
    "            return True\n",
    "        \n",
    "        # ==================================================================\n",
    "        # === NEW LOGIC TO PROCESS ONLY THE PREVIOUS ROUND/WINDOW        ===\n",
    "        # ==================================================================\n",
    "        now = datetime.now()\n",
    "        # Use the global START_AY_TERM to find the current active round based on the system time and schedule\n",
    "        current_round_info = self._get_bidding_round_info_for_term(START_AY_TERM, now)\n",
    "\n",
    "        if not current_round_info:\n",
    "            logger.info(\"‚ÑπÔ∏è Not in an active bidding period. Skipping overall results processing.\")\n",
    "            return True\n",
    "\n",
    "        # Determine the target round/window that we SHOULD be processing from the Excel file\n",
    "        target_round, target_window = self.determine_previous_round_for_overall_results(current_round_info)\n",
    "\n",
    "        if not target_round or not target_window:\n",
    "            logger.info(f\"‚ÑπÔ∏è Current active window ('{current_round_info}') does not require processing of previous results. Skipping.\")\n",
    "            return True\n",
    "            \n",
    "        logger.info(f\"üéØ Current active window is '{current_round_info}'. Targeting results for Round [{target_round}] Window [{target_window}].\")\n",
    "        \n",
    "        # Filter the DataFrame to only include rows for the target round and window\n",
    "        overall_df['round'] = overall_df['round'].astype(str)\n",
    "        overall_df['window'] = pd.to_numeric(overall_df['window']).astype(int)\n",
    "        \n",
    "        original_rows = len(overall_df)\n",
    "        overall_df = overall_df[\n",
    "            (overall_df['round'] == str(target_round)) &\n",
    "            (overall_df['window'] == int(target_window))\n",
    "        ]\n",
    "        \n",
    "        if overall_df.empty:\n",
    "            logger.warning(f\"‚ö†Ô∏è No data found in 'overallBossResults.xlsx' for the target Round [{target_round}] Window [{target_window}]. (Checked {original_rows} rows). Skipping.\")\n",
    "            return True\n",
    "        \n",
    "        logger.info(f\"‚úÖ Filtered 'overallBossResults' to {len(overall_df)} rows for the target round and window.\")\n",
    "\n",
    "        if not hasattr(self, 'update_bid_result'):\n",
    "            self.update_bid_result = []\n",
    "            \n",
    "        overall_df['round'] = overall_df['round'].astype(str)\n",
    "        overall_df['window'] = pd.to_numeric(overall_df['window']).astype(int)\n",
    "        \n",
    "        grouped_results = overall_df.groupby(['round', 'window'])\n",
    "        \n",
    "        new_records_count = 0\n",
    "        updated_records_count = 0\n",
    "        failed_count = 0\n",
    "\n",
    "        for (current_round, current_window), group in grouped_results:\n",
    "            logger.info(f\"üìä Processing results for Round {current_round} Window {current_window}\")\n",
    "            \n",
    "            acad_term_id = \"AY202526T1\"\n",
    "            window_key = (acad_term_id, str(current_round), int(current_window))\n",
    "            bid_window_id = self.bid_window_cache.get(window_key)\n",
    "\n",
    "            if not bid_window_id:\n",
    "                logger.warning(f\"‚ö†Ô∏è Could not find bid_window_id for {window_key}, creating it now.\")\n",
    "                new_bid_window = {\n",
    "                    'id': self.bid_window_id_counter, 'acad_term_id': acad_term_id,\n",
    "                    'round': str(current_round), 'window': int(current_window)\n",
    "                }\n",
    "                self.new_bid_windows.append(new_bid_window)\n",
    "                self.bid_window_cache[window_key] = self.bid_window_id_counter\n",
    "                bid_window_id = self.bid_window_id_counter\n",
    "                self.boss_stats['bid_windows_created'] += 1\n",
    "                self.bid_window_id_counter += 1\n",
    "\n",
    "            # Track rows with bid data for debugging\n",
    "            rows_with_bid_data = 0\n",
    "\n",
    "            for idx, row in group.iterrows():\n",
    "                try:\n",
    "                    # FIXED: Use Course Code + Section instead of class_boss_id\n",
    "                    course_code = self._get_column_value(row, ['Course Code', 'course_code', 'Course_Code'])\n",
    "                    section = self._get_column_value(row, ['Section', 'section'])\n",
    "                    \n",
    "                    if pd.isna(course_code) or pd.isna(section):\n",
    "                        continue\n",
    "\n",
    "                    # Find class_boss_id from the raw data using course_code + section\n",
    "                    class_boss_id = self._find_class_boss_id_from_course_section(course_code, section, acad_term_id)\n",
    "                    \n",
    "                    if not class_boss_id:\n",
    "                        failed_count += 1\n",
    "                        self.failed_mappings.append({\n",
    "                            'course_code': course_code, 'section': section, 'acad_term_id': acad_term_id,\n",
    "                            'round': current_round, 'window': current_window, 'reason': 'class_boss_id_not_found',\n",
    "                            'bidding_window_str': row.get(bidding_window_col, '')\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    class_ids = self.find_all_class_ids(acad_term_id, class_boss_id)\n",
    "                    \n",
    "                    if not class_ids:\n",
    "                        failed_count += 1\n",
    "                        self.failed_mappings.append({\n",
    "                            'course_code': course_code, 'section': section, 'acad_term_id': acad_term_id,\n",
    "                            'round': current_round, 'window': current_window, 'reason': 'class_not_found',\n",
    "                            'bidding_window_str': row.get(bidding_window_col, '')\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    # Find the correct column names (case-insensitive)\n",
    "                    median_bid = None\n",
    "                    min_bid = None\n",
    "                    \n",
    "                    # Map standardized names to actual column names\n",
    "                    for col in row.index:\n",
    "                        col_lower = str(col).lower()\n",
    "                        if 'median' in col_lower and 'bid' in col_lower:\n",
    "                            median_bid = row[col]\n",
    "                        elif 'min' in col_lower and 'bid' in col_lower:\n",
    "                            min_bid = row[col]\n",
    "\n",
    "                    # DEBUG: Log first few rows with bid data\n",
    "                    if pd.notna(median_bid) or pd.notna(min_bid):\n",
    "                        rows_with_bid_data += 1\n",
    "                        if rows_with_bid_data <= 3:\n",
    "                            logger.info(f\"üîç DEBUG: {course_code}-{section} has median={median_bid}, min={min_bid}\")\n",
    "\n",
    "                    for class_id in class_ids:\n",
    "                        def safe_int(val): return int(val) if pd.notna(val) else None\n",
    "                        def safe_float(val): return float(val) if pd.notna(val) else None\n",
    "\n",
    "                        # Map all the column names properly\n",
    "                        result_data = {\n",
    "                            'bid_window_id': bid_window_id, \n",
    "                            'class_id': class_id,\n",
    "                            'vacancy': safe_int(self._get_column_value(row, ['Vacancy', 'vacancy'])),\n",
    "                            'opening_vacancy': safe_int(self._get_column_value(row, ['Opening Vacancy', 'opening_vacancy', 'Opening_Vacancy'])),\n",
    "                            'before_process_vacancy': safe_int(self._get_column_value(row, ['Before Process Vacancy', 'before_process_vacancy', 'Before_Process_Vacancy'])),\n",
    "                            'dice': safe_int(self._get_column_value(row, ['D.I.C.E', 'dice', 'd_i_c_e', 'DICE'])),\n",
    "                            'after_process_vacancy': safe_int(self._get_column_value(row, ['After Process Vacancy', 'after_process_vacancy', 'After_Process_Vacancy'])),\n",
    "                            'enrolled_students': safe_int(self._get_column_value(row, ['Enrolled Students', 'enrolled_students', 'Enrolled_Students'])),\n",
    "                            'median': safe_float(median_bid),\n",
    "                            'min': safe_float(min_bid)\n",
    "                        }\n",
    "                        \n",
    "                        exists, existing_record = self.check_record_exists_in_cache(\n",
    "                            'bid_result',\n",
    "                            {'bid_window_id': bid_window_id, 'class_id': class_id},\n",
    "                            ['bid_window_id', 'class_id']\n",
    "                        )\n",
    "\n",
    "                        if exists:\n",
    "                            # Always update when processing overall results (they have the final bid data)\n",
    "                            self.update_bid_result.append(result_data)\n",
    "                            updated_records_count += 1\n",
    "                            if (pd.notna(median_bid) or pd.notna(min_bid)) and updated_records_count <= 5:\n",
    "                                logger.info(f\"üìä UPDATE: {course_code}-{section} with median={median_bid}, min={min_bid}\")\n",
    "                        else:\n",
    "                            self.new_bid_result.append(result_data)\n",
    "                            self.boss_stats['bid_results_created'] += 1\n",
    "                            new_records_count += 1\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row for {row.get('Course Code', 'unknown')}-{row.get('Section', 'unknown')}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            logger.info(f\"‚úÖ Round {current_round} Window {current_window}: {rows_with_bid_data} rows had bid data\")\n",
    "\n",
    "        self.boss_stats['failed_mappings'] += failed_count\n",
    "        logger.info(\"‚úÖ Overall Results Processing Complete.\")\n",
    "        logger.info(f\"  - Records to CREATE: {new_records_count}\")\n",
    "        logger.info(f\"  - Records to UPDATE: {updated_records_count}\")\n",
    "        logger.info(f\"  - Failed Mappings: {failed_count}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _get_column_value(self, row, possible_names):\n",
    "        \"\"\"Helper method to get column value by trying multiple possible column names\"\"\"\n",
    "        for name in possible_names:\n",
    "            if name in row.index:\n",
    "                return row[name]\n",
    "        return None\n",
    "\n",
    "    def _find_class_boss_id_from_course_section(self, course_code, section, acad_term_id):\n",
    "        \"\"\"Find class_boss_id from course_code + section + acad_term_id\"\"\"\n",
    "        if not hasattr(self, 'standalone_data') or self.standalone_data is None:\n",
    "            return None\n",
    "        \n",
    "        # Look up in standalone_data\n",
    "        matches = self.standalone_data[\n",
    "            (self.standalone_data['course_code'] == course_code) &\n",
    "            (self.standalone_data['section'].astype(str) == str(section).strip()) &\n",
    "            (self.standalone_data['acad_term_id'] == acad_term_id)\n",
    "        ]\n",
    "        \n",
    "        if not matches.empty:\n",
    "            return matches.iloc[0].get('class_boss_id')\n",
    "        \n",
    "        return None\n",
    "     \n",
    "    def _parse_boss_aliases(self, boss_aliases_val: any) -> list[str]:\n",
    "        \"\"\"\n",
    "        Robustly parses the boss_aliases value from various formats into a clean list of strings.\n",
    "\n",
    "        This function correctly handles:\n",
    "        - None, pd.isna(), or other \"empty\" values.\n",
    "        - A standard Python list.\n",
    "        - A NumPy array.\n",
    "        - A raw PostgreSQL array string (e.g., '{\"item1\",\"item2\"}').\n",
    "        - A JSON-formatted string array (e.g., '[\"item1\", \"item2\"]').\n",
    "\n",
    "        Returns:\n",
    "            A clean Python list of strings. Returns an empty list for any invalid or empty input.\n",
    "        \"\"\"\n",
    "        # Return an empty list for any \"empty\" or None-like value.\n",
    "        if boss_aliases_val is None:\n",
    "            return []\n",
    "        \n",
    "        # Handle arrays/lists before using pd.isna\n",
    "        if hasattr(boss_aliases_val, '__len__') and not isinstance(boss_aliases_val, str):\n",
    "            # It's already an array/list, so check if it's empty\n",
    "            if len(boss_aliases_val) == 0:\n",
    "                return []\n",
    "            # If it's a non-empty array, process it\n",
    "            if isinstance(boss_aliases_val, list):\n",
    "                return [str(item).strip() for item in boss_aliases_val if item and str(item).strip()]\n",
    "            elif hasattr(boss_aliases_val, 'tolist'):\n",
    "                # NumPy array\n",
    "                return [str(item).strip() for item in boss_aliases_val.tolist() if item and str(item).strip()]\n",
    "            else:\n",
    "                # Other iterable\n",
    "                return [str(item).strip() for item in boss_aliases_val if item and str(item).strip()]\n",
    "        \n",
    "        # Now safe to use pd.isna for non-array values\n",
    "        try:\n",
    "            if pd.isna(boss_aliases_val):\n",
    "                return []\n",
    "        except:\n",
    "            # If pd.isna fails for any reason, continue processing\n",
    "            pass\n",
    "\n",
    "        # Handle standard Python list.\n",
    "        if isinstance(boss_aliases_val, list):\n",
    "            return [str(item).strip() for item in boss_aliases_val if item and str(item).strip()]\n",
    "\n",
    "        # Handle NumPy array by checking for the .tolist() method.\n",
    "        if hasattr(boss_aliases_val, 'tolist'):\n",
    "            return [str(item).strip() for item in boss_aliases_val.tolist() if item and str(item).strip()]\n",
    "\n",
    "        # Handle various string formats.\n",
    "        if isinstance(boss_aliases_val, str):\n",
    "            aliases_str = boss_aliases_val.strip()\n",
    "            \n",
    "            if not aliases_str:\n",
    "                return []\n",
    "                \n",
    "            # Case 1: PostgreSQL array format '{\"item1\",\"item2\"}'\n",
    "            if aliases_str.startswith('{') and aliases_str.endswith('}'):\n",
    "                content = aliases_str[1:-1]\n",
    "                # Split by comma, then strip whitespace and quotes from each item.\n",
    "                return [item.strip().strip('\"') for item in content.split(',') if item.strip()]\n",
    "\n",
    "            # Case 2: JSON array format '[\"item1\", \"item2\"]'\n",
    "            if aliases_str.startswith('[') and aliases_str.endswith(']'):\n",
    "                try:\n",
    "                    parsed_list = json.loads(aliases_str)\n",
    "                    if isinstance(parsed_list, list):\n",
    "                        return [str(item).strip() for item in parsed_list if item and str(item).strip()]\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    # If JSON is malformed, fall back to treating it as a plain string.\n",
    "                    pass\n",
    "\n",
    "            # Case 3: A single alias provided as a plain string.\n",
    "            return [aliases_str]\n",
    "\n",
    "        # Fallback for other iterable types like tuples or sets.\n",
    "        if hasattr(boss_aliases_val, '__iter__'):\n",
    "            return [str(item).strip() for item in boss_aliases_val if item and str(item).strip()]\n",
    "            \n",
    "        return []\n",
    "\n",
    "    def _extract_unique_professors(self) -> Tuple[set, dict]:\n",
    "        \"\"\"Extracts unique professor names and their variations from the raw data.\"\"\"\n",
    "        unique_professors = set()\n",
    "        professor_variations = defaultdict(set)\n",
    "\n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            prof_name_raw = row.get('professor_name')\n",
    "            if prof_name_raw is None or pd.isna(prof_name_raw):\n",
    "                continue\n",
    "            \n",
    "            prof_name = str(prof_name_raw).strip()\n",
    "            if not prof_name or prof_name.lower() in ['nan', 'tba', 'to be announced']:\n",
    "                continue\n",
    "            \n",
    "            split_professors = self._split_professor_names(prof_name)\n",
    "            for individual in split_professors:\n",
    "                clean_prof = individual.strip()\n",
    "                if clean_prof:\n",
    "                    unique_professors.add(clean_prof)\n",
    "                    if ', ' in clean_prof:\n",
    "                        parts = clean_prof.split(', ')\n",
    "                        if len(parts) == 2:\n",
    "                            base_name = parts[0].strip()\n",
    "                            extension = parts[1].strip()\n",
    "                            if len(extension.split()) == 1:\n",
    "                                professor_variations[clean_prof].add(base_name)\n",
    "                                professor_variations[clean_prof].add(clean_prof)\n",
    "                                if base_name in professor_variations:\n",
    "                                    professor_variations[base_name].add(clean_prof)\n",
    "                    else:\n",
    "                        professor_variations[clean_prof].add(clean_prof)\n",
    "        \n",
    "        return unique_professors, professor_variations\n",
    "\n",
    "    def _normalize_professors_batch(self, names_to_process: list) -> dict:\n",
    "        \"\"\"\n",
    "        Normalizes a list of professor names using the pre-configured LLM model,\n",
    "        with a rule-based fallback.\n",
    "        \"\"\"\n",
    "        normalized_map = {}\n",
    "        if not names_to_process:\n",
    "            return normalized_map\n",
    "\n",
    "        # --- LLM Pathway ---\n",
    "        try:\n",
    "            # Check if the model was successfully initialized in __init__\n",
    "            if not self.llm_model:\n",
    "                raise ValueError(\"LLM model not configured. Check API key.\")\n",
    "\n",
    "            total_batches = (len(names_to_process) + self.llm_batch_size - 1) // self.llm_batch_size\n",
    "            logger.info(f\"üß™ Normalizing {len(names_to_process)} names in {total_batches} batches using '{self.llm_model_name}'...\")\n",
    "\n",
    "            for i in range(0, len(names_to_process), self.llm_batch_size):\n",
    "                batch_names = names_to_process[i:i + self.llm_batch_size]\n",
    "                logger.info(f\"  -> Processing batch {i//self.llm_batch_size + 1} of {total_batches} ({len(batch_names)} names)...\")\n",
    "                \n",
    "                response = self.llm_model.generate_content(\n",
    "                    contents=f\"{self.llm_prompt}\\n\\n{json.dumps(batch_names)}\"\n",
    "                )\n",
    "\n",
    "                # Robustly find the JSON block within the response text\n",
    "                match = re.search(r'\\[.*\\]', response.text, re.DOTALL)\n",
    "                if not match:\n",
    "                    raise ValueError(\"LLM response did not contain a valid JSON array.\")\n",
    "                \n",
    "                json_text = match.group(0)\n",
    "                surnames = json.loads(json_text)\n",
    "                \n",
    "                if not isinstance(surnames, list) or len(surnames) != len(batch_names):\n",
    "                    raise ValueError(f\"LLM returned malformed data for batch {i//self.llm_batch_size + 1}.\")\n",
    "\n",
    "                for original_name, surname in zip(batch_names, surnames):\n",
    "                    name_str = str(original_name).strip().replace(\"‚Äô\", \"'\")\n",
    "                    name_str = re.sub(r'\\s*\\(.*\\)\\s*', ' ', name_str).strip()\n",
    "                    words = name_str.split()\n",
    "                    words_no_initials = [word for word in words if not (len(word) == 1 and word.isalpha()) and not (len(word) == 2 and word.endswith('.'))]\n",
    "                    boss_name = ' '.join(words_no_initials).upper()\n",
    "\n",
    "                    name_parts = re.split(r'([ ,])', original_name)\n",
    "                    afterclass_parts = []\n",
    "                    surname_found = False\n",
    "                    for part in name_parts:\n",
    "                        if not surname_found and part.strip(\" ,\").upper() == surname.upper():\n",
    "                            afterclass_parts.append(part.upper())\n",
    "                            surname_found = True\n",
    "                        else:\n",
    "                            afterclass_parts.append(part.capitalize())\n",
    "                    afterclass_name = \"\".join(afterclass_parts)\n",
    "                    normalized_map[original_name] = (boss_name, afterclass_name)\n",
    "                \n",
    "                time.sleep(6)\n",
    "            \n",
    "            logger.info(\"‚úÖ Batch normalization completed using Gemini LLM.\")\n",
    "\n",
    "        # --- Fallback Pathway ---\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è LLM normalization failed ({e}). Falling back to rule-based method.\")\n",
    "            normalized_map.clear() # Ensure map is empty before filling\n",
    "            for name in names_to_process:\n",
    "                normalized_map[name] = self._normalize_professor_name_fallback(name)\n",
    "        \n",
    "        return normalized_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Phase 1 - Professor and Course Processing with Enhanced Schema Support**\n",
    "\n",
    "**What This Does:**\n",
    "- Initializes the TableBuilder system and connects to PostgreSQL database for existing data validation\n",
    "- Processes professors from raw data with advanced name normalization handling Asian, Western, and mixed naming patterns using enhanced schema with `boss_aliases` JSON arrays\n",
    "- Resolves professor email addresses automatically using Microsoft Outlook integration with improved duplicate detection that excludes default emails\n",
    "- Handles hardcoded multi-instructor combinations and prevents duplicate professor creation through multiple validation strategies including better NaN handling from raw_data.xlsx\n",
    "- Creates new courses from standalone data and automatically maps them to SMU faculties using course code prefix patterns from existing database courses\n",
    "- Generates academic terms with proper ID formatting and date range extraction from multiple sources\n",
    "- Outputs verification files for manual review: `new_professors.csv` for name corrections with enhanced validation and `new_courses.csv` for faculty validation\n",
    "- Provides detailed statistics on professors created, courses processed, automated faculty mappings applied, and comprehensive error tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TableBuilder\n",
    "builder = TableBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 1 (professors, courses, acad_terms)\n",
    "success = builder.run_phase1_professors_and_courses()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Phase 1 completed successfully!\")\n",
    "    print(\"üìù Next steps:\")\n",
    "    print(\"   1. Review script_output/verify/new_professors.csv\")\n",
    "    print(\"   2. Manually correct any professor names if needed\")\n",
    "    print(\"   3. Run Phase 2 in the next cell\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Phase 1 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 2: Professor Name Review and Correction Interface**\n",
    "\n",
    "**What This Does:**\n",
    "- Loads the generated `new_professors.csv` file from the verification directory\n",
    "- Displays a comparison table showing four name formats: original scraped name, boss format (ALL CAPS), afterclass format (Title Case), and the final processed name\n",
    "- Provides clear instructions for manual correction focusing only on the 'name' column (afterclass format)\n",
    "- Guides users to preserve the boss_name format while correcting any parsing errors or name formatting issues\n",
    "- Handles empty files gracefully when all professors already exist in the database\n",
    "- Prepares corrected data for Phase 2 processing by maintaining proper name mapping relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display new professors for review\n",
    "new_prof_path = os.path.join('script_output', 'verify', 'new_professors.csv')\n",
    "if os.path.exists(new_prof_path):\n",
    "    df = pd.read_csv(new_prof_path)\n",
    "    if not df.empty:\n",
    "        print(f\"üìã {len(df)} new professors created:\")\n",
    "        print(\"\\nüîç Review these professor names:\")\n",
    "        display(df[['name', 'boss_aliases', 'original_scraped_name']])\n",
    "        print(\"\\nüìù If any names need correction, edit the 'name' column in:\")\n",
    "        print(f\"   {new_prof_path}\")\n",
    "        print(\"\\n‚ö†Ô∏è  Only edit the 'name' column (afterclass format)\")\n",
    "        print(\"   Keep 'boss_aliases' unchanged\")\n",
    "    else:\n",
    "        print(\"‚úÖ No new professors created - all professors already exist in database\")\n",
    "else:\n",
    "    print(\"‚úÖ No new professors file - all professors already exist in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 3: Phase 2 - Class and Timing Processing with Corrected Professor Data**\n",
    "\n",
    "**What This Does:**\n",
    "- Reads manually corrected professor names from verification CSV files and updates internal lookup tables\n",
    "- Processes classes from standalone data using corrected professor mappings and established course relationships\n",
    "- Handles complex professor assignments including single professors, JSON arrays for multi-instructor classes, and missing professor scenarios\n",
    "- Generates class timing records (weekly schedules) and exam timing records with proper foreign key relationships\n",
    "- Links all timing data to valid class IDs while maintaining referential integrity\n",
    "- Creates complete set of database-ready CSV files: `new_classes.csv`, `new_class_timing.csv`, `new_class_exam_timing.csv`\n",
    "- Provides comprehensive error reporting for validation issues and successful record creation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 2 (classes, timings) after manual correction\n",
    "success = builder.run_phase2_remaining_tables()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Phase 2 completed successfully!\")\n",
    "    print(\"üìù All tables generated with corrected professor names\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Phase 2 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 4: Interactive Faculty Assignment for Unmapped Courses**\n",
    "\n",
    "**What This Does:**\n",
    "- Identifies courses that still require manual faculty assignment after automated BOSS-based mapping\n",
    "- Opens scraped HTML course outline files in web browser for informed faculty assignment decisions  \n",
    "- Presents interactive menu of SMU's schools and centers with options to create new faculties for unmapped departments\n",
    "- Provides course code, name, and content preview to guide proper faculty placement decisions\n",
    "- Updates course records with selected faculty assignments and maintains faculty cache consistency\n",
    "- Allows skipping courses that need additional research while preserving assignment workflow\n",
    "- Re-saves updated CSV files with complete faculty information for database insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run faculty assignment process if needed\n",
    "if hasattr(builder, 'courses_needing_faculty') and builder.courses_needing_faculty:\n",
    "    builder.assign_course_faculties()\n",
    "    print(\"\\n‚úÖ Faculty assignment completed!\")\n",
    "else:\n",
    "    print(\"‚úÖ No courses need faculty assignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 5: BOSS Bidding Data Processing from Raw Data Integration**\n",
    "\n",
    "**What This Does:**\n",
    "- Processes BOSS bidding data directly from `script_input/raw_data.xlsx` standalone sheet containing integrated bidding information from the HTML extraction phase\n",
    "- Extracts bidding metrics including total capacity, current enrollment, reserved seats, available seats, extraction timestamps, and bidding window information\n",
    "- Parses complex bidding window formats including standard rounds (Round 1, 1A, 1B), incoming exchange rounds (Round 1C), and incoming freshmen rounds (Round 1F) with proper hierarchical ordering\n",
    "- Creates comprehensive bid window records following SMU's bidding system rules with automatic deduplication based on academic term, round, and window combinations\n",
    "- Maps course codes and sections from raw bidding data to existing class records using multiple fallback strategies including memory cache, database cache, and CSV file lookups\n",
    "- Generates three interconnected database tables: `new_bid_window.csv` for bidding periods, `new_class_availability.csv` for seat availability tracking, and `new_bid_result.csv` for bidding outcomes\n",
    "- Handles complex class-professor relationships by creating records for all class IDs associated with each course/section/term combination to support multi-instructor scenarios\n",
    "- Provides comprehensive error tracking and failed mapping analysis with detailed logging for troubleshooting data integration issues\n",
    "- Creates processing logs with timestamps, validation statistics, and detailed analysis of unmapped records for quality assurance\n",
    "- Supports academic year variations and bidding rule changes while maintaining backward compatibility with historical data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete Phase 3 pipeline\n",
    "print(\"üöÄ Starting Phase 3: BOSS Results Processing\")\n",
    "success = builder.run_phase3_boss_processing()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Phase 3 completed successfully!\")\n",
    "    builder.close_connection()\n",
    "else:\n",
    "    print(\"\\n‚ùå Phase 3 failed. Check logs for details.\")\n",
    "\n",
    "# Check failed mappings (if any)\n",
    "failed_path = os.path.join('script_output', 'failed_boss_results_mapping.csv')\n",
    "if os.path.exists(failed_path):\n",
    "    failed_df = pd.read_csv(failed_path)\n",
    "    print(f\"‚ö†Ô∏è {len(failed_df)} failed mappings found:\")\n",
    "    display(failed_df.head(10))\n",
    "    print(f\"\\nüìù Review failed mappings in: {failed_path}\")\n",
    "else:\n",
    "    print(\"‚úÖ No failed mappings - all BOSS results mapped successfully!\")\n",
    "\n",
    "# Inspect generated data\n",
    "print(\"üìã Generated Data Summary:\")\n",
    "\n",
    "# Check bid windows\n",
    "bid_window_path = os.path.join('script_output', 'new_bid_window.csv')\n",
    "if os.path.exists(bid_window_path):\n",
    "    bw_df = pd.read_csv(bid_window_path)\n",
    "    print(f\"\\nü™ü Bid Windows ({len(bw_df)} records):\")\n",
    "\n",
    "# Check class availability\n",
    "availability_path = os.path.join('script_output', 'new_class_availability.csv')\n",
    "if os.path.exists(availability_path):\n",
    "    av_df = pd.read_csv(availability_path)\n",
    "    print(f\"\\nüìä Class Availability ({len(av_df)} records):\")\n",
    "\n",
    "# Check bid results\n",
    "result_path = os.path.join('script_output', 'new_bid_result.csv')\n",
    "if os.path.exists(result_path):\n",
    "    br_df = pd.read_csv(result_path)\n",
    "    print(f\"\\nüìà Bid Results ({len(br_df)} records):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 6: Comprehensive Data Integrity Validation**\n",
    "\n",
    "**What This Does:**\n",
    "- Validates referential integrity across all generated CSV files by checking foreign key relationships between tables\n",
    "- Loads valid IDs from multiple sources: database cache files, new CSV files, professor lookup tables, and verification files\n",
    "- Performs comprehensive validation of course_id references in classes, professor_id fields (both single UUIDs and JSON arrays), and class_id references in timing tables\n",
    "- Checks UUID format validity and ensures all referenced IDs exist in their respective source tables\n",
    "- Generates detailed error reports with specific row numbers, invalid IDs, and raw professor names for debugging\n",
    "- Creates validation summary statistics including total records checked, error counts by type, and data loading metrics\n",
    "- Provides categorized error analysis for professor ID issues including format errors, missing references, and null assignments\n",
    "- Saves validation results to CSV files: `validation_errors.csv`, `validation_warnings.csv`, and `validation_summary.csv` for comprehensive quality assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataIntegrityValidator:\n",
    "    \"\"\"Validates data integrity across generated CSV files and database cache\"\"\"\n",
    "    \n",
    "    def __init__(self, output_base='script_output', cache_dir='db_cache'):\n",
    "        self.output_base = output_base\n",
    "        self.verify_dir = os.path.join(output_base, 'verify')\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        os.makedirs(self.verify_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Data containers\n",
    "        self.valid_course_ids = set()\n",
    "        self.valid_professor_ids = set()\n",
    "        self.valid_class_ids = set()\n",
    "        \n",
    "        # Professor lookup mapping\n",
    "        self.professor_lookup = {}\n",
    "        \n",
    "        # Validation results\n",
    "        self.validation_errors = []\n",
    "        self.validation_warnings = []\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_classes_checked': 0,\n",
    "            'total_timings_checked': 0,\n",
    "            'total_exam_timings_checked': 0,\n",
    "            'course_id_errors': 0,\n",
    "            'professor_id_errors': 0,\n",
    "            'professor_id_format_errors': 0,\n",
    "            'class_id_errors': 0,\n",
    "            'warnings': 0,\n",
    "            'professors_created': 0,\n",
    "            'professors_updated': 0,\n",
    "            'courses_created': 0,\n",
    "            'courses_updated': 0,\n",
    "            'courses_needing_faculty': 0,\n",
    "            'classes_created': 0,\n",
    "            'timings_created': 0,\n",
    "            'exams_created': 0\n",
    "        }\n",
    "        \n",
    "        # Initialize new_acad_terms for compatibility\n",
    "        self.new_acad_terms = []\n",
    "    \n",
    "    def is_valid_uuid(self, uuid_string):\n",
    "        \"\"\"Check if a string is a valid UUID format\"\"\"\n",
    "        if not uuid_string or pd.isna(uuid_string):\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            uuid_pattern = re.compile(\n",
    "                r'^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$',\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            return bool(uuid_pattern.match(str(uuid_string).strip()))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"UUID validation error for {uuid_string}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def safe_read_csv(self, file_path, required_columns=None):\n",
    "        \"\"\"Safely read CSV file with error handling\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            if os.path.getsize(file_path) == 0:\n",
    "                logger.warning(f\"File is empty: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if df.empty:\n",
    "                logger.warning(f\"CSV file is empty: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            if required_columns:\n",
    "                missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                if missing_columns:\n",
    "                    logger.warning(f\"Missing columns in {file_path}: {missing_columns}\")\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading CSV {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def safe_read_pickle(self, file_path):\n",
    "        \"\"\"Safely read pickle file with error handling\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"Pickle file not found: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.read_pickle(file_path)\n",
    "            return df if not df.empty else pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading pickle {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_valid_course_ids(self):\n",
    "        \"\"\"Load valid course IDs from new_courses.csv and database cache\"\"\"\n",
    "        logger.info(\"üìö Loading valid course IDs...\")\n",
    "        \n",
    "        # Load from new_courses.csv (verify folder)\n",
    "        new_courses_path = os.path.join(self.verify_dir, 'new_courses.csv')\n",
    "        df = self.safe_read_csv(new_courses_path, ['id'])\n",
    "        if not df.empty:\n",
    "            new_course_ids = set(df['id'].astype(str))\n",
    "            self.valid_course_ids.update(new_course_ids)\n",
    "            logger.info(f\"   ‚úÖ Loaded {len(new_course_ids)} course IDs from new_courses.csv\")\n",
    "        \n",
    "        # Load from database cache\n",
    "        cache_file = os.path.join(self.cache_dir, 'courses_cache.pkl')\n",
    "        courses_df = self.safe_read_pickle(cache_file)\n",
    "        if not courses_df.empty and 'id' in courses_df.columns:\n",
    "            cache_course_ids = set(courses_df['id'].astype(str))\n",
    "            self.valid_course_ids.update(cache_course_ids)\n",
    "            logger.info(f\"   ‚úÖ Loaded {len(cache_course_ids)} course IDs from database cache\")\n",
    "        \n",
    "        logger.info(f\"   üìä Total valid course IDs: {len(self.valid_course_ids)}\")\n",
    "    \n",
    "    def load_valid_professor_ids(self):\n",
    "        \"\"\"Load valid professor IDs from multiple sources including professor_lookup.csv\"\"\"\n",
    "        logger.info(\"üë• Loading valid professor IDs...\")\n",
    "        \n",
    "        # PRIORITY 1: Load from professor_lookup.csv (most authoritative)\n",
    "        lookup_file = 'script_input/professor_lookup.csv'\n",
    "        if os.path.exists(lookup_file):\n",
    "            lookup_df = self.safe_read_csv(lookup_file, ['database_id'])\n",
    "            if not lookup_df.empty and 'database_id' in lookup_df.columns:\n",
    "                lookup_professor_ids = set(lookup_df['database_id'].astype(str))\n",
    "                self.valid_professor_ids.update(lookup_professor_ids)\n",
    "                logger.info(f\"   ‚úÖ Loaded {len(lookup_professor_ids)} professor IDs from professor_lookup.csv\")\n",
    "                \n",
    "                # Also build lookup mapping for analysis\n",
    "                for _, row in lookup_df.iterrows():\n",
    "                    boss_name = row.get('boss_name')\n",
    "                    database_id = str(row.get('database_id'))\n",
    "                    if pd.notna(boss_name) and pd.notna(database_id):\n",
    "                        self.professor_lookup[boss_name] = database_id\n",
    "        \n",
    "        # PRIORITY 2: Load from database cache (professors table)\n",
    "        cache_file = os.path.join(self.cache_dir, 'professors_cache.pkl')\n",
    "        professors_df = self.safe_read_pickle(cache_file)\n",
    "        if not professors_df.empty and 'id' in professors_df.columns:\n",
    "            cache_professor_ids = set(professors_df['id'].astype(str))\n",
    "            self.valid_professor_ids.update(cache_professor_ids)\n",
    "            logger.info(f\"   ‚úÖ Loaded {len(cache_professor_ids)} professor IDs from database cache\")\n",
    "        \n",
    "        # PRIORITY 3: Load from new_professors.csv (verify folder)\n",
    "        new_professors_path = os.path.join(self.verify_dir, 'new_professors.csv')\n",
    "        df = self.safe_read_csv(new_professors_path, ['id'])\n",
    "        if not df.empty:\n",
    "            new_professor_ids = set(df['id'].astype(str))\n",
    "            self.valid_professor_ids.update(new_professor_ids)\n",
    "            logger.info(f\"   ‚úÖ Loaded {len(new_professor_ids)} professor IDs from new_professors.csv\")\n",
    "        \n",
    "        logger.info(f\"   üìä Total valid professor IDs: {len(self.valid_professor_ids)}\")\n",
    "    \n",
    "    def load_valid_class_ids(self):\n",
    "        \"\"\"Load valid class IDs from new_classes.csv\"\"\"\n",
    "        logger.info(\"üè´ Loading valid class IDs...\")\n",
    "        \n",
    "        classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "        df = self.safe_read_csv(classes_path, ['id'])\n",
    "        if not df.empty:\n",
    "            self.valid_class_ids = set(df['id'].astype(str))\n",
    "            logger.info(f\"   ‚úÖ Loaded {len(self.valid_class_ids)} class IDs from new_classes.csv\")\n",
    "        else:\n",
    "            logger.error(f\"   ‚ùå Could not load class IDs from {classes_path}\")\n",
    "        \n",
    "        logger.info(f\"   üìä Total valid class IDs: {len(self.valid_class_ids)}\")\n",
    "    \n",
    "    def parse_professor_ids(self, professor_id_field):\n",
    "        \"\"\"Safely parse professor ID field which can be single ID or JSON array\"\"\"\n",
    "        if pd.isna(professor_id_field) or str(professor_id_field).strip() == '':\n",
    "            return []\n",
    "        \n",
    "        professor_id_str = str(professor_id_field).strip()\n",
    "        \n",
    "        # Check if it's a JSON array\n",
    "        if professor_id_str.startswith('[') and professor_id_str.endswith(']'):\n",
    "            try:\n",
    "                # Handle both single and double quotes\n",
    "                normalized_json = professor_id_str.replace(\"'\", '\"')\n",
    "                parsed_ids = json.loads(normalized_json)\n",
    "                \n",
    "                if isinstance(parsed_ids, list):\n",
    "                    return [str(pid).strip() for pid in parsed_ids if pd.notna(pid)]\n",
    "                else:\n",
    "                    return []\n",
    "            except (json.JSONDecodeError, TypeError) as e:\n",
    "                logger.warning(f\"JSON parsing error for professor_id: {professor_id_str} - {e}\")\n",
    "                return []\n",
    "        else:\n",
    "            # Single professor ID\n",
    "            return [professor_id_str] if professor_id_str else []\n",
    "    \n",
    "    def validate_classes(self):\n",
    "        \"\"\"Validate course_id and professor_id references in new_classes.csv\"\"\"\n",
    "        logger.info(\"üîç Validating new_classes.csv...\")\n",
    "        \n",
    "        classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "        df = self.safe_read_csv(classes_path, ['id', 'course_id'])\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(f\"   ‚ùå Could not validate classes - file not found or empty\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.stats['total_classes_checked'] = len(df)\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    class_id = str(row['id'])\n",
    "                    course_id = str(row['course_id'])\n",
    "                    professor_id_field = row.get('professor_id')\n",
    "                    raw_professor_name = row.get('raw_professor_name', '')\n",
    "                    \n",
    "                    # Validate course_id\n",
    "                    if course_id not in self.valid_course_ids:\n",
    "                        error = {\n",
    "                            'type': 'course_id_missing',\n",
    "                            'file': 'new_classes.csv',\n",
    "                            'row': idx,\n",
    "                            'class_id': class_id,\n",
    "                            'invalid_id': course_id,\n",
    "                            'field': 'course_id'\n",
    "                        }\n",
    "                        self.validation_errors.append(error)\n",
    "                        self.stats['course_id_errors'] += 1\n",
    "                    \n",
    "                    # Validate professor_id\n",
    "                    professor_ids_to_check = self.parse_professor_ids(professor_id_field)\n",
    "                    \n",
    "                    if professor_ids_to_check:\n",
    "                        for prof_id in professor_ids_to_check:\n",
    "                            prof_id_str = str(prof_id).strip()\n",
    "                            \n",
    "                            # Check UUID format\n",
    "                            if not self.is_valid_uuid(prof_id_str):\n",
    "                                error = {\n",
    "                                    'type': 'professor_id_invalid_uuid',\n",
    "                                    'file': 'new_classes.csv',\n",
    "                                    'row': idx,\n",
    "                                    'class_id': class_id,\n",
    "                                    'invalid_id': prof_id_str,\n",
    "                                    'field': 'professor_id',\n",
    "                                    'raw_professor_name': raw_professor_name,\n",
    "                                    'course_id': course_id\n",
    "                                }\n",
    "                                self.validation_errors.append(error)\n",
    "                                self.stats['professor_id_format_errors'] += 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Check if professor exists\n",
    "                            if prof_id_str not in self.valid_professor_ids:\n",
    "                                error = {\n",
    "                                    'type': 'professor_id_not_found',\n",
    "                                    'file': 'new_classes.csv',\n",
    "                                    'row': idx,\n",
    "                                    'class_id': class_id,\n",
    "                                    'invalid_id': prof_id_str,\n",
    "                                    'field': 'professor_id',\n",
    "                                    'raw_professor_name': raw_professor_name,\n",
    "                                    'course_id': course_id\n",
    "                                }\n",
    "                                self.validation_errors.append(error)\n",
    "                                self.stats['professor_id_errors'] += 1\n",
    "                    else:\n",
    "                        # Warning for missing professor\n",
    "                        warning = {\n",
    "                            'type': 'professor_id_null',\n",
    "                            'file': 'new_classes.csv',\n",
    "                            'row': idx,\n",
    "                            'class_id': class_id,\n",
    "                            'message': 'No professors found for class',\n",
    "                            'raw_professor_name': raw_professor_name,\n",
    "                            'course_id': course_id\n",
    "                        }\n",
    "                        self.validation_warnings.append(warning)\n",
    "                        self.stats['warnings'] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"   ‚úÖ Validated {len(df)} classes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   ‚ùå Error validating classes: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def validate_class_timings(self):\n",
    "        \"\"\"Validate class_id references in new_class_timing.csv\"\"\"\n",
    "        logger.info(\"‚è∞ Validating new_class_timing.csv...\")\n",
    "        \n",
    "        timings_path = os.path.join(self.output_base, 'new_class_timing.csv')\n",
    "        df = self.safe_read_csv(timings_path, ['class_id'])\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(f\"   ‚ö†Ô∏è new_class_timing.csv not found or empty\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.stats['total_timings_checked'] = len(df)\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    class_id = str(row['class_id'])\n",
    "                    \n",
    "                    if class_id not in self.valid_class_ids:\n",
    "                        error = {\n",
    "                            'type': 'class_id_missing',\n",
    "                            'file': 'new_class_timing.csv',\n",
    "                            'row': idx,\n",
    "                            'invalid_id': class_id,\n",
    "                            'field': 'class_id'\n",
    "                        }\n",
    "                        self.validation_errors.append(error)\n",
    "                        self.stats['class_id_errors'] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing timing row {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"   ‚úÖ Validated {len(df)} class timings\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   ‚ùå Error validating class timings: {e}\")\n",
    "    \n",
    "    def validate_exam_timings(self):\n",
    "        \"\"\"Validate class_id references in new_class_exam_timing.csv\"\"\"\n",
    "        logger.info(\"üìù Validating new_class_exam_timing.csv...\")\n",
    "        \n",
    "        exam_timings_path = os.path.join(self.output_base, 'new_class_exam_timing.csv')\n",
    "        df = self.safe_read_csv(exam_timings_path, ['class_id'])\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(f\"   ‚ö†Ô∏è new_class_exam_timing.csv not found or empty\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.stats['total_exam_timings_checked'] = len(df)\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    class_id = str(row['class_id'])\n",
    "                    \n",
    "                    if class_id not in self.valid_class_ids:\n",
    "                        error = {\n",
    "                            'type': 'class_id_missing',\n",
    "                            'file': 'new_class_exam_timing.csv',\n",
    "                            'row': idx,\n",
    "                            'invalid_id': class_id,\n",
    "                            'field': 'class_id'\n",
    "                        }\n",
    "                        self.validation_errors.append(error)\n",
    "                        self.stats['class_id_errors'] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing exam timing row {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"   ‚úÖ Validated {len(df)} exam timings\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   ‚ùå Error validating exam timings: {e}\")\n",
    "    \n",
    "    def analyze_professor_issues(self):\n",
    "        \"\"\"Analyze professor-related issues in detail\"\"\"\n",
    "        logger.info(\"üî¨ Analyzing professor issues...\")\n",
    "        \n",
    "        # Group professor errors by type\n",
    "        error_types = {}\n",
    "        for error in self.validation_errors:\n",
    "            if 'professor_id' in error['type']:\n",
    "                error_type = error['type']\n",
    "                if error_type not in error_types:\n",
    "                    error_types[error_type] = []\n",
    "                error_types[error_type].append(error)\n",
    "        \n",
    "        if error_types:\n",
    "            print(f\"\\nüìä PROFESSOR ID ERROR ANALYSIS:\")\n",
    "            for error_type, errors in error_types.items():\n",
    "                print(f\"\\n   Error Type: {error_type}\")\n",
    "                print(f\"   Count: {len(errors)}\")\n",
    "                \n",
    "                # Show unique invalid IDs for this error type\n",
    "                unique_invalid_ids = set()\n",
    "                for error in errors:\n",
    "                    unique_invalid_ids.add(error['invalid_id'])\n",
    "                \n",
    "                print(f\"   Unique Invalid IDs: {len(unique_invalid_ids)}\")\n",
    "                \n",
    "                # Show sample errors\n",
    "                print(f\"   Sample errors:\")\n",
    "                for i, error in enumerate(errors[:3]):\n",
    "                    print(f\"     {i+1}. Class {error['class_id']} - Raw name: {error.get('raw_professor_name', 'N/A')}\")\n",
    "                    print(f\"        Invalid ID: {error['invalid_id']}\")\n",
    "                \n",
    "                if len(errors) > 3:\n",
    "                    print(f\"     ... and {len(errors) - 3} more\")\n",
    "    \n",
    "    def save_validation_report(self):\n",
    "        \"\"\"Save validation errors and warnings to CSV files\"\"\"\n",
    "        logger.info(\"üíæ Saving validation report...\")\n",
    "        \n",
    "        try:\n",
    "            # Save validation errors\n",
    "            if self.validation_errors:\n",
    "                errors_df = pd.DataFrame(self.validation_errors)\n",
    "                errors_path = os.path.join(self.output_base, 'validation_errors.csv')\n",
    "                errors_df.to_csv(errors_path, index=False)\n",
    "                logger.info(f\"   ‚ùå Saved {len(self.validation_errors)} validation errors to validation_errors.csv\")\n",
    "            \n",
    "            # Save validation warnings\n",
    "            if self.validation_warnings:\n",
    "                warnings_df = pd.DataFrame(self.validation_warnings)\n",
    "                warnings_path = os.path.join(self.output_base, 'validation_warnings.csv')\n",
    "                warnings_df.to_csv(warnings_path, index=False)\n",
    "                logger.info(f\"   ‚ö†Ô∏è Saved {len(self.validation_warnings)} validation warnings to validation_warnings.csv\")\n",
    "            \n",
    "            # Save summary report\n",
    "            summary = {\n",
    "                'validation_timestamp': datetime.now().isoformat(),\n",
    "                'total_classes_checked': self.stats['total_classes_checked'],\n",
    "                'total_timings_checked': self.stats['total_timings_checked'],\n",
    "                'total_exam_timings_checked': self.stats['total_exam_timings_checked'],\n",
    "                'total_errors': len(self.validation_errors),\n",
    "                'total_warnings': len(self.validation_warnings),\n",
    "                'course_id_errors': self.stats['course_id_errors'],\n",
    "                'professor_id_errors': self.stats['professor_id_errors'],\n",
    "                'professor_id_format_errors': self.stats['professor_id_format_errors'],\n",
    "                'class_id_errors': self.stats['class_id_errors'],\n",
    "                'valid_course_ids_loaded': len(self.valid_course_ids),\n",
    "                'valid_professor_ids_loaded': len(self.valid_professor_ids),\n",
    "                'valid_class_ids_loaded': len(self.valid_class_ids)\n",
    "            }\n",
    "            \n",
    "            summary_df = pd.DataFrame([summary])\n",
    "            summary_path = os.path.join(self.output_base, 'validation_summary.csv')\n",
    "            summary_df.to_csv(summary_path, index=False)\n",
    "            logger.info(f\"   üìä Saved validation summary to validation_summary.csv\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving validation report: {e}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary with enhanced statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚úÖ Professors created: {self.stats['professors_created']}\")\n",
    "        print(f\"‚úÖ Professors updated: {self.stats.get('professors_updated', 0)}\")\n",
    "        print(f\"‚úÖ Courses created: {self.stats['courses_created']}\")\n",
    "        print(f\"‚úÖ Courses updated: {self.stats['courses_updated']}\")\n",
    "        print(f\"‚ö†Ô∏è  Courses needing faculty: {self.stats['courses_needing_faculty']}\")\n",
    "        print(f\"‚úÖ Classes created: {self.stats['classes_created']}\")\n",
    "        print(f\"‚úÖ Class timings created: {self.stats['timings_created']}\")\n",
    "        print(f\"‚úÖ Exam timings created: {self.stats['exams_created']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìÅ OUTPUT FILES:\")\n",
    "        print(f\"   Verify folder: {self.verify_dir}/\")\n",
    "        print(f\"   - new_professors.csv ({self.stats['professors_created']} records)\")\n",
    "        print(f\"   - new_courses.csv ({self.stats['courses_created']} records)\")\n",
    "        print(f\"   Output folder: {self.output_base}/\")\n",
    "        print(f\"   - update_courses.csv ({self.stats['courses_updated']} records)\")\n",
    "        print(f\"   - update_professor.csv ({self.stats.get('professors_updated', 0)} records)\")\n",
    "        if hasattr(self, 'update_classes') and self.update_classes:\n",
    "            print(f\"   - update_classes.csv ({len(self.update_classes)} records)\")\n",
    "        if hasattr(self, 'update_bid_result') and self.update_bid_result:\n",
    "            print(f\"   - update_bid_result.csv ({len(self.update_bid_result)} records)\")\n",
    "        print(f\"   - new_acad_term.csv ({len(self.new_acad_terms)} records)\")\n",
    "        print(f\"   - new_classes.csv ({self.stats['classes_created']} records)\")\n",
    "        print(f\"   - new_class_timing.csv ({self.stats['timings_created']} records)\")\n",
    "        print(f\"   - new_class_exam_timing.csv ({self.stats['exams_created']} records)\")\n",
    "        print(f\"   - professor_lookup.csv (updated)\")\n",
    "        print(f\"   - courses_needing_faculty.csv ({self.stats['courses_needing_faculty']} records)\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    def run_validation(self):\n",
    "        \"\"\"Run complete data integrity validation\"\"\"\n",
    "        try:\n",
    "            logger.info(\"üöÄ Starting Data Integrity Validation\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Load valid IDs from all sources\n",
    "            self.load_valid_course_ids()\n",
    "            self.load_valid_professor_ids()\n",
    "            self.load_valid_class_ids()\n",
    "            \n",
    "            # Step 2: Validate references\n",
    "            self.validate_classes()\n",
    "            self.validate_class_timings()\n",
    "            self.validate_exam_timings()\n",
    "            \n",
    "            # Step 3: Analyze issues\n",
    "            self.analyze_professor_issues()\n",
    "            \n",
    "            # Step 4: Save and display results\n",
    "            self.save_validation_report()\n",
    "            self.print_summary()\n",
    "            \n",
    "            logger.info(\"\\n‚úÖ Data integrity validation completed!\")\n",
    "            \n",
    "            # Return validation status\n",
    "            return len(self.validation_errors) == 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Validation failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = DataIntegrityValidator()\n",
    "success = validator.run_validation()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ All data integrity checks passed!\")\n",
    "    exit(0)\n",
    "else:\n",
    "    print(\"\\nüí• Data integrity issues found - check error reports!\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_coverage_standalone(output_dir='script_output'):\n",
    "    \"\"\"Standalone function to analyze class coverage and generate detailed report\"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç CLASS COVERAGE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load new_classes.csv\n",
    "    classes_path = os.path.join(output_dir, 'new_classes.csv')\n",
    "    if not os.path.exists(classes_path):\n",
    "        print(f\"‚ùå File not found: {classes_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        classes_df = pd.read_csv(classes_path)\n",
    "        total_classes = len(classes_df)\n",
    "        all_class_ids = set(classes_df['id'].unique())\n",
    "        \n",
    "        # Files to check\n",
    "        files_to_check = {\n",
    "            'new_class_availability.csv': 'class_availability',\n",
    "            'new_class_exam_timing.csv': 'class_exam_timing', \n",
    "            'new_class_timing.csv': 'class_timing',\n",
    "            'new_bid_result.csv': 'bid_result'\n",
    "        }\n",
    "        \n",
    "        coverage_results = {}\n",
    "        orphan_class_ids = defaultdict(list)\n",
    "        \n",
    "        # Load each file and analyze\n",
    "        for filename, table_name in files_to_check.items():\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                coverage_results[table_name] = {\n",
    "                    'found_ids': set(),\n",
    "                    'orphan_ids': set()\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'class_id' in df.columns:\n",
    "                found_class_ids = set(df['class_id'].unique())\n",
    "                \n",
    "                # Check for orphan class_ids\n",
    "                orphan_ids = found_class_ids - all_class_ids\n",
    "                if orphan_ids:\n",
    "                    orphan_class_ids[table_name] = orphan_ids\n",
    "                \n",
    "                # Store valid class_ids\n",
    "                valid_class_ids = found_class_ids & all_class_ids\n",
    "                \n",
    "                coverage_results[table_name] = {\n",
    "                    'found_ids': valid_class_ids,\n",
    "                    'orphan_ids': orphan_ids\n",
    "                }\n",
    "        \n",
    "        # Calculate statistics\n",
    "        print(\"\\nüìä STATISTICS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 1. Total class rows\n",
    "        print(f\"1. Total class rows created: {total_classes}\")\n",
    "        \n",
    "        # 2. Unique course/section/term combinations\n",
    "        unique_combinations = classes_df.groupby(['course_id', 'section', 'acad_term_id']).size()\n",
    "        num_unique_combinations = len(unique_combinations)\n",
    "        print(f\"2. Unique course/section/term combinations: {num_unique_combinations}\")\n",
    "        \n",
    "        # 3. Classes from multiple professors\n",
    "        multi_professor_combinations = unique_combinations[unique_combinations > 1]\n",
    "        total_multi_professor_classes = multi_professor_combinations.sum()\n",
    "        print(f\"3. Class records from multiple professors: {total_multi_professor_classes}\")\n",
    "        \n",
    "        # 4. Unique classes duplicated due to multiple professors\n",
    "        num_duplicated_unique_classes = len(multi_professor_combinations)\n",
    "        print(f\"4. Unique classes duplicated due to multiple professors: {num_duplicated_unique_classes}\")\n",
    "        \n",
    "        # 5. Classes with no BOSS results\n",
    "        no_boss_classes = []\n",
    "        for class_id in all_class_ids:\n",
    "            in_availability = class_id in coverage_results.get('class_availability', {}).get('found_ids', set())\n",
    "            in_bid_result = class_id in coverage_results.get('bid_result', {}).get('found_ids', set())\n",
    "            if not in_availability and not in_bid_result:\n",
    "                no_boss_classes.append(class_id)\n",
    "        print(f\"5. Classes with no BOSS results (no availability/bid_result): {len(no_boss_classes)}\")\n",
    "        \n",
    "        # 6. Classes with no exams but have class timings\n",
    "        has_timing = coverage_results.get('class_timing', {}).get('found_ids', set())\n",
    "        has_exam = coverage_results.get('class_exam_timing', {}).get('found_ids', set())\n",
    "        no_exam_with_timing = has_timing - has_exam\n",
    "        print(f\"6. Classes with class timings but no exams: {len(no_exam_with_timing)}\")\n",
    "        \n",
    "        # 7. Classes with exams but no class timings\n",
    "        exam_no_timing = has_exam - has_timing\n",
    "        print(f\"7. Classes with exams but no class timings: {len(exam_no_timing)}\")\n",
    "        \n",
    "        # 8. Classes with both exams and class timings\n",
    "        both_exam_timing = has_exam & has_timing\n",
    "        print(f\"8. Classes with both exams and class timings: {len(both_exam_timing)}\")\n",
    "        \n",
    "        # 9. Orphan class_ids\n",
    "        total_orphans = sum(len(ids) for ids in orphan_class_ids.values())\n",
    "        print(f\"9. Orphan class_ids (in tables but not in new_classes): {total_orphans}\")\n",
    "        if total_orphans > 0:\n",
    "            for table, ids in orphan_class_ids.items():\n",
    "                print(f\"   - {table}: {len(ids)} orphan IDs\")\n",
    "        \n",
    "        # 10. BOSS records not mapped to scraped data\n",
    "        print(\"\\nüìä Checking BOSS records not mapped to scraped data...\")\n",
    "        \n",
    "        # Load BOSS data to check unmapped records\n",
    "        boss_unmapped_count = 0\n",
    "        try:\n",
    "            import glob\n",
    "            boss_files = glob.glob(os.path.join('script_input', 'overallBossResults', '*.xlsx'))\n",
    "            if boss_files:\n",
    "                # Get unique course/section/term from scraped data\n",
    "                scraped_combinations = set()\n",
    "                for _, row in classes_df.iterrows():\n",
    "                    # Need to get course code from course_id\n",
    "                    course_id = row['course_id']\n",
    "                    section = str(row['section'])\n",
    "                    acad_term_id = row['acad_term_id']\n",
    "                    scraped_combinations.add((course_id, section, acad_term_id))\n",
    "                \n",
    "                # Count unique BOSS combinations not in scraped\n",
    "                boss_unique_combinations = set()\n",
    "                for file_path in boss_files[:1]:  # Sample first file for performance\n",
    "                    boss_df = pd.read_excel(file_path)\n",
    "                    if all(col in boss_df.columns for col in ['Course Code', 'Section', 'Term']):\n",
    "                        for _, row in boss_df.iterrows():\n",
    "                            if pd.notna(row['Course Code']) and pd.notna(row['Section']) and pd.notna(row['Term']):\n",
    "                                course_code = row['Course Code']\n",
    "                                section = str(row['Section'])\n",
    "                                term = row['Term']\n",
    "                                # Convert term to acad_term_id format\n",
    "                                if isinstance(term, str) and '-' in term:\n",
    "                                    import re\n",
    "                                    match = re.match(r'(\\d{4})-(\\d{2})\\s+Term\\s+(\\w+)', term)\n",
    "                                    if match:\n",
    "                                        acad_term_id = f\"AY{match.group(1)}{match.group(2)}T{match.group(3)}\"\n",
    "                                        boss_unique_combinations.add((course_code, section, acad_term_id))\n",
    "                \n",
    "                # Note: This is approximate as we're comparing course_code vs course_id\n",
    "                print(f\"10. Unique BOSS combinations sampled: {len(boss_unique_combinations)}\")\n",
    "                print(\"    (Note: Exact count requires course_code to course_id mapping)\")\n",
    "        except Exception as e:\n",
    "            print(f\"10. Could not analyze BOSS unmapped records: {e}\")\n",
    "        \n",
    "        # Generate detailed report\n",
    "        print(\"\\nüíæ Generating detailed report...\")\n",
    "        \n",
    "        missing_report = []\n",
    "        for _, class_row in classes_df.iterrows():\n",
    "            class_id = class_row['id']\n",
    "            \n",
    "            row = {\n",
    "                'class_id': class_id,\n",
    "                'course_id': class_row.get('course_id'),\n",
    "                'section': class_row.get('section'),\n",
    "                'professor_id': class_row.get('professor_id'),\n",
    "                'acad_term_id': class_row.get('acad_term_id'),\n",
    "                'boss_id': class_row.get('boss_id'),\n",
    "                'raw_professor_name': class_row.get('raw_professor_name', ''),\n",
    "                'warn_inaccuracy': class_row.get('warn_inaccuracy', False)\n",
    "            }\n",
    "            \n",
    "            # Check each table\n",
    "            for table_name, result in coverage_results.items():\n",
    "                row[f'in_{table_name}'] = 'Yes' if class_id in result['found_ids'] else 'No'\n",
    "            \n",
    "            # Add summary flags\n",
    "            row['has_boss_data'] = 'Yes' if (\n",
    "                row.get('in_class_availability') == 'Yes' or \n",
    "                row.get('in_bid_result') == 'Yes'\n",
    "            ) else 'No'\n",
    "            \n",
    "            row['has_timing_data'] = 'Yes' if row.get('in_class_timing') == 'Yes' else 'No'\n",
    "            row['has_exam_data'] = 'Yes' if row.get('in_class_exam_timing') == 'Yes' else 'No'\n",
    "            \n",
    "            missing_report.append(row)\n",
    "        \n",
    "        # Save report\n",
    "        report_df = pd.DataFrame(missing_report)\n",
    "        report_path = os.path.join(output_dir, 'class_coverage_detailed_report.csv')\n",
    "        report_df.to_csv(report_path, index=False)\n",
    "        print(f\"‚úÖ Detailed report saved to: {report_path}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total classes analyzed: {total_classes}\")\n",
    "        print(f\"Report generated: class_coverage_detailed_report.csv\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Usage:\n",
    "check_class_coverage_standalone('script_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_in_tables(output_dir='script_output'):\n",
    "    \"\"\"Check for duplicate records in class_availability, class_exam_timing, class_timing, and bid_result\"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç DUPLICATE RECORDS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Tables to check with their unique key combinations\n",
    "    tables_to_check = {\n",
    "        'new_class_availability.csv': {\n",
    "            'name': 'class_availability',\n",
    "            'key_columns': ['class_id', 'bid_window_id'],\n",
    "            'description': 'class_id + bid_window_id'\n",
    "        },\n",
    "        'new_bid_result.csv': {\n",
    "            'name': 'bid_result',\n",
    "            'key_columns': ['bid_window_id', 'class_id'],\n",
    "            'description': 'bid_window_id + class_id'\n",
    "        },\n",
    "        'new_class_timing.csv': {\n",
    "            'name': 'class_timing',\n",
    "            'key_columns': None,  # No composite key, check for exact duplicates\n",
    "            'description': 'all columns (no defined unique key)'\n",
    "        },\n",
    "        'new_class_exam_timing.csv': {\n",
    "            'name': 'class_exam_timing',\n",
    "            'key_columns': None,  # No composite key, check for exact duplicates\n",
    "            'description': 'all columns (no defined unique key)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_duplicates = {}\n",
    "    \n",
    "    for filename, config in tables_to_check.items():\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        table_name = config['name']\n",
    "        \n",
    "        print(f\"\\nüìä Checking {table_name}...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è  {filename} not found - skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            total_rows = len(df)\n",
    "            print(f\"Total rows: {total_rows}\")\n",
    "            \n",
    "            if total_rows == 0:\n",
    "                print(\"‚ùå No data in file\")\n",
    "                continue\n",
    "            \n",
    "            duplicates_found = []\n",
    "            \n",
    "            if config['key_columns']:\n",
    "                # Check for duplicates based on composite key\n",
    "                key_cols = config['key_columns']\n",
    "                \n",
    "                # Verify columns exist\n",
    "                missing_cols = [col for col in key_cols if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    print(f\"‚ùå Missing required columns: {missing_cols}\")\n",
    "                    continue\n",
    "                \n",
    "                # Find duplicates\n",
    "                duplicated_mask = df.duplicated(subset=key_cols, keep=False)\n",
    "                duplicates = df[duplicated_mask].copy()\n",
    "                \n",
    "                if len(duplicates) > 0:\n",
    "                    # Sort by key columns for better visualization\n",
    "                    duplicates = duplicates.sort_values(by=key_cols)\n",
    "                    \n",
    "                    # Group duplicates\n",
    "                    duplicate_groups = duplicates.groupby(key_cols).size().reset_index(name='count')\n",
    "                    num_duplicate_groups = len(duplicate_groups)\n",
    "                    \n",
    "                    print(f\"‚ùå Found {len(duplicates)} duplicate rows\")\n",
    "                    print(f\"   Duplicate groups: {num_duplicate_groups}\")\n",
    "                    print(f\"   Unique constraint violated: {config['description']}\")\n",
    "                    \n",
    "                    # Show sample duplicates\n",
    "                    print(\"\\n   Sample duplicate groups:\")\n",
    "                    for idx, group in duplicate_groups.head(5).iterrows():\n",
    "                        key_values = {col: group[col] for col in key_cols}\n",
    "                        print(f\"   ‚Ä¢ {key_values} appears {group['count']} times\")\n",
    "                    \n",
    "                    if num_duplicate_groups > 5:\n",
    "                        print(f\"   ... and {num_duplicate_groups - 5} more duplicate groups\")\n",
    "                    \n",
    "                    duplicates_found = duplicates\n",
    "                else:\n",
    "                    print(f\"‚úÖ No duplicates found on composite key: {config['description']}\")\n",
    "            \n",
    "            else:\n",
    "                # Check for exact row duplicates (all columns)\n",
    "                duplicated_mask = df.duplicated(keep=False)\n",
    "                duplicates = df[duplicated_mask].copy()\n",
    "                \n",
    "                if len(duplicates) > 0:\n",
    "                    print(f\"‚ùå Found {len(duplicates)} duplicate rows (exact matches)\")\n",
    "                    \n",
    "                    # Show sample duplicates\n",
    "                    print(\"\\n   Sample duplicate rows:\")\n",
    "                    shown = 0\n",
    "                    for idx, row in duplicates.head(10).iterrows():\n",
    "                        if shown < 5:\n",
    "                            print(f\"   ‚Ä¢ Row {idx}: class_id={row.get('class_id', 'N/A')}\")\n",
    "                            if 'start_time' in row:\n",
    "                                print(f\"     Timing: {row.get('day_of_week', '')} {row.get('start_time', '')}-{row.get('end_time', '')}\")\n",
    "                            if 'date' in row:\n",
    "                                print(f\"     Exam: {row.get('date', '')} {row.get('start_time', '')}-{row.get('end_time', '')}\")\n",
    "                            shown += 1\n",
    "                    \n",
    "                    duplicates_found = duplicates\n",
    "                else:\n",
    "                    print(f\"‚úÖ No exact duplicate rows found\")\n",
    "            \n",
    "            # Additional checks for timing tables\n",
    "            if table_name == 'class_timing' and len(df) > 0:\n",
    "                # Check for same class with overlapping timings\n",
    "                print(\"\\nüîç Checking for overlapping class timings...\")\n",
    "                if all(col in df.columns for col in ['class_id', 'day_of_week', 'start_time', 'end_time']):\n",
    "                    overlap_issues = []\n",
    "                    for class_id in df['class_id'].unique():\n",
    "                        class_timings = df[df['class_id'] == class_id]\n",
    "                        if len(class_timings) > 1:\n",
    "                            # Check each pair of timings\n",
    "                            timings_list = class_timings.to_dict('records')\n",
    "                            for i in range(len(timings_list)):\n",
    "                                for j in range(i + 1, len(timings_list)):\n",
    "                                    t1 = timings_list[i]\n",
    "                                    t2 = timings_list[j]\n",
    "                                    if (t1['day_of_week'] == t2['day_of_week'] and \n",
    "                                        pd.notna(t1['day_of_week']) and pd.notna(t2['day_of_week'])):\n",
    "                                        # Same day - check for time overlap\n",
    "                                        overlap_issues.append({\n",
    "                                            'class_id': class_id,\n",
    "                                            'day': t1['day_of_week'],\n",
    "                                            'timing1': f\"{t1['start_time']}-{t1['end_time']}\",\n",
    "                                            'timing2': f\"{t2['start_time']}-{t2['end_time']}\"\n",
    "                                        })\n",
    "                    \n",
    "                    if overlap_issues:\n",
    "                        print(f\"‚ö†Ô∏è  Found {len(overlap_issues)} potential timing conflicts\")\n",
    "                        for issue in overlap_issues[:3]:\n",
    "                            print(f\"   ‚Ä¢ Class {issue['class_id']} on {issue['day']}: {issue['timing1']} and {issue['timing2']}\")\n",
    "                    else:\n",
    "                        print(\"‚úÖ No overlapping timings found\")\n",
    "            \n",
    "            # Store results\n",
    "            if len(duplicates_found) > 0:\n",
    "                all_duplicates[table_name] = {\n",
    "                    'dataframe': duplicates_found,\n",
    "                    'total_duplicates': len(duplicates_found),\n",
    "                    'key_columns': config['key_columns'],\n",
    "                    'description': config['description']\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Export duplicate reports\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üíæ EXPORTING DUPLICATE REPORTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if all_duplicates:\n",
    "        for table_name, dup_info in all_duplicates.items():\n",
    "            output_filename = f\"duplicates_{table_name}.csv\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Add duplicate group numbers\n",
    "            df = dup_info['dataframe']\n",
    "            if dup_info['key_columns']:\n",
    "                # Add group number for easier identification\n",
    "                df['duplicate_group'] = df.groupby(dup_info['key_columns']).ngroup() + 1\n",
    "                df = df.sort_values(by=['duplicate_group'] + dup_info['key_columns'])\n",
    "            \n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"‚úÖ Exported {dup_info['total_duplicates']} duplicate records to: {output_filename}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicates found in any table!\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_duplicates = sum(info['total_duplicates'] for info in all_duplicates.values())\n",
    "    print(f\"Total duplicate records found: {total_duplicates}\")\n",
    "    \n",
    "    if all_duplicates:\n",
    "        print(\"\\nDuplicates by table:\")\n",
    "        for table_name, info in all_duplicates.items():\n",
    "            print(f\"  ‚Ä¢ {table_name}: {info['total_duplicates']} duplicates on {info['description']}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Usage:\n",
    "check_duplicates_in_tables('script_output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bidly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
