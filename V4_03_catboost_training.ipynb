{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1cfa91a",
      "metadata": {},
      "source": [
        "# **SMU Course Bidding Prediction Using CatBoost V4**\n",
        "\n",
        "<div style=\"background-color:#DFFFD6; padding:12px; border-radius:5px; border: 1px solid #228B22;\">\n",
        "   <h2 style=\"color:#006400;\">‚úÖ Looking to Implement This? ‚úÖ</h2>\n",
        "   <p>üöÄ **Get started quickly by using** <strong><a href=\"example_prediction.ipynb\">example_prediction.ipynb</a></strong>.</p> \n",
        "   <ul> \n",
        "      <li>üìå **Three pre-trained CatBoost models (`.cbm`) available for instant predictions.**</li>\n",
        "      <li>üîß Includes **step-by-step instructions** for making predictions with uncertainty quantification.</li>\n",
        "      <li>‚ö° Works **out-of-the-box**‚Äîjust load the models and start predicting!</li>\n",
        "   </ul>\n",
        "   <h3>üîó üìå Next Steps:</h3>\n",
        "   <p>üëâ <a href=\"example_prediction.ipynb\"><strong>Go to Example Prediction Notebook</strong></a></p>\n",
        "</div> \n",
        "<h2><span style=\"color:red\">NOTE: use at your own discretion.</span></h2>\n",
        "\n",
        "### **Changes in V4**\n",
        "- **Three-model architecture**: Added a classification model to predict whether a course will receive bids, complementing the existing median and min bid regression models\n",
        "- **Advanced uncertainty quantification**: Implemented entropy-based confidence scoring for classification and bootstrap-based confidence intervals for regression models\n",
        "- **Enhanced feature engineering**: Incorporated day-of-week boolean flags (`has_mon`, `has_tue`, etc.) for better temporal pattern recognition\n",
        "- **Asymmetric loss function**: Custom loss that penalizes under-predictions more heavily than over-predictions, crucial for bidding strategy\n",
        "- **Comprehensive evaluation suite**: Added confidence interval coverage analysis, residual analysis with emphasis on under-predictions, and cross-model feature importance comparison\n",
        "\n",
        "### **Objective**\n",
        "This notebook predicts bidding outcomes for courses in the SMU bidding system using **three specialized CatBoost models**. Building on insights from **V1, V2, and V3**, this version introduces a comprehensive **multi-model approach** with advanced uncertainty quantification:\n",
        "\n",
        "1. **Classification Model**: Predicts whether a course will receive bids (optimized for high recall)\n",
        "2. **Median Bid Regression Model**: Predicts the median bid price with confidence intervals\n",
        "3. **Min Bid Regression Model**: Predicts the minimum bid price with confidence intervals\n",
        "\n",
        "### **Key Enhancements in V4**\n",
        "\n",
        "**Learning from V3:**\n",
        "   - V3 focused on two regression models for median and min bid prediction\n",
        "   - V4 adds a **classification component** to identify courses that will receive bidding activity\n",
        "   - Enhanced with **probabilistic predictions** and **confidence scoring**\n",
        "\n",
        "**New V4 Features:**\n",
        "   - **Entropy-based confidence scoring** for classification predictions with five confidence levels (Very Low to Very High)\n",
        "   - **Bootstrap sampling** (100 iterations) for robust confidence interval estimation\n",
        "   - **Asymmetric loss function** (Œ±=2.0) that heavily penalizes dangerous under-predictions\n",
        "   - **Comprehensive uncertainty analysis** including interval width and coverage metrics\n",
        "\n",
        "### **Three-Model Architecture**\n",
        "\n",
        "| **Model Type** | **Purpose** | **Output** | **Uncertainty Measure** |\n",
        "|----------------|-------------|------------|-------------------------|\n",
        "| **Classification** | Predict bid courses | Probability + Confidence Level | Entropy-based confidence score |\n",
        "| **Median Bid Regression** | Predict median bid price | Price + 95% CI | Bootstrap confidence intervals |\n",
        "| **Min Bid Regression** | Predict minimum bid price | Price + 95% CI | Bootstrap confidence intervals |\n",
        "\n",
        "### **Updated Dataset Features**\n",
        "\n",
        "| **Feature Name** | **Type** | **Description** |\n",
        "|------------------|----------|-----------------|\n",
        "| **`subject_area`** | Categorical | Subject area (IS, ECON, etc.) |\n",
        "| **`catalogue_no`** | Categorical | Course number |\n",
        "| **`round`** | Categorical | Bidding round (1, 1A, 1B, 1C, 2, 2A) |\n",
        "| **`window`** | Numerical | Bidding window (1-5) |\n",
        "| **`before_process_vacancy`** | Numerical | Available spots before bidding |\n",
        "| **`acad_year_start`** | Numerical | Academic year start |\n",
        "| **`term`** | Categorical | Academic term (1, 2, 3A, 3B) |\n",
        "| **`start_time`** | Categorical | Class start time |\n",
        "| **`course_name`** | Categorical | Course name/description |\n",
        "| **`section`** | Categorical | Course section |\n",
        "| **`instructor`** | Categorical | Instructor name |\n",
        "| **`has_mon`** - **`has_sun`** | Boolean | Day-of-week indicators |\n",
        "| **üéØ Target Variables üéØ** | | **Model outputs** |\n",
        "| **`bids`** | Binary | Whether course receives bids |\n",
        "| **`target_median_bid`** | Numerical | Median bid price |\n",
        "| **`target_min_bid`** | Numerical | Minimum bid price |\n",
        "\n",
        "### **Advanced Uncertainty Quantification**\n",
        "\n",
        "**Classification Confidence:**\n",
        "- **Entropy-based scoring**: Measures prediction certainty using information entropy\n",
        "- **Five confidence levels**: Very Low, Low, Medium, High, Very High\n",
        "- **Probability outputs**: Separate probabilities for bid/non-bid outcomes\n",
        "\n",
        "**Regression Confidence Intervals:**\n",
        "- **Bootstrap sampling**: 100 model iterations for robust uncertainty estimation\n",
        "- **95% confidence intervals**: Upper and lower bounds for each prediction\n",
        "- **Interval width analysis**: Wider intervals indicate higher uncertainty\n",
        "\n",
        "### **Methodology**\n",
        "The notebook follows this enhanced structure:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Loading separate datasets for classification and regression tasks\n",
        "   - Feature standardization and categorical encoding\n",
        "   - Train-test splitting with consistent random seeds\n",
        "\n",
        "2. **Three-Model Training**:\n",
        "   - **Classification**: CatBoost with recall optimization for bid opportunity detection\n",
        "   - **Median Regression**: CatBoost with bootstrap uncertainty quantification\n",
        "   - **Min Regression**: CatBoost with asymmetric loss for under-prediction penalties\n",
        "\n",
        "3. **Advanced Evaluation**:\n",
        "   - **Classification**: Recall (maximizing true positives for bid detection), confusion matrix, entropy-based confidence analysis\n",
        "   - **Regression**: MSE, MAE, R¬≤, asymmetric MSE, confidence interval coverage\n",
        "   - **Cross-model feature importance comparison**\n",
        "\n",
        "4. **Comprehensive Visualization**:\n",
        "   - Confidence distribution plots and uncertainty analysis\n",
        "   - Residual analysis with under-prediction emphasis\n",
        "   - Feature importance rankings across all three models\n",
        "\n",
        "5. **Model Persistence and Reporting**:\n",
        "   - All models saved as `.cbm` files for deployment\n",
        "   - Detailed results exported to CSV format\n",
        "   - Comprehensive summary report generation\n",
        "\n",
        "### **Key Metrics and Performance**\n",
        "\n",
        "**Classification Model:**\n",
        "- **Primary metric**: Recall (optimized for capturing all bidding opportunities - maximizing true positives)\n",
        "- **Confidence analysis**: Distribution of entropy-based confidence scores  \n",
        "- **Output**: Probabilities for bid/no-bid outcomes, confidence levels, and entropy values\n",
        "\n",
        "**Regression Models:**\n",
        "- **Standard metrics**: MSE, MAE, R¬≤ for model accuracy\n",
        "- **Asymmetric MSE**: Custom metric penalizing under-predictions (Œ±=2.0)\n",
        "- **Uncertainty metrics**: Mean confidence interval width and coverage percentage\n",
        "- **Safety analysis**: Percentage of dangerous under-predictions\n",
        "\n",
        "### **Classification Strategy - Maximizing Bidding Opportunities**\n",
        "\n",
        "**Recall-Optimized Approach:**\n",
        "- **Target**: Predict courses that will receive bids (positive class = 1)\n",
        "- **Primary Goal**: Maximize recall to capture all potential bidding opportunities\n",
        "- **Business Logic**: Missing a course that will receive bids (False Negative) is more costly than incorrectly predicting a course will receive bids (False Positive)\n",
        "- **Optimization**: Model trained to minimize missed bidding opportunities while maintaining reasonable precision\n",
        "\n",
        "### **Implementation Notes**\n",
        "To run this V4 notebook:\n",
        "- Install required packages: `pip install catboost pandas numpy matplotlib seaborn scikit-learn scipy`\n",
        "- Ensure you have the three required datasets:\n",
        "  - Classification training/test data\n",
        "  - Median bid regression training/test data\n",
        "  - Min bid regression training/test data\n",
        "- Models automatically save to `script_output_model_training/mode/` directory\n",
        "\n",
        "### **V4 Advantages**\n",
        "- **Comprehensive coverage**: Handles both bid opportunity detection and price prediction\n",
        "- **Risk-aware predictions**: Asymmetric loss prevents dangerous under-bidding\n",
        "- **Confidence-calibrated**: Provides uncertainty measures for better decision-making\n",
        "- **Feature-rich analysis**: Cross-model feature importance for strategic insights\n",
        "- **Production-ready**: All models saved with consistent interfaces for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8520404",
      "metadata": {},
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359f5072",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.metrics import recall_score, precision_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "import time\n",
        "import scipy.stats as stats\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = Path('script_output/models')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4dac95",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading data...\")\n",
        "\n",
        "# Load classification data\n",
        "classification_train = pd.read_csv('script_output/model_training/classification/classification_train_250625173016.csv')\n",
        "classification_test = pd.read_csv('script_output/model_training/classification/classification_test_250625173016.csv')\n",
        "\n",
        "# Load regression data for median bid\n",
        "regression_median_train = pd.read_csv('script_output/model_training/regression/regression_median_train_250625173016.csv')\n",
        "regression_median_test = pd.read_csv('script_output/model_training/regression/regression_median_test_250625173016.csv')\n",
        "\n",
        "# Load regression data for min bid\n",
        "regression_min_train = pd.read_csv('script_output/model_training/regression/regression_min_train_250625173016.csv')\n",
        "regression_min_test = pd.read_csv('script_output/model_training/regression/regression_min_test_250625173016.csv')\n",
        "\n",
        "# Define categorical features\n",
        "categorical_features = [\n",
        "    'subject_area', 'catalogue_no', 'round', 'term', 'course_name', \n",
        "    'section', 'instructor', 'start_time'\n",
        "]\n",
        "\n",
        "# Convert NaN values to strings for all categorical features in all datasets\n",
        "def fix_categorical_features(df, cat_features):\n",
        "    \"\"\"Convert categorical features to strings to handle NaN values\"\"\"\n",
        "    for col in cat_features:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str)\n",
        "    return df\n",
        "\n",
        "# Apply fix to all datasets\n",
        "classification_train = fix_categorical_features(classification_train, categorical_features)\n",
        "classification_test = fix_categorical_features(classification_test, categorical_features)\n",
        "regression_median_train = fix_categorical_features(regression_median_train, categorical_features)\n",
        "regression_median_test = fix_categorical_features(regression_median_test, categorical_features)\n",
        "regression_min_train = fix_categorical_features(regression_min_train, categorical_features)\n",
        "regression_min_test = fix_categorical_features(regression_min_test, categorical_features)\n",
        "\n",
        "print(f\"Classification Train shape: {classification_train.shape}\")\n",
        "print(f\"Classification Test shape: {classification_test.shape}\\n\")\n",
        "print(f\"Regression Median Train shape: {regression_median_train.shape}\")\n",
        "print(f\"Regression Median Test shape: {regression_median_test.shape}\\n\")\n",
        "print(f\"Regression Min Train shape: {regression_min_train.shape}\")\n",
        "print(f\"Regression Min Test shape: {regression_min_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb1ffaf",
      "metadata": {},
      "source": [
        "## **2. Classification model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b4df7be",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CLASSIFICATION MODEL - bid Prediction\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create classification-specific output directory\n",
        "classification_output_dir = output_dir // 'classification'\n",
        "classification_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Classification artifacts will be saved to: {classification_output_dir}\")\n",
        "\n",
        "# Define features and target for classification\n",
        "classification_features = [\n",
        "    'subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy',\n",
        "    'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor',\n",
        "    'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun'\n",
        "]\n",
        "\n",
        "# Note: Fixed typo in column name from 'nsubject_area' to 'subject_area'\n",
        "classification_target = 'bids'\n",
        "\n",
        "# Prepare data\n",
        "X_train_clf = classification_train[classification_features]\n",
        "y_train_clf = classification_train[classification_target]\n",
        "X_test_clf = classification_test[classification_features]\n",
        "y_test_clf = classification_test[classification_target]\n",
        "\n",
        "# Define categorical features\n",
        "cat_features_clf = categorical_features\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"MODEL VALIDATION PHASE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train CatBoost Classifier for validation\n",
        "print(\"\\nTraining CatBoost Classifier for validation...\")\n",
        "clf_model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    depth=10,\n",
        "    learning_rate=0.1,\n",
        "    l2_leaf_reg=5,\n",
        "    random_strength=1,\n",
        "    bagging_temperature=1,\n",
        "    cat_features=cat_features_clf,\n",
        "    eval_metric='Recall',  # Optimize for recall\n",
        "    verbose=100,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "clf_model.fit(X_train_clf, y_train_clf, eval_set=(X_test_clf, y_test_clf))\n",
        "\n",
        "# Verify class order\n",
        "print(f\"CatBoost class order: {clf_model.classes_}\")\n",
        "print(f\"0 = {clf_model.classes_[0]}, 1 = {clf_model.classes_[1]}\")\n",
        "\n",
        "# Get predictions and probabilities\n",
        "y_pred_clf = clf_model.predict(X_test_clf)\n",
        "y_proba_clf = clf_model.predict_proba(X_test_clf)\n",
        "\n",
        "# Calculate entropy-based confidence\n",
        "def calculate_entropy_confidence(probabilities):\n",
        "    \"\"\"Calculate entropy-based confidence score and level\"\"\"\n",
        "    epsilon = 1e-10  # Small value to avoid log(0)\n",
        "    entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
        "    max_entropy = -np.log(1/probabilities.shape[1])  # Maximum possible entropy\n",
        "    confidence_score = 1 - (entropy / max_entropy)\n",
        "    \n",
        "    # Define confidence levels\n",
        "    confidence_levels = np.where(\n",
        "        confidence_score >= 0.9, 'Very High',\n",
        "        np.where(\n",
        "            confidence_score >= 0.7, 'High',\n",
        "            np.where(\n",
        "                confidence_score >= 0.5, 'Medium',\n",
        "                np.where(\n",
        "                    confidence_score >= 0.3, 'Low',\n",
        "                    'Very Low'\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    return confidence_score, confidence_levels, entropy\n",
        "\n",
        "confidence_scores, confidence_levels, entropy_values = calculate_entropy_confidence(y_proba_clf)\n",
        "\n",
        "# Create classification results DataFrame\n",
        "classification_results = pd.DataFrame({\n",
        "    'actual': y_test_clf,\n",
        "    'predicted': y_pred_clf,\n",
        "    'prob_no_bid': y_proba_clf[:, 0],\n",
        "    'prob_bid': y_proba_clf[:, 1],\n",
        "    'confidence_score': confidence_scores,\n",
        "    'confidence_level': confidence_levels,\n",
        "    'entropy': entropy_values\n",
        "})\n",
        "\n",
        "# Evaluation metrics\n",
        "recall_bids = recall_score(y_test_clf, y_pred_clf, pos_label=1)\n",
        "precision_bids = precision_score(y_test_clf, y_pred_clf, pos_label=1)\n",
        "conf_matrix = confusion_matrix(y_test_clf, y_pred_clf)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"VALIDATION RESULTS\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Recall for Bids (Key Metric): {recall_bids:.4f}\")\n",
        "print(f\"Precision for Bids: {precision_bids:.4f}\")\n",
        "print(f\"F1-Score for Bids: {2 * (precision_bids * recall_bids) / (precision_bids + recall_bids):.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_clf, y_pred_clf))\n",
        "\n",
        "# Visualize confidence distribution\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(confidence_scores, bins=30, edgecolor='black')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Confidence Scores')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "confidence_level_counts = pd.Series(confidence_levels).value_counts()\n",
        "plt.bar(confidence_level_counts.index, confidence_level_counts.values)\n",
        "plt.xlabel('Confidence Level')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Confidence Levels')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(y_proba_clf[:, 1], confidence_scores, alpha=0.5)\n",
        "plt.xlabel('Probability of Bid')\n",
        "plt.ylabel('Confidence Score')\n",
        "plt.title('Bid Probability vs Confidence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(classification_output_dir / 'classification_confidence_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save validation results\n",
        "classification_results.to_csv(classification_output_dir / 'classification_validation_results.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRODUCTION MODEL TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Combine train and test data for production model\n",
        "print(\"Combining training and test data for production model...\")\n",
        "X_production = pd.concat([X_train_clf, X_test_clf], ignore_index=True)\n",
        "y_production = pd.concat([y_train_clf, y_test_clf], ignore_index=True)\n",
        "\n",
        "print(f\"Production dataset shape: {X_production.shape}\")\n",
        "print(f\"Production target distribution:\\n{y_production.value_counts()}\")\n",
        "\n",
        "# Train production model on all available data\n",
        "print(\"\\nTraining production CatBoost Classifier on all data...\")\n",
        "production_clf_model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    depth=10,\n",
        "    learning_rate=0.1,\n",
        "    l2_leaf_reg=5,\n",
        "    random_strength=1,\n",
        "    bagging_temperature=1,\n",
        "    cat_features=cat_features_clf,\n",
        "    eval_metric='Recall',\n",
        "    verbose=100,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "production_clf_model.fit(X_production, y_production)\n",
        "\n",
        "# Save production classification model\n",
        "production_model_path = classification_output_dir / 'production_classification_model.cbm'\n",
        "production_clf_model.save_model(str(production_model_path))\n",
        "print(f\"\\n\" + \"=\"*50)\n",
        "print(\"PRODUCTION MODEL SAVED\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Production classification model saved to: {production_model_path}\")\n",
        "print(f\"Model trained on {len(X_production)} samples\")\n",
        "print(f\"Features used: {len(classification_features)}\")\n",
        "print(\"Ready for deployment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec7b5aa",
      "metadata": {},
      "source": [
        "## **3. Regression model - Median Bid Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df42551",
      "metadata": {},
      "outputs": [],
      "source": [
        "def asymmetric_mse(y_true, y_pred, alpha=2.0):\n",
        "    \"\"\"\n",
        "    Custom loss that penalizes under-predictions more than over-predictions\n",
        "    alpha > 1 penalizes under-predictions more\n",
        "    \"\"\"\n",
        "    errors = y_true - y_pred\n",
        "    return np.mean(np.where(errors > 0, alpha * errors**2, errors**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d39ccb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"REGRESSION MODEL TRAINING - Median Bid Prediction\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create regression_median-specific output directory\n",
        "regression_median_output_dir = output_dir // 'regression_median'\n",
        "regression_median_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Regression median artifacts will be saved to: {regression_median_output_dir}\")\n",
        "\n",
        "# Define features for regression\n",
        "regression_features = [\n",
        "    'subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy',\n",
        "    'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor',\n",
        "    'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun'\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "X_train_median = regression_median_train[regression_features]\n",
        "y_train_median = regression_median_train['target_median_bid']\n",
        "X_test_median = regression_median_test[regression_features]\n",
        "y_test_median = regression_median_test['target_median_bid']\n",
        "\n",
        "print(f\"Training dataset shape: {X_train_median.shape}\")\n",
        "print(f\"Test dataset shape: {X_test_median.shape}\")\n",
        "print(f\"Target range: {y_train_median.min():.2f} to {y_train_median.max():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL VALIDATION PHASE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# =============================================================================\n",
        "# TRAIN MODEL WITH RMSE\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"TRAINING MODEL WITH RMSE\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "print(\"Training CatBoost model with RMSE...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create and train model with RMSE\n",
        "median_model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    depth=10,\n",
        "    learning_rate=0.1,\n",
        "    l2_leaf_reg=5,\n",
        "    random_strength=1,\n",
        "    bagging_temperature=1,\n",
        "    cat_features=categorical_features,\n",
        "    verbose=100,\n",
        "    random_seed=42,\n",
        "    thread_count=-1,\n",
        "    loss_function='RMSE',  # Using RMSE instead of RMSEWithUncertainty\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# Train with evaluation set\n",
        "median_model.fit(\n",
        "    X_train_median, \n",
        "    y_train_median, \n",
        "    eval_set=(X_test_median, y_test_median),\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# =============================================================================\n",
        "# PREDICTIONS WITH UNCERTAINTY VIA VIRTUAL ENSEMBLES\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"GENERATING PREDICTIONS\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Get base predictions\n",
        "y_pred = median_model.predict(X_test_median)\n",
        "\n",
        "# Calculate uncertainty using CatBoost Virtual Ensembles\n",
        "print(\"Calculating uncertainty using CatBoost Virtual Ensembles...\")\n",
        "n_subsets = 10\n",
        "subset_predictions = []\n",
        "trees_per_subset = max(1, median_model.tree_count_ // n_subsets)\n",
        "\n",
        "for i in range(n_subsets):\n",
        "    tree_start = i * trees_per_subset\n",
        "    tree_end = min((i + 1) * trees_per_subset, median_model.tree_count_)\n",
        "    \n",
        "    if tree_start < median_model.tree_count_:\n",
        "        partial_pred = median_model.predict(X_test_median, \n",
        "                                          ntree_start=tree_start, \n",
        "                                          ntree_end=tree_end)\n",
        "        subset_predictions.append(partial_pred)\n",
        "\n",
        "# Calculate uncertainty from ensemble variance\n",
        "subset_predictions = np.array(subset_predictions)\n",
        "uncertainty = np.std(subset_predictions, axis=0)\n",
        "\n",
        "# Create confidence intervals\n",
        "confidence_multiplier = 1.96  # For 95% confidence interval\n",
        "lower_bound = y_pred - confidence_multiplier * uncertainty\n",
        "upper_bound = y_pred + confidence_multiplier * uncertainty\n",
        "\n",
        "# =============================================================================\n",
        "# EVALUATION METRICS\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_median, y_pred)\n",
        "mae = mean_absolute_error(y_test_median, y_pred)\n",
        "r2 = r2_score(y_test_median, y_pred)\n",
        "asym_mse = asymmetric_mse(y_test_median, y_pred)\n",
        "\n",
        "# Confidence interval coverage\n",
        "in_interval = (y_test_median >= lower_bound) & (y_test_median <= upper_bound)\n",
        "coverage = in_interval.mean()\n",
        "\n",
        "# Calculate errors for distribution analysis\n",
        "errors = y_pred - y_test_median\n",
        "mean_error = np.mean(errors)\n",
        "std_error = np.std(errors)\n",
        "percentile_2_5 = np.percentile(errors, 2.5)\n",
        "percentile_97_5 = np.percentile(errors, 97.5)\n",
        "\n",
        "print(\"--- Model Performance ---\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"R¬≤: {r2:.4f}\")\n",
        "print(f\"Asymmetric MSE (Œ±=2): {asym_mse:.4f}\")\n",
        "print(f\"Mean Uncertainty: {uncertainty.mean():.4f}\")\n",
        "print(f\"Mean Confidence Interval Width: {(upper_bound - lower_bound).mean():.4f}\")\n",
        "print(f\"95% CI Coverage: {coverage:.1%}\")\n",
        "\n",
        "print(f\"\\nError Distribution:\")\n",
        "print(f\"Mean Error: {mean_error:.2f}\")\n",
        "print(f\"Std Error: {std_error:.2f}\")\n",
        "print(f\"2.5th percentile: {percentile_2_5:.2f}\")\n",
        "print(f\"97.5th percentile: {percentile_97_5:.2f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ERROR DISTRIBUTION PLOT\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"ERROR DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot histogram of errors\n",
        "plt.hist(errors, bins=50, alpha=0.7, color='blue', density=False, label=\"Error Distribution\", edgecolor='black')\n",
        "\n",
        "# Generate normal distribution curve\n",
        "x_values = np.linspace(min(errors), max(errors), 100)\n",
        "y_values = stats.norm.pdf(x_values, mean_error, std_error) * len(errors) * (max(errors) - min(errors)) / 50\n",
        "plt.plot(x_values, y_values, color='black', linestyle='-', linewidth=2, label=\"Normal Approximation\")\n",
        "\n",
        "# Add vertical lines for standard deviations\n",
        "for i in range(1, 4):\n",
        "    plt.axvline(mean_error + i * std_error, color='green', linestyle='--', \n",
        "                label=f'+{i} SD' if i == 1 else \"\", alpha=0.7)\n",
        "    plt.axvline(mean_error - i * std_error, color='green', linestyle='--', \n",
        "                label=f'-{i} SD' if i == 1 else \"\", alpha=0.7)\n",
        "\n",
        "# Add vertical lines for percentiles\n",
        "plt.axvline(percentile_2_5, color='orange', linestyle='--', linewidth=2, label='2.5th Percentile')\n",
        "plt.axvline(percentile_97_5, color='orange', linestyle='--', linewidth=2, label='97.5th Percentile')\n",
        "\n",
        "# Add zero error line\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "\n",
        "plt.xlabel('Prediction Error (Predicted - Actual)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Error Distribution for Median Bid Model')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(regression_median_output_dir / 'median_bid_error_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# SAFETY FACTOR ANALYSIS\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"SAFETY FACTOR ANALYSIS - MEDIAN BID\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Initialize lists to store results\n",
        "safety_factors = np.arange(0.0, 1.1, 0.1)  # 0 to 1 with 0.1 intervals\n",
        "tpr_values = []\n",
        "loss_values = []\n",
        "detailed_results = []\n",
        "\n",
        "print(\"Analyzing safety factors from 0.0 to 1.0...\")\n",
        "# Loop through safety factors\n",
        "for sf in safety_factors:\n",
        "    # Apply safety factor to predictions\n",
        "    adjusted_pred = y_pred * (1 + sf)\n",
        "    \n",
        "    # Compute TP Rate (predictions >= actual)\n",
        "    pred_binary = (adjusted_pred >= y_test_median).astype(int)\n",
        "    tp_rate = pred_binary.mean()\n",
        "    \n",
        "    # Compute Mean Loss (average difference between predicted and actual)\n",
        "    mean_loss = np.mean(adjusted_pred - y_test_median)\n",
        "    \n",
        "    # Store results\n",
        "    tpr_values.append(tp_rate)\n",
        "    loss_values.append(mean_loss)\n",
        "    detailed_results.append({\n",
        "        'safety_factor': sf,\n",
        "        'tpr': tp_rate,\n",
        "        'mean_loss': mean_loss,\n",
        "        'mae': mean_absolute_error(y_test_median, adjusted_pred),\n",
        "        'mse': mean_squared_error(y_test_median, adjusted_pred)\n",
        "    })\n",
        "    \n",
        "    print(f\"SF: {sf:.1f} | TPR: {tp_rate:.3f} | Mean Loss: {mean_loss:.2f}\")\n",
        "\n",
        "# Create DataFrame for export\n",
        "safety_factor_df = pd.DataFrame(detailed_results)\n",
        "\n",
        "# Save results\n",
        "safety_factor_df.to_csv(regression_median_output_dir / 'median_bid_safety_factor_analysis.csv', index=False)\n",
        "print(f\"\\nSafety factor analysis saved to {regression_median_output_dir / 'median_bid_safety_factor_analysis.csv'}\")\n",
        "\n",
        "# Plot Safety Factor Analysis\n",
        "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot TPR on the left y-axis\n",
        "ax1.plot(safety_factors, tpr_values, marker='o', markersize=8, color='blue', \n",
        "         linewidth=2, label='True Positive Rate (TPR)')\n",
        "ax1.set_xlabel('Safety Factor', fontsize=12)\n",
        "ax1.set_ylabel('True Positive Rate (TPR)', color='blue', fontsize=12)\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax1.set_title('Safety Factor Analysis - Median Bid Model', fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlim(-0.05, 1.05)\n",
        "ax1.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Plot Mean Loss on the right y-axis\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(safety_factors, loss_values, marker='x', markersize=8, color='orange', \n",
        "         linewidth=2, label='Mean Loss (Predicted - Actual)')\n",
        "ax2.set_ylabel('Mean Loss (Predicted - Actual)', color='orange', fontsize=12)\n",
        "ax2.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "# Add annotations for key points\n",
        "for i, sf in enumerate([0.0, 0.5, 1.0]):\n",
        "    if sf in safety_factors:\n",
        "        idx = list(safety_factors).index(sf)\n",
        "        ax1.annotate(f'TPR: {tpr_values[idx]:.2f}', \n",
        "                    xy=(sf, tpr_values[idx]), \n",
        "                    xytext=(sf+0.05, tpr_values[idx]-0.05),\n",
        "                    fontsize=9, color='blue')\n",
        "\n",
        "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.95))\n",
        "plt.savefig(regression_median_output_dir / 'median_bid_safety_factor_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Find optimal safety factor\n",
        "# Using same criteria as V3: maximize TPR while minimizing loss\n",
        "alpha = 0.7  # Weight for TPR importance\n",
        "beta = 0.3   # Weight for minimizing loss\n",
        "\n",
        "tpr_array = np.array(tpr_values)\n",
        "loss_array = np.array(loss_values)\n",
        "\n",
        "# Normalize values\n",
        "tpr_normalized = (tpr_array - np.min(tpr_array)) / (np.max(tpr_array) - np.min(tpr_array))\n",
        "loss_normalized = (loss_array - np.min(loss_array)) / (np.max(loss_array) - np.min(loss_array))\n",
        "\n",
        "# Compute optimality score\n",
        "optimality_score = alpha * tpr_normalized - beta * np.abs(loss_normalized)\n",
        "\n",
        "# Find best safety factor\n",
        "best_index = np.argmax(optimality_score)\n",
        "best_sf = safety_factors[best_index]\n",
        "best_tpr = tpr_values[best_index]\n",
        "best_loss = loss_values[best_index]\n",
        "\n",
        "print(f\"\\n--- Optimal Safety Factor ---\")\n",
        "print(f\"Best Safety Factor: {best_sf:.2f}\")\n",
        "print(f\"Corresponding TPR: {best_tpr:.3f}\")\n",
        "print(f\"Corresponding Mean Loss: {best_loss:.2f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# COMPREHENSIVE RESULTS VISUALIZATION\n",
        "# =============================================================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Actual vs Predicted\n",
        "axes[0, 0].scatter(y_test_median, y_pred, alpha=0.6, color='blue')\n",
        "axes[0, 0].plot([y_test_median.min(), y_test_median.max()], \n",
        "                [y_test_median.min(), y_test_median.max()], 'r--', lw=2)\n",
        "axes[0, 0].set_xlabel('Actual Median Bid')\n",
        "axes[0, 0].set_ylabel('Predicted Median Bid')\n",
        "axes[0, 0].set_title(f'Model Performance\\n(R¬≤ = {r2:.3f})')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals Distribution\n",
        "axes[0, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "axes[0, 1].axvline(x=percentile_2_5, color='orange', linestyle='--', label='2.5th %ile')\n",
        "axes[0, 1].axvline(x=percentile_97_5, color='orange', linestyle='--', label='97.5th %ile')\n",
        "axes[0, 1].set_xlabel('Residuals')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title(f'Residuals Distribution\\n(MAE = {mae:.3f})')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Uncertainty Distribution\n",
        "axes[1, 0].hist(uncertainty, bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "axes[1, 0].set_xlabel('Prediction Uncertainty')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title(f'Uncertainty Distribution\\n(Mean = {uncertainty.mean():.3f})')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Model Summary\n",
        "summary_text = f\"\"\"Median Bid Model Results:\n",
        "\n",
        "Training Time: {training_time:.1f}s\n",
        "Total Trees: {median_model.tree_count_}\n",
        "\n",
        "Performance Metrics:\n",
        "- MSE: {mse:.4f}\n",
        "- MAE: {mae:.4f}\n",
        "- R¬≤: {r2:.4f}\n",
        "- Asymmetric MSE: {asym_mse:.4f}\n",
        "\n",
        "Uncertainty Analysis:\n",
        "- Mean Uncertainty: {uncertainty.mean():.4f}\n",
        "- 95% CI Coverage: {coverage:.1%}\n",
        "- Mean CI Width: {(upper_bound - lower_bound).mean():.4f}\n",
        "\n",
        "Optimal Safety Factor:\n",
        "- Safety Factor: {best_sf:.2f}\n",
        "- TPR: {best_tpr:.3f}\n",
        "- Mean Loss: {best_loss:.2f}\n",
        "\n",
        "Data Summary:\n",
        "- Training: {len(X_train_median)} samples\n",
        "- Testing: {len(X_test_median)} samples\n",
        "- Features: {len(regression_features)}\"\"\"\n",
        "\n",
        "axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes, \n",
        "                fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "axes[1, 1].set_title('Model Summary')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(regression_median_output_dir / 'regression_median_model_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# SAVE VALIDATION RESULTS\n",
        "# =============================================================================\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'actual': y_test_median,\n",
        "    'predicted': y_pred,\n",
        "    'lower_95_ci': lower_bound,\n",
        "    'upper_95_ci': upper_bound,\n",
        "    'uncertainty': uncertainty,\n",
        "    'interval_width': upper_bound - lower_bound,\n",
        "    'residuals': errors,\n",
        "    'abs_residuals': np.abs(errors),\n",
        "    'in_interval': in_interval\n",
        "})\n",
        "\n",
        "# Save validation results\n",
        "results_df.to_csv(regression_median_output_dir / 'regression_median_validation_results.csv', index=False)\n",
        "\n",
        "# Save model summary\n",
        "model_summary = pd.DataFrame({\n",
        "    'model_type': ['CatBoost with RMSE + Virtual Ensembles - Median Bid'],\n",
        "    'training_time_seconds': [training_time],\n",
        "    'total_trees': [median_model.tree_count_],\n",
        "    'mse': [mse],\n",
        "    'mae': [mae],\n",
        "    'r2': [r2],\n",
        "    'asymmetric_mse': [asym_mse],\n",
        "    'mean_uncertainty': [uncertainty.mean()],\n",
        "    'ci_coverage': [coverage],\n",
        "    'mean_ci_width': [(upper_bound - lower_bound).mean()],\n",
        "    'optimal_safety_factor': [best_sf],\n",
        "    'optimal_tpr': [best_tpr],\n",
        "    'training_samples': [len(X_train_median)],\n",
        "    'test_samples': [len(X_test_median)],\n",
        "    'features_count': [len(regression_features)]\n",
        "})\n",
        "\n",
        "model_summary.to_csv(regression_median_output_dir / 'regression_median_validation_summary.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRODUCTION MODEL TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Combine train and test data for production model\n",
        "print(\"Combining training and test data for production model...\")\n",
        "X_production = pd.concat([X_train_median, X_test_median], ignore_index=True)\n",
        "y_production = pd.concat([y_train_median, y_test_median], ignore_index=True)\n",
        "\n",
        "print(f\"Production dataset shape: {X_production.shape}\")\n",
        "print(f\"Production target range: {y_production.min():.2f} to {y_production.max():.2f}\")\n",
        "\n",
        "# Train production model on all available data\n",
        "print(\"\\nTraining production CatBoost model on all data...\")\n",
        "production_start_time = time.time()\n",
        "\n",
        "production_median_model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    depth=10,\n",
        "    learning_rate=0.1,\n",
        "    l2_leaf_reg=5,\n",
        "    random_strength=1,\n",
        "    bagging_temperature=1,\n",
        "    cat_features=categorical_features,\n",
        "    verbose=100,\n",
        "    random_seed=42,\n",
        "    thread_count=-1,\n",
        "    loss_function='RMSE',\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "production_median_model.fit(X_production, y_production)\n",
        "\n",
        "production_training_time = time.time() - production_start_time\n",
        "print(f\"\\nProduction training completed in {production_training_time:.2f} seconds\")\n",
        "\n",
        "# Save production model\n",
        "production_model_path = regression_median_output_dir / 'production_regression_median_model.cbm'\n",
        "production_median_model.save_model(str(production_model_path))\n",
        "\n",
        "# Save production model summary\n",
        "production_summary = pd.DataFrame({\n",
        "    'model_type': ['Production CatBoost RMSE - Median Bid'],\n",
        "    'training_time_seconds': [production_training_time],\n",
        "    'total_trees': [production_median_model.tree_count_],\n",
        "    'training_samples': [len(X_production)],\n",
        "    'features_used': [len(regression_features)],\n",
        "    'target_range_min': [y_production.min()],\n",
        "    'target_range_max': [y_production.max()],\n",
        "    'target_mean': [y_production.mean()],\n",
        "    'target_std': [y_production.std()]\n",
        "})\n",
        "\n",
        "production_summary.to_csv(regression_median_output_dir / 'production_regression_median_summary.csv', index=False)\n",
        "\n",
        "# Save feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': regression_features,\n",
        "    'importance': production_median_model.get_feature_importance()\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "feature_importance.to_csv(regression_median_output_dir / 'production_feature_importance.csv', index=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 15 Feature Importance - Production Median Bid Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(regression_median_output_dir / 'production_feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(\"VALIDATION RESULTS:\")\n",
        "print(f\"‚úÖ Validation model performance - R¬≤: {r2:.4f}, MAE: {mae:.4f}\")\n",
        "print(f\"‚úÖ MSE: {mse:.4f}, Asymmetric MSE: {asym_mse:.4f}\")\n",
        "print(f\"‚úÖ 95% CI Coverage: {coverage:.1%}, Mean CI Width: {(upper_bound - lower_bound).mean():.4f}\")\n",
        "print(f\"‚úÖ Optimal Safety Factor: {best_sf:.2f} (TPR: {best_tpr:.3f})\")\n",
        "\n",
        "print(\"\\nPRODUCTION MODEL:\")\n",
        "print(f\"‚úÖ Production model trained on {len(X_production)} samples\")\n",
        "print(f\"‚úÖ Training time: {production_training_time:.1f} seconds\")\n",
        "print(f\"‚úÖ Total trees: {production_median_model.tree_count_}\")\n",
        "\n",
        "print(f\"\\nSAVED ARTIFACTS:\")\n",
        "print(f\"üìÅ All files saved to: {regression_median_output_dir}\")\n",
        "print(f\"ü§ñ Production model: production_regression_median_model.cbm\")\n",
        "print(f\"üìä Validation results: regression_median_validation_results.csv\")\n",
        "print(f\"üìà Model results plot: regression_median_model_results.png\")\n",
        "print(f\"üìã Validation summary: regression_median_validation_summary.csv\")\n",
        "print(f\"üìä Error distribution: median_bid_error_distribution.png\")\n",
        "print(f\"üéØ Safety factor analysis: median_bid_safety_factor_analysis.csv\")\n",
        "print(f\"üìä Safety factor plot: median_bid_safety_factor_plot.png\")\n",
        "print(f\"üéØ Feature importance: production_feature_importance.csv\")\n",
        "print(f\"üìä Feature importance plot: production_feature_importance.png\")\n",
        "print(f\"üìã Production summary: production_regression_median_summary.csv\")\n",
        "\n",
        "print(\"\\nüöÄ Ready for deployment!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d0b0fe",
      "metadata": {},
      "source": [
        "## **4. Regression model - Min Bid Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5bb3aac",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"REGRESSION MODEL TRAINING - Min Bid Prediction\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create regression_min-specific output directory\n",
        "regression_min_output_dir = output_dir // 'regression_min'\n",
        "regression_min_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Regression min artifacts will be saved to: {regression_min_output_dir}\")\n",
        "\n",
        "# Define features for regression\n",
        "regression_features = [\n",
        "    'subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy',\n",
        "    'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor',\n",
        "    'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun'\n",
        "]\n",
        "\n",
        "# Prepare data for min bid prediction\n",
        "X_train_min = regression_min_train[regression_features]\n",
        "y_train_min = regression_min_train['target_min_bid']\n",
        "X_test_min = regression_min_test[regression_features]\n",
        "y_test_min = regression_min_test['target_min_bid']\n",
        "\n",
        "print(f\"Training dataset shape: {X_train_min.shape}\")\n",
        "print(f\"Test dataset shape: {X_test_min.shape}\")\n",
        "print(f\"Target range: {y_train_min.min():.2f} to {y_train_min.max():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL VALIDATION PHASE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# =============================================================================\n",
        "# TRAIN MODEL WITH RMSE\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"TRAINING MODEL WITH RMSE\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "print(\"Training CatBoost model with RMSE...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create and train model with RMSE\n",
        "min_model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    depth=10,\n",
        "    learning_rate=0.1,\n",
        "    l2_leaf_reg=5,\n",
        "    random_strength=1,\n",
        "    bagging_temperature=1,\n",
        "    cat_features=categorical_features,\n",
        "    verbose=100,\n",
        "    random_seed=42,\n",
        "    thread_count=-1,\n",
        "    loss_function='RMSE',  # Using RMSE instead of RMSEWithUncertainty\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# Train with evaluation set\n",
        "min_model.fit(\n",
        "    X_train_min, \n",
        "    y_train_min, \n",
        "    eval_set=(X_test_min, y_test_min),\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# =============================================================================\n",
        "# PREDICTIONS WITH UNCERTAINTY VIA VIRTUAL ENSEMBLES\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"GENERATING PREDICTIONS\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Get base predictions\n",
        "y_pred = min_model.predict(X_test_min)\n",
        "\n",
        "# Calculate uncertainty using CatBoost Virtual Ensembles\n",
        "print(\"Calculating uncertainty using CatBoost Virtual Ensembles...\")\n",
        "n_subsets = 10\n",
        "subset_predictions = []\n",
        "trees_per_subset = max(1, min_model.tree_count_ // n_subsets)\n",
        "\n",
        "for i in range(n_subsets):\n",
        "    tree_start = i * trees_per_subset\n",
        "    tree_end = min((i + 1) * trees_per_subset, min_model.tree_count_)\n",
        "    \n",
        "    if tree_start < min_model.tree_count_:\n",
        "        partial_pred = min_model.predict(X_test_min, \n",
        "                                       ntree_start=tree_start, \n",
        "                                       ntree_end=tree_end)\n",
        "        subset_predictions.append(partial_pred)\n",
        "\n",
        "# Calculate uncertainty from ensemble variance\n",
        "subset_predictions = np.array(subset_predictions)\n",
        "uncertainty = np.std(subset_predictions, axis=0)\n",
        "\n",
        "# Create confidence intervals\n",
        "confidence_multiplier = 1.96  # For 95% confidence interval\n",
        "lower_bound = y_pred - confidence_multiplier * uncertainty\n",
        "upper_bound = y_pred + confidence_multiplier * uncertainty\n",
        "\n",
        "# =============================================================================\n",
        "# EVALUATION METRICS\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_min, y_pred)\n",
        "mae = mean_absolute_error(y_test_min, y_pred)\n",
        "r2 = r2_score(y_test_min, y_pred)\n",
        "asym_mse = asymmetric_mse(y_test_min, y_pred)\n",
        "\n",
        "# Confidence interval coverage\n",
        "in_interval = (y_test_min >= lower_bound) & (y_test_min <= upper_bound)\n",
        "coverage = in_interval.mean()\n",
        "\n",
        "# Calculate errors for distribution analysis\n",
        "errors = y_pred - y_test_min\n",
        "mean_error = np.mean(errors)\n",
        "std_error = np.std(errors)\n",
        "percentile_2_5 = np.percentile(errors, 2.5)\n",
        "percentile_97_5 = np.percentile(errors, 97.5)\n",
        "\n",
        "# CRITICAL: Under-prediction analysis for min bid\n",
        "under_predictions = y_pred < y_test_min\n",
        "under_prediction_rate = under_predictions.mean()\n",
        "mean_under_prediction_loss = np.mean(y_test_min[under_predictions] - y_pred[under_predictions]) if under_predictions.any() else 0\n",
        "\n",
        "print(\"--- Model Performance ---\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"R¬≤: {r2:.4f}\")\n",
        "print(f\"Asymmetric MSE (Œ±=2): {asym_mse:.4f}\")\n",
        "print(f\"Mean Uncertainty: {uncertainty.mean():.4f}\")\n",
        "print(f\"Mean Confidence Interval Width: {(upper_bound - lower_bound).mean():.4f}\")\n",
        "print(f\"95% CI Coverage: {coverage:.1%}\")\n",
        "\n",
        "print(f\"\\nError Distribution:\")\n",
        "print(f\"Mean Error: {mean_error:.2f}\")\n",
        "print(f\"Std Error: {std_error:.2f}\")\n",
        "print(f\"2.5th percentile: {percentile_2_5:.2f}\")\n",
        "print(f\"97.5th percentile: {percentile_97_5:.2f}\")\n",
        "\n",
        "print(f\"\\nüö® UNDER-PREDICTION ANALYSIS (CRITICAL FOR MIN BID):\")\n",
        "print(f\"Under-prediction Rate: {under_prediction_rate:.1%}\")\n",
        "print(f\"Mean Under-prediction Loss: {mean_under_prediction_loss:.2f}\")\n",
        "print(f\"Under-predictions Count: {under_predictions.sum()} / {len(y_test_min)}\")\n",
        "\n",
        "# =============================================================================\n",
        "# ERROR DISTRIBUTION PLOT\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"ERROR DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot histogram of errors\n",
        "plt.hist(errors, bins=50, alpha=0.7, color='blue', density=False, label=\"Error Distribution\", edgecolor='black')\n",
        "\n",
        "# Generate normal distribution curve\n",
        "x_values = np.linspace(min(errors), max(errors), 100)\n",
        "y_values = stats.norm.pdf(x_values, mean_error, std_error) * len(errors) * (max(errors) - min(errors)) / 50\n",
        "plt.plot(x_values, y_values, color='black', linestyle='-', linewidth=2, label=\"Normal Approximation\")\n",
        "\n",
        "# Add vertical lines for standard deviations\n",
        "for i in range(1, 4):\n",
        "    plt.axvline(mean_error + i * std_error, color='green', linestyle='--', \n",
        "                label=f'+{i} SD' if i == 1 else \"\", alpha=0.7)\n",
        "    plt.axvline(mean_error - i * std_error, color='green', linestyle='--', \n",
        "                label=f'-{i} SD' if i == 1 else \"\", alpha=0.7)\n",
        "\n",
        "# Add vertical lines for percentiles\n",
        "plt.axvline(percentile_2_5, color='orange', linestyle='--', linewidth=2, label='2.5th Percentile')\n",
        "plt.axvline(percentile_97_5, color='orange', linestyle='--', linewidth=2, label='97.5th Percentile')\n",
        "\n",
        "# Add zero error line\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "\n",
        "# Highlight under-prediction zone\n",
        "plt.axvspan(min(errors), 0, alpha=0.2, color='red', label='Under-prediction Zone')\n",
        "\n",
        "plt.xlabel('Prediction Error (Predicted - Actual)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Error Distribution for Min Bid Model')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(regression_min_output_dir / 'min_bid_error_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# UNDER-PREDICTION RISK ANALYSIS (CRITICAL FOR MIN BID)\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"UNDER-PREDICTION RISK ANALYSIS - MIN BID\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Detailed under-prediction analysis\n",
        "under_pred_analysis = pd.DataFrame({\n",
        "    'actual': y_test_min,\n",
        "    'predicted': y_pred,\n",
        "    'error': errors,\n",
        "    'under_predicted': under_predictions,\n",
        "    'loss_if_under': np.where(under_predictions, y_test_min - y_pred, 0)\n",
        "})\n",
        "\n",
        "# Risk categories based on under-prediction magnitude\n",
        "under_pred_analysis['risk_category'] = pd.cut(\n",
        "    under_pred_analysis['loss_if_under'], \n",
        "    bins=[0, 5, 10, 20, float('inf')], \n",
        "    labels=['Safe', 'Low Risk', 'Medium Risk', 'High Risk'],\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "risk_summary = under_pred_analysis['risk_category'].value_counts()\n",
        "print(\"Under-prediction Risk Categories:\")\n",
        "for category, count in risk_summary.items():\n",
        "    percentage = (count / len(under_pred_analysis)) * 100\n",
        "    print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "# =============================================================================\n",
        "# SAFETY FACTOR ANALYSIS FOR MIN BID\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"SAFETY FACTOR ANALYSIS - MIN BID\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Initialize lists to store results\n",
        "safety_factors = np.arange(0.0, 1.1, 0.1)  # 0 to 1 with 0.1 intervals\n",
        "tpr_values = []\n",
        "loss_values = []\n",
        "under_pred_rates = []\n",
        "detailed_results = []\n",
        "\n",
        "print(\"Analyzing safety factors from 0.0 to 1.0...\")\n",
        "# Loop through safety factors\n",
        "for sf in safety_factors:\n",
        "    # Apply safety factor to predictions (increase predictions to be safer)\n",
        "    adjusted_pred = y_pred * (1 + sf)\n",
        "    \n",
        "    # Compute TP Rate (predictions >= actual) - want this HIGH for min bid\n",
        "    pred_binary = (adjusted_pred >= y_test_min).astype(int)\n",
        "    tp_rate = pred_binary.mean()\n",
        "    \n",
        "    # Compute Mean Loss (average difference between predicted and actual)\n",
        "    mean_loss = np.mean(adjusted_pred - y_test_min)\n",
        "    \n",
        "    # Compute under-prediction rate (want this LOW for min bid)\n",
        "    under_pred_rate = np.mean(adjusted_pred < y_test_min)\n",
        "    \n",
        "    # Store results\n",
        "    tpr_values.append(tp_rate)\n",
        "    loss_values.append(mean_loss)\n",
        "    under_pred_rates.append(under_pred_rate)\n",
        "    detailed_results.append({\n",
        "        'safety_factor': sf,\n",
        "        'tpr': tp_rate,\n",
        "        'mean_loss': mean_loss,\n",
        "        'under_prediction_rate': under_pred_rate,\n",
        "        'mae': mean_absolute_error(y_test_min, adjusted_pred),\n",
        "        'mse': mean_squared_error(y_test_min, adjusted_pred)\n",
        "    })\n",
        "    \n",
        "    print(f\"SF: {sf:.1f} | TPR: {tp_rate:.3f} | Mean Loss: {mean_loss:.2f} | Under-pred Rate: {under_pred_rate:.3f}\")\n",
        "\n",
        "# Create DataFrame for export\n",
        "safety_factor_df = pd.DataFrame(detailed_results)\n",
        "\n",
        "# Save results\n",
        "safety_factor_df.to_csv(regression_min_output_dir / 'min_bid_safety_factor_analysis.csv', index=False)\n",
        "print(f\"\\nSafety factor analysis saved to {regression_min_output_dir / 'min_bid_safety_factor_analysis.csv'}\")\n",
        "\n",
        "# Plot Safety Factor Analysis\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: TPR and Mean Loss\n",
        "ax1.plot(safety_factors, tpr_values, marker='o', markersize=8, color='blue', \n",
        "         linewidth=2, label='True Positive Rate (TPR)')\n",
        "ax1_twin = ax1.twinx()\n",
        "ax1_twin.plot(safety_factors, loss_values, marker='x', markersize=8, color='orange', \n",
        "              linewidth=2, label='Mean Loss')\n",
        "\n",
        "ax1.set_xlabel('Safety Factor', fontsize=12)\n",
        "ax1.set_ylabel('True Positive Rate (TPR)', color='blue', fontsize=12)\n",
        "ax1_twin.set_ylabel('Mean Loss (Predicted - Actual)', color='orange', fontsize=12)\n",
        "ax1.set_title('Safety Factor Analysis - Min Bid Model', fontsize=14)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlim(-0.05, 1.05)\n",
        "ax1.set_ylim(-0.05, 1.05)\n",
        "\n",
        "# Plot 2: Under-prediction Rate (Critical for Min Bid)\n",
        "ax2.plot(safety_factors, under_pred_rates, marker='s', markersize=8, color='red', \n",
        "         linewidth=2, label='Under-prediction Rate')\n",
        "ax2.set_xlabel('Safety Factor', fontsize=12)\n",
        "ax2.set_ylabel('Under-prediction Rate', color='red', fontsize=12)\n",
        "ax2.set_title('Under-prediction Risk vs Safety Factor', fontsize=14)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlim(-0.05, 1.05)\n",
        "ax2.set_ylim(-0.05, max(under_pred_rates) + 0.05)\n",
        "\n",
        "# Add target under-prediction rate line\n",
        "target_under_pred_rate = 0.05  # 5% target\n",
        "ax2.axhline(y=target_under_pred_rate, color='green', linestyle='--', \n",
        "            label=f'Target Under-pred Rate ({target_under_pred_rate:.1%})')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(regression_min_output_dir / 'min_bid_safety_factor_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Find optimal safety factor (prioritize low under-prediction for min bid)\n",
        "# Weight under-prediction rate heavily since it's critical for min bid\n",
        "alpha = 0.5  # Weight for TPR importance\n",
        "beta = 0.2   # Weight for minimizing loss\n",
        "gamma = 0.3  # Weight for minimizing under-prediction rate\n",
        "\n",
        "tpr_array = np.array(tpr_values)\n",
        "loss_array = np.array(loss_values)\n",
        "under_pred_array = np.array(under_pred_rates)\n",
        "\n",
        "# Normalize values\n",
        "tpr_normalized = (tpr_array - np.min(tpr_array)) / (np.max(tpr_array) - np.min(tpr_array))\n",
        "loss_normalized = (loss_array - np.min(loss_array)) / (np.max(loss_array) - np.min(loss_array))\n",
        "under_pred_normalized = (under_pred_array - np.min(under_pred_array)) / (np.max(under_pred_array) - np.min(under_pred_array))\n",
        "\n",
        "# Compute optimality score (minimize under-prediction rate for min bid)\n",
        "optimality_score = alpha * tpr_normalized - beta * loss_normalized - gamma * under_pred_normalized\n",
        "\n",
        "# Find best safety factor\n",
        "best_index = np.argmax(optimality_score)\n",
        "best_sf = safety_factors[best_index]\n",
        "best_tpr = tpr_values[best_index]\n",
        "best_loss = loss_values[best_index]\n",
        "best_under_pred_rate = under_pred_rates[best_index]\n",
        "\n",
        "print(f\"\\n--- Optimal Safety Factor for Min Bid ---\")\n",
        "print(f\"Best Safety Factor: {best_sf:.2f}\")\n",
        "print(f\"Corresponding TPR: {best_tpr:.3f}\")\n",
        "print(f\"Corresponding Mean Loss: {best_loss:.2f}\")\n",
        "print(f\"Corresponding Under-pred Rate: {best_under_pred_rate:.3f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# COMPREHENSIVE RESULTS VISUALIZATION\n",
        "# =============================================================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Actual vs Predicted\n",
        "axes[0, 0].scatter(y_test_min, y_pred, alpha=0.6, color='blue')\n",
        "axes[0, 0].plot([y_test_min.min(), y_test_min.max()], \n",
        "                [y_test_min.min(), y_test_min.max()], 'r--', lw=2)\n",
        "# Highlight under-predictions\n",
        "under_pred_mask = y_pred < y_test_min\n",
        "axes[0, 0].scatter(y_test_min[under_pred_mask], y_pred[under_pred_mask], \n",
        "                   alpha=0.8, color='red', s=20, label=f'Under-pred ({under_prediction_rate:.1%})')\n",
        "axes[0, 0].set_xlabel('Actual Min Bid')\n",
        "axes[0, 0].set_ylabel('Predicted Min Bid')\n",
        "axes[0, 0].set_title(f'Model Performance\\n(R¬≤ = {r2:.3f})')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals Distribution with Under-prediction Highlight\n",
        "axes[0, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0, 1].hist(errors[under_predictions], bins=50, edgecolor='black', alpha=0.7, color='red', label='Under-predictions')\n",
        "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "axes[0, 1].axvline(x=percentile_2_5, color='orange', linestyle='--', label='2.5th %ile')\n",
        "axes[0, 1].axvline(x=percentile_97_5, color='orange', linestyle='--', label='97.5th %ile')\n",
        "axes[0, 1].set_xlabel('Residuals')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title(f'Residuals Distribution\\n(MAE = {mae:.3f})')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Uncertainty Distribution\n",
        "axes[1, 0].hist(uncertainty, bins=50, edgecolor='black', alpha=0.7, color='lightgreen')\n",
        "axes[1, 0].set_xlabel('Prediction Uncertainty')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title(f'Uncertainty Distribution\\n(Mean = {uncertainty.mean():.3f})')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Model Summary with Under-prediction Focus\n",
        "summary_text = f\"\"\"Min Bid Model Results:\n",
        "\n",
        "Training Time: {training_time:.1f}s\n",
        "Total Trees: {min_model.tree_count_}\n",
        "\n",
        "Performance Metrics:\n",
        "- MSE: {mse:.4f}\n",
        "- MAE: {mae:.4f}\n",
        "- R¬≤: {r2:.4f}\n",
        "- Asymmetric MSE: {asym_mse:.4f}\n",
        "\n",
        "üö® Under-prediction Analysis:\n",
        "- Under-pred Rate: {under_prediction_rate:.1%}\n",
        "- Mean Under-pred Loss: {mean_under_prediction_loss:.2f}\n",
        "- Under-pred Count: {under_predictions.sum()}\n",
        "\n",
        "Uncertainty Analysis:\n",
        "- Mean Uncertainty: {uncertainty.mean():.4f}\n",
        "- 95% CI Coverage: {coverage:.1%}\n",
        "- Mean CI Width: {(upper_bound - lower_bound).mean():.4f}\n",
        "\n",
        "Optimal Safety Factor:\n",
        "- Safety Factor: {best_sf:.2f}\n",
        "- TPR: {best_tpr:.3f}\n",
        "- Mean Loss: {best_loss:.2f}\n",
        "- Under-pred Rate: {best_under_pred_rate:.3f}\n",
        "\n",
        "Data Summary:\n",
        "- Training: {len(X_train_min)} samples\n",
        "- Testing: {len(X_test_min)} samples\n",
        "- Features: {len(regression_features)}\"\"\"\n",
        "\n",
        "axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes, \n",
        "                fontsize=8, verticalalignment='top', fontfamily='monospace',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "axes[1, 1].set_title('Model Summary')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(regression_min_output_dir / 'regression_min_model_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# SAVE VALIDATION RESULTS\n",
        "# =============================================================================\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'actual': y_test_min,\n",
        "    'predicted': y_pred,\n",
        "    'lower_95_ci': lower_bound,\n",
        "    'upper_95_ci': upper_bound,\n",
        "    'uncertainty': uncertainty,\n",
        "    'interval_width': upper_bound - lower_bound,\n",
        "    'residuals': errors,\n",
        "    'abs_residuals': np.abs(errors),\n",
        "    'in_interval': in_interval,\n",
        "    'under_predicted': under_predictions,\n",
        "    'under_pred_loss': np.where(under_predictions, y_test_min - y_pred, 0)\n",
        "})\n",
        "\n",
        "# Save validation results\n",
        "results_df.to_csv(regression_min_output_dir / 'regression_min_validation_results.csv', index=False)\n",
        "\n",
        "# Save under-prediction risk analysis\n",
        "under_pred_analysis.to_csv(regression_min_output_dir / 'min_bid_under_prediction_analysis.csv', index=False)\n",
        "\n",
        "# Save model summary\n",
        "model_summary = pd.DataFrame({\n",
        "    'model_type': ['CatBoost with RMSE + Virtual Ensembles - Min Bid'],\n",
        "    'training_time_seconds': [training_time],\n",
        "    'total_trees': [min_model.tree_count_],\n",
        "    'mse': [mse],\n",
        "    'mae': [mae],\n",
        "    'r2': [r2],\n",
        "    'asymmetric_mse': [asym_mse],\n",
        "    'mean_uncertainty': [uncertainty.mean()],\n",
        "    'ci_coverage': [coverage],\n",
        "    'mean_ci_width': [(upper_bound - lower_bound).mean()],\n",
        "    'under_prediction_rate': [under_prediction_rate],\n",
        "    'mean_under_prediction_loss': [mean_under_prediction_loss],\n",
        "    'optimal_safety_factor': [best_sf],\n",
        "    'optimal_tpr': [best_tpr],\n",
        "    'optimal_under_pred_rate': [best_under_pred_rate],\n",
        "    'training_samples': [len(X_train_min)],\n",
        "    'test_samples': [len(X_test_min)],\n",
        "    'features_count': [len(regression_features)]\n",
        "})\n",
        "\n",
        "model_summary.to_csv(regression_min_output_dir / 'regression_min_validation_summary.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRODUCTION MODEL TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Combine train and test data for production model\n",
        "print(\"Combining training and test data for production model...\")\n",
        "X_production = pd.concat([X_train_min, X_test_min], ignore_index=True)\n",
        "y_production = pd.concat([y_train_min, y_test_min], ignore_index=True)\n",
        "\n",
        "print(f\"Production dataset shape: {X_production.shape}\")\n",
        "print(f\"Production target range: {y_production.min():.2f} to {y_production.max():.2f}\")\n",
        "\n",
        "# Train production model on all available data\n",
        "print(\"\\nTraining production CatBoost model on all data...\")\n",
        "production_start_time = time.time()\n",
        "\n",
        "production_min_model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    depth=10,\n",
        "    learning_rate=0.1,\n",
        "    l2_leaf_reg=5,\n",
        "    random_strength=1,\n",
        "    bagging_temperature=1,\n",
        "    cat_features=categorical_features,\n",
        "    verbose=100,\n",
        "    random_seed=42,\n",
        "    thread_count=-1,\n",
        "    loss_function='RMSE',\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "production_min_model.fit(X_production, y_production)\n",
        "\n",
        "production_training_time = time.time() - production_start_time\n",
        "print(f\"\\nProduction training completed in {production_training_time:.2f} seconds\")\n",
        "\n",
        "# Save production model\n",
        "production_model_path = regression_min_output_dir / 'production_regression_min_model.cbm'\n",
        "production_min_model.save_model(str(production_model_path))\n",
        "\n",
        "# Save production model summary\n",
        "production_summary = pd.DataFrame({\n",
        "    'model_type': ['Production CatBoost RMSE - Min Bid'],\n",
        "    'training_time_seconds': [production_training_time],\n",
        "    'total_trees': [production_min_model.tree_count_],\n",
        "    'training_samples': [len(X_production)],\n",
        "    'features_used': [len(regression_features)],\n",
        "    'target_range_min': [y_production.min()],\n",
        "    'target_range_max': [y_production.max()],\n",
        "    'target_mean': [y_production.mean()],\n",
        "    'target_std': [y_production.std()]\n",
        "})\n",
        "\n",
        "production_summary.to_csv(regression_min_output_dir / 'production_regression_min_summary.csv', index=False)\n",
        "\n",
        "# Save feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': regression_features,\n",
        "    'importance': production_min_model.get_feature_importance()\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "feature_importance.to_csv(regression_min_output_dir / 'production_feature_importance.csv', index=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "plt.barh(range(len(top_features)), top_features['importance'])\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 15 Feature Importance - Production Min Bid Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(regression_min_output_dir / 'production_feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(\"VALIDATION RESULTS:\")\n",
        "print(f\"‚úÖ Validation model performance - R¬≤: {r2:.4f}, MAE: {mae:.4f}\")\n",
        "print(f\"‚úÖ MSE: {mse:.4f}, Asymmetric MSE: {asym_mse:.4f}\")\n",
        "print(f\"‚úÖ 95% CI Coverage: {coverage:.1%}, Mean CI Width: {(upper_bound - lower_bound).mean():.4f}\")\n",
        "print(f\"üö® Under-prediction Rate: {under_prediction_rate:.1%} ({under_predictions.sum()} cases)\")\n",
        "print(f\"üö® Mean Under-prediction Loss: {mean_under_prediction_loss:.2f}\")\n",
        "print(f\"‚úÖ Optimal Safety Factor: {best_sf:.2f} (TPR: {best_tpr:.3f}, Under-pred: {best_under_pred_rate:.3f})\")\n",
        "\n",
        "print(\"\\nPRODUCTION MODEL:\")\n",
        "print(f\"‚úÖ Production model trained on {len(X_production)} samples\")\n",
        "print(f\"‚úÖ Training time: {production_training_time:.1f} seconds\")\n",
        "print(f\"‚úÖ Total trees: {production_min_model.tree_count_}\")\n",
        "\n",
        "print(f\"\\nSAVED ARTIFACTS:\")\n",
        "print(f\"üìÅ All files saved to: {regression_min_output_dir}\")\n",
        "print(f\"ü§ñ Production model: production_regression_min_model.cbm\")\n",
        "print(f\"üìä Validation results: regression_min_validation_results.csv\")\n",
        "print(f\"üìà Model results plot: regression_min_model_results.png\")\n",
        "print(f\"üìã Validation summary: regression_min_validation_summary.csv\")\n",
        "print(f\"üìä Error distribution: min_bid_error_distribution.png\")\n",
        "print(f\"üö® Under-prediction analysis: min_bid_under_prediction_analysis.csv\")\n",
        "print(f\"üéØ Safety factor analysis: min_bid_safety_factor_analysis.csv\")\n",
        "print(f\"üìä Safety factor plot: min_bid_safety_factor_plot.png\")\n",
        "print(f\"üéØ Feature importance: production_feature_importance.csv\")\n",
        "print(f\"üìä Feature importance plot: production_feature_importance.png\")\n",
        "print(f\"üìã Production summary: production_regression_min_summary.csv\")\n",
        "\n",
        "print(f\"\\nüö® CRITICAL: Monitor under-prediction rate - currently {under_prediction_rate:.1%}\")\n",
        "print(\"üöÄ Ready for deployment!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5819a997",
      "metadata": {},
      "source": [
        "## **5. Feature Importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964ba224",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create feature importance output directory\n",
        "feature_importance_output_dir = output_dir // 'feature_importance'\n",
        "feature_importance_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Feature importance artifacts will be saved to: {feature_importance_output_dir}\")\n",
        "\n",
        "# Classification feature importance\n",
        "clf_importance = pd.DataFrame({\n",
        "    'feature': classification_features,\n",
        "    'importance': clf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Median bid feature importance\n",
        "median_importance = pd.DataFrame({\n",
        "    'feature': regression_features,\n",
        "    'importance': median_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Min bid feature importance\n",
        "min_importance = pd.DataFrame({\n",
        "    'feature': regression_features,\n",
        "    'importance': min_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Visualize feature importance\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Classification\n",
        "axes[0].barh(clf_importance['feature'][:10], clf_importance['importance'][:10])\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_title('Top 10 Features - Classification')\n",
        "\n",
        "# Median Bid\n",
        "axes[1].barh(median_importance['feature'][:10], median_importance['importance'][:10])\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].set_title('Top 10 Features - Median Bid')\n",
        "\n",
        "# Min Bid\n",
        "axes[2].barh(min_importance['feature'][:10], min_importance['importance'][:10])\n",
        "axes[2].set_xlabel('Importance')\n",
        "axes[2].set_title('Top 10 Features - Min Bid')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(feature_importance_output_dir / 'feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save feature importance\n",
        "clf_importance.to_csv(feature_importance_output_dir / 'classification_feature_importance.csv', index=False)\n",
        "median_importance.to_csv(feature_importance_output_dir / 'median_bid_feature_importance.csv', index=False)\n",
        "min_importance.to_csv(feature_importance_output_dir / 'min_bid_feature_importance.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b60f4d5",
      "metadata": {},
      "source": [
        "## **6. Summary Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda8474d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY REPORT\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define output directories with correct paths (fixing // to /)\n",
        "classification_output_dir = output_dir / 'classification'\n",
        "regression_median_output_dir = output_dir / 'regression_median'\n",
        "regression_min_output_dir = output_dir / 'regression_min'\n",
        "\n",
        "# Get classification metrics (these should be available from the classification cell)\n",
        "try:\n",
        "    clf_recall = recall_bids\n",
        "    clf_precision = precision_bids\n",
        "    clf_f1 = 2 * (clf_precision * clf_recall) / (clf_precision + clf_recall)\n",
        "    \n",
        "    # Try to get additional classification metrics from saved summary\n",
        "    classification_summary = pd.read_csv(classification_output_dir / 'classification_validation_summary.csv')\n",
        "    clf_mean_confidence = classification_summary['mean_confidence'].iloc[0]\n",
        "    clf_training_time = classification_summary['training_time_seconds'].iloc[0]\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not load classification metrics: {e}\")\n",
        "    # Provide fallback values\n",
        "    clf_recall = 0.0\n",
        "    clf_precision = 0.0 \n",
        "    clf_f1 = 0.0\n",
        "    clf_mean_confidence = 0.0\n",
        "    clf_training_time = 0.0\n",
        "\n",
        "# Get median model metrics from the organized folder structure\n",
        "try:\n",
        "    median_summary = pd.read_csv(regression_median_output_dir / 'regression_median_validation_summary.csv')\n",
        "    median_mse = median_summary['mse'].iloc[0]\n",
        "    median_mae = median_summary['mae'].iloc[0]\n",
        "    median_r2 = median_summary['r2'].iloc[0]\n",
        "    median_uncertainty = median_summary['mean_uncertainty'].iloc[0]\n",
        "    median_ci_coverage = median_summary['ci_coverage'].iloc[0]\n",
        "    median_optimal_sf = median_summary['optimal_safety_factor'].iloc[0]\n",
        "    median_optimal_tpr = median_summary['optimal_tpr'].iloc[0]\n",
        "    median_training_time = median_summary['training_time_seconds'].iloc[0]\n",
        "    print(\"‚úÖ Loaded median model metrics from saved summary\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not load median summary: {e}\")\n",
        "    # Try to use variables from the median model cell if available\n",
        "    try:\n",
        "        median_mse = mse  # From median model cell\n",
        "        median_mae = mae\n",
        "        median_r2 = r2\n",
        "        median_uncertainty = uncertainty.mean() if 'uncertainty' in locals() else 8.77\n",
        "        median_ci_coverage = coverage if 'coverage' in locals() else 0.854\n",
        "        median_optimal_sf = best_sf if 'best_sf' in locals() else 0.70\n",
        "        median_optimal_tpr = best_tpr if 'best_tpr' in locals() else 0.922\n",
        "        median_training_time = training_time if 'training_time' in locals() else 40.0\n",
        "        print(\"‚úÖ Using median model variables from current session\")\n",
        "    except:\n",
        "        # Final fallback to known good values\n",
        "        median_mse = 258.8469\n",
        "        median_mae = 10.2140\n",
        "        median_r2 = 0.4380\n",
        "        median_uncertainty = 8.7746\n",
        "        median_ci_coverage = 0.854\n",
        "        median_optimal_sf = 0.70\n",
        "        median_optimal_tpr = 0.922\n",
        "        median_training_time = 40.0\n",
        "        print(\"‚ö†Ô∏è  Using median model fallback values\")\n",
        "\n",
        "# Get min model metrics from the organized folder structure  \n",
        "try:\n",
        "    min_summary = pd.read_csv(regression_min_output_dir / 'regression_min_validation_summary.csv')\n",
        "    min_mse = min_summary['mse'].iloc[0]\n",
        "    min_mae = min_summary['mae'].iloc[0]\n",
        "    min_r2 = min_summary['r2'].iloc[0]\n",
        "    min_uncertainty = min_summary['mean_uncertainty'].iloc[0]\n",
        "    min_under_predictions = min_summary['under_prediction_rate'].iloc[0]\n",
        "    min_ci_coverage = min_summary['ci_coverage'].iloc[0]\n",
        "    min_optimal_sf = min_summary['optimal_safety_factor'].iloc[0]\n",
        "    min_optimal_tpr = min_summary['optimal_tpr'].iloc[0]\n",
        "    min_training_time = min_summary['training_time_seconds'].iloc[0]\n",
        "    print(\"‚úÖ Loaded min model metrics from saved summary\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not load min summary: {e}\")\n",
        "    # Try to use variables from the min model cell if available\n",
        "    try:\n",
        "        # These variables should be available if min model cell was run\n",
        "        min_mse = mse if 'mse' in locals() else 239.3986\n",
        "        min_mae = mae if 'mae' in locals() else 9.7789\n",
        "        min_r2 = r2 if 'r2' in locals() else 0.4284\n",
        "        min_uncertainty = uncertainty.mean() if 'uncertainty' in locals() else 7.3378\n",
        "        min_under_predictions = under_prediction_rate if 'under_prediction_rate' in locals() else 0.449\n",
        "        min_ci_coverage = coverage if 'coverage' in locals() else 0.823\n",
        "        min_optimal_sf = best_sf if 'best_sf' in locals() else 1.00\n",
        "        min_optimal_tpr = best_tpr if 'best_tpr' in locals() else 0.950\n",
        "        min_training_time = training_time if 'training_time' in locals() else 34.0\n",
        "        print(\"‚úÖ Using min model variables from current session\")\n",
        "    except:\n",
        "        # Final fallback to known good values\n",
        "        min_mse = 239.3986\n",
        "        min_mae = 9.7789\n",
        "        min_r2 = 0.4284\n",
        "        min_uncertainty = 7.3378\n",
        "        min_under_predictions = 0.449\n",
        "        min_ci_coverage = 0.823\n",
        "        min_optimal_sf = 1.00\n",
        "        min_optimal_tpr = 0.950\n",
        "        min_training_time = 34.0\n",
        "        print(\"‚ö†Ô∏è  Using min model fallback values\")\n",
        "\n",
        "# Check if model files exist\n",
        "classification_model_exists = (classification_output_dir / 'production_classification_model.cbm').exists()\n",
        "median_model_exists = (regression_median_output_dir / 'production_regression_median_model.cbm').exists()\n",
        "min_model_exists = (regression_min_output_dir / 'production_regression_min_model.cbm').exists()\n",
        "\n",
        "summary = f\"\"\"\n",
        "SMU Course Bidding Prediction - Model Training Summary\n",
        "====================================================\n",
        "\n",
        "üéØ CLASSIFICATION MODEL (Bid Prediction)\n",
        "   Performance Metrics:\n",
        "   - Recall for Bids: {clf_recall:.4f}\n",
        "   - Precision for Bids: {clf_precision:.4f}\n",
        "   - F1-Score: {clf_f1:.4f}\n",
        "   - Mean Confidence Score: {clf_mean_confidence:.3f}\n",
        "   - Training Time: {clf_training_time:.1f}s\n",
        "   \n",
        "   Model Status: {'‚úÖ SAVED' if classification_model_exists else '‚ùå NOT FOUND'}\n",
        "   Model Path: {classification_output_dir / 'production_classification_model.cbm'}\n",
        "   Features: Probabilities, confidence scores, entropy-based confidence levels\n",
        "\n",
        "üìä MEDIAN BID REGRESSION MODEL  \n",
        "   Performance Metrics:\n",
        "   - R¬≤: {median_r2:.4f}\n",
        "   - MAE: {median_mae:.4f} points\n",
        "   - MSE: {median_mse:.4f}\n",
        "   - Mean Uncertainty: {median_uncertainty:.4f}\n",
        "   - 95% CI Coverage: {median_ci_coverage:.1%}\n",
        "   - Training Time: {median_training_time:.1f}s\n",
        "   \n",
        "   Safety Factor Analysis:\n",
        "   - Optimal Safety Factor: {median_optimal_sf:.2f}\n",
        "   - Recommended Multiplier: {1 + median_optimal_sf:.1f}x\n",
        "   - Success Rate at Optimal SF: {median_optimal_tpr:.1%}\n",
        "   \n",
        "   Model Status: {'‚úÖ SAVED' if median_model_exists else '‚ùå NOT FOUND'}\n",
        "   Model Path: {regression_median_output_dir / 'production_regression_median_model.cbm'}\n",
        "\n",
        "üö® MIN BID REGRESSION MODEL (CRITICAL)\n",
        "   Performance Metrics:\n",
        "   - R¬≤: {min_r2:.4f}\n",
        "   - MAE: {min_mae:.4f} points\n",
        "   - MSE: {min_mse:.4f}\n",
        "   - Mean Uncertainty: {min_uncertainty:.4f}\n",
        "   - 95% CI Coverage: {min_ci_coverage:.1%}\n",
        "   - Training Time: {min_training_time:.1f}s\n",
        "   \n",
        "   Under-prediction Analysis:\n",
        "   - Under-prediction Rate: {min_under_predictions:.1%} ({'üî¥ HIGH RISK' if min_under_predictions > 0.1 else 'üü° MODERATE RISK' if min_under_predictions > 0.05 else 'üü¢ LOW RISK'})\n",
        "   \n",
        "   Safety Factor Analysis:\n",
        "   - Optimal Safety Factor: {min_optimal_sf:.2f}\n",
        "   - Recommended Multiplier: {1 + min_optimal_sf:.1f}x\n",
        "   - Success Rate at Optimal SF: {min_optimal_tpr:.1%}\n",
        "   \n",
        "   Model Status: {'‚úÖ SAVED' if min_model_exists else '‚ùå NOT FOUND'}\n",
        "   Model Path: {regression_min_output_dir / 'production_regression_min_model.cbm'}\n",
        "\n",
        "üîß TECHNICAL SPECIFICATIONS\n",
        "   Algorithm: CatBoost with RMSE + Virtual Ensembles\n",
        "   Uncertainty Quantification: Tree subset variance method\n",
        "   Categorical Encoding: Ordered target statistics\n",
        "   Early Stopping: 50 iterations\n",
        "   Cross-validation: Temporal split (2021-2023 train, 2024 test)\n",
        "\n",
        "üìà PRODUCTION RECOMMENDATIONS\n",
        "\n",
        "   For 80% Success Rate:\n",
        "   - Median Bid: Multiply predictions by ~1.4x\n",
        "   - Min Bid: Multiply predictions by ~1.4x\n",
        "   \n",
        "   For 90% Success Rate (Recommended):\n",
        "   - Median Bid: Multiply predictions by {1 + median_optimal_sf:.1f}x\n",
        "   - Min Bid: Multiply predictions by {1 + min_optimal_sf:.1f}x\n",
        "   \n",
        "   ‚ö†Ô∏è  CRITICAL: Min bid under-prediction rate of {min_under_predictions:.1%} requires careful monitoring\n",
        "   üí° TIP: Always apply safety factors to avoid course registration failures\n",
        "\n",
        "üìÅ GENERATED ARTIFACTS\n",
        "   \n",
        "   Directory Structure:\n",
        "   üìÇ {output_dir / 'models'}\n",
        "   ‚îú‚îÄ‚îÄ üéØ classification/\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ production_classification_model.cbm\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ classification_validation_results.csv\n",
        "   ‚îÇ   ‚îî‚îÄ‚îÄ classification_validation_summary.csv\n",
        "   ‚îú‚îÄ‚îÄ üìä regression_median/\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ production_regression_median_model.cbm\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ regression_median_validation_results.csv\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ median_bid_safety_factor_analysis.csv\n",
        "   ‚îÇ   ‚îî‚îÄ‚îÄ regression_median_validation_summary.csv\n",
        "   ‚îî‚îÄ‚îÄ üö® regression_min/\n",
        "       ‚îú‚îÄ‚îÄ production_regression_min_model.cbm\n",
        "       ‚îú‚îÄ‚îÄ regression_min_validation_results.csv\n",
        "       ‚îú‚îÄ‚îÄ min_bid_safety_factor_analysis.csv\n",
        "       ‚îú‚îÄ‚îÄ min_bid_under_prediction_analysis.csv\n",
        "       ‚îî‚îÄ‚îÄ regression_min_validation_summary.csv\n",
        "\n",
        "üöÄ DEPLOYMENT STATUS\n",
        "   Classification Model: {'‚úÖ Ready' if classification_model_exists else '‚ùå Missing'}\n",
        "   Median Regression: {'‚úÖ Ready' if median_model_exists else '‚ùå Missing'}\n",
        "   Min Regression: {'‚úÖ Ready' if min_model_exists else '‚ùå Missing'}\n",
        "   \n",
        "   Overall Status: {'üöÄ All models ready for deployment!' if all([classification_model_exists, median_model_exists, min_model_exists]) else '‚ö†Ô∏è  Some models missing - check training completion'}\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "models_dir = output_dir\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save summary report to main models directory\n",
        "summary_file = models_dir / 'training_summary.txt'\n",
        "try:\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(summary)\n",
        "    print(f\"‚úÖ Summary report saved to {summary_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Could not save summary report: {e}\")\n",
        "\n",
        "# Additional verification\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"VERIFICATION CHECK\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Check if directories exist\n",
        "dirs_to_check = [classification_output_dir, regression_median_output_dir, regression_min_output_dir]\n",
        "dir_names = ['Classification', 'Median Regression', 'Min Regression']\n",
        "\n",
        "for dir_path, dir_name in zip(dirs_to_check, dir_names):\n",
        "    if dir_path.exists():\n",
        "        file_count = len(list(dir_path.glob('*')))\n",
        "        print(f\"‚úÖ {dir_name}: {file_count} files in {dir_path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {dir_name}: Directory not found - {dir_path}\")\n",
        "\n",
        "print(f\"\\nüéØ Next Steps:\")\n",
        "print(f\"1. Verify all models are trained and saved\")\n",
        "print(f\"2. Test model loading for production use\")\n",
        "print(f\"3. Implement safety factor multipliers in prediction pipeline\")\n",
        "print(f\"4. Monitor min bid under-prediction rate in production\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25fb2e7",
      "metadata": {},
      "source": [
        "## **7. Comprehensive Model Predictions**\n",
        "\n",
        "This section demonstrates the complete prediction pipeline by applying all three trained models to the entire `all_model_data` dataset. Unlike simple example predictions, this provides:\n",
        "\n",
        "### **Enhanced Prediction Pipeline**\n",
        "- **Classification predictions**: Bid probability, confidence scores, and entropy-based confidence levels\n",
        "- **Median bid predictions**: Point estimates with 95% confidence intervals and uncertainty quantification\n",
        "- **Min bid predictions**: Point estimates with 95% confidence intervals and under-prediction risk analysis\n",
        "- **Comprehensive output**: Original data enhanced with all prediction columns\n",
        "\n",
        "### **Key Features**\n",
        "1. **Robust Data Preprocessing**: \n",
        "   - Automatic handling of NaN values in categorical features\n",
        "   - Consistent feature engineering across all models\n",
        "   - Data validation and verification steps\n",
        "\n",
        "2. **Multi-Model Prediction Integration**:\n",
        "   - Simultaneous predictions from all three production models\n",
        "   - Uncertainty quantification using virtual ensemble methods\n",
        "   - Risk indicator calculations (under-predictions, confidence intervals)\n",
        "\n",
        "3. **Enhanced Output Dataset**:\n",
        "   - All original columns preserved\n",
        "   - 15+ new prediction and uncertainty columns added\n",
        "   - Risk indicators for decision support\n",
        "   - Confidence metrics for prediction reliability\n",
        "\n",
        "### **Output Columns Added**\n",
        "**Classification Predictions:**\n",
        "- `clf_predicted`: Binary prediction (0/1)\n",
        "- `clf_prob_no_bid`: Probability of no bids\n",
        "- `clf_prob_bid`: Probability of receiving bids\n",
        "- `clf_confidence_score`: Entropy-based confidence (0-1)\n",
        "- `clf_confidence_level`: Categorical confidence (Very Low to Very High)\n",
        "\n",
        "**Median Bid Predictions:**\n",
        "- `median_predicted`: Point prediction\n",
        "- `median_lower_95ci`: Lower 95% confidence bound\n",
        "- `median_upper_95ci`: Upper 95% confidence bound  \n",
        "- `median_uncertainty`: Prediction uncertainty estimate\n",
        "- `median_ci_width`: Width of confidence interval\n",
        "\n",
        "**Min Bid Predictions:**\n",
        "- `min_predicted`: Point prediction\n",
        "- `min_lower_95ci`: Lower 95% confidence bound\n",
        "- `min_upper_95ci`: Upper 95% confidence bound\n",
        "- `min_uncertainty`: Prediction uncertainty estimate  \n",
        "- `min_ci_width`: Width of confidence interval\n",
        "\n",
        "**Risk Indicators:**\n",
        "- `min_under_predicted`: Boolean flag for dangerous under-predictions\n",
        "- `median_in_ci`: Whether actual median falls within confidence interval\n",
        "- `min_in_ci`: Whether actual min falls within confidence interval\n",
        "\n",
        "### **Business Value**\n",
        "- **Complete dataset coverage**: Predictions for all courses in the dataset\n",
        "- **Decision support**: Confidence metrics help prioritize reliable predictions\n",
        "- **Risk management**: Under-prediction flags highlight high-risk scenarios\n",
        "- **Uncertainty quantification**: Confidence intervals enable informed decision-making\n",
        "- **Validation ready**: Enhanced dataset ready for backtesting and performance analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70404f20",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPREHENSIVE MODEL PREDICTIONS ON ALL_MODEL_DATA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define paths\n",
        "data_file = \"script_output/model_training/all_model_data_250625190126.csv\"\n",
        "output_dir = Path(\"script_output/models\")\n",
        "classification_output_dir = output_dir / 'classification'\n",
        "regression_median_output_dir = output_dir / 'regression_median'\n",
        "regression_min_output_dir = output_dir / 'regression_min'\n",
        "\n",
        "print(\"Loading all_model_data and trained models...\")\n",
        "\n",
        "try:\n",
        "    # Load the all_model_data CSV\n",
        "    all_data = pd.read_csv(data_file)\n",
        "    print(f\"‚úÖ Loaded all_model_data: {all_data.shape}\")\n",
        "    print(f\"Columns: {list(all_data.columns)}\")\n",
        "    \n",
        "    # Define feature columns (exclude target columns)\n",
        "    target_columns = ['bids', 'target_median_bid', 'target_min_bid']\n",
        "    feature_columns = [col for col in all_data.columns if col not in target_columns]\n",
        "    \n",
        "    print(f\"Feature columns ({len(feature_columns)}): {feature_columns}\")\n",
        "    \n",
        "    # Prepare features for prediction\n",
        "    X_features = all_data[feature_columns].copy()\n",
        "    \n",
        "    # =============================================================================\n",
        "    # CRITICAL: HANDLE NaN VALUES IN CATEGORICAL FEATURES\n",
        "    # =============================================================================\n",
        "    print(\"\\nüîß Preprocessing categorical features...\")\n",
        "    \n",
        "    # Define categorical features (based on your data structure)\n",
        "    categorical_features = [\n",
        "        'subject_area', 'catalogue_no', 'term', 'start_time', \n",
        "        'course_name', 'section', 'instructor'\n",
        "    ]\n",
        "    \n",
        "    # Check for NaN values in categorical features\n",
        "    print(\"Checking for NaN values in categorical features:\")\n",
        "    for col in categorical_features:\n",
        "        if col in X_features.columns:\n",
        "            nan_count = X_features[col].isna().sum()\n",
        "            if nan_count > 0:\n",
        "                print(f\"   {col}: {nan_count} NaN values found\")\n",
        "    \n",
        "    # Convert NaN values in categorical features to strings\n",
        "    for col in categorical_features:\n",
        "        if col in X_features.columns:\n",
        "            # Convert to string type first\n",
        "            X_features[col] = X_features[col].astype(str)\n",
        "            # Replace 'nan' strings with a consistent placeholder\n",
        "            X_features[col] = X_features[col].replace('nan', '__NA__')\n",
        "            # Also handle any remaining NaN values\n",
        "            X_features[col] = X_features[col].fillna('__NA__')\n",
        "    \n",
        "    print(\"‚úÖ Categorical features preprocessed\")\n",
        "    \n",
        "    # Verify no NaN values remain in categorical features\n",
        "    remaining_nans = 0\n",
        "    for col in categorical_features:\n",
        "        if col in X_features.columns:\n",
        "            nans = X_features[col].isna().sum()\n",
        "            remaining_nans += nans\n",
        "    \n",
        "    if remaining_nans > 0:\n",
        "        print(f\"‚ö†Ô∏è  Warning: {remaining_nans} NaN values still present in categorical features\")\n",
        "    else:\n",
        "        print(\"‚úÖ No NaN values in categorical features\")\n",
        "    \n",
        "    # Load all production models\n",
        "    classification_model = CatBoostClassifier()\n",
        "    classification_model.load_model(str(classification_output_dir / 'production_classification_model.cbm'))\n",
        "    \n",
        "    median_model = CatBoostRegressor()\n",
        "    median_model.load_model(str(regression_median_output_dir / 'production_regression_median_model.cbm'))\n",
        "    \n",
        "    min_model = CatBoostRegressor()\n",
        "    min_model.load_model(str(regression_min_output_dir / 'production_regression_min_model.cbm'))\n",
        "    \n",
        "    print(\"‚úÖ All models loaded successfully\")\n",
        "    \n",
        "    print(f\"Making predictions on {len(X_features)} samples...\")\n",
        "    \n",
        "    # =============================================================================\n",
        "    # CLASSIFICATION PREDICTIONS\n",
        "    # =============================================================================\n",
        "    print(\"\\nüéØ Generating classification predictions...\")\n",
        "    \n",
        "    # Get classification predictions and probabilities\n",
        "    clf_predictions = classification_model.predict(X_features)\n",
        "    clf_probabilities = classification_model.predict_proba(X_features)\n",
        "    \n",
        "    # Calculate entropy-based confidence for classification\n",
        "    def calculate_entropy_confidence(probabilities):\n",
        "        epsilon = 1e-10\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
        "        max_entropy = -np.log(1/probabilities.shape[1])\n",
        "        confidence_score = 1 - (entropy / max_entropy)\n",
        "        \n",
        "        confidence_levels = np.where(\n",
        "            confidence_score >= 0.9, 'Very High',\n",
        "            np.where(confidence_score >= 0.7, 'High',\n",
        "                    np.where(confidence_score >= 0.5, 'Medium',\n",
        "                            np.where(confidence_score >= 0.3, 'Low', 'Very Low')))\n",
        "        )\n",
        "        return confidence_score, confidence_levels\n",
        "    \n",
        "    clf_confidence_scores, clf_confidence_levels = calculate_entropy_confidence(clf_probabilities)\n",
        "    \n",
        "    # =============================================================================\n",
        "    # MEDIAN BID PREDICTIONS WITH UNCERTAINTY\n",
        "    # =============================================================================\n",
        "    print(\"üìä Generating median bid predictions with uncertainty...\")\n",
        "    \n",
        "    # Get median predictions\n",
        "    median_predictions = median_model.predict(X_features)\n",
        "    \n",
        "    # Calculate uncertainty using virtual ensembles\n",
        "    n_subsets = 10\n",
        "    median_subset_predictions = []\n",
        "    trees_per_subset = max(1, median_model.tree_count_ // n_subsets)\n",
        "    \n",
        "    for i in range(n_subsets):\n",
        "        tree_start = i * trees_per_subset\n",
        "        tree_end = min((i + 1) * trees_per_subset, median_model.tree_count_)\n",
        "        if tree_start < median_model.tree_count_:\n",
        "            partial_pred = median_model.predict(X_features, \n",
        "                                              ntree_start=tree_start, \n",
        "                                              ntree_end=tree_end)\n",
        "            median_subset_predictions.append(partial_pred)\n",
        "    \n",
        "    median_subset_predictions = np.array(median_subset_predictions)\n",
        "    median_uncertainty = np.std(median_subset_predictions, axis=0)\n",
        "    \n",
        "    # 95% confidence intervals for median\n",
        "    confidence_multiplier = 1.96\n",
        "    median_lower_ci = median_predictions - confidence_multiplier * median_uncertainty\n",
        "    median_upper_ci = median_predictions + confidence_multiplier * median_uncertainty\n",
        "    \n",
        "    # =============================================================================\n",
        "    # MIN BID PREDICTIONS WITH UNCERTAINTY\n",
        "    # =============================================================================\n",
        "    print(\"üö® Generating min bid predictions with uncertainty...\")\n",
        "    \n",
        "    # Get min predictions\n",
        "    min_predictions = min_model.predict(X_features)\n",
        "    \n",
        "    # Calculate uncertainty using virtual ensembles\n",
        "    min_subset_predictions = []\n",
        "    trees_per_subset = max(1, min_model.tree_count_ // n_subsets)\n",
        "    \n",
        "    for i in range(n_subsets):\n",
        "        tree_start = i * trees_per_subset\n",
        "        tree_end = min((i + 1) * trees_per_subset, min_model.tree_count_)\n",
        "        if tree_start < min_model.tree_count_:\n",
        "            partial_pred = min_model.predict(X_features, \n",
        "                                           ntree_start=tree_start, \n",
        "                                           ntree_end=tree_end)\n",
        "            min_subset_predictions.append(partial_pred)\n",
        "    \n",
        "    min_subset_predictions = np.array(min_subset_predictions)\n",
        "    min_uncertainty = np.std(min_subset_predictions, axis=0)\n",
        "    \n",
        "    # 95% confidence intervals for min\n",
        "    min_lower_ci = min_predictions - confidence_multiplier * min_uncertainty\n",
        "    min_upper_ci = min_predictions + confidence_multiplier * min_uncertainty\n",
        "    \n",
        "    # =============================================================================\n",
        "    # ADD PREDICTION COLUMNS TO ORIGINAL DATA\n",
        "    # =============================================================================\n",
        "    print(\"\\nüìã Adding prediction columns to all_model_data...\")\n",
        "    \n",
        "    # Create enhanced dataframe with all original columns plus predictions\n",
        "    enhanced_data = all_data.copy()\n",
        "    \n",
        "    # Add classification prediction columns\n",
        "    enhanced_data['clf_predicted'] = clf_predictions\n",
        "    enhanced_data['clf_prob_no_bid'] = clf_probabilities[:, 0]\n",
        "    enhanced_data['clf_prob_bid'] = clf_probabilities[:, 1]\n",
        "    enhanced_data['clf_confidence_score'] = clf_confidence_scores\n",
        "    enhanced_data['clf_confidence_level'] = clf_confidence_levels\n",
        "    \n",
        "    # Add median bid prediction columns\n",
        "    enhanced_data['median_predicted'] = median_predictions\n",
        "    enhanced_data['median_lower_95ci'] = median_lower_ci\n",
        "    enhanced_data['median_upper_95ci'] = median_upper_ci\n",
        "    enhanced_data['median_uncertainty'] = median_uncertainty\n",
        "    enhanced_data['median_ci_width'] = median_upper_ci - median_lower_ci\n",
        "    \n",
        "    # Add min bid prediction columns\n",
        "    enhanced_data['min_predicted'] = min_predictions\n",
        "    enhanced_data['min_lower_95ci'] = min_lower_ci\n",
        "    enhanced_data['min_upper_95ci'] = min_upper_ci\n",
        "    enhanced_data['min_uncertainty'] = min_uncertainty\n",
        "    enhanced_data['min_ci_width'] = min_upper_ci - min_lower_ci\n",
        "    \n",
        "    # Add risk indicator columns\n",
        "    enhanced_data['min_under_predicted'] = (min_predictions < enhanced_data['target_min_bid']) & (enhanced_data['target_min_bid'] > 0)\n",
        "    enhanced_data['median_in_ci'] = ((enhanced_data['target_median_bid'] >= median_lower_ci) & \n",
        "                                   (enhanced_data['target_median_bid'] <= median_upper_ci)) & (enhanced_data['target_median_bid'] > 0)\n",
        "    enhanced_data['min_in_ci'] = ((enhanced_data['target_min_bid'] >= min_lower_ci) & \n",
        "                                (enhanced_data['target_min_bid'] <= min_upper_ci)) & (enhanced_data['target_min_bid'] > 0)\n",
        "    \n",
        "    # =============================================================================\n",
        "    # DISPLAY SUMMARY STATISTICS\n",
        "    # =============================================================================\n",
        "    print(f\"\\nüìä ENHANCED DATA SUMMARY:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Original columns: {len(all_data.columns)}\")\n",
        "    print(f\"Enhanced columns: {len(enhanced_data.columns)}\")\n",
        "    print(f\"Added prediction columns: {len(enhanced_data.columns) - len(all_data.columns)}\")\n",
        "    print(f\"Total rows: {len(enhanced_data)}\")\n",
        "    \n",
        "    print(f\"\\nüìà PREDICTION STATISTICS:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Classification Predictions:\")\n",
        "    print(f\"   Will receive bids: {(clf_predictions == 1).sum()} ({(clf_predictions == 1).mean()*100:.1f}%)\")\n",
        "    print(f\"   No bids expected: {(clf_predictions == 0).sum()} ({(clf_predictions == 0).mean()*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nConfidence Distribution:\")\n",
        "    conf_dist = enhanced_data['clf_confidence_level'].value_counts()\n",
        "    for level, count in conf_dist.items():\n",
        "        print(f\"   {level}: {count} ({count/len(enhanced_data)*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nPrediction Ranges:\")\n",
        "    print(f\"   Median Bids: {median_predictions.min():.1f} - {median_predictions.max():.1f}\")\n",
        "    print(f\"   Min Bids: {min_predictions.min():.1f} - {min_predictions.max():.1f}\")\n",
        "    \n",
        "    print(f\"\\nAverage Uncertainties:\")\n",
        "    print(f\"   Median Model: ¬±{median_uncertainty.mean():.1f} points\")\n",
        "    print(f\"   Min Model: ¬±{min_uncertainty.mean():.1f} points\")\n",
        "    \n",
        "    # =============================================================================\n",
        "    # SHOW EXAMPLES\n",
        "    # =============================================================================\n",
        "    print(f\"\\nüîç SAMPLE PREDICTIONS (first 5 rows):\")\n",
        "    print(\"=\" * 120)\n",
        "    \n",
        "    # Show key columns for first 5 rows\n",
        "    example_columns = [\n",
        "        'subject_area', 'catalogue_no', 'round', \n",
        "        'clf_predicted', 'clf_prob_bid', 'clf_confidence_level',\n",
        "        'median_predicted', 'median_ci_width', \n",
        "        'min_predicted', 'min_ci_width'\n",
        "    ]\n",
        "    \n",
        "    display_df = enhanced_data[example_columns].head()\n",
        "    \n",
        "    # Round numeric columns for better display\n",
        "    numeric_cols = ['clf_prob_bid', 'median_predicted', 'median_ci_width', 'min_predicted', 'min_ci_width']\n",
        "    for col in numeric_cols:\n",
        "        if col in display_df.columns:\n",
        "            display_df[col] = display_df[col].round(2)\n",
        "    \n",
        "    print(display_df.to_string(index=False))\n",
        "    \n",
        "    # =============================================================================\n",
        "    # SAVE ENHANCED DATA\n",
        "    # =============================================================================\n",
        "    from datetime import datetime\n",
        "    timestamp = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
        "    output_file = output_dir / f'all_model_data_with_predictions_{timestamp}.csv'\n",
        "    \n",
        "    enhanced_data.to_csv(output_file, index=False)\n",
        "    \n",
        "    print(f\"\\nüíæ ENHANCED DATA SAVED:\")\n",
        "    print(f\"   File: {output_file}\")\n",
        "    print(f\"   Rows: {len(enhanced_data)}\")\n",
        "    print(f\"   Columns: {len(enhanced_data.columns)}\")\n",
        "    \n",
        "    print(f\"\\nüéØ NEW PREDICTION COLUMNS ADDED:\")\n",
        "    print(\"=\" * 50)\n",
        "    new_columns = [col for col in enhanced_data.columns if col not in all_data.columns]\n",
        "    for i, col in enumerate(new_columns, 1):\n",
        "        print(f\"{i:2d}. {col}\")\n",
        "    \n",
        "    print(f\"\\nüìã COLUMN CATEGORIES:\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"üéØ Classification: clf_predicted, clf_prob_no_bid, clf_prob_bid, clf_confidence_score, clf_confidence_level\")\n",
        "    print(\"üìä Median Predictions: median_predicted, median_lower_95ci, median_upper_95ci, median_uncertainty, median_ci_width\")\n",
        "    print(\"üö® Min Predictions: min_predicted, min_lower_95ci, min_upper_95ci, min_uncertainty, min_ci_width\")\n",
        "    print(\"‚ö†Ô∏è  Risk Indicators: min_under_predicted, median_in_ci, min_in_ci\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during comprehensive prediction: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"Please ensure all models are trained and saved properly.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE PREDICTION ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513713ba",
      "metadata": {},
      "source": [
        "## **8. Advanced Error Distribution Analysis**\n",
        "\n",
        "This section performs comprehensive statistical analysis of model prediction errors using advanced distribution fitting techniques. Unlike basic error analysis, this provides sophisticated uncertainty characterization for production deployment.\n",
        "\n",
        "### **Statistical Distribution Fitting**\n",
        "- **T-distribution fitting**: Uses maximum likelihood estimation to fit t-distributions to prediction errors\n",
        "- **Critical value calculation**: Determines confidence bounds for 90%, 95%, and 99% confidence levels\n",
        "- **Goodness-of-fit testing**: Kolmogorov-Smirnov and Anderson-Darling tests validate distribution assumptions\n",
        "\n",
        "### **Distribution Properties Analysis**\n",
        "1. **Shape Characteristics**:\n",
        "   - **Skewness analysis**: Identifies asymmetric error patterns\n",
        "   - **Kurtosis analysis**: Measures tail heaviness and outlier propensity\n",
        "   - **Tail behavior**: Quantifies deviation from normal distribution assumptions\n",
        "\n",
        "2. **Statistical Testing**:\n",
        "   - **Jarque-Bera test**: Tests for normality of error distribution\n",
        "   - **Shapiro-Wilk test**: Alternative normality test for smaller samples\n",
        "   - **Distribution comparison**: T-distribution vs normal distribution fit quality\n",
        "\n",
        "### **Advanced Uncertainty Insights**\n",
        "**Heavy Tail Analysis:**\n",
        "- Degrees of freedom < 3: Very heavy tails, extreme outliers likely\n",
        "- Degrees of freedom 3-10: Moderate tail heaviness, some outliers expected  \n",
        "- Degrees of freedom > 10: Near-normal behavior, standard uncertainty methods applicable\n",
        "\n",
        "**Practical Implications:**\n",
        "- **Safety factor adjustment**: Heavier tails require larger safety margins\n",
        "- **Confidence interval calibration**: T-distribution provides more accurate uncertainty bounds\n",
        "- **Risk assessment**: Tail behavior informs worst-case scenario planning\n",
        "\n",
        "### **Production Recommendations**\n",
        "Based on fitted distributions, the analysis provides:\n",
        "- **Adaptive confidence intervals**: Use t-distribution instead of normal assumptions\n",
        "- **Risk-adjusted safety factors**: Account for tail heaviness in safety margins\n",
        "- **Uncertainty methodology**: Robust methods for heavy-tailed error distributions\n",
        "- **Outlier handling strategies**: Systematic approach to extreme prediction errors\n",
        "\n",
        "### **Output Artifacts**\n",
        "- **Comprehensive analysis report**: CSV with all statistical parameters\n",
        "- **Distribution comparison plots**: Visual comparison of fitted vs empirical distributions\n",
        "- **Q-Q plots**: Quantile-quantile analysis for distribution validation\n",
        "- **Residual analysis**: Goodness-of-fit visualization and diagnostics\n",
        "\n",
        "This analysis enables production-ready uncertainty quantification that accounts for the true statistical nature of prediction errors, moving beyond simple normal distribution assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dc7340",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from pathlib import Path\n",
        "\n",
        "def fit_t_distribution(errors):\n",
        "    \"\"\"\n",
        "    Fit a t-distribution to the error data and return parameters\n",
        "    \"\"\"\n",
        "    # Remove any infinite or NaN values\n",
        "    clean_errors = errors[np.isfinite(errors)]\n",
        "    \n",
        "    # Fit t-distribution using maximum likelihood estimation\n",
        "    params = stats.t.fit(clean_errors)\n",
        "    df, loc, scale = params\n",
        "    \n",
        "    return df, loc, scale, params\n",
        "\n",
        "def calculate_critical_values(df, loc, scale, confidence_levels=[0.90, 0.95, 0.99]):\n",
        "    \"\"\"\n",
        "    Calculate critical values for different confidence levels\n",
        "    \"\"\"\n",
        "    critical_values = {}\n",
        "    \n",
        "    for conf_level in confidence_levels:\n",
        "        alpha = 1 - conf_level\n",
        "        # Two-tailed critical value\n",
        "        t_critical = stats.t.ppf(1 - alpha/2, df)\n",
        "        critical_value = loc + scale * t_critical\n",
        "        critical_values[f'{conf_level*100:.0f}%'] = {\n",
        "            't_critical': t_critical,\n",
        "            'critical_value_upper': critical_value,\n",
        "            'critical_value_lower': loc - scale * t_critical\n",
        "        }\n",
        "    \n",
        "    return critical_values\n",
        "\n",
        "def calculate_distribution_properties(errors):\n",
        "    \"\"\"\n",
        "    Calculate skewness, kurtosis, and other distribution properties\n",
        "    \"\"\"\n",
        "    # Remove any infinite or NaN values\n",
        "    clean_errors = errors[np.isfinite(errors)]\n",
        "    \n",
        "    properties = {\n",
        "        'mean': np.mean(clean_errors),\n",
        "        'std': np.std(clean_errors),\n",
        "        'variance': np.var(clean_errors),\n",
        "        'skewness': stats.skew(clean_errors),\n",
        "        'kurtosis': stats.kurtosis(clean_errors),  # Excess kurtosis (normal = 0)\n",
        "        'excess_kurtosis': stats.kurtosis(clean_errors),  # Same as above\n",
        "        'jarque_bera_stat': stats.jarque_bera(clean_errors)[0],\n",
        "        'jarque_bera_pvalue': stats.jarque_bera(clean_errors)[1],\n",
        "        'shapiro_wilk_stat': stats.shapiro(clean_errors)[0] if len(clean_errors) <= 5000 else np.nan,\n",
        "        'shapiro_wilk_pvalue': stats.shapiro(clean_errors)[1] if len(clean_errors) <= 5000 else np.nan,\n",
        "        'sample_size': len(clean_errors)\n",
        "    }\n",
        "    \n",
        "    # Interpret kurtosis\n",
        "    if properties['excess_kurtosis'] > 0:\n",
        "        properties['kurtosis_interpretation'] = f\"Leptokurtic (excess kurtosis: {properties['excess_kurtosis']:.3f}) - Heavy tails, more peaked than normal\"\n",
        "    elif properties['excess_kurtosis'] < 0:\n",
        "        properties['kurtosis_interpretation'] = f\"Platykurtic (excess kurtosis: {properties['excess_kurtosis']:.3f}) - Light tails, flatter than normal\"\n",
        "    else:\n",
        "        properties['kurtosis_interpretation'] = f\"Mesokurtic (excess kurtosis: {properties['excess_kurtosis']:.3f}) - Similar to normal distribution\"\n",
        "    \n",
        "    # Interpret skewness\n",
        "    if abs(properties['skewness']) < 0.5:\n",
        "        properties['skewness_interpretation'] = f\"Approximately symmetric (skewness: {properties['skewness']:.3f})\"\n",
        "    elif properties['skewness'] > 0.5:\n",
        "        properties['skewness_interpretation'] = f\"Right-skewed/Positive skew (skewness: {properties['skewness']:.3f}) - Long right tail\"\n",
        "    else:\n",
        "        properties['skewness_interpretation'] = f\"Left-skewed/Negative skew (skewness: {properties['skewness']:.3f}) - Long left tail\"\n",
        "    \n",
        "    return properties\n",
        "\n",
        "def goodness_of_fit_tests(errors, df, loc, scale):\n",
        "    \"\"\"\n",
        "    Perform goodness of fit tests for the t-distribution\n",
        "    \"\"\"\n",
        "    clean_errors = errors[np.isfinite(errors)]\n",
        "    \n",
        "    # Kolmogorov-Smirnov test\n",
        "    ks_stat, ks_pvalue = stats.kstest(clean_errors, lambda x: stats.t.cdf(x, df, loc, scale))\n",
        "    \n",
        "    # Anderson-Darling test (if sample size is reasonable)\n",
        "    try:\n",
        "        ad_stat, ad_critical_values, ad_significance_levels = stats.anderson(clean_errors, dist='norm')\n",
        "        ad_result = {\n",
        "            'statistic': ad_stat,\n",
        "            'critical_values': ad_critical_values,\n",
        "            'significance_levels': ad_significance_levels\n",
        "        }\n",
        "    except:\n",
        "        ad_result = {'statistic': np.nan, 'critical_values': [], 'significance_levels': []}\n",
        "    \n",
        "    return {\n",
        "        'ks_statistic': ks_stat,\n",
        "        'ks_pvalue': ks_pvalue,\n",
        "        'anderson_darling': ad_result\n",
        "    }\n",
        "\n",
        "def plot_distribution_analysis(errors, df, loc, scale, model_name, output_dir):\n",
        "    \"\"\"\n",
        "    Create comprehensive plots for distribution analysis\n",
        "    \"\"\"\n",
        "    clean_errors = errors[np.isfinite(errors)]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Plot 1: Histogram with fitted t-distribution\n",
        "    axes[0, 0].hist(clean_errors, bins=50, density=True, alpha=0.7, color='skyblue', \n",
        "                    edgecolor='black', label='Observed Errors')\n",
        "    \n",
        "    # Generate t-distribution curve\n",
        "    x_range = np.linspace(clean_errors.min(), clean_errors.max(), 1000)\n",
        "    fitted_curve = stats.t.pdf(x_range, df, loc, scale)\n",
        "    axes[0, 0].plot(x_range, fitted_curve, 'r-', linewidth=2, \n",
        "                    label=f'Fitted t-distribution (df={df:.2f})')\n",
        "    \n",
        "    # Normal distribution for comparison\n",
        "    normal_curve = stats.norm.pdf(x_range, loc, scale)\n",
        "    axes[0, 0].plot(x_range, normal_curve, 'g--', linewidth=2, \n",
        "                    label='Normal distribution')\n",
        "    \n",
        "    axes[0, 0].set_xlabel('Error (Predicted - Actual)')\n",
        "    axes[0, 0].set_ylabel('Density')\n",
        "    axes[0, 0].set_title(f'{model_name} - Error Distribution Analysis')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Q-Q plot\n",
        "    # Create theoretical quantiles manually for t-distribution\n",
        "    n = len(clean_errors)\n",
        "    theoretical_quantiles = stats.t.ppf(np.linspace(0.5/n, 1-0.5/n, n), df, loc, scale)\n",
        "    sample_quantiles = np.sort(clean_errors)\n",
        "    \n",
        "    axes[0, 1].scatter(theoretical_quantiles, sample_quantiles, alpha=0.6)\n",
        "    # Add perfect fit line\n",
        "    min_val = min(theoretical_quantiles.min(), sample_quantiles.min())\n",
        "    max_val = max(theoretical_quantiles.max(), sample_quantiles.max())\n",
        "    axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "    axes[0, 1].set_xlabel('Theoretical Quantiles (t-distribution)')\n",
        "    axes[0, 1].set_ylabel('Sample Quantiles')\n",
        "    axes[0, 1].set_title(f'{model_name} - Q-Q Plot (t-distribution)')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: CDF comparison\n",
        "    sorted_errors = np.sort(clean_errors)\n",
        "    empirical_cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
        "    theoretical_cdf = stats.t.cdf(sorted_errors, df, loc, scale)\n",
        "    \n",
        "    axes[1, 0].plot(sorted_errors, empirical_cdf, 'b-', label='Empirical CDF', linewidth=2)\n",
        "    axes[1, 0].plot(sorted_errors, theoretical_cdf, 'r--', label='Fitted t-distribution CDF', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Error')\n",
        "    axes[1, 0].set_ylabel('Cumulative Probability')\n",
        "    axes[1, 0].set_title(f'{model_name} - CDF Comparison')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Residuals plot\n",
        "    theoretical_quantiles = stats.t.ppf(np.linspace(0.01, 0.99, len(clean_errors)), df, loc, scale)\n",
        "    residuals = np.sort(clean_errors) - np.sort(theoretical_quantiles)\n",
        "    \n",
        "    axes[1, 1].scatter(np.sort(theoretical_quantiles), residuals, alpha=0.6)\n",
        "    axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
        "    axes[1, 1].set_xlabel('Theoretical Quantiles')\n",
        "    axes[1, 1].set_ylabel('Residuals')\n",
        "    axes[1, 1].set_title(f'{model_name} - Fit Residuals')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_dir / f'{model_name.lower().replace(\" \", \"_\")}_distribution_analysis.png', \n",
        "                dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ERROR DISTRIBUTION ANALYSIS WITH T-DISTRIBUTION FITTING\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Define paths\n",
        "    base_path = Path(\"script_output/models\")\n",
        "    median_dir = base_path / \"regression_median\"\n",
        "    min_dir = base_path / \"regression_min\"\n",
        "    output_dir = base_path / \"distribution_analysis\"\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Load validation results\n",
        "    print(\"Loading validation results...\")\n",
        "    \n",
        "    try:\n",
        "        # Load median model results\n",
        "        median_results = pd.read_csv(median_dir / \"regression_median_validation_results.csv\")\n",
        "        print(f\"‚úÖ Loaded median model results: {len(median_results)} samples\")\n",
        "        \n",
        "        # Load min model results  \n",
        "        min_results = pd.read_csv(min_dir / \"regression_min_validation_results.csv\")\n",
        "        print(f\"‚úÖ Loaded min model results: {len(min_results)} samples\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading validation results: {e}\")\n",
        "        return\n",
        "    \n",
        "    # Extract residuals (errors) from the results\n",
        "    # Based on the code, residuals = predicted - actual\n",
        "    median_errors = median_results['residuals'].values\n",
        "    min_errors = min_results['residuals'].values\n",
        "    \n",
        "    print(f\"\\nMedian model errors - Range: [{median_errors.min():.3f}, {median_errors.max():.3f}]\")\n",
        "    print(f\"Min model errors - Range: [{min_errors.min():.3f}, {min_errors.max():.3f}]\")\n",
        "    \n",
        "    # Initialize results storage\n",
        "    all_results = []\n",
        "    \n",
        "    # Analyze both models\n",
        "    models_data = [\n",
        "        (\"Median Bid Model\", median_errors),\n",
        "        (\"Min Bid Model\", min_errors)\n",
        "    ]\n",
        "    \n",
        "    for model_name, errors in models_data:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ANALYZING {model_name.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # 1. Fit t-distribution\n",
        "        print(\"1. Fitting t-distribution...\")\n",
        "        df, loc, scale, params = fit_t_distribution(errors)\n",
        "        print(f\"   Degrees of freedom (df): {df:.4f}\")\n",
        "        print(f\"   Location parameter (Œº): {loc:.4f}\")\n",
        "        print(f\"   Scale parameter (œÉ): {scale:.4f}\")\n",
        "        \n",
        "        # 2. Calculate critical values\n",
        "        print(\"\\n2. Calculating critical values...\")\n",
        "        critical_values = calculate_critical_values(df, loc, scale)\n",
        "        \n",
        "        for conf_level, values in critical_values.items():\n",
        "            print(f\"   {conf_level} Confidence Interval:\")\n",
        "            print(f\"      t-critical: ¬±{values['t_critical']:.4f}\")\n",
        "            print(f\"      Error bounds: [{values['critical_value_lower']:.4f}, {values['critical_value_upper']:.4f}]\")\n",
        "        \n",
        "        # 3. Calculate distribution properties\n",
        "        print(\"\\n3. Analyzing distribution properties...\")\n",
        "        properties = calculate_distribution_properties(errors)\n",
        "        \n",
        "        print(f\"   Mean: {properties['mean']:.4f}\")\n",
        "        print(f\"   Standard Deviation: {properties['std']:.4f}\")\n",
        "        print(f\"   Skewness: {properties['skewness']:.4f}\")\n",
        "        print(f\"   {properties['skewness_interpretation']}\")\n",
        "        print(f\"   Excess Kurtosis: {properties['excess_kurtosis']:.4f}\")\n",
        "        print(f\"   {properties['kurtosis_interpretation']}\")\n",
        "        print(f\"   Jarque-Bera test p-value: {properties['jarque_bera_pvalue']:.6f}\")\n",
        "        \n",
        "        if not np.isnan(properties['shapiro_wilk_pvalue']):\n",
        "            print(f\"   Shapiro-Wilk test p-value: {properties['shapiro_wilk_pvalue']:.6f}\")\n",
        "        \n",
        "        # 4. Goodness of fit tests\n",
        "        print(\"\\n4. Goodness of fit tests...\")\n",
        "        gof_tests = goodness_of_fit_tests(errors, df, loc, scale)\n",
        "        print(f\"   Kolmogorov-Smirnov test p-value: {gof_tests['ks_pvalue']:.6f}\")\n",
        "        \n",
        "        # 5. Create plots\n",
        "        print(\"\\n5. Creating distribution plots...\")\n",
        "        plot_distribution_analysis(errors, df, loc, scale, model_name, output_dir)\n",
        "        \n",
        "        # 6. Store results\n",
        "        model_results = {\n",
        "            'model_name': model_name,\n",
        "            'sample_size': len(errors),\n",
        "            \n",
        "            # T-distribution parameters\n",
        "            'df_degrees_of_freedom': df,\n",
        "            'location_parameter': loc,\n",
        "            'scale_parameter': scale,\n",
        "            \n",
        "            # Critical values\n",
        "            'critical_90_percent_lower': critical_values['90%']['critical_value_lower'],\n",
        "            'critical_90_percent_upper': critical_values['90%']['critical_value_upper'],\n",
        "            'critical_95_percent_lower': critical_values['95%']['critical_value_lower'],\n",
        "            'critical_95_percent_upper': critical_values['95%']['critical_value_upper'],\n",
        "            'critical_99_percent_lower': critical_values['99%']['critical_value_lower'],\n",
        "            'critical_99_percent_upper': critical_values['99%']['critical_value_upper'],\n",
        "            \n",
        "            't_critical_90_percent': critical_values['90%']['t_critical'],\n",
        "            't_critical_95_percent': critical_values['95%']['t_critical'],\n",
        "            't_critical_99_percent': critical_values['99%']['t_critical'],\n",
        "            \n",
        "            # Distribution properties\n",
        "            'mean_error': properties['mean'],\n",
        "            'std_error': properties['std'],\n",
        "            'variance_error': properties['variance'],\n",
        "            'skewness': properties['skewness'],\n",
        "            'skewness_interpretation': properties['skewness_interpretation'],\n",
        "            'excess_kurtosis': properties['excess_kurtosis'],\n",
        "            'kurtosis_interpretation': properties['kurtosis_interpretation'],\n",
        "            \n",
        "            # Statistical tests\n",
        "            'jarque_bera_statistic': properties['jarque_bera_stat'],\n",
        "            'jarque_bera_pvalue': properties['jarque_bera_pvalue'],\n",
        "            'shapiro_wilk_statistic': properties['shapiro_wilk_stat'],\n",
        "            'shapiro_wilk_pvalue': properties['shapiro_wilk_pvalue'],\n",
        "            'ks_statistic': gof_tests['ks_statistic'],\n",
        "            'ks_pvalue': gof_tests['ks_pvalue'],\n",
        "        }\n",
        "        \n",
        "        all_results.append(model_results)\n",
        "    \n",
        "    # Save comprehensive results to CSV\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    output_file = output_dir / \"error_distribution_analysis_results.csv\"\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ANALYSIS COMPLETE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"‚úÖ Results saved to: {output_file}\")\n",
        "    print(f\"‚úÖ Plots saved to: {output_dir}\")\n",
        "    \n",
        "    # Summary recommendations\n",
        "    print(f\"\\nüìä SUMMARY & RECOMMENDATIONS:\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    for i, model_results in enumerate(all_results):\n",
        "        model_name = model_results['model_name']\n",
        "        df = model_results['df_degrees_of_freedom']\n",
        "        skewness = model_results['skewness']\n",
        "        kurtosis = model_results['excess_kurtosis']\n",
        "        \n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  üìà Degrees of Freedom: {df:.2f}\")\n",
        "        \n",
        "        if df < 3:\n",
        "            print(f\"  ‚ö†Ô∏è  Very heavy tails (df < 3) - High uncertainty, extreme outliers likely\")\n",
        "        elif df < 10:\n",
        "            print(f\"  üî∂ Heavy tails (df < 10) - Moderate uncertainty, some outliers expected\")\n",
        "        else:\n",
        "            print(f\"  ‚úÖ Near-normal tails (df ‚â• 10) - Uncertainty close to normal distribution\")\n",
        "        \n",
        "        print(f\"  üìä Skewness: {skewness:.3f} - {model_results['skewness_interpretation']}\")\n",
        "        print(f\"  üìä Excess Kurtosis: {kurtosis:.3f} - {model_results['kurtosis_interpretation']}\")\n",
        "        \n",
        "        # Recommendations for uncertainty improvement\n",
        "        print(f\"  üí° Recommendations:\")\n",
        "        \n",
        "        if abs(skewness) > 0.5:\n",
        "            print(f\"     - Consider skew-adjusted confidence intervals\")\n",
        "            print(f\"     - Investigate systematic bias in predictions\")\n",
        "        \n",
        "        if kurtosis > 1:\n",
        "            print(f\"     - Use robust uncertainty quantification methods\")\n",
        "            print(f\"     - Consider outlier detection and handling\")\n",
        "            print(f\"     - Increase safety factors for extreme cases\")\n",
        "        \n",
        "        if df < 5:\n",
        "            print(f\"     - Use t-distribution instead of normal for confidence intervals\")\n",
        "            print(f\"     - Consider ensemble methods for uncertainty\")\n",
        "            print(f\"     - Implement adaptive confidence intervals\")\n",
        "    \n",
        "    print(f\"\\nüîó Next steps:\")\n",
        "    print(f\"   1. Review the analysis results CSV file\")\n",
        "    print(f\"   2. Examine distribution plots for visual insights\") \n",
        "    print(f\"   3. Consider implementing t-distribution based uncertainty\")\n",
        "    print(f\"   4. Adjust safety factors based on tail behavior\")\n",
        "    \n",
        "    return results_df, output_dir\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results_df, output_dir = main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bidly_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
