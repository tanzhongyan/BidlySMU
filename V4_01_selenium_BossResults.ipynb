{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SMU Course Scraping Using Selenium**\n",
    "\n",
    "<div style=\"background-color:#FFD700; padding:15px; border-radius:5px; border: 2px solid #FF4500;\">\n",
    "    \n",
    "  <h1 style=\"color:#8B0000;\">‚ö†Ô∏èüö® SCRAPE THIS DATA AT YOUR OWN RISK üö®‚ö†Ô∏è</h1>\n",
    "  \n",
    "  <p><strong>üìå If you need the data, please contact me directly.</strong> Only available for **existing students**.</p>\n",
    "\n",
    "  <h3>üîó üì© How to Get the Data?</h3>\n",
    "  <p>üì® <strong>Reach out to me for access</strong> instead of scraping manually.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:#FFF8DC; padding:12px; border-radius:5px; border: 1px solid #DAA520;\">\n",
    "    \n",
    "  <h2 style=\"color:#8B8000;\">‚ú® Looking for the Latest Model? Consider V4! ‚ú®</h2>\n",
    "  <p>üëâ <a href=\"V4_example_prediction.ipynb\"><strong>Check out V4 Here</strong></a></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Objective**\n",
    "This script is designed to scrape SMU course details from the BOSS system using Selenium. The process involves:\n",
    "1. Logging into the system manually to bypass authentication.\n",
    "2. Iteratively scraping class details for specified academic years and terms.\n",
    "3. Writing the scraped data to structured CSV files.\n",
    "\n",
    "### **Script Structure**\n",
    "1. **Setup**: Import libraries and initialize Selenium WebDriver.\n",
    "2. **Login**: Wait for manual login and authentication.\n",
    "3. **Scraping Logic**:\n",
    "    - `scrape_class_details`: Scrapes course details for a specific class number, academic year, and term.\n",
    "    - `main`: Manages the scraping process for multiple academic years and terms.\n",
    "4. **Execution**: Log in and start scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PGGSSENCMODE'] = 'disable'\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import win32com.client as win32\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import logging\n",
    "import psycopg2\n",
    "from typing import List, Optional, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import webbrowser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Scrape all BOSS data**\n",
    "\n",
    "### **BOSS Class Scraper Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `BOSSClassScraper` class automates the extraction of class timing data from SMU's BOSS (Banner Online Self-Service) system with intelligent resume capabilities. It systematically scrapes class details across multiple academic terms and saves them as HTML files for further processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Automated Web Scraping**: Navigates through BOSS class detail pages using Selenium WebDriver\n",
    "- **Resume Capability**: Automatically detects existing scraped files and continues from the last scraped class number, preventing duplicate work\n",
    "- **Flexible Term Range**: Dynamically derives academic years from input parameters (e.g., '2025-26_T1' to '2028-29_T2') rather than hardcoded lists\n",
    "- **Smart Pagination**: Scans class numbers from 1000-5000 with intelligent termination after 300 consecutive empty records\n",
    "- **Progress Tracking**: Monitors existing files and resumes scraping from the highest class number found for each term\n",
    "- **Data Organization**: Saves HTML files in structured directories by academic term (`script_input/classTimingsFull/`)\n",
    "- **Incremental CSV Updates**: Appends only new valid files to the existing CSV index, avoiding duplicates\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, standard libraries (`os`, `time`, `csv`, `re`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- Network access to SMU's BOSS system\n",
    "\n",
    "**User Requirements:**\n",
    "- **Manual Authentication**: User must manually log in and complete Microsoft Authenticator process when prompted\n",
    "- **SMU Credentials**: Valid access to BOSS system\n",
    "- **Directory Structure**: Code creates `script_input/classTimingsFull/` for HTML files and `script_input/scraped_filepaths.csv` for the file index\n",
    "\n",
    "**Resume Functionality:**\n",
    "- **Interruption Handling**: If scraping stops halfway due to network issues or manual interruption, the next run automatically resumes from the exact point it left off\n",
    "- **Duplicate Prevention**: Existing files are automatically detected and skipped, preventing re-downloading of already scraped data\n",
    "- **Natural Termination**: Uses 300 consecutive empty records threshold to handle BOSS system inconsistencies without hardcoded limits\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "scraper = BOSSClassScraper()\n",
    "# Will automatically resume from previous progress if files exist\n",
    "success = scraper.run_full_scraping_process('2025-26_T1', '2025-26_T3B')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOSSClassScraper:\n",
    "    \"\"\"\n",
    "    A class to scrape class details from BOSS (SMU's online class registration system)\n",
    "    and save them as HTML files for further processing with resume capability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BOSS Class Scraper with configuration parameters.\n",
    "        \"\"\"\n",
    "        self.term_code_map = {'T1': '10', 'T2': '20', 'T3A': '31', 'T3B': '32'}\n",
    "        self.all_terms = ['T1', 'T2', 'T3A', 'T3B']\n",
    "        self.driver = None\n",
    "        self.min_class_number = 1000\n",
    "        self.max_class_number = 5000\n",
    "        self.consecutive_empty_threshold = 300\n",
    "        \n",
    "    def _derive_academic_years(self, start_ay_term, end_ay_term):\n",
    "        \"\"\"\n",
    "        Derive academic years from start and end terms.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting term (e.g., '2025-26_T1')\n",
    "            end_ay_term: Ending term (e.g., '2028-29_T2')\n",
    "            \n",
    "        Returns:\n",
    "            List of academic years in format ['2025-26', '2026-27', ...]\n",
    "        \"\"\"\n",
    "        start_year = int(start_ay_term[:4])\n",
    "        end_year = int(end_ay_term[:4])\n",
    "        \n",
    "        academic_years = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            next_year = (year + 1) % 100\n",
    "            ay = f\"{year}-{next_year:02d}\"\n",
    "            academic_years.append(ay)\n",
    "            \n",
    "        return academic_years\n",
    "    \n",
    "    def _get_existing_files_progress(self, base_dir):\n",
    "        \"\"\"\n",
    "        Check existing files and determine the last scraped position for each term.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory where HTML files are stored\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with term as key and last scraped class number as value\n",
    "        \"\"\"\n",
    "        progress = {}\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            return progress\n",
    "            \n",
    "        for term_folder in os.listdir(base_dir):\n",
    "            term_path = os.path.join(base_dir, term_folder)\n",
    "            if os.path.isdir(term_path):\n",
    "                max_class_num = 0\n",
    "                \n",
    "                for filename in os.listdir(term_path):\n",
    "                    if filename.endswith('.html'):\n",
    "                        # Extract class number from filename\n",
    "                        # Format: SelectedAcadTerm=XXYY&SelectedClassNumber=ZZZZ.html\n",
    "                        match = re.search(r'SelectedClassNumber=(\\d+)\\.html', filename)\n",
    "                        if match:\n",
    "                            class_num = int(match.group(1))\n",
    "                            max_class_num = max(max_class_num, class_num)\n",
    "                \n",
    "                if max_class_num > 0:\n",
    "                    progress[term_folder] = max_class_num\n",
    "                    print(f\"Found existing progress for {term_folder}: last class number {max_class_num}\")\n",
    "        \n",
    "        return progress\n",
    "    \n",
    "    def wait_for_manual_login(self):\n",
    "        \"\"\"\n",
    "        Wait for manual login and Microsoft Authenticator process completion.\n",
    "        \"\"\"\n",
    "        print(\"Please log in manually and complete the Microsoft Authenticator process.\")\n",
    "        print(\"Waiting for BOSS dashboard to load...\")\n",
    "        \n",
    "        wait = WebDriverWait(self.driver, 120)\n",
    "        \n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"Label_UserName\")))\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(text(),'Sign out')]\")))\n",
    "            \n",
    "            username = self.driver.find_element(By.ID, \"Label_UserName\").text\n",
    "            print(f\"Login successful! Logged in as {username}\")\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Login failed or timed out. Could not detect login elements.\")\n",
    "            raise Exception(\"Login failed\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    def scrape_and_save_html(self, start_ay_term='2025-26_T1', end_ay_term='2025-26_T1', base_dir='script_input/classTimingsFull'):\n",
    "        \"\"\"\n",
    "        Scrapes class details from BOSS and saves them as HTML files with resume capability.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting academic year and term (e.g., '2025-26_T1')\n",
    "            end_ay_term: Ending academic year and term (e.g., '2025-26_T1')\n",
    "            base_dir: Base directory to save the HTML files\n",
    "        \"\"\"\n",
    "        # Check existing progress\n",
    "        existing_progress = self._get_existing_files_progress(base_dir)\n",
    "        \n",
    "        # Derive academic years from input terms\n",
    "        all_academic_years = self._derive_academic_years(start_ay_term, end_ay_term)\n",
    "        \n",
    "        # Generate all possible AY_TERM combinations\n",
    "        all_ay_terms = []\n",
    "        for ay in all_academic_years:\n",
    "            for term in self.all_terms:\n",
    "                all_ay_terms.append(f\"{ay}_{term}\")\n",
    "        \n",
    "        # Find the indices of the start and end terms\n",
    "        try:\n",
    "            start_idx = all_ay_terms.index(start_ay_term)\n",
    "            end_idx = all_ay_terms.index(end_ay_term)\n",
    "        except ValueError:\n",
    "            print(\"Invalid start or end term provided. Using full range.\")\n",
    "            start_idx = 0\n",
    "            end_idx = len(all_ay_terms) - 1\n",
    "        \n",
    "        # Select the range to scrape\n",
    "        ay_terms_to_scrape = all_ay_terms[start_idx:end_idx+1]\n",
    "        \n",
    "        # Create base directory if needed\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each AY_TERM\n",
    "        for ay_term in ay_terms_to_scrape:\n",
    "            print(f\"Processing {ay_term}...\")\n",
    "            \n",
    "            # Parse AY_TERM for URL\n",
    "            ay, term = ay_term.split('_')\n",
    "            ay_short = ay[2:4]  # last two digits of first year\n",
    "            term_code = self.term_code_map.get(term, '10')\n",
    "            \n",
    "            # Create folder for AY_TERM\n",
    "            folder_path = os.path.join(base_dir, ay_term)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            # Determine starting class number based on existing progress\n",
    "            start_class_num = self.min_class_number\n",
    "            if ay_term in existing_progress:\n",
    "                start_class_num = existing_progress[ay_term] + 1\n",
    "                print(f\"Resuming {ay_term} from class number {start_class_num}\")\n",
    "            \n",
    "            consecutive_empty = 0\n",
    "            \n",
    "            # Scrape each class number in range\n",
    "            for class_num in range(start_class_num, self.max_class_number + 1):\n",
    "                # Check if file already exists\n",
    "                filename = f\"SelectedAcadTerm={ay_short}{term_code}&SelectedClassNumber={class_num:04}.html\"\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"File already exists: {filepath}, skipping...\")\n",
    "                    consecutive_empty = 0  # Reset counter since we have data\n",
    "                    continue\n",
    "                \n",
    "                url = f\"https://boss.intranet.smu.edu.sg/ClassDetails.aspx?SelectedClassNumber={class_num:04}&SelectedAcadTerm={ay_short}{term_code}&SelectedAcadCareer=UGRD\"\n",
    "                \n",
    "                try:\n",
    "                    self.driver.get(url)\n",
    "                    \n",
    "                    wait = WebDriverWait(self.driver, 15)\n",
    "                    try:\n",
    "                        element = wait.until(EC.any_of(\n",
    "                            EC.presence_of_element_located((By.ID, \"lblClassInfoHeader\")),\n",
    "                            EC.presence_of_element_located((By.ID, \"lblErrorDetails\"))\n",
    "                        ))\n",
    "                        \n",
    "                        error_elements = self.driver.find_elements(By.ID, \"lblErrorDetails\")\n",
    "                        has_data = True\n",
    "                        \n",
    "                        for error in error_elements:\n",
    "                            if \"No record found\" in error.text:\n",
    "                                has_data = False\n",
    "                                break\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"Wait error: {e}\")\n",
    "                        has_data = False\n",
    "                    \n",
    "                    if not has_data:\n",
    "                        consecutive_empty += 1\n",
    "                        print(f\"No record found for {ay_term}, class {class_num:04}. Consecutive empty: {consecutive_empty}\")\n",
    "                        \n",
    "                        if consecutive_empty >= self.consecutive_empty_threshold:\n",
    "                            print(f\"{self.consecutive_empty_threshold} consecutive empty records reached for {ay_term}, moving on.\")\n",
    "                            break\n",
    "                        \n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "                    \n",
    "                    # Reset consecutive empty counter if data found\n",
    "                    consecutive_empty = 0\n",
    "                    \n",
    "                    # Save HTML file\n",
    "                    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(self.driver.page_source)\n",
    "                    \n",
    "                    print(f\"Saved {filepath}\")\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {str(e)}\")\n",
    "                    time.sleep(5)\n",
    "        \n",
    "        print(\"Scraping completed.\")\n",
    "    \n",
    "    def generate_scraped_filepaths_csv(self, base_dir='script_input/classTimingsFull', output_csv='script_input/scraped_filepaths.csv'):\n",
    "        \"\"\"\n",
    "        Generates a CSV file with paths to all valid HTML files (those without \"No record found\").\n",
    "        Updates existing CSV by appending new valid files.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory where HTML files are stored\n",
    "            output_csv: Path to the output CSV file\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated CSV file or None if error\n",
    "        \"\"\"\n",
    "        # Read existing filepaths if CSV exists\n",
    "        existing_filepaths = set()\n",
    "        if os.path.exists(output_csv):\n",
    "            try:\n",
    "                with open(output_csv, 'r', encoding='utf-8') as csvfile:\n",
    "                    reader = csv.reader(csvfile)\n",
    "                    next(reader)  # Skip header\n",
    "                    for row in reader:\n",
    "                        if row:\n",
    "                            existing_filepaths.add(row[0])\n",
    "                print(f\"Found {len(existing_filepaths)} existing filepaths in CSV\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading existing CSV: {str(e)}\")\n",
    "        \n",
    "        filepaths = []\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            print(f\"Directory '{base_dir}' does not exist.\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "        \n",
    "        # Walk through directory structure\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.html'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    \n",
    "                    # Skip if already in existing filepaths\n",
    "                    if filepath in existing_filepaths:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            if 'No record found' not in content:\n",
    "                                filepaths.append(filepath)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {filepath}: {str(e)}\")\n",
    "        \n",
    "        # Append new filepaths to CSV\n",
    "        mode = 'a' if existing_filepaths else 'w'\n",
    "        with open(output_csv, mode, newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not existing_filepaths:  # Write header only if new file\n",
    "                writer.writerow(['Filepath'])\n",
    "            for path in filepaths:\n",
    "                writer.writerow([path])\n",
    "        \n",
    "        total_valid_files = len(existing_filepaths) + len(filepaths)\n",
    "        print(f\"CSV updated with {len(filepaths)} new valid file paths. Total: {total_valid_files} files at {output_csv}\")\n",
    "        return output_csv\n",
    "    \n",
    "    def run_full_scraping_process(self, start_ay_term='2025-26_T1', end_ay_term='2025-26_T1'):\n",
    "        \"\"\"\n",
    "        Run the complete scraping process from login to CSV generation with resume capability.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting academic year and term\n",
    "            end_ay_term: Ending academic year and term\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up WebDriver\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            # Navigate to login page and wait for manual login\n",
    "            self.driver.get(\"https://boss.intranet.smu.edu.sg/\")\n",
    "            self.wait_for_manual_login()\n",
    "            \n",
    "            # Run the main scraping function\n",
    "            self.scrape_and_save_html(start_ay_term, end_ay_term)\n",
    "            \n",
    "            # Generate CSV with valid file paths\n",
    "            self.generate_scraped_filepaths_csv()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping process: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "scraper = BOSSClassScraper()\n",
    "success = scraper.run_full_scraping_process('2025-26_T1', '2025-26_T1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Extract Data from HTML Files**\n",
    "\n",
    "### **HTML Data Extractor Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `HTMLDataExtractor` class processes previously scraped HTML files from SMU's BOSS system and extracts structured data into Excel format. It systematically parses course information, class timings, academic terms, and exam schedules from local HTML files without requiring network access or authentication.\n",
    "\n",
    "**Key Features:**\n",
    "- **Local File Processing**: Uses Selenium WebDriver to parse local HTML files without network connectivity requirements\n",
    "- **Comprehensive Data Extraction**: Extracts course details, academic terms, class timings, exam schedules, grading information, and professor names\n",
    "- **Test-First Approach**: Includes `run_test()` function to validate extraction logic on a small sample before processing all files\n",
    "- **Structured Output**: Organizes extracted data into two Excel sheets - standalone records (one per HTML file) and multiple records (class/exam timings)\n",
    "- **Error Tracking**: Captures and logs parsing errors in a separate sheet for debugging and quality assurance\n",
    "- **Flexible Data Parsing**: Handles multiple academic term naming conventions and date formats used across different years\n",
    "- **Record Linking**: Uses record keys to maintain relationships between standalone and multiple data records\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, `pandas`, `openpyxl`, standard libraries (`os`, `re`, `datetime`, `pathlib`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- No network access required (processes local files only)\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Scraped HTML Files**: Previously downloaded HTML files from BOSS system stored locally\n",
    "- **File Path CSV**: `script_input/scraped_filepaths.csv` containing paths to valid HTML files\n",
    "- **Directory Structure**: HTML files organized in the expected folder structure (typically `script_input/classTimingsFull/`)\n",
    "\n",
    "**Output Structure:**\n",
    "- **Excel File**: `script_input/raw_data.xlsx` (or custom path) with multiple sheets:\n",
    "  - `standalone`: One record per HTML file with course and class information\n",
    "  - `multiple`: Multiple records for class timings and exam schedules\n",
    "  - `errors`: Parsing errors and problematic files for debugging\n",
    "\n",
    "**Data Extraction Capabilities:**\n",
    "- **Course Information**: Course codes, names, descriptions, credit units, course areas, enrollment requirements\n",
    "- **Academic Terms**: Term IDs, academic years, start/end dates, BOSS IDs\n",
    "- **Class Details**: Sections, grading basis, course outline URLs, professor names\n",
    "- **Timing Data**: Class schedules, exam dates, venues, day-of-week information\n",
    "- **Cross-References**: Maintains linking keys between related records across sheets\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "# Initialize extractor\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Test with sample files first (recommended)\n",
    "test_success = extractor.run_test(test_count=10)\n",
    "\n",
    "if test_success:\n",
    "    # Run full extraction\n",
    "    extractor.run()\n",
    "    \n",
    "# Or run directly without testing\n",
    "extractor.run(\n",
    "    scraped_filepaths_csv='script_input/scraped_filepaths.csv',\n",
    "    output_path='script_input/raw_data.xlsx'\n",
    ")\n",
    "```\n",
    "\n",
    "The class provides a crucial intermediate step between raw HTML scraping and database insertion, creating clean, structured data that can be further processed for database integration or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLDataExtractor:\n",
    "    \"\"\"\n",
    "    Extract raw data from scraped HTML files and save to Excel format using Selenium\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        self.driver = None\n",
    "        \n",
    "    def setup_selenium_driver(self):\n",
    "        \"\"\"Set up Selenium WebDriver for local file access\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--headless')  # Run in headless mode for efficiency\n",
    "            options.add_argument('--disable-gpu')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            print(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Selenium WebDriver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def safe_find_element_text(self, by, value):\n",
    "        \"\"\"Safely find element and return its text\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            return element.text.strip() if element else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def safe_find_element_attribute(self, by, value, attribute):\n",
    "        \"\"\"Safely find element and return its attribute\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            return element.get_attribute(attribute) if element else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def convert_date_to_timestamp(self, date_str):\n",
    "        \"\"\"Convert DD-Mmm-YYYY to database timestamp format\"\"\"\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "            return date_obj.strftime('%Y-%m-%d 00:00:00.000 +0800')\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def parse_acad_term(self, term_text):\n",
    "        \"\"\"Parse academic term text and return structured data\"\"\"\n",
    "        try:\n",
    "            # Pattern like \"2021-22 Term 2\" or \"2021-22 Session 1\"\n",
    "            pattern = r'(\\d{4})-(\\d{2})\\s+(.*)'\n",
    "            match = re.search(pattern, term_text)\n",
    "            \n",
    "            if not match:\n",
    "                return None, None, None, None\n",
    "            \n",
    "            start_year = int(match.group(1))\n",
    "            end_year_short = int(match.group(2))\n",
    "            term_desc = match.group(3).lower()\n",
    "            \n",
    "            # Convert 2-digit year to 4-digit\n",
    "            if end_year_short < 50:\n",
    "                end_year = 2000 + end_year_short\n",
    "            else:\n",
    "                end_year = 1900 + end_year_short\n",
    "            \n",
    "            # Determine term code\n",
    "            if 'term 1' in term_desc or 'session 1' in term_desc or 'august term' in term_desc:\n",
    "                term_code = 'T1'\n",
    "            elif 'term 2' in term_desc or 'session 2' in term_desc or 'january term' in term_desc:\n",
    "                term_code = 'T2'\n",
    "            elif 'term 3a' in term_desc:\n",
    "                term_code = 'T3A'\n",
    "            elif 'term 3b' in term_desc:\n",
    "                term_code = 'T3B'\n",
    "            else:\n",
    "                return start_year, end_year, None, None\n",
    "            \n",
    "            acad_term_id = f\"AY{start_year}{end_year_short:02d}{term_code}\"\n",
    "            \n",
    "            return start_year, end_year, term_code, acad_term_id\n",
    "        except Exception as e:\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def parse_course_and_section(self, header_text):\n",
    "        \"\"\"Parse course code and section from header text\"\"\"\n",
    "        try:\n",
    "            # Clean the text first\n",
    "            clean_text = re.sub(r'<[^>]+>', '', header_text)\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text.strip())\n",
    "            \n",
    "            # Try multiple regex patterns\n",
    "            patterns = [\n",
    "                r'([A-Z0-9_-]+)\\s+-\\s+(.+)',  # Standard format\n",
    "                r'([A-Z]+)\\s+(\\d+[A-Z0-9_]*)\\s+-\\s+(.+)',  # Split format\n",
    "                r'([A-Z0-9_\\s-]+?)\\s*[-‚Äì‚Äî]\\s*(.+)'  # Fallback\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(patterns):\n",
    "                match = re.match(pattern, clean_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    if i == 1:  # Split format\n",
    "                        course_code = match.group(1) + match.group(2)\n",
    "                        section = match.group(3)\n",
    "                    else:\n",
    "                        course_code = match.group(1)\n",
    "                        section = match.group(2)\n",
    "                    \n",
    "                    course_code = re.sub(r'\\s+', '', course_code.upper())\n",
    "                    section = section.strip()\n",
    "                    \n",
    "                    return course_code, section\n",
    "            \n",
    "            return None, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def parse_date_range(self, date_text):\n",
    "        \"\"\"Parse date range text and return start and end timestamps\"\"\"\n",
    "        try:\n",
    "            # Example: \"10-Jan-2022 to 01-May-2022\"\n",
    "            pattern = r'(\\d{1,2}-\\w{3}-\\d{4})\\s+to\\s+(\\d{1,2}-\\w{3}-\\d{4})'\n",
    "            match = re.search(pattern, date_text)\n",
    "            \n",
    "            if not match:\n",
    "                return None, None\n",
    "            \n",
    "            start_date = self.convert_date_to_timestamp(match.group(1))\n",
    "            end_date = self.convert_date_to_timestamp(match.group(2))\n",
    "            \n",
    "            return start_date, end_date\n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_course_areas_list(self):\n",
    "        \"\"\"Extract course areas as comma-separated string using Selenium\"\"\"\n",
    "        try:\n",
    "            course_areas_element = self.driver.find_element(By.ID, 'lblCourseAreas')\n",
    "            course_areas_html = course_areas_element.get_attribute('innerHTML')\n",
    "            \n",
    "            # Extract list items\n",
    "            areas_list = re.findall(r'<li>(.*?)</li>', course_areas_html)\n",
    "            if areas_list:\n",
    "                return ', '.join(areas_list)\n",
    "            else:\n",
    "                # Fallback to text content\n",
    "                return course_areas_element.text.strip()\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def extract_course_outline_url(self):\n",
    "        \"\"\"Extract course outline URL from HTML using Selenium\"\"\"\n",
    "        try:\n",
    "            onclick_attr = self.safe_find_element_attribute(By.ID, 'imgCourseOutline', 'onclick')\n",
    "            if onclick_attr:\n",
    "                url_match = re.search(r\"window\\.open\\('([^']+)'\", onclick_attr)\n",
    "                if url_match:\n",
    "                    return url_match.group(1)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def extract_boss_ids_from_filepath(self, filepath):\n",
    "        \"\"\"Extract BOSS IDs from filepath\"\"\"\n",
    "        try:\n",
    "            filename = os.path.basename(filepath)\n",
    "            acad_term_match = re.search(r'SelectedAcadTerm=(\\d+)', filename)\n",
    "            class_match = re.search(r'SelectedClassNumber=(\\d+)', filename)\n",
    "            \n",
    "            acad_term_boss_id = int(acad_term_match.group(1)) if acad_term_match else None\n",
    "            class_boss_id = int(class_match.group(1)) if class_match else None\n",
    "            \n",
    "            return acad_term_boss_id, class_boss_id\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_meeting_information(self, record_key):\n",
    "        \"\"\"Extract class timing and exam timing information using Selenium\"\"\"\n",
    "        try:\n",
    "            meeting_table = self.driver.find_element(By.ID, 'RadGrid_MeetingInfo_ctl00')\n",
    "            tbody = meeting_table.find_element(By.TAG_NAME, 'tbody')\n",
    "            rows = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if len(cells) < 7:\n",
    "                    continue\n",
    "                \n",
    "                meeting_type = cells[0].text.strip()\n",
    "                start_date_text = cells[1].text.strip()\n",
    "                end_date_text = cells[2].text.strip()\n",
    "                day_of_week = cells[3].text.strip()\n",
    "                start_time = cells[4].text.strip()\n",
    "                end_time = cells[5].text.strip()\n",
    "                venue = cells[6].text.strip() if len(cells) > 6 else \"\"\n",
    "                professor_name = cells[7].text.strip() if len(cells) > 7 else \"\"\n",
    "                \n",
    "                # Assume CLASS if meeting_type is empty\n",
    "                if not meeting_type:\n",
    "                    meeting_type = 'CLASS'\n",
    "                \n",
    "                if meeting_type == 'CLASS':\n",
    "                    # Convert dates to timestamp format\n",
    "                    start_date = self.convert_date_to_timestamp(start_date_text)\n",
    "                    end_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    timing_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'CLASS',\n",
    "                        'start_date': start_date,\n",
    "                        'end_date': end_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(timing_record)\n",
    "                \n",
    "                elif meeting_type == 'EXAM':\n",
    "                    # For exams, use the second date (end_date_text) as the exam date\n",
    "                    exam_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    exam_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'EXAM',\n",
    "                        'date': exam_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(exam_record)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'record_key': record_key,\n",
    "                'error': f'Error extracting meeting information: {str(e)}',\n",
    "                'type': 'parse_error'\n",
    "            })\n",
    "    \n",
    "    def process_html_file(self, filepath):\n",
    "        \"\"\"Process a single HTML file and extract all data using Selenium\"\"\"\n",
    "        try:\n",
    "            # Load HTML file\n",
    "            html_file = Path(filepath).resolve()\n",
    "            file_url = html_file.as_uri()\n",
    "            self.driver.get(file_url)\n",
    "            \n",
    "            # Create unique record key\n",
    "            record_key = f\"{os.path.basename(filepath)}\"\n",
    "            \n",
    "            # Extract basic information\n",
    "            class_header_text = self.safe_find_element_text(By.ID, 'lblClassInfoHeader')\n",
    "            if not class_header_text:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': 'Missing class header',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            course_code, section = self.parse_course_and_section(class_header_text)\n",
    "            \n",
    "            # Extract academic term\n",
    "            term_text = self.safe_find_element_text(By.ID, 'lblClassInfoSubHeader')\n",
    "            acad_year_start, acad_year_end, term, acad_term_id = self.parse_acad_term(term_text) if term_text else (None, None, None, None)\n",
    "            \n",
    "            # Extract course information\n",
    "            course_name = self.safe_find_element_text(By.ID, 'lblClassSection')\n",
    "            course_description = self.safe_find_element_text(By.ID, 'lblCourseDescription')\n",
    "            credit_units_text = self.safe_find_element_text(By.ID, 'lblUnits')\n",
    "            course_areas = self.extract_course_areas_list()\n",
    "            enrolment_requirements = self.safe_find_element_text(By.ID, 'lblEnrolmentRequirements')\n",
    "            \n",
    "            # Process credit units\n",
    "            try:\n",
    "                credit_units = float(credit_units_text) if credit_units_text else None\n",
    "            except (ValueError, TypeError):\n",
    "                credit_units = None\n",
    "            \n",
    "            # Extract grading basis\n",
    "            grading_text = self.safe_find_element_text(By.ID, 'lblGradingBasis')\n",
    "            grading_basis = None\n",
    "            if grading_text:\n",
    "                if grading_text.lower() == 'graded':\n",
    "                    grading_basis = 'Graded'\n",
    "                elif grading_text.lower() in ['pass/fail', 'pass fail']:\n",
    "                    grading_basis = 'Pass/Fail'\n",
    "                else:\n",
    "                    grading_basis = 'NA'\n",
    "            \n",
    "            # Extract course outline URL\n",
    "            course_outline_url = self.extract_course_outline_url()\n",
    "            \n",
    "            # Extract dates\n",
    "            period_text = self.safe_find_element_text(By.ID, 'lblDates')\n",
    "            start_dt, end_dt = self.parse_date_range(period_text) if period_text else (None, None)\n",
    "            \n",
    "            # Extract BOSS IDs\n",
    "            acad_term_boss_id, class_boss_id = self.extract_boss_ids_from_filepath(filepath)\n",
    "            \n",
    "            # Create standalone record\n",
    "            standalone_record = {\n",
    "                'record_key': record_key,\n",
    "                'filepath': filepath,\n",
    "                'course_code': course_code,\n",
    "                'section': section,\n",
    "                'course_name': course_name,\n",
    "                'course_description': course_description,\n",
    "                'credit_units': credit_units,\n",
    "                'course_area': course_areas,\n",
    "                'enrolment_requirements': enrolment_requirements,\n",
    "                'acad_term_id': acad_term_id,\n",
    "                'acad_year_start': acad_year_start,\n",
    "                'acad_year_end': acad_year_end,\n",
    "                'term': term,\n",
    "                'start_dt': start_dt,\n",
    "                'end_dt': end_dt,\n",
    "                'grading_basis': grading_basis,\n",
    "                'course_outline_url': course_outline_url,\n",
    "                'acad_term_boss_id': acad_term_boss_id,\n",
    "                'class_boss_id': class_boss_id,\n",
    "                'term_text': term_text,\n",
    "                'period_text': period_text\n",
    "            }\n",
    "            \n",
    "            self.standalone_data.append(standalone_record)\n",
    "            \n",
    "            # Extract meeting information\n",
    "            self.extract_meeting_information(record_key)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'filepath': filepath,\n",
    "                'error': str(e),\n",
    "                'type': 'processing_error'\n",
    "            })\n",
    "            return False\n",
    "    \n",
    "    def run_test(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv', test_count=10):\n",
    "        \"\"\"Randomly test the extraction on a subset of files\"\"\"\n",
    "        try:\n",
    "            print(f\"Starting test run with {test_count} randomly selected files...\")\n",
    "\n",
    "            # Reset data containers\n",
    "            self.standalone_data = []\n",
    "            self.multiple_data = []\n",
    "            self.errors = []\n",
    "\n",
    "            # Set up Selenium driver\n",
    "            self.setup_selenium_driver()\n",
    "\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "\n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            all_filepaths = df[filepath_column].dropna().tolist()\n",
    "\n",
    "            if len(all_filepaths) == 0:\n",
    "                raise ValueError(\"No valid filepaths found in CSV\")\n",
    "\n",
    "            # Randomly sample filepaths\n",
    "            sample_size = min(test_count, len(all_filepaths))\n",
    "            sampled_filepaths = random.sample(all_filepaths, sample_size)\n",
    "\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "\n",
    "            for i, filepath in enumerate(sampled_filepaths, start=1):\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"Processing test file {i}/{sample_size}: {os.path.basename(filepath)}\")\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "\n",
    "            print(f\"\\nTest run complete: {successful_files}/{processed_files} files successful\")\n",
    "            print(f\"Standalone records extracted: {len(self.standalone_data)}\")\n",
    "            print(f\"Multiple records extracted: {len(self.multiple_data)}\")\n",
    "            if self.errors:\n",
    "                print(f\"Errors encountered: {len(self.errors)}\")\n",
    "                for error in self.errors[:3]:  # Show only the first 3 errors\n",
    "                    print(f\"  - {error['type']}: {error['error']}\")\n",
    "\n",
    "            # Save test results\n",
    "            test_output_path = 'script_input/test_raw_data.xlsx'\n",
    "            self.save_to_excel(test_output_path)\n",
    "\n",
    "            return successful_files > 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in test run: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Test selenium driver closed\")\n",
    "    \n",
    "    def process_all_files(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv'):\n",
    "        \"\"\"Process all files listed in the scraped filepaths CSV\"\"\"\n",
    "        try:\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "            \n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            \n",
    "            total_files = len(df)\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "            \n",
    "            print(f\"Starting to process {total_files} files\")\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                filepath = row[filepath_column]\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                    \n",
    "                    if processed_files % 100 == 0:\n",
    "                        print(f\"Processed {processed_files}/{total_files} files\")\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "            \n",
    "            print(f\"Processing complete: {successful_files}/{processed_files} files successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_all_files: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_to_excel(self, output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Save extracted data to Excel file with two sheets\"\"\"\n",
    "        try:\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Create DataFrames\n",
    "            standalone_df = pd.DataFrame(self.standalone_data)\n",
    "            multiple_df = pd.DataFrame(self.multiple_data)\n",
    "            \n",
    "            # Save to Excel with multiple sheets\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                standalone_df.to_excel(writer, sheet_name='standalone', index=False)\n",
    "                multiple_df.to_excel(writer, sheet_name='multiple', index=False)\n",
    "                \n",
    "                # Also save errors if any\n",
    "                if self.errors:\n",
    "                    errors_df = pd.DataFrame(self.errors)\n",
    "                    errors_df.to_excel(writer, sheet_name='errors', index=False)\n",
    "            \n",
    "            print(f\"Data saved to {output_path}\")\n",
    "            print(f\"Standalone records: {len(self.standalone_data)}\")\n",
    "            print(f\"Multiple records: {len(self.multiple_data)}\")\n",
    "            if self.errors:\n",
    "                print(f\"Errors: {len(self.errors)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Excel: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv', output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Run the complete extraction process\"\"\"\n",
    "        print(\"Starting HTML data extraction...\")\n",
    "        \n",
    "        # Reset data containers\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        \n",
    "        # Set up Selenium driver\n",
    "        self.setup_selenium_driver()\n",
    "        \n",
    "        try:\n",
    "            # Process all files\n",
    "            self.process_all_files(scraped_filepaths_csv)\n",
    "            \n",
    "            # Save to Excel\n",
    "            self.save_to_excel(output_path)\n",
    "            \n",
    "            print(\"HTML data extraction completed!\")\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Selenium driver closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 18:08:34,790 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HTML data extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 18:08:38,277 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-06 18:08:38,298 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-06 18:08:38,323 - INFO - Driver [C:\\Users\\tanzh\\.wdm\\drivers\\chromedriver\\win64\\137.0.7151.68\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium WebDriver initialized successfully\n",
      "Starting to process 12976 files\n",
      "Processed 100/12976 files\n",
      "Processed 200/12976 files\n",
      "Processed 300/12976 files\n",
      "Processed 400/12976 files\n",
      "Processed 500/12976 files\n",
      "Processed 600/12976 files\n",
      "Processed 700/12976 files\n",
      "Processed 800/12976 files\n",
      "Processed 900/12976 files\n",
      "Processed 1000/12976 files\n",
      "Processed 1100/12976 files\n",
      "Processed 1200/12976 files\n",
      "Processed 1300/12976 files\n",
      "Processed 1400/12976 files\n",
      "Processed 1500/12976 files\n",
      "Processed 1600/12976 files\n",
      "Processed 1700/12976 files\n",
      "Processed 1800/12976 files\n",
      "Processed 1900/12976 files\n",
      "Processed 2000/12976 files\n",
      "Processed 2100/12976 files\n",
      "Processed 2200/12976 files\n",
      "Processed 2300/12976 files\n",
      "Processed 2400/12976 files\n",
      "Processed 2500/12976 files\n",
      "Processed 2600/12976 files\n",
      "Processed 2700/12976 files\n",
      "Processed 2800/12976 files\n",
      "Processed 2900/12976 files\n",
      "Processed 3000/12976 files\n",
      "Processed 3100/12976 files\n",
      "Processed 3200/12976 files\n",
      "Processed 3300/12976 files\n",
      "Processed 3400/12976 files\n",
      "Processed 3500/12976 files\n",
      "Processed 3600/12976 files\n",
      "Processed 3700/12976 files\n",
      "Processed 3800/12976 files\n",
      "Processed 3900/12976 files\n",
      "Processed 4000/12976 files\n",
      "Processed 4100/12976 files\n",
      "Processed 4200/12976 files\n",
      "Processed 4300/12976 files\n",
      "Processed 4400/12976 files\n",
      "Processed 4500/12976 files\n",
      "Processed 4600/12976 files\n",
      "Processed 4700/12976 files\n",
      "Processed 4800/12976 files\n",
      "Processed 4900/12976 files\n",
      "Processed 5000/12976 files\n",
      "Processed 5100/12976 files\n",
      "Processed 5200/12976 files\n",
      "Processed 5300/12976 files\n",
      "Processed 5400/12976 files\n",
      "Processed 5500/12976 files\n",
      "Processed 5600/12976 files\n",
      "Processed 5700/12976 files\n",
      "Processed 5800/12976 files\n",
      "Processed 5900/12976 files\n",
      "Processed 6000/12976 files\n",
      "Processed 6100/12976 files\n",
      "Processed 6200/12976 files\n",
      "Processed 6300/12976 files\n",
      "Processed 6400/12976 files\n",
      "Processed 6500/12976 files\n",
      "Processed 6600/12976 files\n",
      "Processed 6700/12976 files\n",
      "Processed 6800/12976 files\n",
      "Processed 6900/12976 files\n",
      "Processed 7000/12976 files\n",
      "Processed 7100/12976 files\n",
      "Processed 7200/12976 files\n",
      "Processed 7300/12976 files\n",
      "Processed 7400/12976 files\n",
      "Processed 7500/12976 files\n",
      "Processed 7600/12976 files\n",
      "Processed 7700/12976 files\n",
      "Processed 7800/12976 files\n",
      "Processed 7900/12976 files\n",
      "Processed 8000/12976 files\n",
      "Processed 8100/12976 files\n",
      "Processed 8200/12976 files\n",
      "Processed 8300/12976 files\n",
      "Processed 8400/12976 files\n",
      "Processed 8500/12976 files\n",
      "Processed 8600/12976 files\n",
      "Processed 8700/12976 files\n",
      "Processed 8800/12976 files\n",
      "Processed 8900/12976 files\n",
      "Processed 9000/12976 files\n",
      "Processed 9100/12976 files\n",
      "Processed 9200/12976 files\n",
      "Processed 9300/12976 files\n",
      "Processed 9400/12976 files\n",
      "Processed 9500/12976 files\n",
      "Processed 9600/12976 files\n",
      "Processed 9700/12976 files\n",
      "Processed 9800/12976 files\n",
      "Processed 9900/12976 files\n",
      "Processed 10000/12976 files\n",
      "Processed 10100/12976 files\n",
      "Processed 10200/12976 files\n",
      "Processed 10300/12976 files\n",
      "Processed 10400/12976 files\n",
      "Processed 10500/12976 files\n",
      "Processed 10600/12976 files\n",
      "Processed 10700/12976 files\n",
      "Processed 10800/12976 files\n",
      "Processed 10900/12976 files\n",
      "Processed 11000/12976 files\n",
      "Processed 11100/12976 files\n",
      "Processed 11200/12976 files\n",
      "Processed 11300/12976 files\n",
      "Processed 11400/12976 files\n",
      "Processed 11500/12976 files\n",
      "Processed 11600/12976 files\n",
      "Processed 11700/12976 files\n",
      "Processed 11800/12976 files\n",
      "Processed 11900/12976 files\n",
      "Processed 12000/12976 files\n",
      "Processed 12100/12976 files\n",
      "Processed 12200/12976 files\n",
      "Processed 12300/12976 files\n",
      "Processed 12400/12976 files\n",
      "Processed 12500/12976 files\n",
      "Processed 12600/12976 files\n",
      "Processed 12700/12976 files\n",
      "Processed 12800/12976 files\n",
      "Processed 12900/12976 files\n",
      "Processing complete: 12976/12976 files successful\n",
      "Data saved to script_input/raw_data.xlsx\n",
      "Standalone records: 12976\n",
      "Multiple records: 19988\n",
      "HTML data extraction completed!\n",
      "Selenium driver closed\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Run the extraction process\n",
    "extractor.run(scraped_filepaths_csv='script_input/scraped_filepaths.csv', output_path='script_input/raw_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **4. Process Raw Data into Database Tables**\n",
    "\n",
    "### **TableBuilder Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `TableBuilder` class processes structured data from the HTML extractor and transforms it into database-ready CSV files for SMU's class management system. It handles complex data relationships, professor name normalization, duplicate detection, and creates all necessary tables for courses, classes, professors, and timing schedules while maintaining referential integrity.\n",
    "\n",
    "**Key Features:**\n",
    "- **Two-Phase Processing**: Separates professor/course creation from class/timing processing to allow manual review and correction\n",
    "- **Intelligent Professor Matching**: Advanced name normalization and substring matching to prevent duplicate professor creation\n",
    "- **Comprehensive Data Pipeline**: Processes professors, courses, academic terms, classes, class timings, and exam schedules\n",
    "- **Database Cache Integration**: Loads existing data from PostgreSQL to avoid duplicates and maintain consistency\n",
    "- **Manual Review Workflow**: Outputs verification files for human review before final processing\n",
    "- **Asian Name Handling**: Specialized normalization for Asian, Western, and mixed naming conventions common in Singapore\n",
    "- **Faculty Assignment Interface**: Interactive web-based system for assigning courses to appropriate faculties\n",
    "- **Error Recovery**: Robust handling of malformed data and missing information\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `pandas`, `psycopg2`, `openpyxl`, `uuid`, `webbrowser`, standard libraries\n",
    "- PostgreSQL database connection (configured via `.env` file)\n",
    "- Database cache files or live database access for existing data validation\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Raw Data Excel**: `script_input/raw_data.xlsx` from HTML extractor (point 3) with `standalone` and `multiple` sheets\n",
    "- **Database Configuration**: `.env` file with PostgreSQL connection parameters\n",
    "- **Professor Lookup**: `script_input/professor_lookup.csv` for existing professor mappings\n",
    "\n",
    "**Output Structure:**\n",
    "- **Verification Files** (`script_output/verify/`):\n",
    "  - `new_professors.csv`: New professors requiring manual name review\n",
    "  - `new_courses.csv`: New courses for validation\n",
    "- **Database Insert Files** (`script_output/`):\n",
    "  - `new_classes.csv`, `new_class_timing.csv`, `new_class_exam_timing.csv`\n",
    "  - `new_acad_term.csv`, `update_courses.csv`\n",
    "  - `professor_lookup.csv`: Updated professor mapping table\n",
    "\n",
    "**Data Processing Capabilities:**\n",
    "- **Professor Normalization**: Converts names to boss format (ALL CAPS) and afterclass format (Title Case)\n",
    "- **Duplicate Detection**: Substring matching across existing professors, new professors, and cached data\n",
    "- **Course Management**: Creates new courses and updates existing ones with latest information\n",
    "- **Academic Term Generation**: Automatically creates term IDs and manages semester data\n",
    "- **Relationship Mapping**: Maintains foreign key relationships across all generated tables\n",
    "- **Faculty Assignment**: Interactive workflow for assigning courses to SMU's 8 schools/centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TableBuilder:\n",
    "    \"\"\"Comprehensive table builder for university class management system\"\"\"\n",
    "    \n",
    "    def __init__(self, input_file: str = 'script_input/raw_data.xlsx'):\n",
    "        \"\"\"Initialize TableBuilder with database configuration and caching\"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_base = 'script_output'\n",
    "        self.verify_dir = os.path.join(self.output_base, 'verify')\n",
    "        self.cache_dir = 'db_cache'\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(self.output_base, exist_ok=True)\n",
    "        os.makedirs(self.verify_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        self.db_config = {\n",
    "            'host': os.getenv('DB_HOST'),\n",
    "            'database': os.getenv('DB_NAME'),\n",
    "            'user': os.getenv('DB_USER'),\n",
    "            'password': os.getenv('DB_PASSWORD'),\n",
    "            'port': int(os.getenv('DB_PORT', 5432)),\n",
    "            'gssencmode': 'disable'\n",
    "        }\n",
    "        \n",
    "        # Database connection\n",
    "        self.connection = None\n",
    "        \n",
    "        # Data storage\n",
    "        self.standalone_data = None\n",
    "        self.multiple_data = None\n",
    "        \n",
    "        # Caches\n",
    "        self.professors_cache = {}  # name -> professor data\n",
    "        self.courses_cache = {}     # code -> course data\n",
    "        self.acad_term_cache = {}   # id -> acad_term data\n",
    "        self.faculties_cache = {}   # id -> faculty data\n",
    "        self.faculty_acronym_to_id = {}  # acronym -> faculty_id mapping\n",
    "        self.professor_lookup = {}  # scraped_name -> database mapping\n",
    "        \n",
    "        # Output data collectors\n",
    "        self.new_professors = []\n",
    "        self.new_courses = []\n",
    "        self.update_courses = []\n",
    "        self.new_acad_terms = []\n",
    "        self.new_classes = []\n",
    "        self.new_class_timings = []\n",
    "        self.new_class_exam_timings = []\n",
    "        \n",
    "        # Class ID mapping for timing tables\n",
    "        self.class_id_mapping = {}  # record_key -> class_id\n",
    "        \n",
    "        # Courses requiring faculty assignment\n",
    "        self.courses_needing_faculty = []\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'professors_created': 0,\n",
    "            'courses_created': 0,\n",
    "            'courses_updated': 0,\n",
    "            'classes_created': 0,\n",
    "            'timings_created': 0,\n",
    "            'exams_created': 0,\n",
    "            'courses_needing_faculty': 0\n",
    "        }\n",
    "        \n",
    "        # Asian surnames database for name normalization\n",
    "        self.asian_surnames = {\n",
    "            'chinese': ['WANG', 'LI', 'ZHANG', 'LIU', 'CHEN', 'YANG', 'HUANG', 'ZHAO', 'WU', 'ZHOU',\n",
    "                       'XU', 'SUN', 'MA', 'ZHU', 'HU', 'GUO', 'HE', 'LIN', 'GAO', 'LUO'],\n",
    "            'singaporean': ['TAN', 'LIM', 'LEE', 'NG', 'ONG', 'WONG', 'GOH', 'CHUA', 'CHAN', 'KOH',\n",
    "                           'TEO', 'AW', 'CHYE', 'YEO', 'SIM', 'CHIA', 'CHONG', 'LAM', 'CHEW', 'TOH'],\n",
    "            'korean': ['KIM', 'LEE', 'PARK', 'CHOI', 'JUNG', 'KANG', 'CHO', 'YUN', 'JANG', 'LIM'],\n",
    "            'vietnamese': ['NGUYEN', 'TRAN', 'LE', 'PHAM', 'HOANG', 'PHAN', 'VU', 'DANG', 'BUI'],\n",
    "            'indian': ['SHARMA', 'SINGH', 'KUMAR', 'GUPTA', 'KOHLI', 'PATEL', 'MAKHIJA']\n",
    "        }\n",
    "        self.all_asian_surnames = set()\n",
    "        for surnames in self.asian_surnames.values():\n",
    "            self.all_asian_surnames.update(surnames)\n",
    "        \n",
    "        # Western given names\n",
    "        self.western_given_names = {\n",
    "            'AARON', 'ADAM', 'ADRIAN', 'ALEXANDER', 'AMANDA', 'ANDREW', 'ANTHONY',\n",
    "            'BENJAMIN', 'CHRISTOPHER', 'DANIEL', 'DAVID', 'EMILY', 'JAMES', 'JENNIFER',\n",
    "            'JOHN', 'MICHAEL', 'PETER', 'ROBERT', 'SARAH', 'THOMAS', 'WILLIAM'\n",
    "        }\n",
    "\n",
    "        # Bid results data collectors\n",
    "        self.boss_log_file = os.path.join(self.output_base, 'boss_result_log.txt')\n",
    "        self.new_bid_windows = []\n",
    "        self.new_class_availability = []\n",
    "        self.new_bid_results = []\n",
    "\n",
    "    def connect_database(self):\n",
    "        \"\"\"Connect to PostgreSQL database\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(**self.db_config)\n",
    "            logger.info(\"‚úÖ Database connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Database connection failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_or_cache_data(self):\n",
    "        \"\"\"Load data from cache or database\"\"\"\n",
    "        # Try loading from cache first\n",
    "        if self._load_from_cache():\n",
    "            logger.info(\"‚úÖ Loaded data from cache\")\n",
    "            return True\n",
    "        \n",
    "        # Connect to database and download\n",
    "        if not self.connect_database():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self._download_and_cache_data()\n",
    "            logger.info(\"‚úÖ Downloaded and cached data from database\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to download data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _download_and_cache_data(self):\n",
    "        \"\"\"Download data from database and cache locally\"\"\"\n",
    "        # Download professors\n",
    "        query = \"SELECT * FROM professors\"\n",
    "        professors_df = pd.read_sql_query(query, self.connection)\n",
    "        professors_df.to_pickle(os.path.join(self.cache_dir, 'professors_cache.pkl'))\n",
    "        \n",
    "        # Download courses\n",
    "        query = \"SELECT * FROM courses\"\n",
    "        courses_df = pd.read_sql_query(query, self.connection)\n",
    "        courses_df.to_pickle(os.path.join(self.cache_dir, 'courses_cache.pkl'))\n",
    "        \n",
    "        # Download acad_terms\n",
    "        query = \"SELECT * FROM acad_term\"\n",
    "        acad_terms_df = pd.read_sql_query(query, self.connection)\n",
    "        acad_terms_df.to_pickle(os.path.join(self.cache_dir, 'acad_terms_cache.pkl'))\n",
    "        \n",
    "        # Download faculties (NEW)\n",
    "        query = \"SELECT * FROM faculties\"\n",
    "        faculties_df = pd.read_sql_query(query, self.connection)\n",
    "        faculties_df.to_pickle(os.path.join(self.cache_dir, 'faculties_cache.pkl'))\n",
    "        \n",
    "        # Load into memory\n",
    "        self._load_from_cache()\n",
    "\n",
    "    def _load_from_cache(self) -> bool:\n",
    "        \"\"\"Load cached data from files\"\"\"\n",
    "        try:\n",
    "            cache_files = {\n",
    "                'professors': os.path.join(self.cache_dir, 'professors_cache.pkl'),\n",
    "                'courses': os.path.join(self.cache_dir, 'courses_cache.pkl'),\n",
    "                'acad_terms': os.path.join(self.cache_dir, 'acad_terms_cache.pkl'),\n",
    "                'faculties': os.path.join(self.cache_dir, 'faculties_cache.pkl')  # NEW\n",
    "            }\n",
    "            \n",
    "            if all(os.path.exists(f) for f in cache_files.values()):\n",
    "                # Load professors\n",
    "                professors_df = pd.read_pickle(cache_files['professors'])\n",
    "                for _, row in professors_df.iterrows():\n",
    "                    self.professors_cache[row['name'].upper()] = row.to_dict()\n",
    "                \n",
    "                # Load courses\n",
    "                courses_df = pd.read_pickle(cache_files['courses'])\n",
    "                for _, row in courses_df.iterrows():\n",
    "                    self.courses_cache[row['code']] = row.to_dict()\n",
    "                \n",
    "                # Load acad_terms\n",
    "                acad_terms_df = pd.read_pickle(cache_files['acad_terms'])\n",
    "                for _, row in acad_terms_df.iterrows():\n",
    "                    self.acad_term_cache[row['id']] = row.to_dict()\n",
    "                \n",
    "                # Load faculties (NEW)\n",
    "                faculties_df = pd.read_pickle(cache_files['faculties'])\n",
    "                for _, row in faculties_df.iterrows():\n",
    "                    faculty_id = row['id']\n",
    "                    acronym = row['acronym'].upper()\n",
    "                    self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                    self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                \n",
    "                # Load professor lookup if exists\n",
    "                lookup_file = 'script_input/professor_lookup.csv'\n",
    "                if os.path.exists(lookup_file):\n",
    "                    lookup_df = pd.read_csv(lookup_file)\n",
    "                    for _, row in lookup_df.iterrows():\n",
    "                        self.professor_lookup[row['scraped_name']] = {\n",
    "                            'database_id': row['database_id'],\n",
    "                            'boss_name': row.get('boss_name', row['scraped_name'].upper()),\n",
    "                            'afterclass_name': row.get('afterclass_name', row['scraped_name'])\n",
    "                        }\n",
    "                \n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cache loading error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_raw_data(self):\n",
    "        \"\"\"Load raw data from Excel file\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"üìÇ Loading raw data from {self.input_file}\")\n",
    "            \n",
    "            # Load both sheets\n",
    "            self.standalone_data = pd.read_excel(self.input_file, sheet_name='standalone')\n",
    "            self.multiple_data = pd.read_excel(self.input_file, sheet_name='multiple')\n",
    "            \n",
    "            logger.info(f\"‚úÖ Loaded {len(self.standalone_data)} standalone records\")\n",
    "            logger.info(f\"‚úÖ Loaded {len(self.multiple_data)} multiple records\")\n",
    "            \n",
    "            from collections import defaultdict\n",
    "            \n",
    "            self.multiple_lookup = defaultdict(list)\n",
    "            for _, row in self.multiple_data.iterrows():\n",
    "                key = row.get('record_key')\n",
    "                if pd.notna(key):\n",
    "                    self.multiple_lookup[key].append(row)\n",
    "            \n",
    "            logger.info(f\"‚úÖ Created optimized lookup for {len(self.multiple_lookup)} record keys\")\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to load raw data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def normalize_professor_name(self, name: str) -> Tuple[str, str]:\n",
    "        \"\"\"Normalize professor name and return (boss_format, afterclass_format)\"\"\"\n",
    "        if not name or pd.isna(name):\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        # Clean and prepare name\n",
    "        name = str(name).strip()\n",
    "        \n",
    "        # Handle comma-separated names properly\n",
    "        if ',' in name:\n",
    "            comma_count = name.count(',')\n",
    "            if comma_count == 1:\n",
    "                # Single comma - convert \"SURNAME, Given Names\" to \"SURNAME Given Names\"\n",
    "                parts = name.split(',')\n",
    "                if len(parts) == 2:\n",
    "                    surname = parts[0].strip().upper()\n",
    "                    given_names = parts[1].strip()\n",
    "                    # Convert given names to title case\n",
    "                    given_names_title = ' '.join(word.capitalize() for word in given_names.split())\n",
    "                    name = f\"{surname} {given_names_title}\"\n",
    "                # If not exactly 2 parts, keep original\n",
    "            elif comma_count > 1:\n",
    "                # Multiple commas - likely multiple professors, take first\n",
    "                name = name.split(',')[0].strip()\n",
    "\n",
    "        \n",
    "        # Detect naming pattern\n",
    "        words = name.split()\n",
    "        if not words:\n",
    "            return name.upper(), name\n",
    "        \n",
    "        # Detect pattern\n",
    "        pattern = self._detect_name_pattern(words)\n",
    "        \n",
    "        # Format based on pattern\n",
    "        if pattern == 'WESTERN':\n",
    "            # Western: Given SURNAME\n",
    "            boss_name = name.upper()\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == len(words) - 1:  # Last word is surname\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        elif pattern == 'ASIAN':\n",
    "            # Asian: SURNAME Given Given\n",
    "            boss_name = name.upper()\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == 0:  # First word is surname\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        elif pattern == 'SINGAPOREAN':\n",
    "            # Singaporean: Given SURNAME Given\n",
    "            boss_name = name.upper()\n",
    "            surname_idx = self._find_surname_index(words)\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == surname_idx:\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        else:\n",
    "            # Default fallback\n",
    "            boss_name = name.upper()\n",
    "            afterclass_name = ' '.join(word.capitalize() for word in words)\n",
    "        \n",
    "        return boss_name, afterclass_name\n",
    "\n",
    "    def _detect_name_pattern(self, words: List[str]) -> str:\n",
    "        \"\"\"Detect naming pattern: WESTERN, ASIAN, or SINGAPOREAN\"\"\"\n",
    "        if not words:\n",
    "            return 'UNKNOWN'\n",
    "        \n",
    "        # Check for Western pattern\n",
    "        first_upper = words[0].upper()\n",
    "        if first_upper in self.western_given_names:\n",
    "            return 'WESTERN'\n",
    "        \n",
    "        # Check for pure Asian pattern\n",
    "        if first_upper in self.all_asian_surnames:\n",
    "            # Check if no Western names present\n",
    "            has_western = any(w.upper() in self.western_given_names for w in words)\n",
    "            if not has_western:\n",
    "                return 'ASIAN'\n",
    "        \n",
    "        # Check for Singaporean mixed pattern\n",
    "        if len(words) >= 3:\n",
    "            if (words[0].upper() in self.western_given_names and \n",
    "                any(w.upper() in self.all_asian_surnames for w in words[1:])):\n",
    "                return 'SINGAPOREAN'\n",
    "        \n",
    "        # Default to Western if unclear\n",
    "        return 'WESTERN'\n",
    "\n",
    "    def _find_surname_index(self, words: List[str]) -> int:\n",
    "        \"\"\"Find the index of surname in a list of words\"\"\"\n",
    "        for i, word in enumerate(words):\n",
    "            if word.upper() in self.all_asian_surnames:\n",
    "                return i\n",
    "        # Default to last word if no Asian surname found\n",
    "        return len(words) - 1\n",
    "\n",
    "    def resolve_professor_email(self, professor_name):\n",
    "        \"\"\"Resolve professor email using Outlook contacts\"\"\"\n",
    "        try:\n",
    "            # Initialize Outlook\n",
    "            outlook = win32.Dispatch(\"Outlook.Application\")\n",
    "            namespace = outlook.GetNamespace(\"MAPI\")\n",
    "            \n",
    "            # Try exact resolver first\n",
    "            recipient = namespace.CreateRecipient(professor_name)\n",
    "            if recipient.Resolve():\n",
    "                # Try to get SMTP address\n",
    "                address_entry = recipient.AddressEntry\n",
    "                \n",
    "                # Try Exchange user\n",
    "                try:\n",
    "                    exchange_user = address_entry.GetExchangeUser()\n",
    "                    if exchange_user and exchange_user.PrimarySmtpAddress:\n",
    "                        return exchange_user.PrimarySmtpAddress.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try Exchange distribution list\n",
    "                try:\n",
    "                    exchange_dl = address_entry.GetExchangeDistributionList()\n",
    "                    if exchange_dl and exchange_dl.PrimarySmtpAddress:\n",
    "                        return exchange_dl.PrimarySmtpAddress.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try PR_SMTP_ADDRESS property\n",
    "                try:\n",
    "                    property_accessor = address_entry.PropertyAccessor\n",
    "                    smtp_addr = property_accessor.GetProperty(\"http://schemas.microsoft.com/mapi/proptag/0x39FE001E\")\n",
    "                    if smtp_addr:\n",
    "                        return smtp_addr.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Fallback: regex search in Address field\n",
    "                try:\n",
    "                    address = getattr(address_entry, \"Address\", \"\") or \"\"\n",
    "                    match = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", address)\n",
    "                    if match:\n",
    "                        return match.group(0).lower()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # If exact resolve fails, try contacts search\n",
    "            contacts_folder = namespace.GetDefaultFolder(10)  # olFolderContacts\n",
    "            tokens = [t.lower() for t in professor_name.split() if t]\n",
    "            \n",
    "            for item in contacts_folder.Items:\n",
    "                try:\n",
    "                    full_name = (item.FullName or \"\").lower()\n",
    "                    if all(token in full_name for token in tokens):\n",
    "                        # Try the three standard email slots\n",
    "                        for field in (\"Email1Address\", \"Email2Address\", \"Email3Address\"):\n",
    "                            addr = getattr(item, field, \"\") or \"\"\n",
    "                            if addr and \"@\" in addr:\n",
    "                                return addr.lower()\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # If no email found, return default\n",
    "            return 'enquiry@smu.edu.sg'\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Email resolution failed for {professor_name}: {e}\")\n",
    "            return 'enquiry@smu.edu.sg'\n",
    "\n",
    "    def process_professors(self):\n",
    "        \"\"\"Process professors from multiple sheet with email resolution\"\"\"\n",
    "        logger.info(\"üë• Processing professors...\")\n",
    "        \n",
    "        unique_professors = set()\n",
    "        \n",
    "        # Extract unique professor names from multiple sheet\n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            if pd.notna(row.get('professor_name')):\n",
    "                prof_name = str(row['professor_name']).strip()\n",
    "                if prof_name and prof_name.upper() not in ['TBA', 'TO BE ANNOUNCED']:\n",
    "                    # Handle comma-separated names properly during extraction\n",
    "                    comma_count = prof_name.count(',')\n",
    "                    if comma_count == 1:\n",
    "                        parts = prof_name.split(',', 1)\n",
    "                        before_comma = parts[0].strip()\n",
    "                        if len(before_comma.split()) == 1:  # Single-word surname\n",
    "                            prof_name = f\"{before_comma} {parts[1].strip()}\"  # \"SURNAME Given\"\n",
    "                        else:  # Multi-word before comma = multi-instructor\n",
    "                            prof_name = before_comma  # Take first instructor only\n",
    "                    elif comma_count > 1:\n",
    "                        prof_name = prof_name.split(',', 1)[0].strip()  # First instructor\n",
    "                    else:\n",
    "                        prof_name = prof_name  # No commas\n",
    "                    unique_professors.add(prof_name)\n",
    "        \n",
    "        # Process each unique professor\n",
    "        for prof_name in unique_professors:\n",
    "            boss_name, afterclass_name = self.normalize_professor_name(prof_name)\n",
    "            \n",
    "            # Check if professor exists in lookup or cache\n",
    "            if prof_name in self.professor_lookup:\n",
    "                continue\n",
    "            \n",
    "            # Check cache by normalized name\n",
    "            if boss_name in self.professors_cache or afterclass_name.upper() in self.professors_cache:\n",
    "                continue\n",
    "            \n",
    "            # NEW: Add substring matching logic here\n",
    "            duplicate_found = False\n",
    "            \n",
    "            # Check against existing professor_lookup\n",
    "            for existing_scraped_name, prof_data in self.professor_lookup.items():\n",
    "                existing_boss = prof_data.get('boss_name', '')\n",
    "                existing_afterclass = prof_data.get('afterclass_name', '')\n",
    "                \n",
    "                # Check substring matches\n",
    "                if (prof_name.upper() in existing_scraped_name.upper() or \n",
    "                    existing_scraped_name.upper() in prof_name.upper() or\n",
    "                    boss_name.upper() in existing_boss.upper() or\n",
    "                    existing_boss.upper() in boss_name.upper() or\n",
    "                    afterclass_name.upper() in existing_afterclass.upper() or\n",
    "                    existing_afterclass.upper() in afterclass_name.upper()):\n",
    "                    \n",
    "                    # Update lookup to include this variation\n",
    "                    self.professor_lookup[prof_name] = prof_data.copy()\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "            \n",
    "            if duplicate_found:\n",
    "                continue\n",
    "            \n",
    "            # Check against professors_cache\n",
    "            for cached_name, cached_prof in self.professors_cache.items():\n",
    "                cached_boss = cached_prof.get('name', '').upper()\n",
    "                \n",
    "                # Check substring matches with cache\n",
    "                if (prof_name.upper() in cached_name.upper() or \n",
    "                    cached_name.upper() in prof_name.upper() or\n",
    "                    boss_name.upper() in cached_boss or\n",
    "                    cached_boss in boss_name.upper()):\n",
    "                    \n",
    "                    # Update lookup to point to existing professor\n",
    "                    self.professor_lookup[prof_name] = {\n",
    "                        'database_id': cached_prof['id'],\n",
    "                        'boss_name': cached_boss,\n",
    "                        'afterclass_name': cached_prof.get('name', afterclass_name)\n",
    "                    }\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "            \n",
    "            if duplicate_found:\n",
    "                continue\n",
    "            \n",
    "            # Check against new_professors being created in this run\n",
    "            for new_prof in self.new_professors:\n",
    "                new_original = new_prof.get('original_scraped_name', '')\n",
    "                new_boss = new_prof.get('boss_name', '')\n",
    "                new_afterclass = new_prof.get('afterclass_name', '')\n",
    "                \n",
    "                # Check substring matches with new professors\n",
    "                if (prof_name.upper() in new_original.upper() or \n",
    "                    new_original.upper() in prof_name.upper() or\n",
    "                    boss_name.upper() in new_boss.upper() or\n",
    "                    new_boss.upper() in boss_name.upper() or\n",
    "                    afterclass_name.upper() in new_afterclass.upper() or\n",
    "                    new_afterclass.upper() in afterclass_name.upper()):\n",
    "                    \n",
    "                    # Update lookup to point to the new professor\n",
    "                    self.professor_lookup[prof_name] = {\n",
    "                        'database_id': new_prof['id'],\n",
    "                        'boss_name': new_boss,\n",
    "                        'afterclass_name': new_afterclass\n",
    "                    }\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "            \n",
    "            if duplicate_found:\n",
    "                continue\n",
    "            \n",
    "            # Create new professor with email resolution\n",
    "            professor_id = str(uuid.uuid4())\n",
    "            slug = re.sub(r'[^a-zA-Z0-9]+', '-', afterclass_name.lower()).strip('-')\n",
    "            \n",
    "            # FIXED: Resolve email using Outlook\n",
    "            resolved_email = self.resolve_professor_email(afterclass_name)\n",
    "            \n",
    "            new_prof = {\n",
    "                'id': professor_id,\n",
    "                'name': afterclass_name,\n",
    "                'email': resolved_email,  # FIXED: Use resolved email instead of default\n",
    "                'slug': slug,\n",
    "                'photo_url': 'https://smu.edu.sg',\n",
    "                'profile_url': 'https://smu.edu.sg',\n",
    "                'belong_to_university': 1,  # SMU\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'updated_at': datetime.now().isoformat(),\n",
    "                'boss_name': boss_name,\n",
    "                'afterclass_name': afterclass_name,\n",
    "                'original_scraped_name': prof_name\n",
    "            }\n",
    "            \n",
    "            self.new_professors.append(new_prof)\n",
    "            self.stats['professors_created'] += 1\n",
    "            \n",
    "            # Update lookup\n",
    "            self.professor_lookup[prof_name] = {\n",
    "                'database_id': professor_id,\n",
    "                'boss_name': boss_name,\n",
    "                'afterclass_name': afterclass_name\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"‚úÖ Created professor: {afterclass_name} with email: {resolved_email}\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created {self.stats['professors_created']} new professors\")\n",
    "\n",
    "    def process_courses(self):\n",
    "        \"\"\"Process courses from standalone sheet WITHOUT prompting for faculty\"\"\"\n",
    "        logger.info(\"üìö Processing courses...\")\n",
    "        \n",
    "        # Group by course code to handle duplicates\n",
    "        course_groups = defaultdict(list)\n",
    "        for _, row in self.standalone_data.iterrows():\n",
    "            if pd.notna(row.get('course_code')):\n",
    "                course_groups[row['course_code']].append(row)\n",
    "        \n",
    "        for course_code, rows in course_groups.items():\n",
    "            # Helper function to get sortable key for academic term ordering\n",
    "            def get_sort_key(row):\n",
    "                year_start = row.get('acad_year_start', 0)\n",
    "                year_end = row.get('acad_year_end', 0)\n",
    "                term = str(row.get('term', ''))\n",
    "                \n",
    "                # Convert term to sortable format\n",
    "                term_order = {\n",
    "                    'T1': 1,\n",
    "                    'T2': 2,\n",
    "                    'T3A': 3.1,\n",
    "                    'T3B': 3.2\n",
    "                }\n",
    "                term_value = term_order.get(term.upper(), 0)\n",
    "                return (year_start, year_end, term_value)\n",
    "            \n",
    "            # Sort rows to get the latest one (highest year and term)\n",
    "            sorted_rows = sorted(rows, key=get_sort_key, reverse=True)\n",
    "            latest_row = sorted_rows[0]\n",
    "            \n",
    "            # Check if course exists in cache\n",
    "            if course_code in self.courses_cache:\n",
    "                # Course exists - check for updates\n",
    "                existing = self.courses_cache[course_code]\n",
    "                update_needed = False\n",
    "                update_record = {'id': existing['id'], 'code': course_code}\n",
    "                \n",
    "                # Fields that need comparison for changes\n",
    "                comparison_fields = ['name', 'description', 'credit_units']\n",
    "                \n",
    "                # Fields that always need updating (even if null/empty in existing)\n",
    "                always_update_fields = ['course_area', 'enrolment_requirements']\n",
    "                \n",
    "                # Field mapping from raw data to database columns\n",
    "                field_mapping = {\n",
    "                    'name': 'course_name',\n",
    "                    'description': 'course_description',\n",
    "                    'credit_units': 'credit_units'\n",
    "                }\n",
    "                \n",
    "                # Check comparison fields for changes\n",
    "                for field in comparison_fields:\n",
    "                    raw_field = field_mapping.get(field, field)\n",
    "                    new_value = latest_row.get(raw_field)\n",
    "                    old_value = existing.get(field)\n",
    "                    \n",
    "                    # Convert credit_units to float for proper comparison\n",
    "                    if field == 'credit_units':\n",
    "                        new_value = float(new_value) if pd.notna(new_value) else None\n",
    "                        old_value = float(old_value) if pd.notna(old_value) else None\n",
    "                    \n",
    "                    # Only update if new value exists and differs from old\n",
    "                    if pd.notna(new_value) and new_value != old_value:\n",
    "                        update_record[field] = new_value\n",
    "                        update_needed = True\n",
    "                \n",
    "                # Always update course_area and enrolment_requirements if they have values\n",
    "                for field in always_update_fields:\n",
    "                    new_value = latest_row.get(field)\n",
    "                    if pd.notna(new_value):\n",
    "                        # Always add these fields to update, even if unchanged\n",
    "                        update_record[field] = new_value\n",
    "                        update_needed = True\n",
    "                    elif existing.get(field) is None:\n",
    "                        # If existing has no value and new has no value, no update needed\n",
    "                        pass\n",
    "                    else:\n",
    "                        # If existing has value but new doesn't, keep existing (don't overwrite with null)\n",
    "                        pass\n",
    "                \n",
    "                if update_needed:\n",
    "                    self.update_courses.append(update_record)\n",
    "                    self.stats['courses_updated'] += 1\n",
    "                    \n",
    "                    # Update cache with new values\n",
    "                    for field, value in update_record.items():\n",
    "                        if field != 'id' and field != 'code':\n",
    "                            self.courses_cache[course_code][field] = value\n",
    "            else:\n",
    "                # Create new course WITHOUT faculty assignment\n",
    "                course_id = str(uuid.uuid4())\n",
    "                \n",
    "                new_course = {\n",
    "                    'id': course_id,\n",
    "                    'code': course_code,\n",
    "                    'name': latest_row.get('course_name', 'Unknown Course'),\n",
    "                    'description': latest_row.get('course_description', 'No description available'),\n",
    "                    'credit_units': float(latest_row.get('credit_units', 1.0)) if pd.notna(latest_row.get('credit_units')) else 1.0,\n",
    "                    'belong_to_university': 1,  # SMU\n",
    "                    'belong_to_faculty': None,  # Will be assigned later\n",
    "                    'course_area': latest_row.get('course_area'),\n",
    "                    'enrolment_requirements': latest_row.get('enrolment_requirements')\n",
    "                }\n",
    "                \n",
    "                self.new_courses.append(new_course)\n",
    "                self.stats['courses_created'] += 1\n",
    "                \n",
    "                # Store course info for later faculty assignment\n",
    "                self.courses_needing_faculty.append({\n",
    "                    'course_id': course_id,\n",
    "                    'course_code': course_code,\n",
    "                    'course_name': latest_row.get('course_name', 'Unknown Course'),\n",
    "                    'course_outline_url': latest_row.get('course_outline_url')\n",
    "                })\n",
    "                self.stats['courses_needing_faculty'] += 1\n",
    "                \n",
    "                # Update cache\n",
    "                self.courses_cache[course_code] = new_course\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created {self.stats['courses_created']} new courses\")\n",
    "        logger.info(f\"‚úÖ Updated {self.stats['courses_updated']} existing courses\")\n",
    "        logger.info(f\"‚ö†Ô∏è  {self.stats['courses_needing_faculty']} courses need faculty assignment\")\n",
    "\n",
    "    def assign_course_faculties(self):\n",
    "        \"\"\"Separate method to handle faculty assignments for courses\"\"\"\n",
    "        if not self.courses_needing_faculty:\n",
    "            logger.info(\"‚úÖ No courses need faculty assignment\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"üéì Starting faculty assignment for {len(self.courses_needing_faculty)} courses\")\n",
    "        \n",
    "        faculty_assignments = []\n",
    "        \n",
    "        for course_info in self.courses_needing_faculty:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üéì FACULTY ASSIGNMENT NEEDED\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Course Code: {course_info['course_code']}\")\n",
    "            print(f\"Course Name: {course_info['course_name']}\")\n",
    "            \n",
    "            # Open course outline if available\n",
    "            if pd.notna(course_info.get('course_outline_url')):\n",
    "                url = course_info['course_outline_url']\n",
    "                print(f\"Opening course outline: {url}\")\n",
    "                webbrowser.open(url)\n",
    "            \n",
    "            print(\"\\nFaculty Options:\")\n",
    "            print(\"1. Lee Kong Chian School of Business\")\n",
    "            print(\"2. Yong Pung How School of Law\")\n",
    "            print(\"3. School of Economics\")\n",
    "            print(\"4. School of Computing and Information Systems\")\n",
    "            print(\"5. School of Social Sciences\")\n",
    "            print(\"6. School of Accountancy\")\n",
    "            print(\"7. College of Integrative Studies\")\n",
    "            print(\"8. Center for English Communication\")\n",
    "            print(\"0. Skip (will need manual review)\")\n",
    "            \n",
    "            while True:\n",
    "                choice = input(\"\\nEnter faculty number (0-8): \").strip()\n",
    "                if choice == '0':\n",
    "                    faculty_id = None\n",
    "                    break\n",
    "                elif choice in ['1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "                    faculty_id = int(choice)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid choice. Please enter 0-8.\")\n",
    "            \n",
    "            # Store assignment\n",
    "            faculty_assignments.append({\n",
    "                'course_id': course_info['course_id'],\n",
    "                'course_code': course_info['course_code'],\n",
    "                'faculty_id': faculty_id\n",
    "            })\n",
    "        \n",
    "        # Update the new_courses list with faculty assignments\n",
    "        for assignment in faculty_assignments:\n",
    "            if assignment['faculty_id'] is not None:\n",
    "                # Find and update the course in new_courses\n",
    "                for course in self.new_courses:\n",
    "                    if course['id'] == assignment['course_id']:\n",
    "                        course['belong_to_faculty'] = assignment['faculty_id']\n",
    "                        break\n",
    "                \n",
    "                # Update cache\n",
    "                if assignment['course_code'] in self.courses_cache:\n",
    "                    self.courses_cache[assignment['course_code']]['belong_to_faculty'] = assignment['faculty_id']\n",
    "        \n",
    "        # Re-save the new_courses.csv with faculty assignments\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Updated new_courses.csv with faculty assignments\")\n",
    "        \n",
    "        logger.info(\"‚úÖ Faculty assignment completed\")\n",
    "\n",
    "    def process_acad_terms(self):\n",
    "        \"\"\"Process academic terms from standalone sheet\"\"\"\n",
    "        logger.info(\"üìÖ Processing academic terms...\")\n",
    "        \n",
    "        # Group by (acad_year_start, acad_year_end, term)\n",
    "        term_groups = defaultdict(list)\n",
    "        for _, row in self.standalone_data.iterrows():\n",
    "            key = (\n",
    "                row.get('acad_year_start'),\n",
    "                row.get('acad_year_end'),\n",
    "                row.get('term')\n",
    "            )\n",
    "            if all(pd.notna(v) for v in key):\n",
    "                term_groups[key].append(row)\n",
    "        \n",
    "        for (year_start, year_end, term), rows in term_groups.items():\n",
    "            # Generate acad_term_id (keep T for ID)\n",
    "            acad_term_id = f\"AY{int(year_start)}{int(year_end) % 100:02d}{term}\"\n",
    "            \n",
    "            # Check if already exists\n",
    "            if acad_term_id in self.acad_term_cache:\n",
    "                continue\n",
    "            \n",
    "            # Find most common period_text and dates\n",
    "            period_counter = Counter()\n",
    "            date_info = {}\n",
    "            \n",
    "            for row in rows:\n",
    "                period_text = row.get('period_text', '')\n",
    "                if pd.notna(period_text):\n",
    "                    period_counter[period_text] += 1\n",
    "                    if period_text not in date_info:\n",
    "                        date_info[period_text] = {\n",
    "                            'start_dt': row.get('start_dt'),\n",
    "                            'end_dt': row.get('end_dt')\n",
    "                        }\n",
    "            \n",
    "            # Get most common period\n",
    "            if period_counter:\n",
    "                most_common_period = period_counter.most_common(1)[0][0]\n",
    "                dates = date_info[most_common_period]\n",
    "            else:\n",
    "                dates = {'start_dt': None, 'end_dt': None}\n",
    "            \n",
    "            # Get boss_id from first row\n",
    "            boss_id = rows[0].get('acad_term_boss_id')\n",
    "            \n",
    "            # FIXED: Remove T prefix from term field for database storage\n",
    "            clean_term = str(term)[1:] if str(term).startswith('T') else str(term)\n",
    "            \n",
    "            new_term = {\n",
    "                'id': acad_term_id,\n",
    "                'acad_year_start': int(year_start),\n",
    "                'acad_year_end': int(year_end),\n",
    "                'term': clean_term,  # FIXED: Store without T prefix (1, 2, 3A, 3B)\n",
    "                'boss_id': int(boss_id) if pd.notna(boss_id) else None,\n",
    "                'start_dt': dates['start_dt'],\n",
    "                'end_dt': dates['end_dt']\n",
    "            }\n",
    "            \n",
    "            self.new_acad_terms.append(new_term)\n",
    "            self.acad_term_cache[acad_term_id] = new_term\n",
    "            \n",
    "            logger.info(f\"‚úÖ Created academic term: {acad_term_id} (term: {clean_term})\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created {len(self.new_acad_terms)} new academic terms\")\n",
    "\n",
    "    def process_classes(self):\n",
    "        \"\"\"Process classes from standalone sheet\"\"\"\n",
    "        logger.info(\"üè´ Processing classes...\")\n",
    "        \n",
    "        try:\n",
    "            for _, row in self.standalone_data.iterrows():\n",
    "                record_key = row.get('record_key')\n",
    "                if pd.notna(record_key):\n",
    "                    # Use optimized professor lookup\n",
    "                    professor_id = self._find_professor_for_class(record_key)\n",
    "                    # Generate class ID\n",
    "                    class_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    # Extract boss_id from record_key\n",
    "                    record_key = row.get('record_key', '')\n",
    "                    boss_id_match = re.search(r'SelectedClassNumber=(\\d+)', record_key)\n",
    "                    boss_id = int(boss_id_match.group(1)) if boss_id_match else None\n",
    "                    \n",
    "                    # Get course_id\n",
    "                    course_code = row.get('course_code')\n",
    "                    course_id = None\n",
    "                    if course_code and course_code in self.courses_cache:\n",
    "                        course_id = self.courses_cache[course_code]['id']\n",
    "                    \n",
    "                    # Get professor_id from multiple sheet\n",
    "                    professor_id = self._find_professor_for_class(record_key)\n",
    "                    \n",
    "                    new_class = {\n",
    "                        'id': class_id,\n",
    "                        'section': row.get('section', ''),\n",
    "                        'course_id': course_id,\n",
    "                        'professor_id': professor_id,\n",
    "                        'acad_term_id': row.get('acad_term_id'),\n",
    "                        'grading_basis': row.get('grading_basis'),\n",
    "                        'course_outline_url': row.get('course_outline_url'),\n",
    "                        'boss_id': boss_id\n",
    "                    }\n",
    "                    \n",
    "                    self.new_classes.append(new_class)\n",
    "                    self.stats['classes_created'] += 1\n",
    "                    \n",
    "                    # Store mapping for timing tables\n",
    "                    self.class_id_mapping[record_key] = class_id\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing classes: {e}\")\n",
    "            raise\n",
    "        logger.info(f\"‚úÖ Created {self.stats['classes_created']} new classes\")\n",
    "\n",
    "    def _find_professor_for_class(self, record_key: str) -> Optional[str]:\n",
    "        \"\"\"Optimised: Find professor ID for a class using pre-indexed multiple_lookup\"\"\"\n",
    "        rows = self.multiple_lookup.get(record_key, [])\n",
    "        for row in rows:\n",
    "            if pd.notna(row.get('professor_name')):\n",
    "                original_prof_name = str(row['professor_name']).strip()\n",
    "\n",
    "                # Step 1: Try full string match first\n",
    "                if original_prof_name in self.professor_lookup:\n",
    "                    return self.professor_lookup[original_prof_name]['database_id']\n",
    "\n",
    "                # Step 2: Parse name by commas\n",
    "                comma_count = original_prof_name.count(',')\n",
    "                if comma_count == 1:\n",
    "                    # One comma - check substring before comma\n",
    "                    parts = original_prof_name.split(',')\n",
    "                    before_comma = parts[0].strip()\n",
    "                    words_before_comma = before_comma.split()\n",
    "                    \n",
    "                    if len(words_before_comma) == 1:\n",
    "                        # Exactly one word before comma - single professor in \"SURNAME, FirstName\" format\n",
    "                        cleaned_name = original_prof_name  # Use full original string\n",
    "                    else:\n",
    "                        # More than one word before comma - multiple professors\n",
    "                        cleaned_name = before_comma  # Use only part before comma\n",
    "                elif comma_count >= 2:\n",
    "                    # Two or more commas - definitely multiple professors\n",
    "                    cleaned_name = original_prof_name.split(',')[0].strip()\n",
    "                else:\n",
    "                    # No commas - single professor\n",
    "                    cleaned_name = original_prof_name\n",
    "\n",
    "                # Step 3: Try cleaned name match\n",
    "                if cleaned_name in self.professor_lookup:\n",
    "                    return self.professor_lookup[cleaned_name]['database_id']\n",
    "\n",
    "                # NEW: Step 3.5: Try substring matching\n",
    "                # Check if any existing professor names are substrings of the original name\n",
    "                # or if the original name is a substring of existing names\n",
    "                for existing_name, prof_data in self.professor_lookup.items():\n",
    "                    # Check if existing name is in the original name\n",
    "                    if existing_name.upper() in original_prof_name.upper():\n",
    "                        return prof_data['database_id']\n",
    "                    # Check if original name is in existing name  \n",
    "                    if original_prof_name.upper() in existing_name.upper():\n",
    "                        return prof_data['database_id']\n",
    "                \n",
    "                # Also check against new professors being created\n",
    "                for new_prof in self.new_professors:\n",
    "                    new_prof_name = new_prof.get('original_scraped_name', '')\n",
    "                    boss_name = new_prof.get('boss_name', '')\n",
    "                    afterclass_name = new_prof.get('afterclass_name', '')\n",
    "                    \n",
    "                    # Check substring matches against various name formats\n",
    "                    names_to_check = [new_prof_name, boss_name, afterclass_name]\n",
    "                    for name in names_to_check:\n",
    "                        if name and (name.upper() in original_prof_name.upper() or \n",
    "                                    original_prof_name.upper() in name.upper()):\n",
    "                            return new_prof['id']\n",
    "\n",
    "                # Step 4: Use normalisation fallback\n",
    "                boss_name, afterclass_name = self.normalize_professor_name(cleaned_name)\n",
    "                if boss_name in self.professors_cache:\n",
    "                    return self.professors_cache[boss_name]['id']\n",
    "                if afterclass_name.upper() in self.professors_cache:\n",
    "                    return self.professors_cache[afterclass_name.upper()]['id']\n",
    "        return None\n",
    "\n",
    "    def process_timings(self):\n",
    "        \"\"\"Process class timings and exam timings from multiple sheet\"\"\"\n",
    "        logger.info(\"‚è∞ Processing class timings and exam timings...\")\n",
    "        \n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            record_key = row.get('record_key')\n",
    "            if record_key not in self.class_id_mapping:\n",
    "                continue\n",
    "            \n",
    "            class_id = self.class_id_mapping[record_key]\n",
    "            timing_type = row.get('type', 'CLASS')\n",
    "            \n",
    "            if timing_type == 'CLASS':\n",
    "                timing_record = {\n",
    "                    'class_id': class_id,\n",
    "                    'start_date': row.get('start_date'),\n",
    "                    'end_date': row.get('end_date'),\n",
    "                    'day_of_week': row.get('day_of_week'),\n",
    "                    'start_time': row.get('start_time'),\n",
    "                    'end_time': row.get('end_time'),\n",
    "                    'venue': row.get('venue', '')\n",
    "                }\n",
    "                self.new_class_timings.append(timing_record)\n",
    "                self.stats['timings_created'] += 1\n",
    "            \n",
    "            elif timing_type == 'EXAM':\n",
    "                exam_record = {\n",
    "                    'class_id': class_id,\n",
    "                    'date': row.get('date'),\n",
    "                    'day_of_week': row.get('day_of_week'),\n",
    "                    'start_time': row.get('start_time'),\n",
    "                    'end_time': row.get('end_time'),\n",
    "                    'venue': row.get('venue')\n",
    "                }\n",
    "                self.new_class_exam_timings.append(exam_record)\n",
    "                self.stats['exams_created'] += 1\n",
    "        \n",
    "        logger.info(f\"‚úÖ Created {self.stats['timings_created']} class timings\")\n",
    "        logger.info(f\"‚úÖ Created {self.stats['exams_created']} exam timings\")\n",
    "        \n",
    "    def save_outputs(self):\n",
    "        \"\"\"Save all generated CSV files\"\"\"\n",
    "        logger.info(\"üíæ Saving output files...\")\n",
    "        \n",
    "        # In Phase 2, professors have already been saved and corrected\n",
    "        # Only save if we're in Phase 1 or if there are new professors to save\n",
    "        if self.new_professors and not hasattr(self, '_phase2_mode'):\n",
    "            df = pd.DataFrame(self.new_professors)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_professors.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_professors)} new professors\")\n",
    "        \n",
    "        # Save new courses (to verify folder)\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_courses)} new courses\")\n",
    "        \n",
    "        # Save course updates\n",
    "        if self.update_courses:\n",
    "            df = pd.DataFrame(self.update_courses)\n",
    "            df.to_csv(os.path.join(self.output_base, 'update_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.update_courses)} course updates\")\n",
    "        \n",
    "        # Save academic terms\n",
    "        if self.new_acad_terms:\n",
    "            df = pd.DataFrame(self.new_acad_terms)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_acad_term.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_acad_terms)} academic terms\")\n",
    "        \n",
    "        # Save classes\n",
    "        if self.new_classes:\n",
    "            df = pd.DataFrame(self.new_classes)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_classes.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_classes)} classes\")\n",
    "        \n",
    "        # Save class timings\n",
    "        if self.new_class_timings:\n",
    "            df = pd.DataFrame(self.new_class_timings)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_class_timing.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_class_timings)} class timings\")\n",
    "        \n",
    "        # Save exam timings\n",
    "        if self.new_class_exam_timings:\n",
    "            df = pd.DataFrame(self.new_class_exam_timings)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_class_exam_timing.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_class_exam_timings)} exam timings\")\n",
    "        \n",
    "        # Save courses needing faculty assignment\n",
    "        if self.courses_needing_faculty:\n",
    "            df = pd.DataFrame(self.courses_needing_faculty)\n",
    "            df.to_csv(os.path.join(self.output_base, 'courses_needing_faculty.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.courses_needing_faculty)} courses needing faculty assignment\")\n",
    "        \n",
    "        # Create placeholder files only if they don't exist\n",
    "        placeholders = ['new_bid_window.csv', 'new_class_availability.csv', 'new_bid_result.csv']\n",
    "        for filename in placeholders:\n",
    "            filepath = os.path.join(self.output_base, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                df = pd.DataFrame()\n",
    "                df.to_csv(filepath, index=False)\n",
    "                logger.info(f\"‚úÖ Created placeholder: {filename}\")\n",
    "\n",
    "    def _save_professor_lookup(self):\n",
    "        \"\"\"Save updated professor lookup table\"\"\"\n",
    "        lookup_data = []\n",
    "        \n",
    "        # Add all professors from lookup\n",
    "        for scraped_name, data in self.professor_lookup.items():\n",
    "            lookup_data.append({\n",
    "                'boss_name': data.get('boss_name', scraped_name.upper()),\n",
    "                'afterclass_name': data.get('afterclass_name', scraped_name),\n",
    "                'database_id': data['database_id'],\n",
    "                'method': 'exists' if scraped_name not in [p['original_scraped_name'] for p in self.new_professors] else 'created'\n",
    "            })\n",
    "        \n",
    "        # Sort by scraped_name\n",
    "        lookup_data.sort(key=lambda x: x['scraped_name'])\n",
    "        \n",
    "        # Save to output folder\n",
    "        df = pd.DataFrame(lookup_data)\n",
    "        df.to_csv(os.path.join(self.output_base, 'professor_lookup.csv'), index=False)\n",
    "        logger.info(f\"‚úÖ Saved updated professor lookup with {len(lookup_data)} entries\")\n",
    "\n",
    "    def update_professor_lookup_from_corrected_csv(self):\n",
    "        \"\"\"Update professor lookup from manually corrected new_professors.csv\"\"\"\n",
    "        logger.info(\"üîÑ Updating professor lookup from corrected CSV...\")\n",
    "        \n",
    "        # Read corrected new_professors.csv\n",
    "        corrected_csv_path = os.path.join(self.verify_dir, 'new_professors.csv')\n",
    "        if not os.path.exists(corrected_csv_path):\n",
    "            logger.error(f\"‚ùå Corrected CSV not found: {corrected_csv_path}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            corrected_df = pd.read_csv(corrected_csv_path)\n",
    "            logger.info(f\"üìñ Reading {len(corrected_df)} corrected professor records\")\n",
    "            \n",
    "            # Clear and rebuild the new_professors list with corrected data\n",
    "            self.new_professors = []\n",
    "            \n",
    "            # Update internal professor_lookup and rebuild new_professors\n",
    "            updated_count = 0\n",
    "            for _, row in corrected_df.iterrows():\n",
    "                original_name = row.get('original_scraped_name', '')\n",
    "                corrected_afterclass_name = row.get('name', '')  # This is the corrected name\n",
    "                boss_name = row.get('boss_name', '')  # Keep boss name same\n",
    "                professor_id = row.get('id', '')\n",
    "                \n",
    "                # Rebuild the professor record with corrected data\n",
    "                corrected_prof = {\n",
    "                    'id': professor_id,\n",
    "                    'name': corrected_afterclass_name,  # Use corrected name\n",
    "                    'email': row.get('email', 'enquiry@smu.edu.sg'),\n",
    "                    'slug': row.get('slug', ''),\n",
    "                    'photo_url': row.get('photo_url', 'https://smu.edu.sg'),\n",
    "                    'profile_url': row.get('profile_url', 'https://smu.edu.sg'),\n",
    "                    'belong_to_university': row.get('belong_to_university', 1),\n",
    "                    'created_at': row.get('created_at', datetime.now().isoformat()),\n",
    "                    'updated_at': row.get('updated_at', datetime.now().isoformat()),\n",
    "                    'boss_name': boss_name,\n",
    "                    'afterclass_name': corrected_afterclass_name,\n",
    "                    'original_scraped_name': original_name\n",
    "                }\n",
    "                \n",
    "                # Add to new_professors list\n",
    "                self.new_professors.append(corrected_prof)\n",
    "                \n",
    "                if original_name and professor_id:\n",
    "                    # Update lookup with corrected afterclass name but same boss name\n",
    "                    self.professor_lookup[original_name] = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,  # Keep original boss name\n",
    "                        'afterclass_name': corrected_afterclass_name  # Use corrected name\n",
    "                    }\n",
    "                    updated_count += 1\n",
    "                    \n",
    "                    # Also add the corrected name as a lookup key\n",
    "                    self.professor_lookup[corrected_afterclass_name] = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': corrected_afterclass_name\n",
    "                    }\n",
    "                    \n",
    "                    # Add boss name as lookup key too\n",
    "                    self.professor_lookup[boss_name] = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': corrected_afterclass_name\n",
    "                    }\n",
    "            \n",
    "            # Save updated professor lookup to CSV\n",
    "            self._save_corrected_professor_lookup()\n",
    "            \n",
    "            logger.info(f\"‚úÖ Updated {updated_count} professor lookup entries\")\n",
    "            logger.info(f\"‚úÖ Rebuilt {len(self.new_professors)} professor records with corrections\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to update professor lookup: {e}\")\n",
    "            return False\n",
    "\n",
    "    def process_remaining_tables(self):\n",
    "        \"\"\"Process classes and timings after professor lookup is updated\"\"\"\n",
    "        logger.info(\"üè´ Processing remaining tables (classes, timings)...\")\n",
    "        \n",
    "        try:\n",
    "            # Clear any existing data from Phase 1 to avoid duplicates\n",
    "            self.new_classes = []\n",
    "            self.new_class_timings = []\n",
    "            self.new_class_exam_timings = []\n",
    "            self.class_id_mapping = {}\n",
    "            self.stats['classes_created'] = 0\n",
    "            self.stats['timings_created'] = 0\n",
    "            self.stats['exams_created'] = 0\n",
    "            \n",
    "            # Process classes (depends on updated professor lookup)\n",
    "            self.process_classes()\n",
    "            \n",
    "            # Process timings (depends on classes)\n",
    "            self.process_timings()\n",
    "            \n",
    "            logger.info(\"‚úÖ Remaining tables processed successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to process remaining tables: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_corrected_professor_lookup(self):\n",
    "        \"\"\"Save professor lookup with corrected names and proper method tracking\"\"\"\n",
    "        lookup_data = []\n",
    "        \n",
    "        # Load existing professor lookup if it exists\n",
    "        existing_lookup_path = os.path.join(self.output_base, 'professor_lookup.csv')\n",
    "        existing_scraped_names = set()\n",
    "        \n",
    "        if os.path.exists(existing_lookup_path):\n",
    "            existing_df = pd.read_csv(existing_lookup_path)\n",
    "            for _, row in existing_df.iterrows():\n",
    "                lookup_data.append({\n",
    "                    'scraped_name': row.get('scraped_name', ''),\n",
    "                    'boss_name': row.get('boss_name', ''),\n",
    "                    'afterclass_name': row.get('afterclass_name', ''),\n",
    "                    'database_id': row.get('database_id', ''),\n",
    "                    'method': row.get('method', 'exists')  # Keep existing method\n",
    "                })\n",
    "                existing_scraped_names.add(row.get('scraped_name', ''))\n",
    "        \n",
    "        # Add/update with new professor lookup entries\n",
    "        for scraped_name, data in self.professor_lookup.items():\n",
    "            if scraped_name not in existing_scraped_names:\n",
    "                # Determine if this professor was newly created or already existed\n",
    "                professor_id = data['database_id']\n",
    "                \n",
    "                # Check if this professor was created in this run\n",
    "                method = 'exists'  # Default to exists\n",
    "                if any(prof['id'] == professor_id for prof in self.new_professors):\n",
    "                    method = 'created'\n",
    "                elif professor_id in [prof['id'] for prof in self.professors_cache.values()]:\n",
    "                    method = 'exists'\n",
    "                else:\n",
    "                    method = 'created'  # Fallback for new professors\n",
    "                \n",
    "                lookup_data.append({\n",
    "                    'scraped_name': scraped_name,\n",
    "                    'boss_name': data.get('boss_name', scraped_name.upper()),\n",
    "                    'afterclass_name': data.get('afterclass_name', scraped_name),\n",
    "                    'database_id': data['database_id'],\n",
    "                    'method': method\n",
    "                })\n",
    "            else:\n",
    "                # Update existing entry with corrected afterclass name but keep original method\n",
    "                for item in lookup_data:\n",
    "                    if item['scraped_name'] == scraped_name:\n",
    "                        item['afterclass_name'] = data.get('afterclass_name', scraped_name)\n",
    "                        # Don't change the method for existing entries\n",
    "                        break\n",
    "        \n",
    "        # Sort by scraped_name\n",
    "        lookup_data.sort(key=lambda x: x['scraped_name'])\n",
    "        \n",
    "        # Save to output folder\n",
    "        df = pd.DataFrame(lookup_data)\n",
    "        df.to_csv(os.path.join(self.output_base, 'professor_lookup.csv'), index=False)\n",
    "        logger.info(f\"‚úÖ Saved updated professor lookup with {len(lookup_data)} entries\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"‚úÖ Professors created: {self.stats['professors_created']}\")\n",
    "        print(f\"‚úÖ Courses created: {self.stats['courses_created']}\")\n",
    "        print(f\"‚úÖ Courses updated: {self.stats['courses_updated']}\")\n",
    "        print(f\"‚ö†Ô∏è  Courses needing faculty: {self.stats['courses_needing_faculty']}\")\n",
    "        print(f\"‚úÖ Classes created: {self.stats['classes_created']}\")\n",
    "        print(f\"‚úÖ Class timings created: {self.stats['timings_created']}\")\n",
    "        print(f\"‚úÖ Exam timings created: {self.stats['exams_created']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìÅ OUTPUT FILES:\")\n",
    "        print(f\"   Verify folder: {self.verify_dir}/\")\n",
    "        print(f\"   - new_professors.csv ({self.stats['professors_created']} records)\")\n",
    "        print(f\"   - new_courses.csv ({self.stats['courses_created']} records)\")\n",
    "        print(f\"   Output folder: {self.output_base}/\")\n",
    "        print(f\"   - update_courses.csv ({self.stats['courses_updated']} records)\")\n",
    "        print(f\"   - new_acad_term.csv ({len(self.new_acad_terms)} records)\")\n",
    "        print(f\"   - new_classes.csv ({self.stats['classes_created']} records)\")\n",
    "        print(f\"   - new_class_timing.csv ({self.stats['timings_created']} records)\")\n",
    "        print(f\"   - new_class_exam_timing.csv ({self.stats['exams_created']} records)\")\n",
    "        print(f\"   - professor_lookup.csv (updated)\")\n",
    "        print(f\"   - courses_needing_faculty.csv ({self.stats['courses_needing_faculty']} records)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def run_phase1_professors_and_courses(self):\n",
    "        \"\"\"Phase 1: Process professors and courses with automated faculty mapping\"\"\"\n",
    "        try:\n",
    "            logger.info(\"üöÄ Starting Phase 1: Professors and Courses with Automated Faculty Mapping\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Load data\n",
    "            if not self.load_or_cache_data():\n",
    "                logger.error(\"‚ùå Failed to load database data\")\n",
    "                return False\n",
    "            \n",
    "            if not self.load_raw_data():\n",
    "                logger.error(\"‚ùå Failed to load raw data\")\n",
    "                return False\n",
    "            \n",
    "            # Process professors (CSV only, no lookup update)\n",
    "            self.process_professors()\n",
    "            \n",
    "            # Process courses\n",
    "            self.process_courses()\n",
    "            \n",
    "            # NEW: Automated faculty mapping using BOSS data\n",
    "            logger.info(\"\\nüéì Running automated faculty mapping...\")\n",
    "            try:\n",
    "                self.map_courses_to_faculties_from_boss()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Automated faculty mapping failed: {e}\")\n",
    "                logger.info(\"   Continuing with manual faculty assignment...\")\n",
    "            \n",
    "            # Process academic terms\n",
    "            self.process_acad_terms()\n",
    "            \n",
    "            # Save phase 1 outputs\n",
    "            self._save_phase1_outputs()\n",
    "            \n",
    "            # Print faculty mapping summary\n",
    "            if hasattr(self, 'courses_needing_faculty') and self.courses_needing_faculty:\n",
    "                logger.info(f\"\\nüìã Faculty Assignment Summary:\")\n",
    "                logger.info(f\"   ‚Ä¢ Automated mappings applied to {self.stats['courses_created'] - len(self.courses_needing_faculty)} courses\")\n",
    "                logger.info(f\"   ‚Ä¢ {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "                \n",
    "                # Show which courses need manual review\n",
    "                if len(self.courses_needing_faculty) <= 10:\n",
    "                    logger.info(f\"   Courses needing manual review:\")\n",
    "                    for course_info in self.courses_needing_faculty:\n",
    "                        logger.info(f\"     - {course_info['course_code']}: {course_info['course_name']}\")\n",
    "            \n",
    "            logger.info(\"‚úÖ Phase 1 completed - Review files in verify/ folder\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Phase 1 failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run_phase2_remaining_tables(self):\n",
    "        \"\"\"Phase 2: Process classes and timings after professor correction\"\"\"\n",
    "        try:\n",
    "            logger.info(\"üöÄ Starting Phase 2: Classes and Timings\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Set phase 2 mode to prevent overwriting corrected professors\n",
    "            self._phase2_mode = True\n",
    "            \n",
    "            # Update professor lookup from corrected CSV\n",
    "            if not self.update_professor_lookup_from_corrected_csv():\n",
    "                logger.error(\"‚ùå Failed to update professor lookup\")\n",
    "                return False\n",
    "            \n",
    "            # Process remaining tables\n",
    "            if not self.process_remaining_tables():\n",
    "                logger.error(\"‚ùå Failed to process remaining tables\")\n",
    "                return False\n",
    "            \n",
    "            # Save all outputs\n",
    "            self.save_outputs()\n",
    "            \n",
    "            # Print summary\n",
    "            self.print_summary()\n",
    "            \n",
    "            logger.info(\"‚úÖ Phase 2 completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Phase 2 failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_phase1_outputs(self):\n",
    "        \"\"\"Save Phase 1 outputs (professors, courses, acad_terms)\"\"\"\n",
    "        # Save new professors (to verify folder for manual correction)\n",
    "        if self.new_professors:\n",
    "            df = pd.DataFrame(self.new_professors)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_professors.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_professors)} new professors for review\")\n",
    "        \n",
    "        # Save new courses (to verify folder)\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_courses)} new courses\")\n",
    "        \n",
    "        # Save course updates\n",
    "        if self.update_courses:\n",
    "            df = pd.DataFrame(self.update_courses)\n",
    "            df.to_csv(os.path.join(self.output_base, 'update_courses.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.update_courses)} course updates\")\n",
    "        \n",
    "        # Save academic terms\n",
    "        if self.new_acad_terms:\n",
    "            df = pd.DataFrame(self.new_acad_terms)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_acad_term.csv'), index=False)\n",
    "            logger.info(f\"‚úÖ Saved {len(self.new_acad_terms)} academic terms\")\n",
    "\n",
    "    def run(self, skip_faculty_assignment=True):\n",
    "        \"\"\"Run the complete table building process\n",
    "        \n",
    "        Args:\n",
    "            skip_faculty_assignment: If True, faculty assignment is deferred\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"üöÄ Starting TableBuilder process\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Load or cache database data\n",
    "            if not self.load_or_cache_data():\n",
    "                logger.error(\"‚ùå Failed to load database data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 2: Load raw data\n",
    "            if not self.load_raw_data():\n",
    "                logger.error(\"‚ùå Failed to load raw data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Process tables in dependency order\n",
    "            logger.info(\"\\nüìã Processing tables in dependency order...\")\n",
    "            \n",
    "            # 3.1: Process professors first (no dependencies)\n",
    "            self.process_professors()\n",
    "            \n",
    "            # 3.2: Process courses (without faculty assignment)\n",
    "            self.process_courses()\n",
    "            \n",
    "            # 3.3: Process academic terms (no dependencies)\n",
    "            self.process_acad_terms()\n",
    "            \n",
    "            # 3.4: Process classes (depends on courses, professors, acad_terms)\n",
    "            self.process_classes()\n",
    "            \n",
    "            # 3.5: Process timings (depends on classes)\n",
    "            self.process_timings()\n",
    "            \n",
    "            # Step 4: Save all outputs\n",
    "            self.save_outputs()\n",
    "            \n",
    "            # Step 5: Print summary\n",
    "            self.print_summary()\n",
    "            \n",
    "            if self.stats['courses_needing_faculty'] > 0 and not skip_faculty_assignment:\n",
    "                print(\"\\n‚ö†Ô∏è  FACULTY ASSIGNMENT REQUIRED\")\n",
    "                print(f\"   {self.stats['courses_needing_faculty']} courses need faculty assignment\")\n",
    "                print(\"   Run builder.assign_course_faculties() to complete assignment\")\n",
    "            \n",
    "            logger.info(\"\\n‚úÖ TableBuilder process completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Process failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        finally:\n",
    "            # Clean up database connection\n",
    "            if self.connection:\n",
    "                self.connection.close()\n",
    "                logger.info(\"üîí Database connection closed\")\n",
    "\n",
    "    def setup_boss_processing(self):\n",
    "        \"\"\"Initialize BOSS results processing with logging and caches\"\"\"\n",
    "        # Setup logging for BOSS processing\n",
    "        self.boss_log_file = os.path.join(self.output_base, 'boss_result_log.txt')\n",
    "        \n",
    "        # Create the log file and write header\n",
    "        try:\n",
    "            with open(self.boss_log_file, 'w') as f:\n",
    "                f.write(f\"BOSS Results Processing Log - {datetime.now().isoformat()}\\n\")\n",
    "                f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            print(f\"üìù Log file created: {self.boss_log_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not create log file {self.boss_log_file}: {e}\")\n",
    "            self.boss_log_file = None\n",
    "        \n",
    "        # Initialize existing classes cache\n",
    "        self.existing_classes_cache = []\n",
    "        \n",
    "        # Data storage for BOSS results\n",
    "        self.boss_data = []\n",
    "        self.failed_mappings = []\n",
    "        \n",
    "        # Output collectors\n",
    "        self.new_bid_windows = []\n",
    "        self.new_class_availability = []\n",
    "        self.new_bid_result = []\n",
    "        \n",
    "        # Caches for deduplication\n",
    "        self.bid_window_cache = {}  # (acad_term_id, round, window) -> bid_window_id\n",
    "        self.bid_window_id_counter = 1\n",
    "        \n",
    "        # Statistics\n",
    "        self.boss_stats = {\n",
    "            'files_processed': 0,\n",
    "            'total_rows': 0,\n",
    "            'bid_windows_created': 0,\n",
    "            'class_availability_created': 0,\n",
    "            'bid_results_created': 0,\n",
    "            'failed_mappings': 0\n",
    "        }\n",
    "        \n",
    "        print(\"üîÑ BOSS results processing setup completed\")\n",
    "\n",
    "    def log_boss_activity(self, message, print_to_stdout=True):\n",
    "        \"\"\"Log activity to both file and optionally stdout\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"[{timestamp}] {message}\\n\"\n",
    "        \n",
    "        # Only write to file if boss_log_file exists (after setup_boss_processing is called)\n",
    "        if hasattr(self, 'boss_log_file') and self.boss_log_file:\n",
    "            try:\n",
    "                with open(self.boss_log_file, 'a') as f:\n",
    "                    f.write(log_message)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Warning: Could not write to log file: {e}\")\n",
    "        \n",
    "        if print_to_stdout:\n",
    "            print(f\"üìù {message}\")\n",
    "\n",
    "    def parse_term_to_acad_term_id(self, term_str):\n",
    "        \"\"\"Convert term string to acad_term_id format\n",
    "        \n",
    "        Examples:\n",
    "        \"2021-22 Term 1\" -> \"AY202122T1\"\n",
    "        \"2021-22 Term 3A\" -> \"AY202122T3A\"\n",
    "        \"\"\"\n",
    "        if not term_str or pd.isna(term_str):\n",
    "            return None\n",
    "        \n",
    "        # Clean the string\n",
    "        term_str = str(term_str).strip()\n",
    "        \n",
    "        # Pattern: YYYY-YY Term X[A/B]\n",
    "        pattern = r'(\\d{4})-(\\d{2})\\s+Term\\s+(\\w+)'\n",
    "        match = re.match(pattern, term_str)\n",
    "        \n",
    "        if match:\n",
    "            year_start = match.group(1)\n",
    "            year_end = match.group(2)\n",
    "            term = match.group(3)\n",
    "            return f\"AY{year_start}{year_end}T{term}\"\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_bidding_window(self, bidding_window_str):\n",
    "        \"\"\"Complete parser for bidding window string to extract round and window\n",
    "        \n",
    "        Examples:\n",
    "        \"Round 1 Window 1\" -> (\"1\", 1)\n",
    "        \"Round 1A Window 2\" -> (\"1A\", 2)\n",
    "        \"Round 2A Window 3\" -> (\"2A\", 3)\n",
    "        \"Incoming Exchange Rnd 1C Win 1\" -> (\"1C\", 1)\n",
    "        \"Incoming Freshmen Rnd 1 Win 4\" -> (\"1F\", 4)\n",
    "        \"\"\"\n",
    "        if not bidding_window_str or pd.isna(bidding_window_str):\n",
    "            return None, None\n",
    "        \n",
    "        # Clean the string\n",
    "        bidding_window_str = str(bidding_window_str).strip()\n",
    "        \n",
    "        # Pattern 1: Standard format \"Round X[A/B/C] Window Y\"\n",
    "        pattern1 = r'Round\\s+(\\w+)\\s+Window\\s+(\\d+)'\n",
    "        match1 = re.match(pattern1, bidding_window_str)\n",
    "        if match1:\n",
    "            round_str = match1.group(1)\n",
    "            window_num = int(match1.group(2))\n",
    "            return round_str, window_num\n",
    "        \n",
    "        # Pattern 2: Incoming Exchange format \"Incoming Exchange Rnd X[A/B/C] Win Y\"\n",
    "        # Map to same round but keep distinction if needed\n",
    "        pattern2 = r'Incoming\\s+Exchange\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)'\n",
    "        match2 = re.match(pattern2, bidding_window_str)\n",
    "        if match2:\n",
    "            round_str = match2.group(1)  # Keep original round (1C)\n",
    "            window_num = int(match2.group(2))\n",
    "            return round_str, window_num\n",
    "        \n",
    "        # Pattern 3: Incoming Freshmen format \"Incoming Freshmen Rnd X Win Y\"\n",
    "        # Map Round 1 -> Round 1F for distinction\n",
    "        pattern3 = r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)'\n",
    "        match3 = re.match(pattern3, bidding_window_str)\n",
    "        if match3:\n",
    "            original_round = match3.group(1)\n",
    "            window_num = int(match3.group(2))\n",
    "            # Map Incoming Freshmen Round 1 to Round 1F\n",
    "            if original_round == \"1\":\n",
    "                round_str = \"1F\"\n",
    "            else:\n",
    "                round_str = f\"{original_round}F\"  # For other rounds if they exist\n",
    "            return round_str, window_num\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    def get_window_hierarchy(self, acad_term_id):\n",
    "        \"\"\"Get the expected window hierarchy for a given academic term\n",
    "        Updated to include incoming student rounds\"\"\"\n",
    "        if not acad_term_id:\n",
    "            return []\n",
    "        \n",
    "        # Extract year and term from acad_term_id\n",
    "        pattern = r'AY(\\d{4})(\\d{2})T(\\w+)'\n",
    "        match = re.match(pattern, acad_term_id)\n",
    "        if not match:\n",
    "            return []\n",
    "        \n",
    "        year_start = int(match.group(1))\n",
    "        year_end = int(match.group(2))\n",
    "        term = match.group(3)\n",
    "        \n",
    "        # Determine academic year\n",
    "        full_year_end = 2000 + year_end if year_end > 50 else 2000 + year_end\n",
    "        if year_start > full_year_end:\n",
    "            full_year_end += 100\n",
    "        \n",
    "        # Term 3A and 3B have different hierarchy\n",
    "        if term in ['3A', '3B']:\n",
    "            return [\n",
    "                (\"1\", 1), (\"1\", 2), (\"1\", 3), (\"1\", 4),\n",
    "                (\"2\", 1), (\"2\", 2)\n",
    "            ]\n",
    "        \n",
    "        # Regular terms (T1, T2) - includes incoming student rounds\n",
    "        base_hierarchy = []\n",
    "        \n",
    "        if full_year_end < 2025:  # Before AY2024-25\n",
    "            base_hierarchy = [\n",
    "                (\"1\", 1), (\"1\", 2),\n",
    "                (\"1A\", 1), (\"1A\", 2),\n",
    "                (\"1B\", 1), (\"1B\", 2),\n",
    "                (\"1C\", 1), (\"1C\", 2), (\"1C\", 3),\n",
    "                (\"2\", 1), (\"2\", 2), (\"2\", 3),\n",
    "                (\"2A\", 1), (\"2A\", 2), (\"2A\", 3)\n",
    "            ]\n",
    "        else:  # From AY2024-25 onwards\n",
    "            base_hierarchy = [\n",
    "                (\"1\", 1),\n",
    "                (\"1A\", 1), (\"1A\", 2), (\"1A\", 3),\n",
    "                (\"1B\", 1), (\"1B\", 2),\n",
    "                (\"1C\", 1), (\"1C\", 2), (\"1C\", 3),\n",
    "                (\"2\", 1), (\"2\", 2), (\"2\", 3),\n",
    "                (\"2A\", 1), (\"2A\", 2), (\"2A\", 3)\n",
    "            ]\n",
    "        \n",
    "        # Add incoming student rounds\n",
    "        incoming_rounds = [\n",
    "            (\"1F\", 1), (\"1F\", 2), (\"1F\", 3), (\"1F\", 4)  # Incoming Freshmen\n",
    "        ]\n",
    "        \n",
    "        # Combine hierarchies: regular rounds first, then incoming rounds\n",
    "        return base_hierarchy + incoming_rounds\n",
    "\n",
    "    def load_boss_results(self):\n",
    "        \"\"\"Load all BOSS results XLSX files\"\"\"\n",
    "        self.log_boss_activity(\"üîç Loading BOSS results files...\")\n",
    "        \n",
    "        input_pattern = os.path.join('script_input', 'overallBossResults', '*.xlsx')\n",
    "        xlsx_files = glob.glob(input_pattern)\n",
    "        \n",
    "        if not xlsx_files:\n",
    "            self.log_boss_activity(f\"‚ùå No XLSX files found in pattern: {input_pattern}\")\n",
    "            return False\n",
    "        \n",
    "        self.log_boss_activity(f\"üìÇ Found {len(xlsx_files)} XLSX files\")\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        for file_path in xlsx_files:\n",
    "            try:\n",
    "                self.log_boss_activity(f\"üìñ Loading: {os.path.basename(file_path)}\")\n",
    "                df = pd.read_excel(file_path)\n",
    "                \n",
    "                # Add source file for tracking\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                all_data.append(df)\n",
    "                \n",
    "                self.boss_stats['files_processed'] += 1\n",
    "                self.log_boss_activity(f\"‚úÖ Loaded {len(df)} rows from {os.path.basename(file_path)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_boss_activity(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if all_data:\n",
    "            self.boss_data = pd.concat(all_data, ignore_index=True)\n",
    "            self.boss_stats['total_rows'] = len(self.boss_data)\n",
    "            self.log_boss_activity(f\"‚úÖ Combined {self.boss_stats['total_rows']} total rows\")\n",
    "            return True\n",
    "        else:\n",
    "            self.log_boss_activity(\"‚ùå No data loaded successfully\")\n",
    "            return False\n",
    "\n",
    "    def process_bid_windows(self):\n",
    "        \"\"\"Process and create bid_window entries\"\"\"\n",
    "        self.log_boss_activity(\"ü™ü Processing bid windows...\")\n",
    "        \n",
    "        if self.boss_data is None or len(self.boss_data) == 0:\n",
    "            self.log_boss_activity(\"‚ùå No BOSS data loaded\")\n",
    "            return False\n",
    "        \n",
    "        # Group by Term and Bidding Window to identify unique bid windows\n",
    "        unique_windows = self.boss_data.groupby(['Term', 'Bidding Window']).size().reset_index(name='count')\n",
    "        \n",
    "        for _, row in unique_windows.iterrows():\n",
    "            term_str = row['Term']\n",
    "            bidding_window_str = row['Bidding Window']\n",
    "            \n",
    "            # Parse term and bidding window\n",
    "            acad_term_id = self.parse_term_to_acad_term_id(term_str)\n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            \n",
    "            if not all([acad_term_id, round_str, window_num]):\n",
    "                self.log_boss_activity(f\"‚ö†Ô∏è Could not parse: Term='{term_str}', Window='{bidding_window_str}'\")\n",
    "                continue\n",
    "            \n",
    "            # Check if this bid window already exists\n",
    "            window_key = (acad_term_id, round_str, window_num)\n",
    "            if window_key in self.bid_window_cache:\n",
    "                continue\n",
    "            \n",
    "            # Create new bid window\n",
    "            bid_window_id = self.bid_window_id_counter\n",
    "            self.bid_window_id_counter += 1\n",
    "            \n",
    "            new_bid_window = {\n",
    "                'id': bid_window_id,\n",
    "                'acad_term_id': acad_term_id,\n",
    "                'round': round_str,\n",
    "                'window': window_num\n",
    "            }\n",
    "            \n",
    "            self.new_bid_windows.append(new_bid_window)\n",
    "            self.bid_window_cache[window_key] = bid_window_id\n",
    "            self.boss_stats['bid_windows_created'] += 1\n",
    "            \n",
    "            self.log_boss_activity(\n",
    "                f\"‚úÖ Created bid_window {bid_window_id}: {acad_term_id} Round {round_str} Window {window_num}\"\n",
    "            )\n",
    "        \n",
    "        # Sort bid windows by hierarchy\n",
    "        self.sort_bid_windows_by_hierarchy()\n",
    "        \n",
    "        self.log_boss_activity(f\"‚úÖ Processed {self.boss_stats['bid_windows_created']} bid windows\")\n",
    "        return True\n",
    "\n",
    "    def sort_bid_windows_by_hierarchy(self):\n",
    "        \"\"\"Sort bid windows according to the proper hierarchy\"\"\"\n",
    "        self.log_boss_activity(\"üîÑ Sorting bid windows by hierarchy...\")\n",
    "        \n",
    "        # Group by acad_term_id\n",
    "        term_groups = defaultdict(list)\n",
    "        for bw in self.new_bid_windows:\n",
    "            term_groups[bw['acad_term_id']].append(bw)\n",
    "        \n",
    "        sorted_windows = []\n",
    "        new_id_mapping = {}  # old_id -> new_id\n",
    "        new_id_counter = 1\n",
    "        \n",
    "        for acad_term_id in sorted(term_groups.keys()):\n",
    "            windows = term_groups[acad_term_id]\n",
    "            hierarchy = self.get_window_hierarchy(acad_term_id)\n",
    "            \n",
    "            # Create a mapping of (round, window) to bid_window for this term\n",
    "            term_window_map = {(bw['round'], bw['window']): bw for bw in windows}\n",
    "            \n",
    "            # Sort according to hierarchy\n",
    "            for round_str, window_num in hierarchy:\n",
    "                if (round_str, window_num) in term_window_map:\n",
    "                    bw = term_window_map[(round_str, window_num)]\n",
    "                    old_id = bw['id']\n",
    "                    new_id = new_id_counter\n",
    "                    new_id_counter += 1\n",
    "                    \n",
    "                    # Update the bid window with new ID\n",
    "                    bw['id'] = new_id\n",
    "                    sorted_windows.append(bw)\n",
    "                    new_id_mapping[old_id] = new_id\n",
    "                    \n",
    "                    # Update cache\n",
    "                    window_key = (acad_term_id, round_str, window_num)\n",
    "                    self.bid_window_cache[window_key] = new_id\n",
    "        \n",
    "        self.new_bid_windows = sorted_windows\n",
    "        self.bid_window_id_counter = new_id_counter\n",
    "        \n",
    "        self.log_boss_activity(f\"‚úÖ Sorted {len(sorted_windows)} bid windows by hierarchy\")\n",
    "\n",
    "\n",
    "    def find_class_id(self, course_code, section, acad_term_id):\n",
    "        \"\"\"Find class_id using course_code, section, and acad_term_id\n",
    "        Robust version that checks multiple sources in order:\n",
    "        1. Memory cache (new_classes)\n",
    "        2. Database cache (existing_classes_cache)\n",
    "        3. new_classes.csv file\n",
    "        4. Direct database query\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, get course_id from course_code\n",
    "        course_id = self.get_course_id(course_code)\n",
    "        if not course_id:\n",
    "            return None\n",
    "        \n",
    "        # Convert section to string for consistent comparison\n",
    "        section_str = str(section)\n",
    "        \n",
    "        # Source 1: Search in newly created classes (memory)\n",
    "        if hasattr(self, 'new_classes') and self.new_classes:\n",
    "            for class_obj in self.new_classes:\n",
    "                if (class_obj['course_id'] == course_id and \n",
    "                    str(class_obj['section']) == section_str and \n",
    "                    class_obj['acad_term_id'] == acad_term_id):\n",
    "                    return class_obj['id']\n",
    "        \n",
    "        # Source 2: Search in existing database cache\n",
    "        if not hasattr(self, 'existing_classes_cache'):\n",
    "            self.load_existing_classes_cache()\n",
    "        \n",
    "        if hasattr(self, 'existing_classes_cache') and self.existing_classes_cache:\n",
    "            for class_obj in self.existing_classes_cache:\n",
    "                if (class_obj['course_id'] == course_id and \n",
    "                    str(class_obj['section']) == section_str and \n",
    "                    class_obj['acad_term_id'] == acad_term_id):\n",
    "                    return class_obj['id']\n",
    "        \n",
    "        # Source 3: Check new_classes.csv file (if cache is empty/stale)\n",
    "        class_id = self.search_new_classes_csv(course_id, section_str, acad_term_id)\n",
    "        if class_id:\n",
    "            return class_id\n",
    "        \n",
    "        # Source 4: Direct database query (last resort)\n",
    "        if self.connection:\n",
    "            class_id = self.search_database_classes(course_id, section_str, acad_term_id)\n",
    "            if class_id:\n",
    "                return class_id\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def get_course_id(self, course_code):\n",
    "        \"\"\"Get course_id from course_code, checking multiple sources\"\"\"\n",
    "        # Check courses cache (from database)\n",
    "        if course_code in self.courses_cache:\n",
    "            return self.courses_cache[course_code]['id']\n",
    "        \n",
    "        # Check in new_courses (newly created)\n",
    "        for course in self.new_courses:\n",
    "            if course['code'] == course_code:\n",
    "                return course['id']\n",
    "        \n",
    "        # Check new_courses.csv file\n",
    "        try:\n",
    "            new_courses_path = os.path.join(self.output_base, 'new_courses.csv')\n",
    "            verify_courses_path = os.path.join(self.verify_dir, 'new_courses.csv')\n",
    "            \n",
    "            for path in [verify_courses_path, new_courses_path]:\n",
    "                if os.path.exists(path):\n",
    "                    df = pd.read_csv(path)\n",
    "                    matching_courses = df[df['code'] == course_code]\n",
    "                    if not matching_courses.empty:\n",
    "                        return matching_courses.iloc[0]['id']\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Error reading new_courses.csv: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def search_new_classes_csv(self, course_id, section_str, acad_term_id):\n",
    "        \"\"\"Search for class in new_classes.csv file\"\"\"\n",
    "        try:\n",
    "            new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "            if os.path.exists(new_classes_path):\n",
    "                df = pd.read_csv(new_classes_path)\n",
    "                matching_classes = df[\n",
    "                    (df['course_id'] == course_id) & \n",
    "                    (df['section'].astype(str) == section_str) & \n",
    "                    (df['acad_term_id'] == acad_term_id)\n",
    "                ]\n",
    "                if not matching_classes.empty:\n",
    "                    return matching_classes.iloc[0]['id']\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Error reading new_classes.csv: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def search_database_classes(self, course_id, section_str, acad_term_id):\n",
    "        \"\"\"Search for class directly in database\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            SELECT id FROM classes \n",
    "            WHERE course_id = %s AND section = %s AND acad_term_id = %s\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(query, (course_id, section_str, acad_term_id))\n",
    "            result = cursor.fetchone()\n",
    "            cursor.close()\n",
    "            \n",
    "            if result:\n",
    "                return result[0]\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Error querying database: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def load_existing_classes_cache(self):\n",
    "        \"\"\"Load existing classes from database cache with fallback options\"\"\"\n",
    "        self.existing_classes_cache = []\n",
    "        \n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, 'classes_cache.pkl')\n",
    "            \n",
    "            # Try loading from cache file first\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    classes_df = pd.read_pickle(cache_file)\n",
    "                    if not classes_df.empty:\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        self.log_boss_activity(f\"üìö Loaded {len(self.existing_classes_cache)} existing classes from cache\")\n",
    "                        return\n",
    "                    else:\n",
    "                        self.log_boss_activity(\"‚ö†Ô∏è Cache file exists but is empty\")\n",
    "                except Exception as e:\n",
    "                    self.log_boss_activity(f\"‚ö†Ô∏è Error reading cache file: {e}\")\n",
    "            \n",
    "            # If cache doesn't exist or is empty, try database\n",
    "            if self.connection:\n",
    "                try:\n",
    "                    query = \"SELECT * FROM classes\"\n",
    "                    classes_df = pd.read_sql_query(query, self.connection)\n",
    "                    if not classes_df.empty:\n",
    "                        # Save to cache for future use\n",
    "                        classes_df.to_pickle(cache_file)\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        self.log_boss_activity(f\"üìö Downloaded and cached {len(self.existing_classes_cache)} existing classes\")\n",
    "                        return\n",
    "                    else:\n",
    "                        self.log_boss_activity(\"‚ö†Ô∏è Database classes table is empty\")\n",
    "                except Exception as e:\n",
    "                    self.log_boss_activity(f\"‚ö†Ô∏è Error downloading classes from database: {e}\")\n",
    "            \n",
    "            # If all else fails, try reading from new_classes.csv\n",
    "            try:\n",
    "                new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "                if os.path.exists(new_classes_path):\n",
    "                    classes_df = pd.read_csv(new_classes_path)\n",
    "                    if not classes_df.empty:\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        self.log_boss_activity(f\"üìö Loaded {len(self.existing_classes_cache)} classes from new_classes.csv as fallback\")\n",
    "                        return\n",
    "            except Exception as e:\n",
    "                self.log_boss_activity(f\"‚ö†Ô∏è Error reading new_classes.csv as fallback: {e}\")\n",
    "            \n",
    "            # Final fallback\n",
    "            self.log_boss_activity(\"‚ö†Ô∏è All class loading methods failed - using empty cache\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.existing_classes_cache = []\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Critical error in load_existing_classes_cache: {e}\")\n",
    "\n",
    "    def verify_class_lookup_sources(self):\n",
    "        \"\"\"Debug method to verify all class lookup sources\"\"\"\n",
    "        print(\"\\nüîç Verifying Class Lookup Sources...\")\n",
    "        \n",
    "        # Check memory sources\n",
    "        print(f\"\\n1Ô∏è‚É£ Memory Sources:\")\n",
    "        print(f\"   - new_classes: {len(self.new_classes) if hasattr(self, 'new_classes') and self.new_classes else 0} classes\")\n",
    "        print(f\"   - existing_classes_cache: {len(self.existing_classes_cache) if hasattr(self, 'existing_classes_cache') and self.existing_classes_cache else 0} classes\")\n",
    "        \n",
    "        # Check file sources\n",
    "        print(f\"\\n2Ô∏è‚É£ File Sources:\")\n",
    "        new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "        if os.path.exists(new_classes_path):\n",
    "            try:\n",
    "                df = pd.read_csv(new_classes_path)\n",
    "                print(f\"   - new_classes.csv: {len(df)} classes\")\n",
    "            except Exception as e:\n",
    "                print(f\"   - new_classes.csv: Error reading ({e})\")\n",
    "        else:\n",
    "            print(f\"   - new_classes.csv: File not found\")\n",
    "        \n",
    "        # Check database\n",
    "        print(f\"\\n3Ô∏è‚É£ Database Source:\")\n",
    "        if self.connection:\n",
    "            try:\n",
    "                cursor = self.connection.cursor()\n",
    "                cursor.execute(\"SELECT COUNT(*) FROM classes\")\n",
    "                count = cursor.fetchone()[0]\n",
    "                cursor.close()\n",
    "                print(f\"   - Database classes table: {count} classes\")\n",
    "            except Exception as e:\n",
    "                print(f\"   - Database classes table: Error ({e})\")\n",
    "        else:\n",
    "            print(f\"   - Database: No connection\")\n",
    "        \n",
    "        # Test a specific case\n",
    "        print(f\"\\n4Ô∏è‚É£ Testing Specific Case:\")\n",
    "        test_cases = [\n",
    "            (\"ACCT101\", \"G1\", \"AY202122T2\"),\n",
    "            (\"ACCT001\", \"G1\", \"AY202122T2\")\n",
    "        ]\n",
    "        \n",
    "        for course_code, section, term in test_cases:\n",
    "            print(f\"\\n   Testing: {course_code} Section {section} Term {term}\")\n",
    "            course_id = self.get_course_id(course_code)\n",
    "            print(f\"   - Course ID: {course_id}\")\n",
    "            \n",
    "            if course_id:\n",
    "                class_id = self.find_class_id(course_code, section, term)\n",
    "                print(f\"   - Class ID: {class_id}\")\n",
    "            else:\n",
    "                print(f\"   - Cannot test class lookup without course ID\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def process_class_availability(self):\n",
    "        \"\"\"Process class availability data\"\"\"\n",
    "        self.log_boss_activity(\"üìä Processing class availability...\")\n",
    "        \n",
    "        processed_count = 0\n",
    "        \n",
    "        for _, row in self.boss_data.iterrows():\n",
    "            # Parse required fields\n",
    "            course_code = row.get('Course Code')\n",
    "            section = row.get('Section')\n",
    "            term_str = row.get('Term')\n",
    "            bidding_window_str = row.get('Bidding Window')\n",
    "            \n",
    "            # Extract availability data\n",
    "            vacancy = row.get('Vacancy')\n",
    "            enrolled_students = row.get('Enrolled Students')\n",
    "            before_process_vacancy = row.get('Before Process Vacancy')\n",
    "            \n",
    "            # Validate required fields\n",
    "            if pd.isna(course_code) or pd.isna(section) or pd.isna(term_str) or pd.isna(bidding_window_str):\n",
    "                continue\n",
    "            \n",
    "            # Parse term and bidding window\n",
    "            acad_term_id = self.parse_term_to_acad_term_id(term_str)\n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            \n",
    "            if not all([acad_term_id, round_str, window_num]):\n",
    "                continue\n",
    "            \n",
    "            # Find class_id\n",
    "            class_id = self.find_class_id(course_code, str(section), acad_term_id)\n",
    "            if not class_id:\n",
    "                # Record failed mapping\n",
    "                failed_row = {\n",
    "                    'course_code': course_code,\n",
    "                    'section': section,\n",
    "                    'acad_term_id': acad_term_id,\n",
    "                    'term_str': term_str,\n",
    "                    'bidding_window_str': bidding_window_str,\n",
    "                    'reason': 'class_not_found',\n",
    "                    'source_file': row.get('source_file', 'unknown')\n",
    "                }\n",
    "                self.failed_mappings.append(failed_row)\n",
    "                self.boss_stats['failed_mappings'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Get bid_window_id\n",
    "            window_key = (acad_term_id, round_str, window_num)\n",
    "            bid_window_id = self.bid_window_cache.get(window_key)\n",
    "            if not bid_window_id:\n",
    "                self.log_boss_activity(f\"‚ö†Ô∏è No bid_window_id for {window_key}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate fields\n",
    "            total = int(vacancy) if pd.notna(vacancy) else 0\n",
    "            current_enrolled = int(enrolled_students) if pd.notna(enrolled_students) else 0\n",
    "            available = int(before_process_vacancy) if pd.notna(before_process_vacancy) else 0\n",
    "            reserved = max(0, total - current_enrolled - available)\n",
    "            \n",
    "            # Create class availability record\n",
    "            availability_record = {\n",
    "                'class_id': class_id,\n",
    "                'bid_window_id': bid_window_id,\n",
    "                'total': total,\n",
    "                'current_enrolled': current_enrolled,\n",
    "                'reserved': reserved,\n",
    "                'available': available\n",
    "            }\n",
    "            \n",
    "            self.new_class_availability.append(availability_record)\n",
    "            processed_count += 1\n",
    "            self.boss_stats['class_availability_created'] += 1\n",
    "        \n",
    "        self.log_boss_activity(f\"‚úÖ Processed {processed_count} class availability records\")\n",
    "        return True\n",
    "\n",
    "    def process_bid_results(self):\n",
    "        \"\"\"Process bid result data\"\"\"\n",
    "        self.log_boss_activity(\"üìà Processing bid results...\")\n",
    "        \n",
    "        processed_count = 0\n",
    "        \n",
    "        for _, row in self.boss_data.iterrows():\n",
    "            # Parse required fields\n",
    "            course_code = row.get('Course Code')\n",
    "            section = row.get('Section')\n",
    "            term_str = row.get('Term')\n",
    "            bidding_window_str = row.get('Bidding Window')\n",
    "            \n",
    "            # Extract bid result data\n",
    "            vacancy = row.get('Vacancy')\n",
    "            opening_vacancy = row.get('Opening Vacancy')\n",
    "            before_process_vacancy = row.get('Before Process Vacancy')\n",
    "            dice = row.get('D.I.C.E')\n",
    "            after_process_vacancy = row.get('After Process Vacancy', 0)  # Default to 0 if missing\n",
    "            enrolled_students = row.get('Enrolled Students')\n",
    "            median_bid = row.get('Median Bid')\n",
    "            min_bid = row.get('Min Bid')\n",
    "            \n",
    "            # Validate required fields\n",
    "            if pd.isna(course_code) or pd.isna(section) or pd.isna(term_str) or pd.isna(bidding_window_str):\n",
    "                continue\n",
    "            \n",
    "            # Parse term and bidding window\n",
    "            acad_term_id = self.parse_term_to_acad_term_id(term_str)\n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            \n",
    "            if not all([acad_term_id, round_str, window_num]):\n",
    "                continue\n",
    "            \n",
    "            # Find class_id\n",
    "            class_id = self.find_class_id(course_code, str(section), acad_term_id)\n",
    "            if not class_id:\n",
    "                # Failed mapping already recorded in process_class_availability\n",
    "                continue\n",
    "            \n",
    "            # Get bid_window_id\n",
    "            window_key = (acad_term_id, round_str, window_num)\n",
    "            bid_window_id = self.bid_window_cache.get(window_key)\n",
    "            if not bid_window_id:\n",
    "                continue\n",
    "            \n",
    "            # Convert numeric fields\n",
    "            def safe_int(val):\n",
    "                return int(val) if pd.notna(val) else 0\n",
    "            \n",
    "            def safe_float(val):\n",
    "                return float(val) if pd.notna(val) else 0.0\n",
    "            \n",
    "            # Create bid result record\n",
    "            bid_result_record = {\n",
    "                'bid_window_id': bid_window_id,\n",
    "                'class_id': class_id,\n",
    "                'vacancy': safe_int(vacancy),\n",
    "                'opening_vacancy': safe_int(opening_vacancy),\n",
    "                'before_process_vacancy': safe_int(before_process_vacancy),\n",
    "                'dice': safe_int(dice),\n",
    "                'after_process_vacancy': safe_int(after_process_vacancy),\n",
    "                'enrolled_students': safe_int(enrolled_students),\n",
    "                'bid_actual_median': safe_float(median_bid),\n",
    "                'bid_actual_min': safe_float(min_bid),\n",
    "                'bid_predicted_median': 0.0,  # Not in source data\n",
    "                'bid_predicted_min': 0.0      # Not in source data\n",
    "            }\n",
    "            \n",
    "            self.new_bid_result.append(bid_result_record)\n",
    "            processed_count += 1\n",
    "            self.boss_stats['bid_results_created'] += 1\n",
    "        \n",
    "        self.log_boss_activity(f\"‚úÖ Processed {processed_count} bid result records\")\n",
    "        return True\n",
    "\n",
    "    def save_boss_outputs(self):\n",
    "        \"\"\"Save all BOSS-related output files\"\"\"\n",
    "        self.log_boss_activity(\"üíæ Saving BOSS output files...\")\n",
    "        \n",
    "        # Save bid windows\n",
    "        if self.new_bid_windows:\n",
    "            df = pd.DataFrame(self.new_bid_windows)\n",
    "            output_path = os.path.join(self.output_base, 'new_bid_window.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚úÖ Saved {len(self.new_bid_windows)} bid windows to new_bid_window.csv\")\n",
    "        \n",
    "        # Save class availability\n",
    "        if self.new_class_availability:\n",
    "            df = pd.DataFrame(self.new_class_availability)\n",
    "            output_path = os.path.join(self.output_base, 'new_class_availability.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚úÖ Saved {len(self.new_class_availability)} availability records to new_class_availability.csv\")\n",
    "        \n",
    "        # Save bid results\n",
    "        if self.new_bid_result:\n",
    "            df = pd.DataFrame(self.new_bid_result)\n",
    "            output_path = os.path.join(self.output_base, 'new_bid_result.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚úÖ Saved {len(self.new_bid_result)} bid results to new_bid_result.csv\")\n",
    "        \n",
    "        # Save failed mappings\n",
    "        if self.failed_mappings:\n",
    "            df = pd.DataFrame(self.failed_mappings)\n",
    "            output_path = os.path.join(self.output_base, 'failed_boss_results_mapping.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"‚ö†Ô∏è Saved {len(self.failed_mappings)} failed mappings to failed_boss_results_mapping.csv\")\n",
    "        \n",
    "        self.log_boss_activity(\"‚úÖ All BOSS output files saved successfully\")\n",
    "\n",
    "    def print_boss_summary(self):\n",
    "        \"\"\"Print BOSS processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä BOSS RESULTS PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"üìÇ Files processed: {self.boss_stats['files_processed']}\")\n",
    "        print(f\"üìÑ Total rows: {self.boss_stats['total_rows']}\")\n",
    "        print(f\"ü™ü Bid windows created: {self.boss_stats['bid_windows_created']}\")\n",
    "        print(f\"üìä Class availability records: {self.boss_stats['class_availability_created']}\")\n",
    "        print(f\"üìà Bid result records: {self.boss_stats['bid_results_created']}\")\n",
    "        print(f\"‚ùå Failed mappings: {self.boss_stats['failed_mappings']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nüìÅ OUTPUT FILES:\")\n",
    "        print(f\"   - new_bid_window.csv ({self.boss_stats['bid_windows_created']} records)\")\n",
    "        print(f\"   - new_class_availability.csv ({self.boss_stats['class_availability_created']} records)\")\n",
    "        print(f\"   - new_bid_result.csv ({self.boss_stats['bid_results_created']} records)\")\n",
    "        if self.boss_stats['failed_mappings'] > 0:\n",
    "            print(f\"   - failed_boss_results_mapping.csv ({self.boss_stats['failed_mappings']} records)\")\n",
    "        print(f\"   - boss_result_log.txt (processing log)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def run_phase3_boss_processing(self):\n",
    "        \"\"\"Run the complete BOSS results processing pipeline\"\"\"\n",
    "        try:\n",
    "            self.log_boss_activity(\"üöÄ Starting Phase 3: BOSS Results Processing\")\n",
    "            self.log_boss_activity(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Setup\n",
    "            self.setup_boss_processing()\n",
    "            \n",
    "            # Step 2: Load BOSS results\n",
    "            if not self.load_boss_results():\n",
    "                self.log_boss_activity(\"‚ùå Failed to load BOSS results\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Process bid windows\n",
    "            if not self.process_bid_windows():\n",
    "                self.log_boss_activity(\"‚ùå Failed to process bid windows\")\n",
    "                return False\n",
    "            \n",
    "            # Step 4: Process class availability\n",
    "            if not self.process_class_availability():\n",
    "                self.log_boss_activity(\"‚ùå Failed to process class availability\")\n",
    "                return False\n",
    "            \n",
    "            # Step 5: Process bid results\n",
    "            if not self.process_bid_results():\n",
    "                self.log_boss_activity(\"‚ùå Failed to process bid results\")\n",
    "                return False\n",
    "            \n",
    "            # Step 6: Save outputs\n",
    "            self.save_boss_outputs()\n",
    "            \n",
    "            # Step 7: Print summary\n",
    "            self.print_boss_summary()\n",
    "            \n",
    "            self.log_boss_activity(\"‚úÖ Phase 3: BOSS Results Processing completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"‚ùå Phase 3 failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def load_faculties_cache(self):\n",
    "        \"\"\"Load faculties from database cache for mapping\"\"\"\n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, 'faculties_cache.pkl')\n",
    "            \n",
    "            # Try loading from cache file first\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    faculties_df = pd.read_pickle(cache_file)\n",
    "                    if not faculties_df.empty:\n",
    "                        self.faculties_cache = {}\n",
    "                        self.faculty_acronym_to_id = {}\n",
    "                        \n",
    "                        for _, row in faculties_df.iterrows():\n",
    "                            faculty_id = row['id']\n",
    "                            acronym = row['acronym'].upper()\n",
    "                            \n",
    "                            self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                            self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                        \n",
    "                        logger.info(f\"üìö Loaded {len(self.faculties_cache)} faculties from cache\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(\"‚ö†Ô∏è Faculty cache file exists but is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Error reading faculty cache file: {e}\")\n",
    "            \n",
    "            # If cache doesn't exist or failed, try database\n",
    "            if self.connection:\n",
    "                try:\n",
    "                    query = \"SELECT * FROM faculties\"\n",
    "                    faculties_df = pd.read_sql_query(query, self.connection)\n",
    "                    if not faculties_df.empty:\n",
    "                        # Save to cache for future use\n",
    "                        faculties_df.to_pickle(cache_file)\n",
    "                        \n",
    "                        # Load into memory\n",
    "                        self.faculties_cache = {}\n",
    "                        self.faculty_acronym_to_id = {}\n",
    "                        \n",
    "                        for _, row in faculties_df.iterrows():\n",
    "                            faculty_id = row['id']\n",
    "                            acronym = row['acronym'].upper()\n",
    "                            \n",
    "                            self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                            self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                        \n",
    "                        logger.info(f\"üìö Downloaded and cached {len(self.faculties_cache)} faculties from database\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(\"‚ö†Ô∏è Database faculties table is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Error downloading faculties from database: {e}\")\n",
    "            \n",
    "            # Fallback: create basic mapping from known data\n",
    "            logger.warning(\"‚ö†Ô∏è Using fallback faculty mapping\")\n",
    "            self.faculties_cache = {}\n",
    "            self.faculty_acronym_to_id = {\n",
    "                'LKCSB': 1,   # Lee Kong Chian School of Business\n",
    "                'YPHSL': 2,   # Yong Pung How School of Law\n",
    "                'SOE': 3,     # School of Economics\n",
    "                'SCIS': 4,    # School of Computing and Information Systems\n",
    "                'SOSS': 5,    # School of Social Sciences\n",
    "                'SOA': 6,     # School of Accountancy\n",
    "                'CIS': 7,     # College of Integrative Studies\n",
    "                'CEC': 8      # Center for English Communication\n",
    "            }\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Critical error in load_faculties_cache: {e}\")\n",
    "            return False\n",
    "\n",
    "    def map_courses_to_faculties_from_boss(self):\n",
    "        \"\"\"Map courses to faculties using School/Department data from BOSS results\"\"\"\n",
    "        logger.info(\"üéì Starting automated faculty mapping from BOSS data...\")\n",
    "        \n",
    "        # Load faculties cache first\n",
    "        if not self.load_faculties_cache():\n",
    "            logger.error(\"‚ùå Failed to load faculties cache\")\n",
    "            return False\n",
    "        \n",
    "        # Department code to faculty acronym mapping\n",
    "        # Maps BOSS School/Department codes to our faculty acronyms\n",
    "        dept_to_faculty_mapping = {\n",
    "            'SOA': 'SOA',         # School of Accountancy\n",
    "            'SOSS': 'SOSS',       # School of Social Sciences  \n",
    "            'LKCSOB': 'LKCSB',    # Lee Kong Chian School of Business (alternative name)\n",
    "            'LKCSB': 'LKCSB',     # Lee Kong Chian School of Business\n",
    "            'SIS': 'SCIS',        # School of Computing and Information Systems (old name)\n",
    "            'SCIS': 'SCIS',       # School of Computing and Information Systems\n",
    "            'OCC': 'CIS',         # College of Integrative Studies (Office of Core Curriculum)\n",
    "            'CIS': 'CIS',         # College of Integrative Studies\n",
    "            'CEC': 'CEC',         # Center for English Communication\n",
    "            'SOL': 'YPHSL',       # Yong Pung How School of Law (School of Law)\n",
    "            'SOLGPO': 'YPHSL',    # Yong Pung How School of Law (alternative)\n",
    "            'SOE': 'SOE'          # School of Economics\n",
    "        }\n",
    "        \n",
    "        # Track new faculties that need to be created\n",
    "        new_faculties_needed = set()\n",
    "        course_faculty_mappings = {}\n",
    "        \n",
    "        # Load BOSS results to extract School/Department mapping\n",
    "        boss_data_pattern = os.path.join('script_input', 'overallBossResults', '*.xlsx')\n",
    "        boss_files = glob.glob(boss_data_pattern)\n",
    "        \n",
    "        if not boss_files:\n",
    "            logger.warning(\"‚ö†Ô∏è No BOSS results files found for faculty mapping\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"üìÇ Found {len(boss_files)} BOSS files for faculty mapping\")\n",
    "        \n",
    "        # Collect all course-faculty mappings from BOSS data\n",
    "        boss_faculty_data = []\n",
    "        for file_path in boss_files:\n",
    "            try:\n",
    "                df = pd.read_excel(file_path)\n",
    "                if 'Course Code' in df.columns and 'School/Department' in df.columns:\n",
    "                    # Extract unique course-faculty pairs\n",
    "                    course_dept_pairs = df[['Course Code', 'School/Department']].dropna().drop_duplicates()\n",
    "                    boss_faculty_data.append(course_dept_pairs)\n",
    "                    logger.info(f\"‚úÖ Extracted {len(course_dept_pairs)} course-department pairs from {os.path.basename(file_path)}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Could not read {file_path}: {e}\")\n",
    "        \n",
    "        if not boss_faculty_data:\n",
    "            logger.warning(\"‚ö†Ô∏è No valid BOSS faculty data found\")\n",
    "            return False\n",
    "        \n",
    "        # Combine all BOSS faculty data\n",
    "        combined_boss_data = pd.concat(boss_faculty_data, ignore_index=True).drop_duplicates()\n",
    "        logger.info(f\"üìã Combined {len(combined_boss_data)} unique course-department pairs\")\n",
    "        \n",
    "        # Log unique departments found\n",
    "        unique_depts = combined_boss_data['School/Department'].str.strip().str.upper().unique()\n",
    "        logger.info(f\"üèõÔ∏è Unique departments found in BOSS data: {sorted(unique_depts)}\")\n",
    "        \n",
    "        # Process each course-department pair\n",
    "        mapped_count = 0\n",
    "        unmapped_depts = set()\n",
    "        \n",
    "        for _, row in combined_boss_data.iterrows():\n",
    "            course_code = row['Course Code']\n",
    "            dept_code = str(row['School/Department']).strip().upper()\n",
    "            \n",
    "            if not course_code or not dept_code:\n",
    "                continue\n",
    "            \n",
    "            # Check if course exists in our courses (new or existing)\n",
    "            course_exists = False\n",
    "            if course_code in self.courses_cache:\n",
    "                course_exists = True\n",
    "            elif any(course['code'] == course_code for course in self.new_courses):\n",
    "                course_exists = True\n",
    "            \n",
    "            if not course_exists:\n",
    "                continue  # Skip courses we don't have\n",
    "            \n",
    "            # Map department code to faculty acronym, then to faculty ID\n",
    "            if dept_code in dept_to_faculty_mapping:\n",
    "                faculty_acronym = dept_to_faculty_mapping[dept_code]\n",
    "                \n",
    "                # Get faculty ID from acronym\n",
    "                if faculty_acronym in self.faculty_acronym_to_id:\n",
    "                    faculty_id = self.faculty_acronym_to_id[faculty_acronym]\n",
    "                    course_faculty_mappings[course_code] = faculty_id\n",
    "                    mapped_count += 1\n",
    "                    logger.debug(f\"‚úÖ Mapped {course_code}: {dept_code} ‚Üí {faculty_acronym} ‚Üí ID {faculty_id}\")\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Faculty acronym {faculty_acronym} not found in database\")\n",
    "                    unmapped_depts.add(dept_code)\n",
    "            else:\n",
    "                # Track unmapped department for new faculty creation\n",
    "                unmapped_depts.add(dept_code)\n",
    "                logger.info(f\"üÜï Unmapped department: {dept_code} (for course {course_code})\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Mapped {mapped_count} courses to existing faculties\")\n",
    "        logger.info(f\"üÜï Found {len(unmapped_depts)} unmapped departments: {sorted(unmapped_depts)}\")\n",
    "        \n",
    "        # Create new faculties for unmapped departments\n",
    "        new_faculty_mappings = {}\n",
    "        if unmapped_depts:\n",
    "            new_faculty_mappings = self._create_new_faculties(unmapped_depts)\n",
    "            \n",
    "            # Update our faculty mapping caches\n",
    "            for dept_code, faculty_data in new_faculty_mappings.items():\n",
    "                faculty_id = faculty_data['id']\n",
    "                faculty_acronym = faculty_data['acronym']\n",
    "                \n",
    "                # Update caches\n",
    "                self.faculties_cache[faculty_id] = faculty_data\n",
    "                self.faculty_acronym_to_id[faculty_acronym] = faculty_id\n",
    "                dept_to_faculty_mapping[dept_code] = faculty_acronym\n",
    "            \n",
    "            # Re-process courses with new faculty mappings\n",
    "            for _, row in combined_boss_data.iterrows():\n",
    "                course_code = row['Course Code']\n",
    "                dept_code = str(row['School/Department']).strip().upper()\n",
    "                \n",
    "                if course_code and dept_code and dept_code in new_faculty_mappings:\n",
    "                    # Check if course exists\n",
    "                    course_exists = False\n",
    "                    if course_code in self.courses_cache:\n",
    "                        course_exists = True\n",
    "                    elif any(course['code'] == course_code for course in self.new_courses):\n",
    "                        course_exists = True\n",
    "                    \n",
    "                    if course_exists:\n",
    "                        faculty_id = new_faculty_mappings[dept_code]['id']\n",
    "                        course_faculty_mappings[course_code] = faculty_id\n",
    "                        mapped_count += 1\n",
    "        \n",
    "        # Apply faculty mappings to courses\n",
    "        self._apply_faculty_mappings_to_courses(course_faculty_mappings)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Automated faculty mapping completed:\")\n",
    "        logger.info(f\"   ‚Ä¢ {mapped_count} courses mapped to faculties\")\n",
    "        logger.info(f\"   ‚Ä¢ {len(new_faculty_mappings)} new faculties created\")\n",
    "        logger.info(f\"   ‚Ä¢ {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _create_new_faculties(self, unmapped_dept_codes):\n",
    "        \"\"\"Create new faculties for unmapped department codes\"\"\"\n",
    "        logger.info(f\"üèóÔ∏è Creating {len(unmapped_dept_codes)} new faculties...\")\n",
    "        \n",
    "        # Get next available faculty ID\n",
    "        if hasattr(self, 'faculties_cache') and self.faculties_cache:\n",
    "            next_faculty_id = max(self.faculties_cache.keys()) + 1\n",
    "        else:\n",
    "            next_faculty_id = 9  # Start after existing 8 faculties\n",
    "        \n",
    "        # Faculty name mappings for known codes\n",
    "        faculty_name_mapping = {\n",
    "            'OCS': 'Office of Corporate & Student Relations',\n",
    "            'C4SR': 'Centre for Strategic & Regional Studies',\n",
    "            'LSM': 'Lee Shau Kee Business School',\n",
    "            'SICS': 'Singapore Institute for Clinical Sciences',\n",
    "            # Add more as needed based on actual BOSS data\n",
    "        }\n",
    "        \n",
    "        new_faculties = []\n",
    "        new_faculty_mappings = {}\n",
    "        \n",
    "        for dept_code in sorted(unmapped_dept_codes):\n",
    "            # Generate faculty name\n",
    "            if dept_code in faculty_name_mapping:\n",
    "                faculty_name = faculty_name_mapping[dept_code]\n",
    "            else:\n",
    "                # Generate a reasonable name from the code\n",
    "                # Try to expand common abbreviations\n",
    "                if 'SOC' in dept_code:\n",
    "                    faculty_name = f\"School of {dept_code.replace('SOC', 'Social Sciences')}\"\n",
    "                elif 'SCI' in dept_code:\n",
    "                    faculty_name = f\"School of {dept_code.replace('SCI', 'Science')}\"\n",
    "                elif 'ENG' in dept_code:\n",
    "                    faculty_name = f\"School of {dept_code.replace('ENG', 'Engineering')}\"\n",
    "                else:\n",
    "                    faculty_name = f\"Faculty of {dept_code}\"\n",
    "            \n",
    "            new_faculty = {\n",
    "                'id': next_faculty_id,\n",
    "                'name': faculty_name,\n",
    "                'acronym': dept_code,  # Use the original dept_code as acronym\n",
    "                'site_url': 'https://smu.edu.sg',  # Default URL\n",
    "                'belong_to_university': 1,  # SMU\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'updated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            new_faculties.append(new_faculty)\n",
    "            new_faculty_mappings[dept_code] = new_faculty  # Return full faculty data\n",
    "            next_faculty_id += 1\n",
    "            \n",
    "            logger.info(f\"‚úÖ Created faculty: {faculty_name} ({dept_code}) with ID {new_faculty['id']}\")\n",
    "        \n",
    "        # Save new faculties to verify folder\n",
    "        if new_faculties:\n",
    "            df = pd.DataFrame(new_faculties)\n",
    "            output_path = os.path.join(self.verify_dir, 'new_faculties.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            logger.info(f\"üíæ Saved {len(new_faculties)} new faculties to {output_path}\")\n",
    "        \n",
    "        return new_faculty_mappings     \n",
    "\n",
    "    def _apply_faculty_mappings_to_courses(self, course_faculty_mappings):\n",
    "        \"\"\"Apply faculty mappings to new courses and update courses needing faculty\"\"\"\n",
    "        logger.info(f\"üîÑ Applying faculty mappings to {len(course_faculty_mappings)} courses...\")\n",
    "        \n",
    "        mapped_count = 0\n",
    "        \n",
    "        # Update new_courses\n",
    "        for course in self.new_courses:\n",
    "            course_code = course['code']\n",
    "            if course_code in course_faculty_mappings:\n",
    "                course['belong_to_faculty'] = course_faculty_mappings[course_code]\n",
    "                mapped_count += 1\n",
    "        \n",
    "        # Update courses_cache\n",
    "        for course_code, faculty_id in course_faculty_mappings.items():\n",
    "            if course_code in self.courses_cache:\n",
    "                self.courses_cache[course_code]['belong_to_faculty'] = faculty_id\n",
    "        \n",
    "        # Remove mapped courses from courses_needing_faculty\n",
    "        original_needing_count = len(self.courses_needing_faculty)\n",
    "        self.courses_needing_faculty = [\n",
    "            course_info for course_info in self.courses_needing_faculty\n",
    "            if course_info['course_code'] not in course_faculty_mappings\n",
    "        ]\n",
    "        \n",
    "        removed_count = original_needing_count - len(self.courses_needing_faculty)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Applied faculty mappings:\")\n",
    "        logger.info(f\"   ‚Ä¢ {mapped_count} courses updated with faculty\")\n",
    "        logger.info(f\"   ‚Ä¢ {removed_count} courses removed from manual review queue\")\n",
    "        logger.info(f\"   ‚Ä¢ {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Phase 1 Initialization**\n",
    "```python\n",
    "# Initialize the TableBuilder\n",
    "builder = TableBuilder()\n",
    "\n",
    "# Run Phase 1 (professors, courses, acad_terms)\n",
    "success = builder.run_phase1_professors_and_courses()\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **First-time setup**: Initial processing of new raw data from HTML extractor\n",
    "- **Semester data ingestion**: Beginning of each new academic term data import\n",
    "- **After HTML extraction**: Following successful completion of point 3 data extraction\n",
    "\n",
    "**What It Does:**\n",
    "- Loads existing database cache and raw Excel data\n",
    "- Processes professors with advanced name normalization and duplicate detection\n",
    "- Creates new courses without faculty assignments (deferred for manual review)\n",
    "- Generates academic terms from date ranges and term codes\n",
    "- Outputs verification files for manual review before proceeding\n",
    "\n",
    "**Success Indicators:**\n",
    "- Creates `script_output/verify/new_professors.csv` with properly normalized names\n",
    "- Generates course files ready for faculty assignment\n",
    "- Displays statistics on professors, courses, and terms processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:22:09,773 - INFO - üöÄ Starting Phase 1: Professors and Courses with Automated Faculty Mapping\n",
      "2025-06-11 15:22:09,773 - INFO - ============================================================\n",
      "2025-06-11 15:22:10,011 - INFO - ‚úÖ Loaded data from cache\n",
      "2025-06-11 15:22:10,012 - INFO - üìÇ Loading raw data from script_input/raw_data.xlsx\n",
      "2025-06-11 15:22:14,974 - INFO - ‚úÖ Loaded 12973 standalone records\n",
      "2025-06-11 15:22:14,975 - INFO - ‚úÖ Loaded 19986 multiple records\n",
      "2025-06-11 15:22:15,963 - INFO - ‚úÖ Created optimized lookup for 11924 record keys\n",
      "2025-06-11 15:22:15,964 - INFO - üë• Processing professors...\n",
      "2025-06-11 15:22:17,640 - INFO - ‚úÖ Created professor: Zeng QINGLI with email: enquiry@smu.edu.sg\n",
      "2025-06-11 15:22:17,665 - INFO - ‚úÖ Created professor: Hong JIAQI with email: enquiry@smu.edu.sg\n",
      "2025-06-11 15:22:17,694 - INFO - ‚úÖ Created professor: Yu QI with email: enquiry@smu.edu.sg\n",
      "2025-06-11 15:22:17,716 - INFO - ‚úÖ Created professor: HU Naiyuan with email: enquiry@smu.edu.sg\n",
      "2025-06-11 15:22:17,746 - INFO - ‚úÖ Created professor: ZHANG Ce with email: enquiry@smu.edu.sg\n",
      "2025-06-11 15:22:17,772 - INFO - ‚úÖ Created professor: Tang TONY with email: tonyt@smu.edu.sg\n",
      "2025-06-11 15:22:17,797 - INFO - ‚úÖ Created professor: Koh ANDREW with email: andrewkoh@smu.edu.sg\n",
      "2025-06-11 15:22:17,818 - INFO - ‚úÖ Created professor: LEE Yun with email: enquiry@smu.edu.sg\n",
      "2025-06-11 15:22:17,845 - INFO - ‚úÖ Created professor: Pepito NONA with email: npepito@smu.edu.sg\n",
      "2025-06-11 15:22:17,866 - INFO - ‚úÖ Created professor: Ricks JACOB with email: jacobricks@smu.edu.sg\n",
      "2025-06-11 15:22:17,890 - INFO - ‚úÖ Created professor: Hara KOTARO with email: kotarohara@smu.edu.sg\n",
      "2025-06-11 15:22:17,890 - INFO - ‚úÖ Created 11 new professors\n",
      "2025-06-11 15:22:17,891 - INFO - üìö Processing courses...\n",
      "2025-06-11 15:22:18,862 - INFO - ‚úÖ Created 141 new courses\n",
      "2025-06-11 15:22:18,862 - INFO - ‚úÖ Updated 1158 existing courses\n",
      "2025-06-11 15:22:18,863 - INFO - ‚ö†Ô∏è  141 courses need faculty assignment\n",
      "2025-06-11 15:22:18,876 - INFO - \n",
      "üéì Running automated faculty mapping...\n",
      "2025-06-11 15:22:18,876 - INFO - üéì Starting automated faculty mapping from BOSS data...\n",
      "2025-06-11 15:22:18,879 - INFO - üìö Loaded 8 faculties from cache\n",
      "2025-06-11 15:22:18,880 - INFO - üìÇ Found 14 BOSS files for faculty mapping\n",
      "2025-06-11 15:22:21,470 - INFO - ‚úÖ Extracted 406 course-department pairs from 2021-22_T2.xlsx\n",
      "2025-06-11 15:22:21,483 - INFO - ‚úÖ Extracted 1 course-department pairs from 2021-22_T3B.xlsx\n",
      "2025-06-11 15:22:24,306 - INFO - ‚úÖ Extracted 348 course-department pairs from 2022-23_T1.xlsx\n",
      "2025-06-11 15:22:26,509 - INFO - ‚úÖ Extracted 392 course-department pairs from 2022-23_T2.xlsx\n",
      "2025-06-11 15:22:26,530 - INFO - ‚úÖ Extracted 10 course-department pairs from 2022-23_T3A.xlsx\n",
      "2025-06-11 15:22:26,543 - INFO - ‚úÖ Extracted 3 course-department pairs from 2022-23_T3B.xlsx\n",
      "2025-06-11 15:22:29,742 - INFO - ‚úÖ Extracted 341 course-department pairs from 2023-24_T1.xlsx\n",
      "2025-06-11 15:22:32,113 - INFO - ‚úÖ Extracted 419 course-department pairs from 2023-24_T2.xlsx\n",
      "2025-06-11 15:22:32,135 - INFO - ‚úÖ Extracted 9 course-department pairs from 2023-24_T3A.xlsx\n",
      "2025-06-11 15:22:32,167 - INFO - ‚úÖ Extracted 22 course-department pairs from 2023-24_T3B.xlsx\n",
      "2025-06-11 15:22:35,214 - INFO - ‚úÖ Extracted 392 course-department pairs from 2024-25_T1.xlsx\n",
      "2025-06-11 15:22:36,144 - INFO - ‚úÖ Extracted 411 course-department pairs from 2024-25_T2.xlsx\n",
      "2025-06-11 15:22:36,180 - INFO - ‚úÖ Extracted 24 course-department pairs from 2024-25_T3B.xlsx\n",
      "2025-06-11 15:22:36,182 - INFO - üìã Combined 802 unique course-department pairs\n",
      "2025-06-11 15:22:36,184 - INFO - üèõÔ∏è Unique departments found in BOSS data: ['C4SR', 'CEC', 'LKCSOB', 'OCC', 'OCS', 'SIS', 'SOA', 'SOE', 'SOL', 'SOLGPO', 'SOSS']\n",
      "2025-06-11 15:22:36,204 - INFO - üÜï Unmapped department: OCS (for course COR1001)\n",
      "2025-06-11 15:22:36,204 - INFO - üÜï Unmapped department: C4SR (for course COR2001)\n",
      "2025-06-11 15:22:36,216 - INFO - üÜï Unmapped department: OCS (for course FTW200)\n",
      "2025-06-11 15:22:36,225 - INFO - ‚úÖ Mapped 797 courses to existing faculties\n",
      "2025-06-11 15:22:36,226 - INFO - üÜï Found 2 unmapped departments: ['C4SR', 'OCS']\n",
      "2025-06-11 15:22:36,226 - INFO - üèóÔ∏è Creating 2 new faculties...\n",
      "2025-06-11 15:22:36,227 - INFO - ‚úÖ Created faculty: Centre for Strategic & Regional Studies (C4SR) with ID 9\n",
      "2025-06-11 15:22:36,227 - INFO - ‚úÖ Created faculty: Office of Corporate & Student Relations (OCS) with ID 10\n",
      "2025-06-11 15:22:36,232 - INFO - üíæ Saved 2 new faculties to script_output\\verify\\new_faculties.csv\n",
      "2025-06-11 15:22:36,270 - INFO - üîÑ Applying faculty mappings to 704 courses...\n",
      "2025-06-11 15:22:36,271 - INFO - ‚úÖ Applied faculty mappings:\n",
      "2025-06-11 15:22:36,272 - INFO -    ‚Ä¢ 71 courses updated with faculty\n",
      "2025-06-11 15:22:36,272 - INFO -    ‚Ä¢ 71 courses removed from manual review queue\n",
      "2025-06-11 15:22:36,273 - INFO -    ‚Ä¢ 70 courses still need manual review\n",
      "2025-06-11 15:22:36,273 - INFO - ‚úÖ Automated faculty mapping completed:\n",
      "2025-06-11 15:22:36,274 - INFO -    ‚Ä¢ 800 courses mapped to faculties\n",
      "2025-06-11 15:22:36,275 - INFO -    ‚Ä¢ 2 new faculties created\n",
      "2025-06-11 15:22:36,275 - INFO -    ‚Ä¢ 70 courses still need manual review\n",
      "2025-06-11 15:22:36,276 - INFO - üìÖ Processing academic terms...\n",
      "2025-06-11 15:22:36,982 - INFO - ‚úÖ Created academic term: AY202122T1 (term: 1)\n",
      "2025-06-11 15:22:36,988 - INFO - ‚úÖ Created academic term: AY202122T2 (term: 2)\n",
      "2025-06-11 15:22:36,988 - INFO - ‚úÖ Created academic term: AY202122T3A (term: 3A)\n",
      "2025-06-11 15:22:36,989 - INFO - ‚úÖ Created academic term: AY202122T3B (term: 3B)\n",
      "2025-06-11 15:22:36,994 - INFO - ‚úÖ Created academic term: AY202223T1 (term: 1)\n",
      "2025-06-11 15:22:37,001 - INFO - ‚úÖ Created academic term: AY202223T2 (term: 2)\n",
      "2025-06-11 15:22:37,001 - INFO - ‚úÖ Created academic term: AY202223T3A (term: 3A)\n",
      "2025-06-11 15:22:37,002 - INFO - ‚úÖ Created academic term: AY202223T3B (term: 3B)\n",
      "2025-06-11 15:22:37,008 - INFO - ‚úÖ Created academic term: AY202324T1 (term: 1)\n",
      "2025-06-11 15:22:37,014 - INFO - ‚úÖ Created academic term: AY202324T2 (term: 2)\n",
      "2025-06-11 15:22:37,015 - INFO - ‚úÖ Created academic term: AY202324T3A (term: 3A)\n",
      "2025-06-11 15:22:37,016 - INFO - ‚úÖ Created academic term: AY202324T3B (term: 3B)\n",
      "2025-06-11 15:22:37,022 - INFO - ‚úÖ Created academic term: AY202425T1 (term: 1)\n",
      "2025-06-11 15:22:37,028 - INFO - ‚úÖ Created academic term: AY202425T2 (term: 2)\n",
      "2025-06-11 15:22:37,028 - INFO - ‚úÖ Created academic term: AY202425T3A (term: 3A)\n",
      "2025-06-11 15:22:37,029 - INFO - ‚úÖ Created academic term: AY202425T3B (term: 3B)\n",
      "2025-06-11 15:22:37,029 - INFO - ‚úÖ Created 16 new academic terms\n",
      "2025-06-11 15:22:37,042 - INFO - ‚úÖ Saved 11 new professors for review\n",
      "2025-06-11 15:22:37,047 - INFO - ‚úÖ Saved 141 new courses\n",
      "2025-06-11 15:22:37,063 - INFO - ‚úÖ Saved 1158 course updates\n",
      "2025-06-11 15:22:37,065 - INFO - ‚úÖ Saved 16 academic terms\n",
      "2025-06-11 15:22:37,066 - INFO - \n",
      "üìã Faculty Assignment Summary:\n",
      "2025-06-11 15:22:37,067 - INFO -    ‚Ä¢ Automated mappings applied to 71 courses\n",
      "2025-06-11 15:22:37,067 - INFO -    ‚Ä¢ 70 courses still need manual review\n",
      "2025-06-11 15:22:37,068 - INFO - ‚úÖ Phase 1 completed - Review files in verify/ folder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Phase 1 completed successfully!\n",
      "üìù Next steps:\n",
      "   1. Review script_output/verify/new_professors.csv\n",
      "   2. Manually correct any professor names if needed\n",
      "   3. Run Phase 2 in the next cell\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TableBuilder\n",
    "builder = TableBuilder()\n",
    "\n",
    "# Run Phase 1 (professors, courses, acad_terms)\n",
    "success = builder.run_phase1_professors_and_courses()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Phase 1 completed successfully!\")\n",
    "    print(\"üìù Next steps:\")\n",
    "    print(\"   1. Review script_output/verify/new_professors.csv\")\n",
    "    print(\"   2. Manually correct any professor names if needed\")\n",
    "    print(\"   3. Run Phase 2 in the next cell\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Phase 1 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 2: Professor Review Interface**\n",
    "```python\n",
    "# Display new professors for review\n",
    "new_prof_path = os.path.join('script_output', 'verify', 'new_professors.csv')\n",
    "df = pd.read_csv(new_prof_path)\n",
    "display(df[['name', 'boss_name', 'afterclass_name', 'original_scraped_name']])\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **After Phase 1 completion**: Review professor names before final processing\n",
    "- **Quality assurance**: Verify name normalization accuracy for Asian and Western names\n",
    "- **Before database insertion**: Ensure all professor names are correctly formatted\n",
    "\n",
    "**What It Does:**\n",
    "- Displays newly created professors with different name formats\n",
    "- Shows original scraped names vs. normalized versions\n",
    "- Provides clear guidance on which column to edit (name = afterclass format)\n",
    "- Preserves boss_name format for database consistency\n",
    "\n",
    "**Manual Review Process:**\n",
    "- Check `name` column for proper Title Case formatting\n",
    "- Verify Asian surnames are correctly identified and positioned\n",
    "- Correct any obvious parsing errors (e.g., \"TSE, JUSTIN K, AIDAN WONG\" cases)\n",
    "- Save changes directly to the CSV file for Phase 2 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã 11 new professors created:\n",
      "\n",
      "üîç Review these professor names:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>boss_name</th>\n",
       "      <th>afterclass_name</th>\n",
       "      <th>original_scraped_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HARA Kotaro</td>\n",
       "      <td>HARA KOTARO</td>\n",
       "      <td>HARA Kotaro</td>\n",
       "      <td>HARA KOTARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HONG Jiaqi</td>\n",
       "      <td>HONG JIAQI</td>\n",
       "      <td>HONG Jiaqi</td>\n",
       "      <td>HONG JIAQI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HU Naiyuan</td>\n",
       "      <td>HU NAIYUAN</td>\n",
       "      <td>HU Naiyuan</td>\n",
       "      <td>HU NAIYUAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KOH Andrew</td>\n",
       "      <td>KOH ANDREW</td>\n",
       "      <td>KOH Andrew</td>\n",
       "      <td>KOH ANDREW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LEE Yun</td>\n",
       "      <td>LEE YUN</td>\n",
       "      <td>LEE Yun</td>\n",
       "      <td>LEE YUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PEPITO Nona</td>\n",
       "      <td>PEPITO NONA</td>\n",
       "      <td>PEPITO Nona</td>\n",
       "      <td>PEPITO NONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RICKS Jacob</td>\n",
       "      <td>RICKS JACOB</td>\n",
       "      <td>RICKS Jacob</td>\n",
       "      <td>RICKS JACOB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TANG Tony</td>\n",
       "      <td>TANG TONY</td>\n",
       "      <td>TANG Tony</td>\n",
       "      <td>TANG TONY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YU Qi</td>\n",
       "      <td>YU QI</td>\n",
       "      <td>YU Qi</td>\n",
       "      <td>YU QI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ZENG Qingli</td>\n",
       "      <td>ZENG QINGLI</td>\n",
       "      <td>ZENG Qingli</td>\n",
       "      <td>ZENG QINGLI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZHANG Ce</td>\n",
       "      <td>ZHANG CE</td>\n",
       "      <td>ZHANG Ce</td>\n",
       "      <td>ZHANG CE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name    boss_name afterclass_name original_scraped_name\n",
       "0   HARA Kotaro  HARA KOTARO     HARA Kotaro           HARA KOTARO\n",
       "1    HONG Jiaqi   HONG JIAQI      HONG Jiaqi            HONG JIAQI\n",
       "2    HU Naiyuan   HU NAIYUAN      HU Naiyuan            HU NAIYUAN\n",
       "3    KOH Andrew   KOH ANDREW      KOH Andrew            KOH ANDREW\n",
       "4       LEE Yun      LEE YUN         LEE Yun               LEE YUN\n",
       "5   PEPITO Nona  PEPITO NONA     PEPITO Nona           PEPITO NONA\n",
       "6   RICKS Jacob  RICKS JACOB     RICKS Jacob           RICKS JACOB\n",
       "7     TANG Tony    TANG TONY       TANG Tony             TANG TONY\n",
       "8         YU Qi        YU QI           YU Qi                 YU QI\n",
       "9   ZENG Qingli  ZENG QINGLI     ZENG Qingli           ZENG QINGLI\n",
       "10     ZHANG Ce     ZHANG CE        ZHANG Ce              ZHANG CE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù If any names need correction, edit the 'name' column in:\n",
      "   script_output\\verify\\new_professors.csv\n",
      "\n",
      "‚ö†Ô∏è  Only edit the 'name' column (afterclass format)\n",
      "   Keep 'boss_name' unchanged\n"
     ]
    }
   ],
   "source": [
    "# Display new professors for review\n",
    "new_prof_path = os.path.join('script_output', 'verify', 'new_professors.csv')\n",
    "if os.path.exists(new_prof_path):\n",
    "    df = pd.read_csv(new_prof_path)\n",
    "    print(f\"üìã {len(df)} new professors created:\")\n",
    "    print(\"\\nüîç Review these professor names:\")\n",
    "    display(df[['name', 'boss_name', 'afterclass_name', 'original_scraped_name']])\n",
    "    print(\"\\nüìù If any names need correction, edit the 'name' column in:\")\n",
    "    print(f\"   {new_prof_path}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Only edit the 'name' column (afterclass format)\")\n",
    "    print(\"   Keep 'boss_name' unchanged\")\n",
    "else:\n",
    "    print(\"‚ùå new_professors.csv not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 3: Phase 2 Completion**\n",
    "```python\n",
    "# Run Phase 2 (classes, timings) after manual correction\n",
    "success = builder.run_phase2_remaining_tables()\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **After manual professor review**: Following corrections to new_professors.csv\n",
    "- **Final data processing**: Complete the database table generation pipeline\n",
    "- **Before database insertion**: Generate all remaining tables with correct relationships\n",
    "\n",
    "**What It Does:**\n",
    "- Updates internal professor lookup from manually corrected CSV files\n",
    "- Processes classes using corrected professor mappings and course relationships\n",
    "- Generates class timing and exam timing records linked to classes\n",
    "- Creates complete set of database-ready CSV files\n",
    "- Maintains referential integrity across all generated tables\n",
    "\n",
    "**Output Generation:**\n",
    "- Links professors to classes using updated lookup mappings\n",
    "- Ensures all timing records reference valid class IDs\n",
    "- Produces final statistics and file summaries for database insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:22:55,992 - INFO - üöÄ Starting Phase 2: Classes and Timings\n",
      "2025-06-11 15:22:55,993 - INFO - ============================================================\n",
      "2025-06-11 15:22:55,993 - INFO - üîÑ Updating professor lookup from corrected CSV...\n",
      "2025-06-11 15:22:55,995 - INFO - üìñ Reading 11 corrected professor records\n",
      "2025-06-11 15:22:56,096 - INFO - ‚úÖ Saved updated professor lookup with 1127 entries\n",
      "2025-06-11 15:22:56,097 - INFO - ‚úÖ Updated 11 professor lookup entries\n",
      "2025-06-11 15:22:56,097 - INFO - ‚úÖ Rebuilt 11 professor records with corrections\n",
      "2025-06-11 15:22:56,097 - INFO - üè´ Processing remaining tables (classes, timings)...\n",
      "2025-06-11 15:22:56,098 - INFO - üè´ Processing classes...\n",
      "2025-06-11 15:22:57,272 - INFO - ‚úÖ Created 12973 new classes\n",
      "2025-06-11 15:22:57,273 - INFO - ‚è∞ Processing class timings and exam timings...\n",
      "2025-06-11 15:22:58,415 - INFO - ‚úÖ Created 13082 class timings\n",
      "2025-06-11 15:22:58,415 - INFO - ‚úÖ Created 6904 exam timings\n",
      "2025-06-11 15:22:58,416 - INFO - ‚úÖ Remaining tables processed successfully\n",
      "2025-06-11 15:22:58,417 - INFO - üíæ Saving output files...\n",
      "2025-06-11 15:22:58,422 - INFO - ‚úÖ Saved 141 new courses\n",
      "2025-06-11 15:22:58,439 - INFO - ‚úÖ Saved 1158 course updates\n",
      "2025-06-11 15:22:58,441 - INFO - ‚úÖ Saved 16 academic terms\n",
      "2025-06-11 15:22:58,500 - INFO - ‚úÖ Saved 12973 classes\n",
      "2025-06-11 15:22:58,542 - INFO - ‚úÖ Saved 13082 class timings\n",
      "2025-06-11 15:22:58,564 - INFO - ‚úÖ Saved 6904 exam timings\n",
      "2025-06-11 15:22:58,566 - INFO - ‚úÖ Saved 70 courses needing faculty assignment\n",
      "2025-06-11 15:22:58,567 - INFO - ‚úÖ Phase 2 completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä PROCESSING SUMMARY\n",
      "======================================================================\n",
      "‚úÖ Professors created: 11\n",
      "‚úÖ Courses created: 141\n",
      "‚úÖ Courses updated: 1158\n",
      "‚ö†Ô∏è  Courses needing faculty: 141\n",
      "‚úÖ Classes created: 12973\n",
      "‚úÖ Class timings created: 13082\n",
      "‚úÖ Exam timings created: 6904\n",
      "======================================================================\n",
      "\n",
      "üìÅ OUTPUT FILES:\n",
      "   Verify folder: script_output\\verify/\n",
      "   - new_professors.csv (11 records)\n",
      "   - new_courses.csv (141 records)\n",
      "   Output folder: script_output/\n",
      "   - update_courses.csv (1158 records)\n",
      "   - new_acad_term.csv (16 records)\n",
      "   - new_classes.csv (12973 records)\n",
      "   - new_class_timing.csv (13082 records)\n",
      "   - new_class_exam_timing.csv (6904 records)\n",
      "   - professor_lookup.csv (updated)\n",
      "   - courses_needing_faculty.csv (141 records)\n",
      "======================================================================\n",
      "\n",
      "üéâ Phase 2 completed successfully!\n",
      "üìù All tables generated with corrected professor names\n"
     ]
    }
   ],
   "source": [
    "# Run Phase 2 (classes, timings) after manual correction\n",
    "success = builder.run_phase2_remaining_tables()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Phase 2 completed successfully!\")\n",
    "    print(\"üìù All tables generated with corrected professor names\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Phase 2 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 4: Faculty Assignment (Optional)**\n",
    "```python\n",
    "# Run faculty assignment process if needed\n",
    "if hasattr(builder, 'courses_needing_faculty') and builder.courses_needing_faculty:\n",
    "    builder.assign_course_faculties()\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **New course processing**: When courses lack faculty assignments in manual mapping\n",
    "- **Interactive assignment**: For courses requiring human judgment on faculty placement\n",
    "- **Policy compliance**: Ensuring all courses are properly assigned to SMU schools\n",
    "\n",
    "**What It Does:**\n",
    "- Opens course outline URLs in web browser for informed decision-making\n",
    "- Presents interactive menu of SMU's 8 schools and centers\n",
    "- Updates course records with selected faculty assignments\n",
    "- Re-saves CSV files with complete faculty information\n",
    "\n",
    "**Faculty Options Available:**\n",
    "1. Lee Kong Chian School of Business\n",
    "2. Yong Pung How School of Law  \n",
    "3. School of Economics\n",
    "4. School of Computing and Information Systems\n",
    "5. School of Social Sciences\n",
    "6. School of Accountancy\n",
    "7. College of Integrative Studies\n",
    "8. Center for English Communication\n",
    "\n",
    "**Best Practices:**\n",
    "- Review course outlines before making faculty assignments\n",
    "- Use existing course patterns as reference for similar courses\n",
    "- Skip courses requiring additional research (can be assigned later)\n",
    "- Ensure consistency with SMU's academic structure and course offerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:24:23,493 - INFO - üéì Starting faculty assignment for 70 courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ISFS603\n",
      "Course Name: Corporate Banking and Blockchain\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2022/SCISGPO/2210/ISFS603_ PAUL GRIFFIN.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW101\n",
      "Course Name: Contract Law 1\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/UGRD2021/SOL/2110/LAW101_LOCKNIE HSU.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW201\n",
      "Course Name: Law of Business Organisations\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/UGRD2021/SOL/2110/LAW201_STEPHEN BULL.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW106\n",
      "Course Name: Legal Research and Writing I\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/UGRD2021/SOL/2110/LAW106_ONG EE ING.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW202\n",
      "Course Name: Law of Property\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/UGRD2021/SOL/2110/LAW202_ALVIN SEE WEI-LIANG.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW301\n",
      "Course Name: Legal Theory & Philosophy\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW108\n",
      "Course Name: The Singapore Legal System\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/UGRD2021/SOL/2110/LAW108_ALVIN SEE WEI-LIANG.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: HIST005\n",
      "Course Name: European Cultural History: From Antiquity to Brexit\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/UGRD2021/SOSS/2110/HIST005_David%20Ocon.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: FNCE6040\n",
      "Course Name: Finance\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2021/LKCSBGPO/2120/FNCE6038_6039_6040_WANG RONG.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: FNCE6038\n",
      "Course Name: Finance\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2021/LKCSBGPO/2120/FNCE6038_6039_6040_WANG RONG.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: FNCE6039\n",
      "Course Name: Finance\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2022/LKCSBGPO/2220/FNCE6039_Wang Rong.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: PPPM103\n",
      "Course Name: Understanding Government: Public Sector Change\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/UGRD2021/SOSS/2110/PPPM103_SEULKI LEE.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: COMM675\n",
      "Course Name: Storytelling for Leaders and Brands\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MGMT6056\n",
      "Course Name: Strategic Management\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IS711\n",
      "Course Name: Learning and Planning in Intelligent Systems\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IS715\n",
      "Course Name: Systems Security\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: HLCR601\n",
      "Course Name: Strategy and Global Healthcare Landscape\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: OBHR621\n",
      "Course Name: Leadership and People Management\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: OBHR645\n",
      "Course Name: Leading YourSelf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: FNCE6063\n",
      "Course Name: Mergers & Acquisitions\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS624\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS625\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS618\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS616\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS615\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS622\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS614\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS617\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS623\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ACCT654\n",
      "Course Name: Accounting Analytics Capstone\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MGMT6058\n",
      "Course Name: Human Capital and Global Business Strategy\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MGMT6057\n",
      "Course Name: Business Acceleration and Growth Strategy\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MKTG639\n",
      "Course Name: Advanced Marketing Strategy\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ISSS610\n",
      "Course Name: Applied Machine Learning\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2021/SCISGPO/2120/ISSS610_DAI BING TIAN.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IS702\n",
      "Course Name: Information Security\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IS706\n",
      "Course Name: Software Mining and Analysis\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IS712\n",
      "Course Name: Machine Learning\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: HLCR602\n",
      "Course Name: Aligning Healthcare Strategy and Customer Value Delivery\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: HLCR603\n",
      "Course Name: Health Economics & Financial Management\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: FTW100\n",
      "Course Name: Finishing Touch Workshops (Year One)\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: QF608\n",
      "Course Name: Research Methods for Quantitative Professionals\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS619\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS620\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IDIS621\n",
      "Course Name: Postgraduate Professional Development\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MGMT6034\n",
      "Course Name: Ethics and Corporate Social Responsibility\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2022/LKCSBGPO/2210/MGMT 6034_Heli Wang and Ronald Tay.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: COMM673\n",
      "Course Name: Crisis Communication & Leadership\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2022/LKCSBGPO/2210/COMM673_Augustine Pang.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ISFS602\n",
      "Course Name: Digital Transformation in Retail Banking Technology\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2022/SCISGPO/2220/ISFS602_RANDALL EUGENE DURAN.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: IS708\n",
      "Course Name: Mobile and Distributed Systems\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MGMT754\n",
      "Course Name: Innovation and Entrepreneurship\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: INTS703\n",
      "Course Name: Ways of Thinking About Asian Smart Cities\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: INTS704\n",
      "Course Name: Sustainable Urban Development: Foundations and Best Practices\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MGMT6092\n",
      "Course Name: Southeast Asian Business Immersion\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ACCT684\n",
      "Course Name: Automation for Finance Transformation\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOAGPO/2420/ACCT684_HE JIANZHONG.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: COR601\n",
      "Course Name: Principles of Circular Economy\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOSSGPO/2420/COR601_AIDAN WONG.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: PPPM604\n",
      "Course Name: Collaborative Governance\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOSSGPO/2420/PPPM604_IJLAL NAQVI.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: SOCG600\n",
      "Course Name: Inequalities in Sustainability\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOSSGPO/2420/SOCG600_YASMIN ORTIGA.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: SSOC600\n",
      "Course Name: Energy Transitions for Sustainability\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOSSGPO/2420/SSOC600_ISHANI MUKHERJEE.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: POSC601\n",
      "Course Name: Development, Poverty and Sustainability\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOSSGPO/2420/POSC601_JOHN ANDREW DONALDSON.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LGST613\n",
      "Course Name: Sustainability Law\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOSSGPO/2420/LGST613_SOSS INSTRUCTOR 15.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: SSOC601\n",
      "Course Name: Digital Technologies for Global Sustainability: Practical Solutions for Industry\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOSSGPO/2420/SSOC601_ANDREW KOH.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW661\n",
      "Course Name: Tax Issues in Private Wealth Practice\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/YPHSLGPO/2420/LAW661_Vincent Ooi_0.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: LAW662\n",
      "Course Name: Comparative Corporate Law and Governance in Asia\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/YPHSLGPO/2420/LAW662_Dan Puchniak.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MGMT6093\n",
      "Course Name: Doing Business in Southeast Asia\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ECON6032\n",
      "Course Name: Strategic Thinking\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SOEGPO/2420/ECON6032_Xue Jingyi.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: CS614\n",
      "Course Name: Generative AI with Large Language Models\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2024/SCISGPO/2420/CS614_LAURA WYNTER_0.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: PSYC607\n",
      "Course Name: Cognitive Psychology\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: MKTG717\n",
      "Course Name: Sustainability Marketing\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ECON755\n",
      "Course Name: Topics in International Macroeconomics\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: COR-INTS2643\n",
      "Course Name: Exploring Music and Culture: An Interdisciplinary Approach\n",
      "Opening course outline: https://courses.smu.edu.sg//sites/courses.smu.edu.sg/files/UGRD2024/CIS/2431/COR-INTS2643_Fiona Clare Williamson.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "\n",
      "============================================================\n",
      "üéì FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: COR-INTS2644\n",
      "Course Name: Arts Entrepreneurship and Public Engagement in Southeast Asia\n",
      "Opening course outline: https://courses.smu.edu.sg//sites/courses.smu.edu.sg/files/UGRD2024/CIS/2431/COR-INTS2644_Darlene ESPENA.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 15:28:52,762 - INFO - ‚úÖ Updated new_courses.csv with faculty assignments\n",
      "2025-06-11 15:28:52,763 - INFO - ‚úÖ Faculty assignment completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Faculty assignment completed!\n"
     ]
    }
   ],
   "source": [
    "# Run faculty assignment process if needed\n",
    "if hasattr(builder, 'courses_needing_faculty') and builder.courses_needing_faculty:\n",
    "    builder.assign_course_faculties()\n",
    "    print(\"\\n‚úÖ Faculty assignment completed!\")\n",
    "else:\n",
    "    print(\"‚úÖ No courses need faculty assignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Running Enhanced Course Synchronization\n",
      "   Recommended timing: After Phase 2, before Phase 3\n",
      "============================================================\n",
      "üîÑ Starting enhanced course synchronization...\n",
      "   Priority: Automated mappings > Manual mappings\n",
      "‚úÖ Loaded 99 manually mapped courses\n",
      "‚úÖ Loaded 141 automated courses\n",
      "\n",
      "üìä Course Analysis:\n",
      "   ‚Ä¢ Automated only: 42 courses\n",
      "   ‚Ä¢ Manual only: 0 courses\n",
      "   ‚Ä¢ Overlapping: 99 courses\n",
      "   ‚Ä¢ Total unique: 141 courses\n",
      "\n",
      "ü§ñ Using 141 automated courses as base\n",
      "\n",
      "üîÑ Processing 99 overlapping courses:\n",
      "\n",
      "‚úÖ Final course set: 141 courses\n",
      "   Strategy: Automated (141) + Manual-only (0)\n",
      "üîó Created 99 UUID mappings (manual ‚Üí automated)\n",
      "üóëÔ∏è  Will delete 99 manual UUIDs (replaced by automated)\n",
      "üíæ Saved final merged courses to script_output\\verify\\new_courses.csv\n",
      "\n",
      "üîÑ Updating dependent tables...\n",
      "üíæ Updated classes: 12973 ‚Üí 12973 (0 deleted)\n",
      "üíæ Updated class timings: 13082 ‚Üí 13082 (0 deleted)\n",
      "üíæ Updated exam timings: 6904 ‚Üí 6904 (0 deleted)\n",
      "\n",
      "‚úÖ Enhanced course synchronization completed!\n",
      "üìä Final Summary:\n",
      "   ‚Ä¢ Total courses: 141\n",
      "   ‚Ä¢ Automated courses: 141 (prioritized)\n",
      "   ‚Ä¢ Manual-only courses: 0 (supplemented)\n",
      "   ‚Ä¢ Conflicts detected: 0 (review conflicts file)\n",
      "   ‚Ä¢ Classes updated: 12973\n",
      "\n",
      "‚úÖ Course synchronization completed successfully!\n",
      "\n",
      "üìù Next steps:\n",
      "   1. Review course_mapping_conflicts.csv if it exists\n",
      "   2. Resolve any conflicts manually if needed\n",
      "   3. Proceed with Phase 3 (BOSS results processing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sync_courses_with_manual_mapping():\n",
    "    \"\"\"\n",
    "    Enhanced sync script: Prioritize automated mappings over manual ones\n",
    "    Run after Phase 2 to merge automated (script-generated) and manual course mappings\n",
    "    \"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    manual_courses_path = r'extracted_data\\3. new_courses.csv'\n",
    "    script_courses_path = r'script_output\\verify\\new_courses.csv'\n",
    "    \n",
    "    print(\"üîÑ Starting enhanced course synchronization...\")\n",
    "    print(\"   Priority: Automated mappings > Manual mappings\")\n",
    "    \n",
    "    # Load manually mapped courses\n",
    "    try:\n",
    "        manual_courses = pd.read_csv(manual_courses_path)\n",
    "        print(f\"‚úÖ Loaded {len(manual_courses)} manually mapped courses\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Manual courses file not found: {manual_courses_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Load script-generated courses (automated)\n",
    "    try:\n",
    "        script_courses = pd.read_csv(script_courses_path)\n",
    "        print(f\"‚úÖ Loaded {len(script_courses)} automated courses\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Automated courses file not found: {script_courses_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Create mapping sets for analysis\n",
    "    manual_codes = set(manual_courses['code'])\n",
    "    script_codes = set(script_courses['code'])\n",
    "    \n",
    "    # Categorize courses\n",
    "    automated_only = script_codes - manual_codes  # Courses only in automated\n",
    "    manual_only = manual_codes - script_codes     # Courses only in manual\n",
    "    overlapping = script_codes & manual_codes     # Courses in both\n",
    "    \n",
    "    print(f\"\\nüìä Course Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Automated only: {len(automated_only)} courses\")\n",
    "    print(f\"   ‚Ä¢ Manual only: {len(manual_only)} courses\") \n",
    "    print(f\"   ‚Ä¢ Overlapping: {len(overlapping)} courses\")\n",
    "    print(f\"   ‚Ä¢ Total unique: {len(script_codes | manual_codes)} courses\")\n",
    "    \n",
    "    # STRATEGY: Use automated mappings as primary, supplement with manual where needed\n",
    "    \n",
    "    # Step 1: Start with all automated courses (these are more accurate)\n",
    "    final_courses = script_courses.copy()\n",
    "    print(f\"\\nü§ñ Using {len(final_courses)} automated courses as base\")\n",
    "    \n",
    "    # Step 2: Add manual-only courses (courses that automation missed)\n",
    "    manual_only_courses = manual_courses[manual_courses['code'].isin(manual_only)].copy()\n",
    "    if len(manual_only_courses) > 0:\n",
    "        # For manual-only courses, we need to verify they're still valid\n",
    "        print(f\"\\nüìã Found {len(manual_only_courses)} manual-only courses:\")\n",
    "        for _, course in manual_only_courses.iterrows():\n",
    "            print(f\"   - {course['code']}: {course.get('name', 'Unknown')}\")\n",
    "        \n",
    "        # Add them to final courses\n",
    "        final_courses = pd.concat([final_courses, manual_only_courses], ignore_index=True)\n",
    "        print(f\"‚úÖ Added {len(manual_only_courses)} manual-only courses\")\n",
    "    \n",
    "    # Step 3: For overlapping courses, prioritize automated mappings but review\n",
    "    print(f\"\\nüîÑ Processing {len(overlapping)} overlapping courses:\")\n",
    "    overlapping_conflicts = []\n",
    "    \n",
    "    for code in overlapping:\n",
    "        auto_course = script_courses[script_courses['code'] == code].iloc[0]\n",
    "        manual_course = manual_courses[manual_courses['code'] == code].iloc[0]\n",
    "        \n",
    "        # Check for conflicts in key fields\n",
    "        conflicts = []\n",
    "        key_fields = ['name', 'belong_to_faculty']\n",
    "        \n",
    "        for field in key_fields:\n",
    "            auto_val = auto_course.get(field)\n",
    "            manual_val = manual_course.get(field)\n",
    "            \n",
    "            if pd.notna(auto_val) and pd.notna(manual_val) and auto_val != manual_val:\n",
    "                conflicts.append(f\"{field}: auto='{auto_val}' vs manual='{manual_val}'\")\n",
    "        \n",
    "        if conflicts:\n",
    "            overlapping_conflicts.append({\n",
    "                'code': code,\n",
    "                'conflicts': conflicts,\n",
    "                'auto_course': auto_course.to_dict(),\n",
    "                'manual_course': manual_course.to_dict()\n",
    "            })\n",
    "    \n",
    "    if overlapping_conflicts:\n",
    "        print(f\"‚ö†Ô∏è  Found {len(overlapping_conflicts)} courses with conflicts:\")\n",
    "        for conflict in overlapping_conflicts[:5]:  # Show first 5\n",
    "            print(f\"   - {conflict['code']}: {'; '.join(conflict['conflicts'])}\")\n",
    "        \n",
    "        # Save conflicts for review\n",
    "        conflicts_df = pd.DataFrame([\n",
    "            {\n",
    "                'course_code': c['code'],\n",
    "                'conflict_summary': '; '.join(c['conflicts']),\n",
    "                **{f\"auto_{k}\": v for k, v in c['auto_course'].items()},\n",
    "                **{f\"manual_{k}\": v for k, v in c['manual_course'].items()}\n",
    "            }\n",
    "            for c in overlapping_conflicts\n",
    "        ])\n",
    "        conflicts_path = r'script_output\\course_mapping_conflicts.csv'\n",
    "        conflicts_df.to_csv(conflicts_path, index=False)\n",
    "        print(f\"üìù Saved conflicts to {conflicts_path}\")\n",
    "        print(f\"   Review and manually resolve if needed\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final course set: {len(final_courses)} courses\")\n",
    "    print(f\"   Strategy: Automated ({len(script_courses)}) + Manual-only ({len(manual_only_courses)})\")\n",
    "    \n",
    "    # Create UUID mappings (manual -> automated for overlapping courses)\n",
    "    uuid_mappings = {}\n",
    "    \n",
    "    # For overlapping courses, map manual UUID -> automated UUID  \n",
    "    for code in overlapping:\n",
    "        manual_uuid = manual_courses[manual_courses['code'] == code].iloc[0]['id']\n",
    "        auto_uuid = script_courses[script_courses['code'] == code].iloc[0]['id']\n",
    "        uuid_mappings[manual_uuid] = auto_uuid\n",
    "    \n",
    "    print(f\"üîó Created {len(uuid_mappings)} UUID mappings (manual ‚Üí automated)\")\n",
    "    \n",
    "    # Identify UUIDs to delete (manual UUIDs that are being replaced)\n",
    "    uuids_to_delete = set(uuid_mappings.keys())\n",
    "    print(f\"üóëÔ∏è  Will delete {len(uuids_to_delete)} manual UUIDs (replaced by automated)\")\n",
    "    \n",
    "    # Save the final merged course set\n",
    "    final_courses.to_csv(script_courses_path, index=False)\n",
    "    print(f\"üíæ Saved final merged courses to {script_courses_path}\")\n",
    "    \n",
    "    # Update dependent tables (classes, timings)\n",
    "    print(f\"\\nüîÑ Updating dependent tables...\")\n",
    "    \n",
    "    # Update classes\n",
    "    classes_path = r'script_output\\new_classes.csv'\n",
    "    if os.path.exists(classes_path):\n",
    "        classes_df = pd.read_csv(classes_path)\n",
    "        original_classes = len(classes_df)\n",
    "        \n",
    "        # Remove classes linked to deleted UUIDs\n",
    "        classes_df_cleaned = classes_df[~classes_df['course_id'].isin(uuids_to_delete)].copy()\n",
    "        deleted_classes = original_classes - len(classes_df_cleaned)\n",
    "        \n",
    "        # Update course_id mappings (manual UUID -> automated UUID)\n",
    "        classes_df_cleaned['course_id'] = classes_df_cleaned['course_id'].map(uuid_mappings).fillna(classes_df_cleaned['course_id'])\n",
    "        \n",
    "        classes_df_cleaned.to_csv(classes_path, index=False)\n",
    "        print(f\"üíæ Updated classes: {original_classes} ‚Üí {len(classes_df_cleaned)} ({deleted_classes} deleted)\")\n",
    "        \n",
    "        valid_class_ids = set(classes_df_cleaned['id'])\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Classes file not found: {classes_path}\")\n",
    "        valid_class_ids = set()\n",
    "    \n",
    "    # Update timing tables\n",
    "    timing_files = [\n",
    "        (r'script_output\\new_class_timing.csv', 'class timings'),\n",
    "        (r'script_output\\new_class_exam_timing.csv', 'exam timings')\n",
    "    ]\n",
    "    \n",
    "    for file_path, description in timing_files:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            original_count = len(df)\n",
    "            \n",
    "            # Filter by valid class_ids\n",
    "            filtered_df = df[df['class_id'].isin(valid_class_ids)].copy()\n",
    "            filtered_count = len(filtered_df)\n",
    "            deleted_count = original_count - filtered_count\n",
    "            \n",
    "            filtered_df.to_csv(file_path, index=False)\n",
    "            print(f\"üíæ Updated {description}: {original_count} ‚Üí {filtered_count} ({deleted_count} deleted)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  File not found: {file_path}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Enhanced course synchronization completed!\")\n",
    "    print(f\"üìä Final Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total courses: {len(final_courses)}\")\n",
    "    print(f\"   ‚Ä¢ Automated courses: {len(script_courses)} (prioritized)\")\n",
    "    print(f\"   ‚Ä¢ Manual-only courses: {len(manual_only_courses)} (supplemented)\")\n",
    "    print(f\"   ‚Ä¢ Conflicts detected: {len(overlapping_conflicts)} (review conflicts file)\")\n",
    "    print(f\"   ‚Ä¢ Classes updated: {len(valid_class_ids) if 'valid_class_ids' in locals() else 'N/A'}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Helper function to run after Phase 2\n",
    "def run_enhanced_course_sync():\n",
    "    \"\"\"\n",
    "    Run this after Phase 2 is completed to merge automated and manual course mappings\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Running Enhanced Course Synchronization\")\n",
    "    print(\"   Recommended timing: After Phase 2, before Phase 3\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    success = sync_courses_with_manual_mapping()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚úÖ Course synchronization completed successfully!\")\n",
    "        print(\"\\nüìù Next steps:\")\n",
    "        print(\"   1. Review course_mapping_conflicts.csv if it exists\")\n",
    "        print(\"   2. Resolve any conflicts manually if needed\")\n",
    "        print(\"   3. Proceed with Phase 3 (BOSS results processing)\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Course synchronization failed. Check file paths and try again.\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Run the synchronization\n",
    "run_enhanced_course_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 5: Extract OverallBossResults Excel Files**\n",
    "\n",
    "```python\n",
    "# Run BOSS results extraction and processing\n",
    "builder.run_phase3_boss_processing()\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **Post-class creation**: After completing Phase 1 (professors/courses) and Phase 2 (classes/timings)\n",
    "- **Bidding data integration**: When processing SMU BOSS bidding results for historical analysis\n",
    "- **Complete pipeline**: As the final step to populate bid windows, class availability, and bid results\n",
    "\n",
    "**What It Does:**\n",
    "- Scans `script_input/overallBossResults/` for all academic year Excel files (e.g., `2021-22_T1.xlsx`)\n",
    "- Parses academic terms into standardized format (e.g., \"2021-22 Term 1\" ‚Üí \"AY202122T1\")\n",
    "- Creates hierarchical bid windows following SMU's round/window progression rules\n",
    "- Maps course codes and sections to existing class records for referential integrity\n",
    "- Extracts bidding metrics: vacancy, enrollment, median/min bids, D.I.C.E scores\n",
    "- Generates comprehensive processing logs with timestamps and statistics\n",
    "\n",
    "**Best Practices:**\n",
    "- Ensure Phase 1 and Phase 2 are completed first (requires existing courses and classes)\n",
    "- Place all BOSS result Excel files in `script_input/overallBossResults/` directory\n",
    "- Review `failed_boss_results_mapping.csv` for any unmatched classes requiring investigation\n",
    "- Check processing logs in `boss_result_log.txt` for detailed operation history\n",
    "- Verify bid window hierarchy follows correct academic year rules (pre/post AY2024-25 differences)\n",
    "- Use step-by-step execution for debugging if full pipeline encounters issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Phase 3: BOSS Results Processing\n",
      "üìù üöÄ Starting Phase 3: BOSS Results Processing\n",
      "üìù ============================================================\n",
      "üìù Log file created: script_output\\boss_result_log.txt\n",
      "üîÑ BOSS results processing setup completed\n",
      "üìù üîç Loading BOSS results files...\n",
      "üìù üìÇ Found 14 XLSX files\n",
      "üìù üìñ Loading: 2021-22_T2.xlsx\n",
      "üìù ‚úÖ Loaded 18032 rows from 2021-22_T2.xlsx\n",
      "üìù üìñ Loading: 2021-22_T3B.xlsx\n",
      "üìù ‚úÖ Loaded 6 rows from 2021-22_T3B.xlsx\n",
      "üìù üìñ Loading: 2022-23_T1.xlsx\n",
      "üìù ‚úÖ Loaded 20103 rows from 2022-23_T1.xlsx\n",
      "üìù üìñ Loading: 2022-23_T2.xlsx\n",
      "üìù ‚úÖ Loaded 15705 rows from 2022-23_T2.xlsx\n",
      "üìù üìñ Loading: 2022-23_T3A.xlsx\n",
      "üìù ‚úÖ Loaded 55 rows from 2022-23_T3A.xlsx\n",
      "üìù üìñ Loading: 2022-23_T3B.xlsx\n",
      "üìù ‚úÖ Loaded 14 rows from 2022-23_T3B.xlsx\n",
      "üìù üìñ Loading: 2023-24_T1.xlsx\n",
      "üìù ‚úÖ Loaded 22100 rows from 2023-24_T1.xlsx\n",
      "üìù üìñ Loading: 2023-24_T2.xlsx\n",
      "üìù ‚úÖ Loaded 16447 rows from 2023-24_T2.xlsx\n",
      "üìù üìñ Loading: 2023-24_T3A.xlsx\n",
      "üìù ‚úÖ Loaded 60 rows from 2023-24_T3A.xlsx\n",
      "üìù üìñ Loading: 2023-24_T3B.xlsx\n",
      "üìù ‚úÖ Loaded 135 rows from 2023-24_T3B.xlsx\n",
      "üìù üìñ Loading: 2024-25_T1.xlsx\n",
      "üìù ‚úÖ Loaded 22317 rows from 2024-25_T1.xlsx\n",
      "üìù üìñ Loading: 2024-25_T2.xlsx\n",
      "üìù ‚úÖ Loaded 6247 rows from 2024-25_T2.xlsx\n",
      "üìù üìñ Loading: 2024-25_T3A.xlsx\n",
      "üìù ‚úÖ Loaded 42 rows from 2024-25_T3A.xlsx\n",
      "üìù üìñ Loading: 2024-25_T3B.xlsx\n",
      "üìù ‚úÖ Loaded 77 rows from 2024-25_T3B.xlsx\n",
      "üìù ‚úÖ Combined 121340 total rows\n",
      "üìù ü™ü Processing bid windows...\n",
      "üìù ‚úÖ Created bid_window 1: AY202122T2 Round 1C Window 1\n",
      "üìù ‚úÖ Created bid_window 2: AY202122T2 Round 1C Window 2\n",
      "üìù ‚úÖ Created bid_window 3: AY202122T2 Round 1C Window 3\n",
      "üìù ‚úÖ Created bid_window 4: AY202122T2 Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 5: AY202122T2 Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 6: AY202122T2 Round 1A Window 1\n",
      "üìù ‚úÖ Created bid_window 7: AY202122T2 Round 1A Window 2\n",
      "üìù ‚úÖ Created bid_window 8: AY202122T2 Round 1B Window 1\n",
      "üìù ‚úÖ Created bid_window 9: AY202122T2 Round 1B Window 2\n",
      "üìù ‚úÖ Created bid_window 10: AY202122T2 Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 11: AY202122T2 Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 12: AY202122T2 Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 13: AY202122T2 Round 2 Window 4\n",
      "üìù ‚úÖ Created bid_window 14: AY202122T2 Round 2 Window 5\n",
      "üìù ‚úÖ Created bid_window 15: AY202122T2 Round 2A Window 1\n",
      "üìù ‚úÖ Created bid_window 16: AY202122T2 Round 2A Window 2\n",
      "üìù ‚úÖ Created bid_window 17: AY202122T2 Round 2A Window 3\n",
      "üìù ‚úÖ Created bid_window 18: AY202122T3B Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 19: AY202122T3B Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 20: AY202122T3B Round 1 Window 3\n",
      "üìù ‚úÖ Created bid_window 21: AY202122T3B Round 1 Window 4\n",
      "üìù ‚úÖ Created bid_window 22: AY202122T3B Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 23: AY202122T3B Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 24: AY202223T1 Round 1C Window 1\n",
      "üìù ‚úÖ Created bid_window 25: AY202223T1 Round 1C Window 2\n",
      "üìù ‚úÖ Created bid_window 26: AY202223T1 Round 1C Window 3\n",
      "üìù ‚úÖ Created bid_window 27: AY202223T1 Round 1F Window 1\n",
      "üìù ‚úÖ Created bid_window 28: AY202223T1 Round 1F Window 2\n",
      "üìù ‚úÖ Created bid_window 29: AY202223T1 Round 1F Window 3\n",
      "üìù ‚úÖ Created bid_window 30: AY202223T1 Round 1F Window 4\n",
      "üìù ‚úÖ Created bid_window 31: AY202223T1 Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 32: AY202223T1 Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 33: AY202223T1 Round 1A Window 1\n",
      "üìù ‚úÖ Created bid_window 34: AY202223T1 Round 1A Window 2\n",
      "üìù ‚úÖ Created bid_window 35: AY202223T1 Round 1B Window 1\n",
      "üìù ‚úÖ Created bid_window 36: AY202223T1 Round 1B Window 2\n",
      "üìù ‚úÖ Created bid_window 37: AY202223T1 Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 38: AY202223T1 Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 39: AY202223T1 Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 40: AY202223T1 Round 2A Window 1\n",
      "üìù ‚úÖ Created bid_window 41: AY202223T1 Round 2A Window 2\n",
      "üìù ‚úÖ Created bid_window 42: AY202223T1 Round 2A Window 3\n",
      "üìù ‚úÖ Created bid_window 43: AY202223T2 Round 1C Window 1\n",
      "üìù ‚úÖ Created bid_window 44: AY202223T2 Round 1C Window 2\n",
      "üìù ‚úÖ Created bid_window 45: AY202223T2 Round 1C Window 3\n",
      "üìù ‚úÖ Created bid_window 46: AY202223T2 Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 47: AY202223T2 Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 48: AY202223T2 Round 1A Window 1\n",
      "üìù ‚úÖ Created bid_window 49: AY202223T2 Round 1A Window 2\n",
      "üìù ‚úÖ Created bid_window 50: AY202223T2 Round 1B Window 1\n",
      "üìù ‚úÖ Created bid_window 51: AY202223T2 Round 1B Window 2\n",
      "üìù ‚úÖ Created bid_window 52: AY202223T2 Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 53: AY202223T2 Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 54: AY202223T2 Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 55: AY202223T2 Round 2A Window 1\n",
      "üìù ‚úÖ Created bid_window 56: AY202223T2 Round 2A Window 2\n",
      "üìù ‚úÖ Created bid_window 57: AY202223T2 Round 2A Window 3\n",
      "üìù ‚úÖ Created bid_window 58: AY202223T3A Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 59: AY202223T3A Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 60: AY202223T3A Round 1 Window 3\n",
      "üìù ‚úÖ Created bid_window 61: AY202223T3A Round 1 Window 4\n",
      "üìù ‚úÖ Created bid_window 62: AY202223T3A Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 63: AY202223T3B Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 64: AY202223T3B Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 65: AY202223T3B Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 66: AY202223T3B Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 67: AY202223T3B Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 68: AY202223T3B Round 2 Window 4\n",
      "üìù ‚úÖ Created bid_window 69: AY202324T1 Round 1C Window 1\n",
      "üìù ‚úÖ Created bid_window 70: AY202324T1 Round 1C Window 2\n",
      "üìù ‚úÖ Created bid_window 71: AY202324T1 Round 1C Window 3\n",
      "üìù ‚úÖ Created bid_window 72: AY202324T1 Round 1F Window 1\n",
      "üìù ‚úÖ Created bid_window 73: AY202324T1 Round 1F Window 2\n",
      "üìù ‚úÖ Created bid_window 74: AY202324T1 Round 1F Window 3\n",
      "üìù ‚úÖ Created bid_window 75: AY202324T1 Round 1F Window 4\n",
      "üìù ‚úÖ Created bid_window 76: AY202324T1 Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 77: AY202324T1 Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 78: AY202324T1 Round 1A Window 1\n",
      "üìù ‚úÖ Created bid_window 79: AY202324T1 Round 1A Window 2\n",
      "üìù ‚úÖ Created bid_window 80: AY202324T1 Round 1B Window 1\n",
      "üìù ‚úÖ Created bid_window 81: AY202324T1 Round 1B Window 2\n",
      "üìù ‚úÖ Created bid_window 82: AY202324T1 Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 83: AY202324T1 Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 84: AY202324T1 Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 85: AY202324T1 Round 2A Window 1\n",
      "üìù ‚úÖ Created bid_window 86: AY202324T1 Round 2A Window 2\n",
      "üìù ‚úÖ Created bid_window 87: AY202324T1 Round 2A Window 3\n",
      "üìù ‚úÖ Created bid_window 88: AY202324T2 Round 1C Window 1\n",
      "üìù ‚úÖ Created bid_window 89: AY202324T2 Round 1C Window 2\n",
      "üìù ‚úÖ Created bid_window 90: AY202324T2 Round 1C Window 3\n",
      "üìù ‚úÖ Created bid_window 91: AY202324T2 Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 92: AY202324T2 Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 93: AY202324T2 Round 1A Window 1\n",
      "üìù ‚úÖ Created bid_window 94: AY202324T2 Round 1A Window 2\n",
      "üìù ‚úÖ Created bid_window 95: AY202324T2 Round 1B Window 1\n",
      "üìù ‚úÖ Created bid_window 96: AY202324T2 Round 1B Window 2\n",
      "üìù ‚úÖ Created bid_window 97: AY202324T2 Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 98: AY202324T2 Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 99: AY202324T2 Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 100: AY202324T2 Round 2A Window 1\n",
      "üìù ‚úÖ Created bid_window 101: AY202324T2 Round 2A Window 2\n",
      "üìù ‚úÖ Created bid_window 102: AY202324T2 Round 2A Window 3\n",
      "üìù ‚úÖ Created bid_window 103: AY202324T3A Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 104: AY202324T3A Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 105: AY202324T3A Round 1 Window 3\n",
      "üìù ‚úÖ Created bid_window 106: AY202324T3A Round 1 Window 4\n",
      "üìù ‚úÖ Created bid_window 107: AY202324T3A Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 108: AY202324T3A Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 109: AY202324T3B Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 110: AY202324T3B Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 111: AY202324T3B Round 1 Window 3\n",
      "üìù ‚úÖ Created bid_window 112: AY202324T3B Round 1 Window 4\n",
      "üìù ‚úÖ Created bid_window 113: AY202324T3B Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 114: AY202324T3B Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 115: AY202324T3B Round 2 Window 4\n",
      "üìù ‚úÖ Created bid_window 116: AY202425T1 Round 1C Window 1\n",
      "üìù ‚úÖ Created bid_window 117: AY202425T1 Round 1C Window 2\n",
      "üìù ‚úÖ Created bid_window 118: AY202425T1 Round 1C Window 3\n",
      "üìù ‚úÖ Created bid_window 119: AY202425T1 Round 1F Window 1\n",
      "üìù ‚úÖ Created bid_window 120: AY202425T1 Round 1F Window 2\n",
      "üìù ‚úÖ Created bid_window 121: AY202425T1 Round 1F Window 3\n",
      "üìù ‚úÖ Created bid_window 122: AY202425T1 Round 1F Window 4\n",
      "üìù ‚úÖ Created bid_window 123: AY202425T1 Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 124: AY202425T1 Round 1A Window 1\n",
      "üìù ‚úÖ Created bid_window 125: AY202425T1 Round 1A Window 2\n",
      "üìù ‚úÖ Created bid_window 126: AY202425T1 Round 1A Window 3\n",
      "üìù ‚úÖ Created bid_window 127: AY202425T1 Round 1B Window 1\n",
      "üìù ‚úÖ Created bid_window 128: AY202425T1 Round 1B Window 2\n",
      "üìù ‚úÖ Created bid_window 129: AY202425T1 Round 2 Window 1\n",
      "üìù ‚úÖ Created bid_window 130: AY202425T1 Round 2 Window 2\n",
      "üìù ‚úÖ Created bid_window 131: AY202425T1 Round 2 Window 3\n",
      "üìù ‚úÖ Created bid_window 132: AY202425T1 Round 2A Window 1\n",
      "üìù ‚úÖ Created bid_window 133: AY202425T1 Round 2A Window 2\n",
      "üìù ‚úÖ Created bid_window 134: AY202425T1 Round 2A Window 3\n",
      "üìù ‚úÖ Created bid_window 135: AY202425T2 Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 136: AY202425T2 Round 1A Window 1\n",
      "üìù ‚úÖ Created bid_window 137: AY202425T2 Round 1A Window 2\n",
      "üìù ‚úÖ Created bid_window 138: AY202425T2 Round 1A Window 3\n",
      "üìù ‚úÖ Created bid_window 139: AY202425T2 Round 1B Window 1\n",
      "üìù ‚úÖ Created bid_window 140: AY202425T2 Round 1B Window 2\n",
      "üìù ‚úÖ Created bid_window 141: AY202425T3B Round 1 Window 1\n",
      "üìù ‚úÖ Created bid_window 142: AY202425T3B Round 1 Window 2\n",
      "üìù ‚úÖ Created bid_window 143: AY202425T3B Round 1 Window 3\n",
      "üìù üîÑ Sorting bid windows by hierarchy...\n",
      "üìù ‚úÖ Sorted 137 bid windows by hierarchy\n",
      "üìù ‚úÖ Processed 143 bid windows\n",
      "üìù üìä Processing class availability...\n",
      "üìù ‚úÖ Processed 121172 class availability records\n",
      "üìù üìà Processing bid results...\n",
      "üìù ‚úÖ Processed 121172 bid result records\n",
      "üìù üíæ Saving BOSS output files...\n",
      "üìù ‚úÖ Saved 137 bid windows to new_bid_window.csv\n",
      "üìù ‚úÖ Saved 121172 availability records to new_class_availability.csv\n",
      "üìù ‚úÖ Saved 121172 bid results to new_bid_result.csv\n",
      "üìù ‚ö†Ô∏è Saved 126 failed mappings to failed_boss_results_mapping.csv\n",
      "üìù ‚úÖ All BOSS output files saved successfully\n",
      "\n",
      "======================================================================\n",
      "üìä BOSS RESULTS PROCESSING SUMMARY\n",
      "======================================================================\n",
      "üìÇ Files processed: 14\n",
      "üìÑ Total rows: 121340\n",
      "ü™ü Bid windows created: 143\n",
      "üìä Class availability records: 121172\n",
      "üìà Bid result records: 121172\n",
      "‚ùå Failed mappings: 126\n",
      "======================================================================\n",
      "\n",
      "üìÅ OUTPUT FILES:\n",
      "   - new_bid_window.csv (143 records)\n",
      "   - new_class_availability.csv (121172 records)\n",
      "   - new_bid_result.csv (121172 records)\n",
      "   - failed_boss_results_mapping.csv (126 records)\n",
      "   - boss_result_log.txt (processing log)\n",
      "======================================================================\n",
      "üìù ‚úÖ Phase 3: BOSS Results Processing completed successfully!\n",
      "\n",
      "üéâ Phase 3 completed successfully!\n",
      "‚ö†Ô∏è 126 failed mappings found:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_code</th>\n",
       "      <th>section</th>\n",
       "      <th>acad_term_id</th>\n",
       "      <th>term_str</th>\n",
       "      <th>bidding_window_str</th>\n",
       "      <th>reason</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CS422</td>\n",
       "      <td>G1</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>2021-22 Term 2</td>\n",
       "      <td>Round 1A Window 2</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2021-22_T2.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CS422</td>\n",
       "      <td>G1</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>2021-22 Term 2</td>\n",
       "      <td>Round 1A Window 1</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2021-22_T2.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COR1305</td>\n",
       "      <td>G1</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>2021-22 Term 2</td>\n",
       "      <td>Round 1 Window 1</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2021-22_T2.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IS213</td>\n",
       "      <td>G10</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>2021-22 Term 2</td>\n",
       "      <td>Round 1 Window 1</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2021-22_T2.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LAW4024</td>\n",
       "      <td>G1</td>\n",
       "      <td>AY202223T1</td>\n",
       "      <td>2022-23 Term 1</td>\n",
       "      <td>Round 1A Window 2</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2022-23_T1.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LAW4024</td>\n",
       "      <td>G61</td>\n",
       "      <td>AY202223T1</td>\n",
       "      <td>2022-23 Term 1</td>\n",
       "      <td>Round 1A Window 2</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2022-23_T1.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LAW4031</td>\n",
       "      <td>G1</td>\n",
       "      <td>AY202223T1</td>\n",
       "      <td>2022-23 Term 1</td>\n",
       "      <td>Round 1A Window 2</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2022-23_T1.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LAW4031</td>\n",
       "      <td>G61</td>\n",
       "      <td>AY202223T1</td>\n",
       "      <td>2022-23 Term 1</td>\n",
       "      <td>Round 1A Window 2</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2022-23_T1.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LAW447</td>\n",
       "      <td>G1</td>\n",
       "      <td>AY202223T1</td>\n",
       "      <td>2022-23 Term 1</td>\n",
       "      <td>Round 1A Window 2</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2022-23_T1.xlsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LAW447</td>\n",
       "      <td>G61</td>\n",
       "      <td>AY202223T1</td>\n",
       "      <td>2022-23 Term 1</td>\n",
       "      <td>Round 1A Window 2</td>\n",
       "      <td>class_not_found</td>\n",
       "      <td>2022-23_T1.xlsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  course_code section acad_term_id        term_str bidding_window_str  \\\n",
       "0       CS422      G1   AY202122T2  2021-22 Term 2  Round 1A Window 2   \n",
       "1       CS422      G1   AY202122T2  2021-22 Term 2  Round 1A Window 1   \n",
       "2     COR1305      G1   AY202122T2  2021-22 Term 2   Round 1 Window 1   \n",
       "3       IS213     G10   AY202122T2  2021-22 Term 2   Round 1 Window 1   \n",
       "4     LAW4024      G1   AY202223T1  2022-23 Term 1  Round 1A Window 2   \n",
       "5     LAW4024     G61   AY202223T1  2022-23 Term 1  Round 1A Window 2   \n",
       "6     LAW4031      G1   AY202223T1  2022-23 Term 1  Round 1A Window 2   \n",
       "7     LAW4031     G61   AY202223T1  2022-23 Term 1  Round 1A Window 2   \n",
       "8      LAW447      G1   AY202223T1  2022-23 Term 1  Round 1A Window 2   \n",
       "9      LAW447     G61   AY202223T1  2022-23 Term 1  Round 1A Window 2   \n",
       "\n",
       "            reason      source_file  \n",
       "0  class_not_found  2021-22_T2.xlsx  \n",
       "1  class_not_found  2021-22_T2.xlsx  \n",
       "2  class_not_found  2021-22_T2.xlsx  \n",
       "3  class_not_found  2021-22_T2.xlsx  \n",
       "4  class_not_found  2022-23_T1.xlsx  \n",
       "5  class_not_found  2022-23_T1.xlsx  \n",
       "6  class_not_found  2022-23_T1.xlsx  \n",
       "7  class_not_found  2022-23_T1.xlsx  \n",
       "8  class_not_found  2022-23_T1.xlsx  \n",
       "9  class_not_found  2022-23_T1.xlsx  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Review failed mappings in: script_output\\failed_boss_results_mapping.csv\n",
      "üìã Generated Data Summary:\n",
      "\n",
      "ü™ü Bid Windows (137 records):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>acad_term_id</th>\n",
       "      <th>round</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>1A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>1A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>AY202122T2</td>\n",
       "      <td>1B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id acad_term_id round  window\n",
       "0   1   AY202122T2     1       1\n",
       "1   2   AY202122T2     1       2\n",
       "2   3   AY202122T2    1A       1\n",
       "3   4   AY202122T2    1A       2\n",
       "4   5   AY202122T2    1B       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Class Availability (121172 records):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>bid_window_id</th>\n",
       "      <th>total</th>\n",
       "      <th>current_enrolled</th>\n",
       "      <th>reserved</th>\n",
       "      <th>available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a5a4bac-6884-4e42-8307-42f48fdf8a6c</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65ead13a-3a22-4fd3-b230-0a95312a4dd6</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a65ae35c-7e8d-4d05-8eb8-a52054e329b9</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a64ff3d5-cbb8-4332-b10e-7a6a04ce4f18</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ffd2e435-64da-4974-8bb8-4022be810d14</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               class_id  bid_window_id  total  \\\n",
       "0  6a5a4bac-6884-4e42-8307-42f48fdf8a6c             15     45   \n",
       "1  65ead13a-3a22-4fd3-b230-0a95312a4dd6             15     25   \n",
       "2  a65ae35c-7e8d-4d05-8eb8-a52054e329b9             15     42   \n",
       "3  a64ff3d5-cbb8-4332-b10e-7a6a04ce4f18             15     45   \n",
       "4  ffd2e435-64da-4974-8bb8-4022be810d14             15     45   \n",
       "\n",
       "   current_enrolled  reserved  available  \n",
       "0                44         0          1  \n",
       "1                24         1          0  \n",
       "2                42         0          0  \n",
       "3                44         0          1  \n",
       "4                45         0          0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Bid Results (121172 records):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_window_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>vacancy</th>\n",
       "      <th>opening_vacancy</th>\n",
       "      <th>before_process_vacancy</th>\n",
       "      <th>dice</th>\n",
       "      <th>after_process_vacancy</th>\n",
       "      <th>enrolled_students</th>\n",
       "      <th>bid_actual_median</th>\n",
       "      <th>bid_actual_min</th>\n",
       "      <th>bid_predicted_median</th>\n",
       "      <th>bid_predicted_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>6a5a4bac-6884-4e42-8307-42f48fdf8a6c</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>65ead13a-3a22-4fd3-b230-0a95312a4dd6</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>a65ae35c-7e8d-4d05-8eb8-a52054e329b9</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>a64ff3d5-cbb8-4332-b10e-7a6a04ce4f18</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>ffd2e435-64da-4974-8bb8-4022be810d14</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bid_window_id                              class_id  vacancy  \\\n",
       "0             15  6a5a4bac-6884-4e42-8307-42f48fdf8a6c       45   \n",
       "1             15  65ead13a-3a22-4fd3-b230-0a95312a4dd6       25   \n",
       "2             15  a65ae35c-7e8d-4d05-8eb8-a52054e329b9       42   \n",
       "3             15  a64ff3d5-cbb8-4332-b10e-7a6a04ce4f18       45   \n",
       "4             15  ffd2e435-64da-4974-8bb8-4022be810d14       45   \n",
       "\n",
       "   opening_vacancy  before_process_vacancy  dice  after_process_vacancy  \\\n",
       "0               45                       1     0                      1   \n",
       "1               24                       0     0                      0   \n",
       "2               42                       0     0                      0   \n",
       "3               45                       1     0                      1   \n",
       "4               45                       0     0                      0   \n",
       "\n",
       "   enrolled_students  bid_actual_median  bid_actual_min  bid_predicted_median  \\\n",
       "0                 44                0.0             0.0                   0.0   \n",
       "1                 24                0.0             0.0                   0.0   \n",
       "2                 42                0.0             0.0                   0.0   \n",
       "3                 44                0.0             0.0                   0.0   \n",
       "4                 45                0.0             0.0                   0.0   \n",
       "\n",
       "   bid_predicted_min  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run complete Phase 3 pipeline\n",
    "print(\"üöÄ Starting Phase 3: BOSS Results Processing\")\n",
    "success = builder.run_phase3_boss_processing()\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Phase 3 completed successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Phase 3 failed. Check logs for details.\")\n",
    "\n",
    "# Check failed mappings (if any)\n",
    "failed_path = os.path.join('script_output', 'failed_boss_results_mapping.csv')\n",
    "if os.path.exists(failed_path):\n",
    "    failed_df = pd.read_csv(failed_path)\n",
    "    print(f\"‚ö†Ô∏è {len(failed_df)} failed mappings found:\")\n",
    "    display(failed_df.head(10))\n",
    "    print(f\"\\nüìù Review failed mappings in: {failed_path}\")\n",
    "else:\n",
    "    print(\"‚úÖ No failed mappings - all BOSS results mapped successfully!\")\n",
    "\n",
    "# Inspect generated data\n",
    "print(\"üìã Generated Data Summary:\")\n",
    "\n",
    "# Check bid windows\n",
    "bid_window_path = os.path.join('script_output', 'new_bid_window.csv')\n",
    "if os.path.exists(bid_window_path):\n",
    "    bw_df = pd.read_csv(bid_window_path)\n",
    "    print(f\"\\nü™ü Bid Windows ({len(bw_df)} records):\")\n",
    "    display(bw_df.head())\n",
    "\n",
    "# Check class availability\n",
    "availability_path = os.path.join('script_output', 'new_class_availability.csv')\n",
    "if os.path.exists(availability_path):\n",
    "    av_df = pd.read_csv(availability_path)\n",
    "    print(f\"\\nüìä Class Availability ({len(av_df)} records):\")\n",
    "    display(av_df.head())\n",
    "\n",
    "# Check bid results\n",
    "result_path = os.path.join('script_output', 'new_bid_result.csv')\n",
    "if os.path.exists(result_path):\n",
    "    br_df = pd.read_csv(result_path)\n",
    "    print(f\"\\nüìà Bid Results ({len(br_df)} records):\")\n",
    "    display(br_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bidly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
