{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SMU Course Scraping Using Selenium**\n",
    "\n",
    "<div style=\"background-color:#FFD700; padding:15px; border-radius:5px; border: 2px solid #FF4500;\">\n",
    "    \n",
    "  <h1 style=\"color:#8B0000;\">⚠️🚨 SCRAPE THIS DATA AT YOUR OWN RISK 🚨⚠️</h1>\n",
    "  \n",
    "  <p><strong>📌 If you need the data, please contact me directly.</strong> Only available for **existing students**.</p>\n",
    "\n",
    "  <h3>🔗 📩 How to Get the Data?</h3>\n",
    "  <p>📨 <strong>Reach out to me for access</strong> instead of scraping manually.</p>\n",
    "  <p>Visit <a href='https://www.afterclass.io/'>AfterClass</a> to use the data for planning</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Objective**\n",
    "This script is designed to scrape SMU course details from the BOSS system using Selenium. The process involves:\n",
    "1. Logging into the system manually to bypass authentication.\n",
    "2. Iteratively scraping class details for specified academic years and terms.\n",
    "3. Writing the scraped data to structured CSV files.\n",
    "\n",
    "The data is then ingested into [AfterClass.io](https://www.afterclass.io/) to serve students.\n",
    "\n",
    "### **Script Structure**\n",
    "1. **Setup**: Import libraries and initialize Selenium WebDriver.\n",
    "2. **Login**: Wait for manual login and authentication.\n",
    "3. **Scraping Logic**:\n",
    "    - `scrape_class_details`: Scrapes course details for a specific class number, academic year, and term.\n",
    "    - `main`: Manages the scraping process for multiple academic years and terms.\n",
    "4. **Execution**: Log in and start scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PGGSSENCMODE'] = 'disable'\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import win32com.client as win32\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import logging\n",
    "import psycopg2\n",
    "from typing import List, Optional, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import webbrowser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Scrape all BOSS data**\n",
    "\n",
    "### **BOSS Class Scraper Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `BOSSClassScraper` class automates the extraction of class timing data from SMU's BOSS system. It systematically scrapes class details across multiple academic terms and saves them as HTML files for further processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Automated Web Scraping**: Navigates through BOSS class detail pages using Selenium WebDriver\n",
    "- **Resume Capability**: Automatically detects existing scraped files and continues from the last scraped class number, preventing duplicate work\n",
    "- **Flexible Term Range**: Dynamically derives academic years from input parameters (e.g., '2025-26_T1' to '2028-29_T2') rather than hardcoded lists\n",
    "- **Smart Pagination**: Scans class numbers from 1000-5000 with intelligent termination after 300 consecutive empty records\n",
    "- **Progress Tracking**: Monitors existing files and resumes scraping from the highest class number found for each term\n",
    "- **Data Organization**: Saves HTML files in structured directories by academic term (`script_input/classTimingsFull/`)\n",
    "- **Incremental CSV Updates**: Appends only new valid files to the existing CSV index, avoiding duplicates\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, standard libraries (`os`, `time`, `csv`, `re`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- Network access to SMU's BOSS system\n",
    "\n",
    "**User Requirements:**\n",
    "- **Manual Authentication**: User must manually log in and complete Microsoft Authenticator process when prompted\n",
    "- **SMU Credentials**: Valid access to BOSS system\n",
    "- **Directory Structure**: Code creates `script_input/classTimingsFull/` for HTML files and `script_input/scraped_filepaths.csv` for the file index\n",
    "\n",
    "**Resume Functionality:**\n",
    "- **Interruption Handling**: If scraping stops halfway due to network issues or manual interruption, the next run automatically resumes from the exact point it left off\n",
    "- **Duplicate Prevention**: Existing files are automatically detected and skipped, preventing re-downloading of already scraped data\n",
    "- **Natural Termination**: Uses 300 consecutive empty records threshold to handle BOSS system inconsistencies without hardcoded limits\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "scraper = BOSSClassScraper()\n",
    "# Will automatically resume from previous progress if files exist\n",
    "success = scraper.run_full_scraping_process('2025-26_T1', '2025-26_T3B')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOSSClassScraper:\n",
    "    \"\"\"\n",
    "    A class to scrape class details from BOSS (SMU's online class registration system)\n",
    "    and save them as HTML files for further processing with resume capability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BOSS Class Scraper with configuration parameters.\n",
    "        \"\"\"\n",
    "        self.term_code_map = {'T1': '10', 'T2': '20', 'T3A': '31', 'T3B': '32'}\n",
    "        self.all_terms = ['T1', 'T2', 'T3A', 'T3B']\n",
    "        self.driver = None\n",
    "        self.min_class_number = 1000\n",
    "        self.max_class_number = 5000\n",
    "        self.consecutive_empty_threshold = 300\n",
    "        \n",
    "    def _derive_academic_years(self, start_ay_term, end_ay_term):\n",
    "        \"\"\"\n",
    "        Derive academic years from start and end terms.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting term (e.g., '2025-26_T1')\n",
    "            end_ay_term: Ending term (e.g., '2028-29_T2')\n",
    "            \n",
    "        Returns:\n",
    "            List of academic years in format ['2025-26', '2026-27', ...]\n",
    "        \"\"\"\n",
    "        start_year = int(start_ay_term[:4])\n",
    "        end_year = int(end_ay_term[:4])\n",
    "        \n",
    "        academic_years = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            next_year = (year + 1) % 100\n",
    "            ay = f\"{year}-{next_year:02d}\"\n",
    "            academic_years.append(ay)\n",
    "            \n",
    "        return academic_years\n",
    "    \n",
    "    def _get_existing_files_progress(self, base_dir):\n",
    "        \"\"\"\n",
    "        Check existing files and determine the last scraped position for each term.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory where HTML files are stored\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with term as key and last scraped class number as value\n",
    "        \"\"\"\n",
    "        progress = {}\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            return progress\n",
    "            \n",
    "        for term_folder in os.listdir(base_dir):\n",
    "            term_path = os.path.join(base_dir, term_folder)\n",
    "            if os.path.isdir(term_path):\n",
    "                max_class_num = 0\n",
    "                \n",
    "                for filename in os.listdir(term_path):\n",
    "                    if filename.endswith('.html'):\n",
    "                        # Extract class number from filename\n",
    "                        # Format: SelectedAcadTerm=XXYY&SelectedClassNumber=ZZZZ.html\n",
    "                        match = re.search(r'SelectedClassNumber=(\\d+)\\.html', filename)\n",
    "                        if match:\n",
    "                            class_num = int(match.group(1))\n",
    "                            max_class_num = max(max_class_num, class_num)\n",
    "                \n",
    "                if max_class_num > 0:\n",
    "                    progress[term_folder] = max_class_num\n",
    "                    print(f\"Found existing progress for {term_folder}: last class number {max_class_num}\")\n",
    "        \n",
    "        return progress\n",
    "    \n",
    "    def wait_for_manual_login(self):\n",
    "        \"\"\"\n",
    "        Wait for manual login and Microsoft Authenticator process completion.\n",
    "        \"\"\"\n",
    "        print(\"Please log in manually and complete the Microsoft Authenticator process.\")\n",
    "        print(\"Waiting for BOSS dashboard to load...\")\n",
    "        \n",
    "        wait = WebDriverWait(self.driver, 120)\n",
    "        \n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"Label_UserName\")))\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(text(),'Sign out')]\")))\n",
    "            \n",
    "            username = self.driver.find_element(By.ID, \"Label_UserName\").text\n",
    "            print(f\"Login successful! Logged in as {username}\")\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Login failed or timed out. Could not detect login elements.\")\n",
    "            raise Exception(\"Login failed\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    def scrape_and_save_html(self, start_ay_term='2025-26_T1', end_ay_term='2025-26_T1', base_dir='script_input/classTimingsFull'):\n",
    "        \"\"\"\n",
    "        Scrapes class details from BOSS and saves them as HTML files with resume capability.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting academic year and term (e.g., '2025-26_T1')\n",
    "            end_ay_term: Ending academic year and term (e.g., '2025-26_T1')\n",
    "            base_dir: Base directory to save the HTML files\n",
    "        \"\"\"\n",
    "        # Check existing progress\n",
    "        existing_progress = self._get_existing_files_progress(base_dir)\n",
    "        \n",
    "        # Derive academic years from input terms\n",
    "        all_academic_years = self._derive_academic_years(start_ay_term, end_ay_term)\n",
    "        \n",
    "        # Generate all possible AY_TERM combinations\n",
    "        all_ay_terms = []\n",
    "        for ay in all_academic_years:\n",
    "            for term in self.all_terms:\n",
    "                all_ay_terms.append(f\"{ay}_{term}\")\n",
    "        \n",
    "        # Find the indices of the start and end terms\n",
    "        try:\n",
    "            start_idx = all_ay_terms.index(start_ay_term)\n",
    "            end_idx = all_ay_terms.index(end_ay_term)\n",
    "        except ValueError:\n",
    "            print(\"Invalid start or end term provided. Using full range.\")\n",
    "            start_idx = 0\n",
    "            end_idx = len(all_ay_terms) - 1\n",
    "        \n",
    "        # Select the range to scrape\n",
    "        ay_terms_to_scrape = all_ay_terms[start_idx:end_idx+1]\n",
    "        \n",
    "        # Create base directory if needed\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each AY_TERM\n",
    "        for ay_term in ay_terms_to_scrape:\n",
    "            print(f\"Processing {ay_term}...\")\n",
    "            \n",
    "            # Parse AY_TERM for URL\n",
    "            ay, term = ay_term.split('_')\n",
    "            ay_short = ay[2:4]  # last two digits of first year\n",
    "            term_code = self.term_code_map.get(term, '10')\n",
    "            \n",
    "            # Create folder for AY_TERM\n",
    "            folder_path = os.path.join(base_dir, ay_term)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            # Determine starting class number based on existing progress\n",
    "            start_class_num = self.min_class_number\n",
    "            if ay_term in existing_progress:\n",
    "                start_class_num = existing_progress[ay_term] + 1\n",
    "                print(f\"Resuming {ay_term} from class number {start_class_num}\")\n",
    "            \n",
    "            consecutive_empty = 0\n",
    "            \n",
    "            # Scrape each class number in range\n",
    "            for class_num in range(start_class_num, self.max_class_number + 1):\n",
    "                # Check if file already exists\n",
    "                filename = f\"SelectedAcadTerm={ay_short}{term_code}&SelectedClassNumber={class_num:04}.html\"\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"File already exists: {filepath}, skipping...\")\n",
    "                    consecutive_empty = 0  # Reset counter since we have data\n",
    "                    continue\n",
    "                \n",
    "                url = f\"https://boss.intranet.smu.edu.sg/ClassDetails.aspx?SelectedClassNumber={class_num:04}&SelectedAcadTerm={ay_short}{term_code}&SelectedAcadCareer=UGRD\"\n",
    "                \n",
    "                try:\n",
    "                    self.driver.get(url)\n",
    "                    \n",
    "                    wait = WebDriverWait(self.driver, 15)\n",
    "                    try:\n",
    "                        element = wait.until(EC.any_of(\n",
    "                            EC.presence_of_element_located((By.ID, \"lblClassInfoHeader\")),\n",
    "                            EC.presence_of_element_located((By.ID, \"lblErrorDetails\"))\n",
    "                        ))\n",
    "                        \n",
    "                        error_elements = self.driver.find_elements(By.ID, \"lblErrorDetails\")\n",
    "                        has_data = True\n",
    "                        \n",
    "                        for error in error_elements:\n",
    "                            if \"No record found\" in error.text:\n",
    "                                has_data = False\n",
    "                                break\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"Wait error: {e}\")\n",
    "                        has_data = False\n",
    "                    \n",
    "                    if not has_data:\n",
    "                        consecutive_empty += 1\n",
    "                        print(f\"No record found for {ay_term}, class {class_num:04}. Consecutive empty: {consecutive_empty}\")\n",
    "                        \n",
    "                        if consecutive_empty >= self.consecutive_empty_threshold:\n",
    "                            print(f\"{self.consecutive_empty_threshold} consecutive empty records reached for {ay_term}, moving on.\")\n",
    "                            break\n",
    "                        \n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "                    \n",
    "                    # Reset consecutive empty counter if data found\n",
    "                    consecutive_empty = 0\n",
    "                    \n",
    "                    # Save HTML file\n",
    "                    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(self.driver.page_source)\n",
    "                    \n",
    "                    print(f\"Saved {filepath}\")\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {str(e)}\")\n",
    "                    time.sleep(5)\n",
    "        \n",
    "        print(\"Scraping completed.\")\n",
    "    \n",
    "    def generate_scraped_filepaths_csv(self, base_dir='script_input/classTimingsFull', output_csv='script_input/scraped_filepaths.csv'):\n",
    "        \"\"\"\n",
    "        Generates a CSV file with paths to all valid HTML files (those without \"No record found\").\n",
    "        Updates existing CSV by appending new valid files.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory where HTML files are stored\n",
    "            output_csv: Path to the output CSV file\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated CSV file or None if error\n",
    "        \"\"\"\n",
    "        # Read existing filepaths if CSV exists\n",
    "        existing_filepaths = set()\n",
    "        if os.path.exists(output_csv):\n",
    "            try:\n",
    "                with open(output_csv, 'r', encoding='utf-8') as csvfile:\n",
    "                    reader = csv.reader(csvfile)\n",
    "                    next(reader)  # Skip header\n",
    "                    for row in reader:\n",
    "                        if row:\n",
    "                            existing_filepaths.add(row[0])\n",
    "                print(f\"Found {len(existing_filepaths)} existing filepaths in CSV\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading existing CSV: {str(e)}\")\n",
    "        \n",
    "        filepaths = []\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            print(f\"Directory '{base_dir}' does not exist.\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "        \n",
    "        # Walk through directory structure\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.html'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    \n",
    "                    # Skip if already in existing filepaths\n",
    "                    if filepath in existing_filepaths:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            if 'No record found' not in content:\n",
    "                                filepaths.append(filepath)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {filepath}: {str(e)}\")\n",
    "        \n",
    "        # Append new filepaths to CSV\n",
    "        mode = 'a' if existing_filepaths else 'w'\n",
    "        with open(output_csv, mode, newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not existing_filepaths:  # Write header only if new file\n",
    "                writer.writerow(['Filepath'])\n",
    "            for path in filepaths:\n",
    "                writer.writerow([path])\n",
    "        \n",
    "        total_valid_files = len(existing_filepaths) + len(filepaths)\n",
    "        print(f\"CSV updated with {len(filepaths)} new valid file paths. Total: {total_valid_files} files at {output_csv}\")\n",
    "        return output_csv\n",
    "    \n",
    "    def run_full_scraping_process(self, start_ay_term='2025-26_T1', end_ay_term='2025-26_T1'):\n",
    "        \"\"\"\n",
    "        Run the complete scraping process from login to CSV generation with resume capability.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting academic year and term\n",
    "            end_ay_term: Ending academic year and term\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up WebDriver\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            # Navigate to login page and wait for manual login\n",
    "            self.driver.get(\"https://boss.intranet.smu.edu.sg/\")\n",
    "            self.wait_for_manual_login()\n",
    "            \n",
    "            # Run the main scraping function\n",
    "            self.scrape_and_save_html(start_ay_term, end_ay_term)\n",
    "            \n",
    "            # Generate CSV with valid file paths\n",
    "            self.generate_scraped_filepaths_csv()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping process: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the scraper\n",
    "scraper = BOSSClassScraper()\n",
    "success = scraper.run_full_scraping_process('2025-26_T1', '2025-26_T1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Extract Data from HTML Files**\n",
    "\n",
    "### **HTML Data Extractor Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `HTMLDataExtractor` class processes previously scraped HTML files from SMU's BOSS system and extracts structured data into Excel format. It systematically parses course information, class timings, academic terms, and exam schedules from local HTML files without requiring network access or authentication.\n",
    "\n",
    "**Key Features:**\n",
    "- **Local File Processing**: Uses Selenium WebDriver to parse local HTML files without network connectivity requirements\n",
    "- **Comprehensive Data Extraction**: Extracts course details, academic terms, class timings, exam schedules, grading information, and professor names\n",
    "- **Test-First Approach**: Includes `run_test()` function to validate extraction logic on a small sample before processing all files\n",
    "- **Structured Output**: Organizes extracted data into two Excel sheets - standalone records (one per HTML file) and multiple records (class/exam timings)\n",
    "- **Error Tracking**: Captures and logs parsing errors in a separate sheet for debugging and quality assurance\n",
    "- **Flexible Data Parsing**: Handles multiple academic term naming conventions and date formats used across different years\n",
    "- **Record Linking**: Uses record keys to maintain relationships between standalone and multiple data records\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, `pandas`, `openpyxl`, standard libraries (`os`, `re`, `datetime`, `pathlib`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- No network access required (processes local files only)\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Scraped HTML Files**: Previously downloaded HTML files from BOSS system stored locally\n",
    "- **File Path CSV**: `script_input/scraped_filepaths.csv` containing paths to valid HTML files\n",
    "- **Directory Structure**: HTML files organized in the expected folder structure (typically `script_input/classTimingsFull/`)\n",
    "\n",
    "**Output Structure:**\n",
    "- **Excel File**: `script_input/raw_data.xlsx` (or custom path) with multiple sheets:\n",
    "  - `standalone`: One record per HTML file with course and class information\n",
    "  - `multiple`: Multiple records for class timings and exam schedules\n",
    "  - `errors`: Parsing errors and problematic files for debugging\n",
    "\n",
    "**Data Extraction Capabilities:**\n",
    "- **Course Information**: Course codes, names, descriptions, credit units, course areas, enrollment requirements\n",
    "- **Academic Terms**: Term IDs, academic years, start/end dates, BOSS IDs\n",
    "- **Class Details**: Sections, grading basis, course outline URLs, professor names\n",
    "- **Timing Data**: Class schedules, exam dates, venues, day-of-week information\n",
    "- **Cross-References**: Maintains linking keys between related records across sheets\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "# Initialize extractor\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Test with sample files first (recommended)\n",
    "test_success = extractor.run_test(test_count=10)\n",
    "\n",
    "if test_success:\n",
    "    # Run full extraction\n",
    "    extractor.run()\n",
    "    \n",
    "# Or run directly without testing\n",
    "extractor.run(\n",
    "    scraped_filepaths_csv='script_input/scraped_filepaths.csv',\n",
    "    output_path='script_input/raw_data.xlsx'\n",
    ")\n",
    "```\n",
    "\n",
    "The class provides a crucial intermediate step between raw HTML scraping and database insertion, creating clean, structured data that can be further processed for database integration or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLDataExtractor:\n",
    "    \"\"\"\n",
    "    Extract raw data from scraped HTML files and save to Excel format using Selenium\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        self.driver = None\n",
    "        \n",
    "    def setup_selenium_driver(self):\n",
    "        \"\"\"Set up Selenium WebDriver for local file access\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--headless')  # Run in headless mode for efficiency\n",
    "            options.add_argument('--disable-gpu')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            print(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Selenium WebDriver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def safe_find_element_text(self, by, value):\n",
    "        \"\"\"Safely find element and return its text with proper encoding handling\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            if element:\n",
    "                raw_text = element.text.strip()\n",
    "                return self.clean_text_encoding(raw_text)\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def safe_find_element_attribute(self, by, value, attribute):\n",
    "        \"\"\"Safely find element and return its attribute with proper encoding handling\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            if element:\n",
    "                raw_attr = element.get_attribute(attribute)\n",
    "                return self.clean_text_encoding(raw_attr) if raw_attr else None\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def convert_date_to_timestamp(self, date_str):\n",
    "        \"\"\"Convert DD-Mmm-YYYY to database timestamp format\"\"\"\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "            return date_obj.strftime('%Y-%m-%d 00:00:00.000 +0800')\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def parse_acad_term(self, term_text, filepath=None):\n",
    "        \"\"\"Parse academic term text and return structured data with folder path fallback\"\"\"\n",
    "        try:\n",
    "            # Clean the term text first\n",
    "            if term_text:\n",
    "                term_text = self.clean_text_encoding(term_text)\n",
    "            \n",
    "            # Pattern like \"2021-22 Term 2\" or \"2021-22 Session 1\"\n",
    "            pattern = r'(\\d{4})-(\\d{2})\\s+(.*)'\n",
    "            match = re.search(pattern, term_text) if term_text else None\n",
    "            \n",
    "            if not match:\n",
    "                return None, None, None, None\n",
    "            \n",
    "            start_year = int(match.group(1))\n",
    "            end_year_short = int(match.group(2))\n",
    "            term_desc = match.group(3).lower()\n",
    "            \n",
    "            # Convert 2-digit year to 4-digit\n",
    "            if end_year_short < 50:\n",
    "                end_year = 2000 + end_year_short\n",
    "            else:\n",
    "                end_year = 1900 + end_year_short\n",
    "            \n",
    "            # Determine term code from text\n",
    "            term_code = None\n",
    "            if 'term 1' in term_desc or 'session 1' in term_desc or 'august term' in term_desc:\n",
    "                term_code = 'T1'\n",
    "            elif 'term 2' in term_desc or 'session 2' in term_desc or 'january term' in term_desc:\n",
    "                term_code = 'T2'\n",
    "            elif 'term 3a' in term_desc:\n",
    "                term_code = 'T3A'\n",
    "            elif 'term 3b' in term_desc:\n",
    "                term_code = 'T3B'\n",
    "            elif 'term 3' in term_desc:\n",
    "                # Generic T3 - need to check folder path for A/B\n",
    "                term_code = 'T3'\n",
    "            \n",
    "            # If term_code is incomplete or missing, use folder path as fallback\n",
    "            if not term_code or term_code == 'T3':\n",
    "                folder_term = self.extract_term_from_folder_path(filepath) if filepath else None\n",
    "                if folder_term:\n",
    "                    # If we have folder term, use it\n",
    "                    if term_code == 'T3' and folder_term in ['T3A', 'T3B']:\n",
    "                        term_code = folder_term\n",
    "                    elif not term_code:\n",
    "                        term_code = folder_term\n",
    "            \n",
    "            # If still no term code, return None\n",
    "            if not term_code:\n",
    "                return start_year, end_year, None, None\n",
    "            \n",
    "            acad_term_id = f\"AY{start_year}{end_year_short:02d}{term_code}\"\n",
    "            \n",
    "            return start_year, end_year, term_code, acad_term_id\n",
    "        except Exception as e:\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def parse_course_and_section(self, header_text):\n",
    "        \"\"\"Parse course code and section from header text with encoding fixes\"\"\"\n",
    "        try:\n",
    "            if not header_text:\n",
    "                return None, None\n",
    "            \n",
    "            # Clean the text first\n",
    "            clean_text = self.clean_text_encoding(header_text)\n",
    "            clean_text = re.sub(r'<[^>]+>', '', clean_text)\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text.strip())\n",
    "            \n",
    "            # Try multiple regex patterns\n",
    "            patterns = [\n",
    "                r'([A-Z0-9_-]+)\\s+—\\s+(.+)',  # Standard format with em-dash\n",
    "                r'([A-Z0-9_-]+)\\s+-\\s+(.+)',  # Standard format with hyphen\n",
    "                r'([A-Z]+)\\s+(\\d+[A-Z0-9_]*)\\s+—\\s+(.+)',  # Split format with em-dash\n",
    "                r'([A-Z]+)\\s+(\\d+[A-Z0-9_]*)\\s+-\\s+(.+)',  # Split format with hyphen\n",
    "                r'([A-Z0-9_\\s-]+?)\\s+—\\s+([^—]+)',  # Flexible format with em-dash\n",
    "                r'([A-Z0-9_\\s-]+?)\\s+-\\s+([^-]+)',  # Flexible format with hyphen\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.match(pattern, clean_text)\n",
    "                if match:\n",
    "                    if len(match.groups()) == 2:\n",
    "                        # Standard format: course_code - section\n",
    "                        course_section = match.group(1).strip()\n",
    "                        section_name = match.group(2).strip()\n",
    "                        \n",
    "                        # Extract section from the end of course_section if it's there\n",
    "                        section_match = re.search(r'^(.+?)\\s+([A-Z]\\d+|G\\d+|\\d+)$', course_section)\n",
    "                        if section_match:\n",
    "                            course_code = section_match.group(1)\n",
    "                            section = section_match.group(2)\n",
    "                        else:\n",
    "                            course_code = course_section\n",
    "                            # Try to extract section from section_name\n",
    "                            section_extract = re.search(r'([A-Z]\\d+|G\\d+|\\d+)', section_name)\n",
    "                            section = section_extract.group(1) if section_extract else None\n",
    "                    else:\n",
    "                        # Split format: course_prefix course_number - section_name\n",
    "                        course_code = f\"{match.group(1)}{match.group(2)}\"\n",
    "                        section_name = match.group(3).strip()\n",
    "                        section_extract = re.search(r'([A-Z]\\d+|G\\d+|\\d+)', section_name)\n",
    "                        section = section_extract.group(1) if section_extract else None\n",
    "                    \n",
    "                    return course_code.strip() if course_code else None, section\n",
    "            \n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def parse_date_range(self, date_text):\n",
    "        \"\"\"Parse date range text and return start and end timestamps\"\"\"\n",
    "        try:\n",
    "            # Example: \"10-Jan-2022 to 01-May-2022\"\n",
    "            pattern = r'(\\d{1,2}-\\w{3}-\\d{4})\\s+to\\s+(\\d{1,2}-\\w{3}-\\d{4})'\n",
    "            match = re.search(pattern, date_text)\n",
    "            \n",
    "            if not match:\n",
    "                return None, None\n",
    "            \n",
    "            start_date = self.convert_date_to_timestamp(match.group(1))\n",
    "            end_date = self.convert_date_to_timestamp(match.group(2))\n",
    "            \n",
    "            return start_date, end_date\n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_course_areas_list(self):\n",
    "        \"\"\"Extract course areas with encoding fixes\"\"\"\n",
    "        try:\n",
    "            course_areas_element = self.driver.find_element(By.ID, 'lblCourseAreas')\n",
    "            if not course_areas_element:\n",
    "                return None\n",
    "            \n",
    "            # Get innerHTML to handle HTML content\n",
    "            course_areas_html = course_areas_element.get_attribute('innerHTML')\n",
    "            if course_areas_html:\n",
    "                # Clean encoding first\n",
    "                course_areas_html = self.clean_text_encoding(course_areas_html)\n",
    "                \n",
    "                # Extract list items\n",
    "                areas_list = re.findall(r'<li[^>]*>([^<]+)</li>', course_areas_html)\n",
    "                if areas_list:\n",
    "                    # Clean each area and join\n",
    "                    cleaned_areas = [self.clean_text_encoding(area.strip()) for area in areas_list]\n",
    "                    return ', '.join(cleaned_areas)\n",
    "                else:\n",
    "                    # Fallback to text content\n",
    "                    text_content = course_areas_element.text.strip()\n",
    "                    return self.clean_text_encoding(text_content)\n",
    "            else:\n",
    "                # Fallback to text content\n",
    "                text_content = course_areas_element.text.strip()\n",
    "                return self.clean_text_encoding(text_content)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def extract_course_outline_url(self):\n",
    "        \"\"\"Extract course outline URL from HTML using Selenium\"\"\"\n",
    "        try:\n",
    "            onclick_attr = self.safe_find_element_attribute(By.ID, 'imgCourseOutline', 'onclick')\n",
    "            if onclick_attr:\n",
    "                url_match = re.search(r\"window\\.open\\('([^']+)'\", onclick_attr)\n",
    "                if url_match:\n",
    "                    return url_match.group(1)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def extract_boss_ids_from_filepath(self, filepath):\n",
    "        \"\"\"Extract BOSS IDs from filepath\"\"\"\n",
    "        try:\n",
    "            filename = os.path.basename(filepath)\n",
    "            acad_term_match = re.search(r'SelectedAcadTerm=(\\d+)', filename)\n",
    "            class_match = re.search(r'SelectedClassNumber=(\\d+)', filename)\n",
    "            \n",
    "            acad_term_boss_id = int(acad_term_match.group(1)) if acad_term_match else None\n",
    "            class_boss_id = int(class_match.group(1)) if class_match else None\n",
    "            \n",
    "            return acad_term_boss_id, class_boss_id\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_meeting_information(self, record_key):\n",
    "        \"\"\"Extract class timing and exam timing information using Selenium\"\"\"\n",
    "        try:\n",
    "            meeting_table = self.driver.find_element(By.ID, 'RadGrid_MeetingInfo_ctl00')\n",
    "            tbody = meeting_table.find_element(By.TAG_NAME, 'tbody')\n",
    "            rows = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if len(cells) < 7:\n",
    "                    continue\n",
    "                \n",
    "                meeting_type = cells[0].text.strip()\n",
    "                start_date_text = cells[1].text.strip()\n",
    "                end_date_text = cells[2].text.strip()\n",
    "                day_of_week = cells[3].text.strip()\n",
    "                start_time = cells[4].text.strip()\n",
    "                end_time = cells[5].text.strip()\n",
    "                venue = cells[6].text.strip() if len(cells) > 6 else \"\"\n",
    "                professor_name = cells[7].text.strip() if len(cells) > 7 else \"\"\n",
    "                \n",
    "                # Assume CLASS if meeting_type is empty\n",
    "                if not meeting_type:\n",
    "                    meeting_type = 'CLASS'\n",
    "                \n",
    "                if meeting_type == 'CLASS':\n",
    "                    # Convert dates to timestamp format\n",
    "                    start_date = self.convert_date_to_timestamp(start_date_text)\n",
    "                    end_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    timing_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'CLASS',\n",
    "                        'start_date': start_date,\n",
    "                        'end_date': end_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(timing_record)\n",
    "                \n",
    "                elif meeting_type == 'EXAM':\n",
    "                    # For exams, use the second date (end_date_text) as the exam date\n",
    "                    exam_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    exam_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'EXAM',\n",
    "                        'date': exam_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(exam_record)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'record_key': record_key,\n",
    "                'error': f'Error extracting meeting information: {str(e)}',\n",
    "                'type': 'parse_error'\n",
    "            })\n",
    "    \n",
    "    def process_html_file(self, filepath):\n",
    "        \"\"\"Process a single HTML file and extract all data using Selenium\"\"\"\n",
    "        try:\n",
    "            # Load HTML file\n",
    "            html_file = Path(filepath).resolve()\n",
    "            file_url = html_file.as_uri()\n",
    "            self.driver.get(file_url)\n",
    "            \n",
    "            # Create unique record key\n",
    "            record_key = f\"{os.path.basename(filepath)}\"\n",
    "            \n",
    "            # Extract basic information\n",
    "            class_header_text = self.safe_find_element_text(By.ID, 'lblClassInfoHeader')\n",
    "            if not class_header_text:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': 'Missing class header',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            course_code, section = self.parse_course_and_section(class_header_text)\n",
    "            \n",
    "            # Extract academic term\n",
    "            term_text = self.safe_find_element_text(By.ID, 'lblClassInfoSubHeader')\n",
    "            acad_year_start, acad_year_end, term, acad_term_id = self.parse_acad_term(term_text, filepath) if term_text else (None, None, None, None)\n",
    "            \n",
    "            # Extract course information\n",
    "            course_name = self.safe_find_element_text(By.ID, 'lblClassSection')\n",
    "            course_description = self.safe_find_element_text(By.ID, 'lblCourseDescription')\n",
    "            credit_units_text = self.safe_find_element_text(By.ID, 'lblUnits')\n",
    "            course_areas = self.extract_course_areas_list()\n",
    "            enrolment_requirements = self.safe_find_element_text(By.ID, 'lblEnrolmentRequirements')\n",
    "            \n",
    "            # Process credit units\n",
    "            try:\n",
    "                credit_units = float(credit_units_text) if credit_units_text else None\n",
    "            except (ValueError, TypeError):\n",
    "                credit_units = None\n",
    "            \n",
    "            # Extract grading basis\n",
    "            grading_text = self.safe_find_element_text(By.ID, 'lblGradingBasis')\n",
    "            grading_basis = None\n",
    "            if grading_text:\n",
    "                if grading_text.lower() == 'graded':\n",
    "                    grading_basis = 'Graded'\n",
    "                elif grading_text.lower() in ['pass/fail', 'pass fail']:\n",
    "                    grading_basis = 'Pass/Fail'\n",
    "                else:\n",
    "                    grading_basis = 'NA'\n",
    "            \n",
    "            # Extract course outline URL\n",
    "            course_outline_url = self.extract_course_outline_url()\n",
    "            \n",
    "            # Extract dates\n",
    "            period_text = self.safe_find_element_text(By.ID, 'lblDates')\n",
    "            start_dt, end_dt = self.parse_date_range(period_text) if period_text else (None, None)\n",
    "            \n",
    "            # Extract BOSS IDs\n",
    "            acad_term_boss_id, class_boss_id = self.extract_boss_ids_from_filepath(filepath)\n",
    "            \n",
    "            # Create standalone record\n",
    "            standalone_record = {\n",
    "                'record_key': record_key,\n",
    "                'filepath': filepath,\n",
    "                'course_code': course_code,\n",
    "                'section': section,\n",
    "                'course_name': course_name,\n",
    "                'course_description': course_description,\n",
    "                'credit_units': credit_units,\n",
    "                'course_area': course_areas,\n",
    "                'enrolment_requirements': enrolment_requirements,\n",
    "                'acad_term_id': acad_term_id,\n",
    "                'acad_year_start': acad_year_start,\n",
    "                'acad_year_end': acad_year_end,\n",
    "                'term': term,\n",
    "                'start_dt': start_dt,\n",
    "                'end_dt': end_dt,\n",
    "                'grading_basis': grading_basis,\n",
    "                'course_outline_url': course_outline_url,\n",
    "                'acad_term_boss_id': acad_term_boss_id,\n",
    "                'class_boss_id': class_boss_id,\n",
    "                'term_text': term_text,\n",
    "                'period_text': period_text\n",
    "            }\n",
    "            \n",
    "            self.standalone_data.append(standalone_record)\n",
    "            \n",
    "            # Extract meeting information\n",
    "            self.extract_meeting_information(record_key)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'filepath': filepath,\n",
    "                'error': str(e),\n",
    "                'type': 'processing_error'\n",
    "            })\n",
    "            return False\n",
    "    \n",
    "    def run_test(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv', test_count=10):\n",
    "        \"\"\"Randomly test the extraction on a subset of files\"\"\"\n",
    "        try:\n",
    "            print(f\"Starting test run with {test_count} randomly selected files...\")\n",
    "\n",
    "            # Reset data containers\n",
    "            self.standalone_data = []\n",
    "            self.multiple_data = []\n",
    "            self.errors = []\n",
    "\n",
    "            # Set up Selenium driver\n",
    "            self.setup_selenium_driver()\n",
    "\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "\n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            all_filepaths = df[filepath_column].dropna().tolist()\n",
    "\n",
    "            if len(all_filepaths) == 0:\n",
    "                raise ValueError(\"No valid filepaths found in CSV\")\n",
    "\n",
    "            # Randomly sample filepaths\n",
    "            sample_size = min(test_count, len(all_filepaths))\n",
    "            sampled_filepaths = random.sample(all_filepaths, sample_size)\n",
    "\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "\n",
    "            for i, filepath in enumerate(sampled_filepaths, start=1):\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"Processing test file {i}/{sample_size}: {os.path.basename(filepath)}\")\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "\n",
    "            print(f\"\\nTest run complete: {successful_files}/{processed_files} files successful\")\n",
    "            print(f\"Standalone records extracted: {len(self.standalone_data)}\")\n",
    "            print(f\"Multiple records extracted: {len(self.multiple_data)}\")\n",
    "            if self.errors:\n",
    "                print(f\"Errors encountered: {len(self.errors)}\")\n",
    "                for error in self.errors[:3]:  # Show only the first 3 errors\n",
    "                    print(f\"  - {error['type']}: {error['error']}\")\n",
    "\n",
    "            # Save test results\n",
    "            test_output_path = 'script_input/test_raw_data.xlsx'\n",
    "            self.save_to_excel(test_output_path)\n",
    "\n",
    "            return successful_files > 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in test run: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Test selenium driver closed\")\n",
    "    \n",
    "    def process_all_files(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv'):\n",
    "        \"\"\"Process all files listed in the scraped filepaths CSV\"\"\"\n",
    "        try:\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "            \n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            \n",
    "            total_files = len(df)\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "            \n",
    "            print(f\"Starting to process {total_files} files\")\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                filepath = row[filepath_column]\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                    \n",
    "                    if processed_files % 100 == 0:\n",
    "                        print(f\"Processed {processed_files}/{total_files} files\")\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "            \n",
    "            print(f\"Processing complete: {successful_files}/{processed_files} files successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_all_files: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_to_excel(self, output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Save extracted data to Excel file with two sheets\"\"\"\n",
    "        try:\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Create DataFrames\n",
    "            standalone_df = pd.DataFrame(self.standalone_data)\n",
    "            multiple_df = pd.DataFrame(self.multiple_data)\n",
    "            \n",
    "            # Save to Excel with multiple sheets\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                standalone_df.to_excel(writer, sheet_name='standalone', index=False)\n",
    "                multiple_df.to_excel(writer, sheet_name='multiple', index=False)\n",
    "                \n",
    "                # Also save errors if any\n",
    "                if self.errors:\n",
    "                    errors_df = pd.DataFrame(self.errors)\n",
    "                    errors_df.to_excel(writer, sheet_name='errors', index=False)\n",
    "            \n",
    "            print(f\"Data saved to {output_path}\")\n",
    "            print(f\"Standalone records: {len(self.standalone_data)}\")\n",
    "            print(f\"Multiple records: {len(self.multiple_data)}\")\n",
    "            if self.errors:\n",
    "                print(f\"Errors: {len(self.errors)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Excel: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv', output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Run the complete extraction process\"\"\"\n",
    "        print(\"Starting HTML data extraction...\")\n",
    "        \n",
    "        # Reset data containers\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        \n",
    "        # Set up Selenium driver\n",
    "        self.setup_selenium_driver()\n",
    "        \n",
    "        try:\n",
    "            # Process all files\n",
    "            self.process_all_files(scraped_filepaths_csv)\n",
    "            \n",
    "            # Save to Excel\n",
    "            self.save_to_excel(output_path)\n",
    "            \n",
    "            print(\"HTML data extraction completed!\")\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Selenium driver closed\")\n",
    "\n",
    "    def clean_text_encoding(self, text):\n",
    "        \"\"\"Clean text to fix encoding issues like â€\" -> —\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        # Common encoding fixes - ORDER MATTERS! Process longer patterns first\n",
    "        encoding_fixes = [\n",
    "            ('â€\"', '—'),   # em-dash\n",
    "            ('â€™', \"'\"),   # right single quotation mark\n",
    "            ('â€œ', '\"'),   # left double quotation mark\n",
    "            ('â€¦', '…'),   # horizontal ellipsis\n",
    "            ('â€¢', '•'),   # bullet\n",
    "            ('â€‹', ''),    # zero-width space\n",
    "            ('â€‚', ' '),   # en space\n",
    "            ('â€ƒ', ' '),   # em space\n",
    "            ('â€‰', ' '),   # thin space\n",
    "            ('â€', '\"'),    # right double quotation mark (shorter pattern, process last)\n",
    "            ('Â', ''),      # non-breaking space artifacts\n",
    "        ]\n",
    "        \n",
    "        cleaned_text = text\n",
    "        # Process in order to avoid substring conflicts\n",
    "        for bad, good in encoding_fixes:\n",
    "            cleaned_text = cleaned_text.replace(bad, good)\n",
    "        \n",
    "        # Remove any remaining problematic characters\n",
    "        cleaned_text = re.sub(r'â€[^\\w]', '', cleaned_text)\n",
    "        \n",
    "        return cleaned_text.strip()\n",
    "    \n",
    "    def extract_term_from_folder_path(self, filepath):\n",
    "        \"\"\"Extract term from folder path as fallback\n",
    "        E.g., script_input\\\\classTimingsFull\\\\2023-24_T3A -> T3A\"\"\"\n",
    "        try:\n",
    "            # Get the folder path\n",
    "            folder_path = os.path.dirname(filepath)\n",
    "            folder_name = os.path.basename(folder_path)\n",
    "            \n",
    "            # Look for term pattern in folder name\n",
    "            # Pattern: YYYY-YY_TXX or YYYY-YY_TXXA\n",
    "            term_match = re.search(r'(\\d{4}-\\d{2})_T(\\w+)', folder_name)\n",
    "            if term_match:\n",
    "                return f\"T{term_match.group(2)}\"\n",
    "            \n",
    "            # Fallback: look for any T followed by alphanumeric\n",
    "            term_fallback = re.search(r'T(\\w+)', folder_name)\n",
    "            if term_fallback:\n",
    "                return f\"T{term_fallback.group(1)}\"\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Run the extraction process\n",
    "extractor.run(scraped_filepaths_csv='script_input/scraped_filepaths.csv', output_path='script_input/raw_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **4. Process Raw Data into Database Tables**\n",
    "\n",
    "### **What This Code Does**\n",
    "The `TableBuilder` class processes structured data from the HTML extractor and transforms it into database-ready CSV files for SMU's class management system. It handles complex data relationships, professor name normalization, duplicate detection, and creates all necessary tables for courses, classes, professors, timing schedules, bidding data, and faculty assignments while maintaining referential integrity across all database tables.\n",
    "\n",
    "**Key Features:**\n",
    "- **Three-Phase Processing**: Phase 1 (professors/courses with automated faculty mapping), Phase 2 (classes/timings), Phase 3 (BOSS bidding results)\n",
    "- **Intelligent Professor Matching**: Advanced name normalization with email resolution via Outlook integration and comprehensive duplicate detection\n",
    "- **Automated Faculty Mapping**: Uses BOSS data to automatically assign courses to SMU's schools and centers based on department codes\n",
    "- **Comprehensive Data Pipeline**: Processes professors, courses, academic terms, classes, class timings, exam schedules, bid windows, class availability, and bid results\n",
    "- **Database Cache Integration**: Loads existing data from PostgreSQL to avoid duplicates and maintain consistency\n",
    "- **Manual Review Workflow**: Outputs verification files for human review and correction before final processing\n",
    "- **Asian Name Handling**: Specialized normalization for Asian, Western, and mixed naming conventions with hardcoded multi-instructor handling\n",
    "- **BOSS Integration**: Complete processing of SMU's bidding system results with hierarchical window ordering and failed mapping tracking\n",
    "- **Data Integrity Validation**: Comprehensive validation system that checks referential integrity across all generated CSV files\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Raw Data Excel**: `script_input/raw_data.xlsx` from HTML extractor with `standalone` and `multiple` sheets\n",
    "- **BOSS Results**: Excel files in `script_input/overallBossResults/` directory for bidding data processing\n",
    "- **Database Configuration**: `.env` file with PostgreSQL connection parameters\n",
    "- **Professor Lookup**: `script_input/professor_lookup.csv` for existing professor mappings (optional)\n",
    "\n",
    "**Output Structure:**\n",
    "- **Verification Files** (`script_output/verify/`): `new_professors.csv`, `new_courses.csv`, `new_faculties.csv`\n",
    "- **Database Insert Files** (`script_output/`): All table CSV files including classes, timings, exams, bid data, and academic terms\n",
    "- **Validation Reports**: Data integrity validation with error/warning reports and statistics\n",
    "- **Processing Logs**: Detailed BOSS processing logs with timestamps and failure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TableBuilder:\n",
    "    \"\"\"Comprehensive table builder for university class management system\"\"\"\n",
    "    \n",
    "    def __init__(self, input_file: str = 'script_input/raw_data.xlsx'):\n",
    "        \"\"\"Initialize TableBuilder with database configuration and caching\"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_base = 'script_output'\n",
    "        self.verify_dir = os.path.join(self.output_base, 'verify')\n",
    "        self.cache_dir = 'db_cache'\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(self.output_base, exist_ok=True)\n",
    "        os.makedirs(self.verify_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        self.db_config = {\n",
    "            'host': os.getenv('DB_HOST'),\n",
    "            'database': os.getenv('DB_NAME'),\n",
    "            'user': os.getenv('DB_USER'),\n",
    "            'password': os.getenv('DB_PASSWORD'),\n",
    "            'port': int(os.getenv('DB_PORT', 5432)),\n",
    "            'gssencmode': 'disable'\n",
    "        }\n",
    "        \n",
    "        # Database connection\n",
    "        self.connection = None\n",
    "        \n",
    "        # Data storage\n",
    "        self.standalone_data = None\n",
    "        self.multiple_data = None\n",
    "        \n",
    "        # Caches\n",
    "        self.professors_cache = {}  # name -> professor data\n",
    "        self.courses_cache = {}     # code -> course data\n",
    "        self.acad_term_cache = {}   # id -> acad_term data\n",
    "        self.faculties_cache = {}   # id -> faculty data\n",
    "        self.faculty_acronym_to_id = {}  # acronym -> faculty_id mapping\n",
    "        self.professor_lookup = {}  # scraped_name -> database mapping\n",
    "        \n",
    "        # Output data collectors\n",
    "        self.new_professors = []\n",
    "        self.new_courses = []\n",
    "        self.update_courses = []\n",
    "        self.new_acad_terms = []\n",
    "        self.new_classes = []\n",
    "        self.new_class_timings = []\n",
    "        self.new_class_exam_timings = []\n",
    "        \n",
    "        # Class ID mapping for timing tables\n",
    "        self.class_id_mapping = {}  # record_key -> class_id\n",
    "        \n",
    "        # Courses requiring faculty assignment\n",
    "        self.courses_needing_faculty = []\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'professors_created': 0,\n",
    "            'courses_created': 0,\n",
    "            'courses_updated': 0,\n",
    "            'classes_created': 0,\n",
    "            'timings_created': 0,\n",
    "            'exams_created': 0,\n",
    "            'courses_needing_faculty': 0\n",
    "        }\n",
    "        \n",
    "        # Asian surnames database for name normalization\n",
    "        self.asian_surnames = {\n",
    "            'chinese': ['WANG', 'LI', 'ZHANG', 'LIU', 'CHEN', 'YANG', 'HUANG', 'ZHAO', 'WU', 'ZHOU',\n",
    "                       'XU', 'SUN', 'MA', 'ZHU', 'HU', 'GUO', 'HE', 'LIN', 'GAO', 'LUO'],\n",
    "            'singaporean': ['TAN', 'LIM', 'LEE', 'NG', 'ONG', 'WONG', 'GOH', 'CHUA', 'CHAN', 'KOH',\n",
    "                           'TEO', 'AW', 'CHYE', 'YEO', 'SIM', 'CHIA', 'CHONG', 'LAM', 'CHEW', 'TOH'],\n",
    "            'korean': ['KIM', 'LEE', 'PARK', 'CHOI', 'JUNG', 'KANG', 'CHO', 'YUN', 'JANG', 'LIM'],\n",
    "            'vietnamese': ['NGUYEN', 'TRAN', 'LE', 'PHAM', 'HOANG', 'PHAN', 'VU', 'DANG', 'BUI'],\n",
    "            'indian': ['SHARMA', 'SINGH', 'KUMAR', 'GUPTA', 'KOHLI', 'PATEL', 'MAKHIJA']\n",
    "        }\n",
    "        self.all_asian_surnames = set()\n",
    "        for surnames in self.asian_surnames.values():\n",
    "            self.all_asian_surnames.update(surnames)\n",
    "        \n",
    "        # Western given names\n",
    "        self.western_given_names = {\n",
    "            'AARON', 'ADAM', 'ADRIAN', 'ALEXANDER', 'AMANDA', 'ANDREW', 'ANTHONY',\n",
    "            'BENJAMIN', 'CHRISTOPHER', 'DANIEL', 'DAVID', 'EMILY', 'JAMES', 'JENNIFER',\n",
    "            'JOHN', 'MICHAEL', 'PETER', 'ROBERT', 'SARAH', 'THOMAS', 'WILLIAM'\n",
    "        }\n",
    "\n",
    "        # Bid results data collectors\n",
    "        self.boss_log_file = os.path.join(self.output_base, 'boss_result_log.txt')\n",
    "        self.new_bid_windows = []\n",
    "        self.new_class_availability = []\n",
    "        self.new_bid_results = []\n",
    "\n",
    "        # Professor lookup from CSV\n",
    "        self.professor_lookup = {}\n",
    "        \n",
    "        # Load professor lookup if available\n",
    "        self.load_professor_lookup_csv()\n",
    "\n",
    "    def connect_database(self):\n",
    "        \"\"\"Connect to PostgreSQL database\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(**self.db_config)\n",
    "            logger.info(\"✅ Database connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Database connection failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_or_cache_data(self):\n",
    "        \"\"\"Load data from cache or database\"\"\"\n",
    "        # Try loading from cache first\n",
    "        if self._load_from_cache():\n",
    "            logger.info(\"✅ Loaded data from cache\")\n",
    "            return True\n",
    "        \n",
    "        # Connect to database and download\n",
    "        if not self.connect_database():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self._download_and_cache_data()\n",
    "            logger.info(\"✅ Downloaded and cached data from database\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to download data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _download_and_cache_data(self):\n",
    "        \"\"\"Download data from database and cache locally - includes all requested tables\"\"\"\n",
    "        try:\n",
    "            # Download professors\n",
    "            query = \"SELECT * FROM professors\"\n",
    "            professors_df = pd.read_sql_query(query, self.connection)\n",
    "            professors_df.to_pickle(os.path.join(self.cache_dir, 'professors_cache.pkl'))\n",
    "            \n",
    "            # Download courses\n",
    "            query = \"SELECT * FROM courses\"\n",
    "            courses_df = pd.read_sql_query(query, self.connection)\n",
    "            courses_df.to_pickle(os.path.join(self.cache_dir, 'courses_cache.pkl'))\n",
    "            \n",
    "            # Download acad_terms\n",
    "            query = \"SELECT * FROM acad_term\"\n",
    "            acad_terms_df = pd.read_sql_query(query, self.connection)\n",
    "            acad_terms_df.to_pickle(os.path.join(self.cache_dir, 'acad_terms_cache.pkl'))\n",
    "            \n",
    "            # Download faculties\n",
    "            query = \"SELECT * FROM faculties\"\n",
    "            faculties_df = pd.read_sql_query(query, self.connection)\n",
    "            faculties_df.to_pickle(os.path.join(self.cache_dir, 'faculties_cache.pkl'))\n",
    "            \n",
    "            # Download bid_result\n",
    "            query = \"SELECT * FROM bid_result\"\n",
    "            bid_result_df = pd.read_sql_query(query, self.connection)\n",
    "            bid_result_df.to_pickle(os.path.join(self.cache_dir, 'bid_result_cache.pkl'))\n",
    "            \n",
    "            # Download bid_window\n",
    "            query = \"SELECT * FROM bid_window\"\n",
    "            bid_window_df = pd.read_sql_query(query, self.connection)\n",
    "            bid_window_df.to_pickle(os.path.join(self.cache_dir, 'bid_window_cache.pkl'))\n",
    "            \n",
    "            # Download class_availability\n",
    "            query = \"SELECT * FROM class_availability\"\n",
    "            class_availability_df = pd.read_sql_query(query, self.connection)\n",
    "            class_availability_df.to_pickle(os.path.join(self.cache_dir, 'class_availability_cache.pkl'))\n",
    "            \n",
    "            # Download class_exam_timing\n",
    "            query = \"SELECT * FROM class_exam_timing\"\n",
    "            class_exam_timing_df = pd.read_sql_query(query, self.connection)\n",
    "            class_exam_timing_df.to_pickle(os.path.join(self.cache_dir, 'class_exam_timing_cache.pkl'))\n",
    "            \n",
    "            # Download class_timing\n",
    "            query = \"SELECT * FROM class_timing\"\n",
    "            class_timing_df = pd.read_sql_query(query, self.connection)\n",
    "            class_timing_df.to_pickle(os.path.join(self.cache_dir, 'class_timing_cache.pkl'))\n",
    "            \n",
    "            # Download classes\n",
    "            query = \"SELECT * FROM classes\"\n",
    "            classes_df = pd.read_sql_query(query, self.connection)\n",
    "            classes_df.to_pickle(os.path.join(self.cache_dir, 'classes_cache.pkl'))\n",
    "            \n",
    "            logger.info(\"✅ Downloaded all tables from database and cached locally\")\n",
    "            \n",
    "            # Load into memory\n",
    "            self._load_from_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to download and cache data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _load_from_cache(self) -> bool:\n",
    "        \"\"\"Load cached data from files\"\"\"\n",
    "        try:\n",
    "            cache_files = {\n",
    "                'professors': os.path.join(self.cache_dir, 'professors_cache.pkl'),\n",
    "                'courses': os.path.join(self.cache_dir, 'courses_cache.pkl'),\n",
    "                'acad_terms': os.path.join(self.cache_dir, 'acad_terms_cache.pkl'),\n",
    "                'faculties': os.path.join(self.cache_dir, 'faculties_cache.pkl'),\n",
    "                'bid_result': os.path.join(self.cache_dir, 'bid_result_cache.pkl'),\n",
    "                'bid_window': os.path.join(self.cache_dir, 'bid_window_cache.pkl'),\n",
    "                'class_availability': os.path.join(self.cache_dir, 'class_availability_cache.pkl'),\n",
    "                'class_exam_timing': os.path.join(self.cache_dir, 'class_exam_timing_cache.pkl'),\n",
    "                'class_timing': os.path.join(self.cache_dir, 'class_timing_cache.pkl'),\n",
    "                'classes': os.path.join(self.cache_dir, 'classes_cache.pkl')\n",
    "            }\n",
    "            \n",
    "            if all(os.path.exists(f) for f in cache_files.values()):\n",
    "                # Load professors - KEY CHANGE: Use boss_name if available\n",
    "                professors_df = pd.read_pickle(cache_files['professors'])\n",
    "                for _, row in professors_df.iterrows():\n",
    "                    # Safely handle None values\n",
    "                    boss_name = row.get('boss_name')\n",
    "                    regular_name = row.get('name')\n",
    "                    \n",
    "                    # Skip records with no valid name data\n",
    "                    if boss_name is None and (regular_name is None or pd.isna(regular_name)):\n",
    "                        continue\n",
    "                        \n",
    "                    # Use boss_name if available and not None, otherwise use name.upper()\n",
    "                    if boss_name is not None and not pd.isna(boss_name):\n",
    "                        cache_key = str(boss_name)\n",
    "                    elif regular_name is not None and not pd.isna(regular_name):\n",
    "                        cache_key = str(regular_name).upper()\n",
    "                    else:\n",
    "                        continue  # Skip if both are None/NaN\n",
    "                        \n",
    "                    self.professors_cache[cache_key] = row.to_dict()\n",
    "                \n",
    "                # Load courses\n",
    "                courses_df = pd.read_pickle(cache_files['courses'])\n",
    "                for _, row in courses_df.iterrows():\n",
    "                    self.courses_cache[row['code']] = row.to_dict()\n",
    "                \n",
    "                # Load acad_terms\n",
    "                acad_terms_df = pd.read_pickle(cache_files['acad_terms'])\n",
    "                for _, row in acad_terms_df.iterrows():\n",
    "                    self.acad_term_cache[row['id']] = row.to_dict()\n",
    "                \n",
    "                # Load faculties\n",
    "                faculties_df = pd.read_pickle(cache_files['faculties'])\n",
    "                for _, row in faculties_df.iterrows():\n",
    "                    faculty_id = row['id']\n",
    "                    acronym = row['acronym'].upper()\n",
    "                    self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                    self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                \n",
    "                # Load professor lookup if exists - FIXED: Proper validation for clean professor_lookup.csv\n",
    "                lookup_file = 'script_input/professor_lookup.csv'\n",
    "                if os.path.exists(lookup_file):\n",
    "                    lookup_df = pd.read_csv(lookup_file)\n",
    "                    for _, row in lookup_df.iterrows():\n",
    "                        boss_name = row.get('boss_name')\n",
    "                        afterclass_name = row.get('afterclass_name')\n",
    "                        database_id = row.get('database_id')\n",
    "                        \n",
    "                        # Validate required fields - professor_lookup.csv should be clean but pandas might introduce NaN\n",
    "                        if pd.isna(boss_name) or pd.isna(database_id):\n",
    "                            continue\n",
    "                        \n",
    "                        # Convert to string and validate\n",
    "                        boss_name_str = str(boss_name).strip()\n",
    "                        if not boss_name_str or boss_name_str.lower() == 'nan':\n",
    "                            continue\n",
    "                        \n",
    "                        self.professor_lookup[boss_name_str.upper()] = {\n",
    "                            'database_id': str(database_id),\n",
    "                            'boss_name': boss_name_str,\n",
    "                            'afterclass_name': str(afterclass_name) if not pd.isna(afterclass_name) else boss_name_str\n",
    "                        }\n",
    "                \n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cache loading error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_raw_data(self):\n",
    "        \"\"\"Load raw data from Excel file\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"📂 Loading raw data from {self.input_file}\")\n",
    "            \n",
    "            # Load both sheets\n",
    "            self.standalone_data = pd.read_excel(self.input_file, sheet_name='standalone')\n",
    "            self.multiple_data = pd.read_excel(self.input_file, sheet_name='multiple')\n",
    "            \n",
    "            logger.info(f\"✅ Loaded {len(self.standalone_data)} standalone records\")\n",
    "            logger.info(f\"✅ Loaded {len(self.multiple_data)} multiple records\")\n",
    "            \n",
    "            from collections import defaultdict\n",
    "            \n",
    "            self.multiple_lookup = defaultdict(list)\n",
    "            for _, row in self.multiple_data.iterrows():\n",
    "                key = row.get('record_key')\n",
    "                if pd.notna(key):\n",
    "                    self.multiple_lookup[key].append(row)\n",
    "            \n",
    "            logger.info(f\"✅ Created optimized lookup for {len(self.multiple_lookup)} record keys\")\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load raw data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def normalize_professor_name(self, name: str) -> Tuple[str, str]:\n",
    "        \"\"\"Normalize professor name with improved handling for special cases\"\"\"\n",
    "        # Handle None, NaN, and empty values\n",
    "        if name is None or pd.isna(name) or name == \"\":\n",
    "            return \"UNKNOWN\", \"Unknown\"\n",
    "        \n",
    "        # Clean and prepare name - ensure it's a string\n",
    "        name = str(name).strip()\n",
    "        \n",
    "        # Additional safety check after conversion\n",
    "        if not name:\n",
    "            return \"UNKNOWN\", \"Unknown\"\n",
    "        \n",
    "        # Handle comma-separated names (SURNAME, GIVEN format)\n",
    "        # These are almost always single professors\n",
    "        if ',' in name:\n",
    "            parts = name.split(',')\n",
    "            if len(parts) == 2:\n",
    "                # Standard \"SURNAME, GIVEN\" format\n",
    "                surname_part = parts[0].strip()\n",
    "                given_part = parts[1].strip()\n",
    "                # Reconstruct as \"SURNAME Given Names\"\n",
    "                given_names_title = ' '.join(word.capitalize() for word in given_part.split())\n",
    "                name = f\"{surname_part.upper()} {given_names_title}\"\n",
    "        \n",
    "        # Detect naming pattern\n",
    "        words = name.split()\n",
    "        if not words:\n",
    "            return name.upper(), name\n",
    "        \n",
    "        # Detect pattern\n",
    "        pattern = self._detect_name_pattern(words)\n",
    "        \n",
    "        # Format based on pattern\n",
    "        if pattern == 'WESTERN':\n",
    "            # Western: Given SURNAME\n",
    "            boss_name = name.upper()\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == len(words) - 1:  # Last word is surname\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        elif pattern == 'ASIAN':\n",
    "            # Asian: SURNAME Given Given\n",
    "            boss_name = name.upper()\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == 0:  # First word is surname\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        elif pattern == 'SINGAPOREAN':\n",
    "            # Singaporean: Given SURNAME Given\n",
    "            boss_name = name.upper()\n",
    "            surname_idx = self._find_surname_index(words)\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == surname_idx:\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        else:\n",
    "            # Default fallback\n",
    "            boss_name = name.upper()\n",
    "            afterclass_name = ' '.join(word.capitalize() for word in words)\n",
    "        \n",
    "        return boss_name, afterclass_name\n",
    "\n",
    "    def _detect_name_pattern(self, words: List[str]) -> str:\n",
    "        \"\"\"Detect naming pattern: WESTERN, ASIAN, or SINGAPOREAN\"\"\"\n",
    "        if not words:\n",
    "            return 'UNKNOWN'\n",
    "        \n",
    "        # Check for Western pattern\n",
    "        first_upper = words[0].upper()\n",
    "        if first_upper in self.western_given_names:\n",
    "            return 'WESTERN'\n",
    "        \n",
    "        # Check for pure Asian pattern\n",
    "        if first_upper in self.all_asian_surnames:\n",
    "            # Check if no Western names present\n",
    "            has_western = any(w.upper() in self.western_given_names for w in words)\n",
    "            if not has_western:\n",
    "                return 'ASIAN'\n",
    "        \n",
    "        # Check for Singaporean mixed pattern\n",
    "        if len(words) >= 3:\n",
    "            if (words[0].upper() in self.western_given_names and \n",
    "                any(w.upper() in self.all_asian_surnames for w in words[1:])):\n",
    "                return 'SINGAPOREAN'\n",
    "        \n",
    "        # Default to Western if unclear\n",
    "        return 'WESTERN'\n",
    "\n",
    "    def _find_surname_index(self, words: List[str]) -> int:\n",
    "        \"\"\"Find the index of surname in a list of words\"\"\"\n",
    "        for i, word in enumerate(words):\n",
    "            if word.upper() in self.all_asian_surnames:\n",
    "                return i\n",
    "        # Default to last word if no Asian surname found\n",
    "        return len(words) - 1\n",
    "\n",
    "    def resolve_professor_email(self, professor_name):\n",
    "        \"\"\"Resolve professor email using Outlook contacts\"\"\"\n",
    "        try:\n",
    "            # Initialize Outlook\n",
    "            outlook = win32.Dispatch(\"Outlook.Application\")\n",
    "            namespace = outlook.GetNamespace(\"MAPI\")\n",
    "            \n",
    "            # Try exact resolver first\n",
    "            recipient = namespace.CreateRecipient(professor_name)\n",
    "            if recipient.Resolve():\n",
    "                # Try to get SMTP address\n",
    "                address_entry = recipient.AddressEntry\n",
    "                \n",
    "                # Try Exchange user\n",
    "                try:\n",
    "                    exchange_user = address_entry.GetExchangeUser()\n",
    "                    if exchange_user and exchange_user.PrimarySmtpAddress:\n",
    "                        return exchange_user.PrimarySmtpAddress.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try Exchange distribution list\n",
    "                try:\n",
    "                    exchange_dl = address_entry.GetExchangeDistributionList()\n",
    "                    if exchange_dl and exchange_dl.PrimarySmtpAddress:\n",
    "                        return exchange_dl.PrimarySmtpAddress.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Try PR_SMTP_ADDRESS property\n",
    "                try:\n",
    "                    property_accessor = address_entry.PropertyAccessor\n",
    "                    smtp_addr = property_accessor.GetProperty(\"http://schemas.microsoft.com/mapi/proptag/0x39FE001E\")\n",
    "                    if smtp_addr:\n",
    "                        return smtp_addr.lower()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Fallback: regex search in Address field\n",
    "                try:\n",
    "                    address = getattr(address_entry, \"Address\", \"\") or \"\"\n",
    "                    match = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", address)\n",
    "                    if match:\n",
    "                        return match.group(0).lower()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # If exact resolve fails, try contacts search\n",
    "            contacts_folder = namespace.GetDefaultFolder(10)  # olFolderContacts\n",
    "            tokens = [t.lower() for t in professor_name.split() if t]\n",
    "            \n",
    "            for item in contacts_folder.Items:\n",
    "                try:\n",
    "                    full_name = (item.FullName or \"\").lower()\n",
    "                    if all(token in full_name for token in tokens):\n",
    "                        # Try the three standard email slots\n",
    "                        for field in (\"Email1Address\", \"Email2Address\", \"Email3Address\"):\n",
    "                            addr = getattr(item, field, \"\") or \"\"\n",
    "                            if addr and \"@\" in addr:\n",
    "                                return addr.lower()\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # If no email found, return default\n",
    "            return 'enquiry@smu.edu.sg'\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Email resolution failed for {professor_name}: {e}\")\n",
    "            return 'enquiry@smu.edu.sg'\n",
    "        \n",
    "    def process_professors(self):\n",
    "        \"\"\"Process professors from multiple sheet with intelligent multi-instructor handling\"\"\"\n",
    "        logger.info(\"👥 Processing professors...\")\n",
    "        \n",
    "        unique_professors = set()\n",
    "        \n",
    "        # Extract unique professor names from multiple sheet - FIXED: Better NaN handling for raw_data.xlsx\n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            prof_name_raw = row.get('professor_name')\n",
    "            \n",
    "            # Handle NaN values from raw_data.xlsx properly\n",
    "            if prof_name_raw is None or pd.isna(prof_name_raw):\n",
    "                continue\n",
    "            \n",
    "            # Convert to string and validate - this handles float NaN values from pandas\n",
    "            prof_name = str(prof_name_raw).strip()\n",
    "            \n",
    "            # Skip empty strings and special values\n",
    "            if not prof_name or prof_name.lower() in ['nan', 'tba', 'to be announced']:\n",
    "                continue\n",
    "            \n",
    "            # Use intelligent splitting instead of hardcoded combinations\n",
    "            split_professors = self._split_professor_names(prof_name)\n",
    "            for individual_prof in split_professors:\n",
    "                if individual_prof and individual_prof.strip():  # Additional safety check\n",
    "                    unique_professors.add(individual_prof.strip())\n",
    "        \n",
    "        # Create email-to-professor mapping from existing professors for duplicate detection\n",
    "        # FIXED: Exclude default email from duplicate detection\n",
    "        email_to_professor = {}\n",
    "        for boss_name, prof_data in self.professors_cache.items():\n",
    "            if 'email' in prof_data and prof_data['email']:\n",
    "                # Skip default email for duplicate detection\n",
    "                if prof_data['email'].lower() != 'enquiry@smu.edu.sg':\n",
    "                    email_to_professor[prof_data['email'].lower()] = prof_data\n",
    "        \n",
    "        # Initialize fuzzy match tracking\n",
    "        fuzzy_matched_professors = []\n",
    "        \n",
    "        # Process each unique professor\n",
    "        for prof_name in unique_professors:\n",
    "            try:\n",
    "                boss_name, afterclass_name = self.normalize_professor_name(prof_name)\n",
    "                \n",
    "                # Step 1: Check professor_lookup.csv first - FIXED: Exact matching priority\n",
    "                if hasattr(self, 'professor_lookup') and prof_name.upper() in self.professor_lookup:\n",
    "                    logger.info(f\"✅ Found in professor_lookup.csv: {prof_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Also check with normalized boss_name\n",
    "                if hasattr(self, 'professor_lookup') and boss_name.upper() in self.professor_lookup:\n",
    "                    # Update the lookup with the original prof_name as key too\n",
    "                    self.professor_lookup[prof_name.upper()] = self.professor_lookup[boss_name.upper()]\n",
    "                    logger.info(f\"✅ Found in professor_lookup.csv by boss_name: {prof_name} → {boss_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 1.5: FIXED: Check for partial name matches in professor_lookup.csv boss_names\n",
    "                if hasattr(self, 'professor_lookup'):\n",
    "                    found_partial_match = False\n",
    "                    for lookup_boss_name, lookup_data in self.professor_lookup.items():\n",
    "                        # Check if prof_name is a substring of any boss_name (exact word matching)\n",
    "                        prof_words = set(prof_name.upper().split())\n",
    "                        lookup_words = set(lookup_boss_name.split())\n",
    "                        \n",
    "                        # If all words in prof_name are found in lookup_boss_name, it's a match\n",
    "                        if prof_words.issubset(lookup_words) and len(prof_words) >= 2:  # At least 2 words must match\n",
    "                            self.professor_lookup[prof_name.upper()] = lookup_data\n",
    "                            logger.info(f\"✅ Found partial match in professor_lookup.csv: {prof_name} → {lookup_boss_name}\")\n",
    "                            found_partial_match = True\n",
    "                            break\n",
    "                    \n",
    "                    if found_partial_match:\n",
    "                        continue\n",
    "                \n",
    "                # Step 2: Check exact matches in professors_cache\n",
    "                if boss_name in self.professors_cache:\n",
    "                    if not hasattr(self, 'professor_lookup'):\n",
    "                        self.professor_lookup = {}\n",
    "                    self.professor_lookup[prof_name.upper()] = {\n",
    "                        'database_id': self.professors_cache[boss_name]['id'],\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': self.professors_cache[boss_name].get('name', afterclass_name)\n",
    "                    }\n",
    "                    logger.info(f\"✅ Found in professors_cache: {prof_name} → {boss_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 3: Enhanced fuzzy matching (100% certain matches only)\n",
    "                fuzzy_match_found = False\n",
    "                normalized_prof = ' '.join(str(prof_name).replace(',', ' ').split()).upper()\n",
    "                \n",
    "                # Check against existing professors\n",
    "                for cached_name, cached_prof in self.professors_cache.items():\n",
    "                    if cached_name is None:\n",
    "                        continue\n",
    "                    cached_normalized = ' '.join(str(cached_name).replace(',', ' ').split()).upper()\n",
    "                    \n",
    "                    # Only match if exactly the same after normalization\n",
    "                    if normalized_prof == cached_normalized:\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = {\n",
    "                            'database_id': cached_prof['id'],\n",
    "                            'boss_name': cached_prof.get('boss_name', cached_prof['name'].upper()),\n",
    "                            'afterclass_name': cached_prof.get('name', afterclass_name)\n",
    "                        }\n",
    "                        fuzzy_match_found = True\n",
    "                        logger.info(f\"✅ Found fuzzy match (100% certain): {prof_name} → {cached_name}\")\n",
    "                        break\n",
    "                \n",
    "                if fuzzy_match_found:\n",
    "                    continue\n",
    "                \n",
    "                # Also check against new professors being created in this session\n",
    "                for new_prof in self.new_professors:\n",
    "                    new_normalized = ' '.join(new_prof.get('boss_name', '').replace(',', ' ').split()).upper()\n",
    "                    if normalized_prof == new_normalized:\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = {\n",
    "                            'database_id': new_prof['id'],\n",
    "                            'boss_name': new_prof['boss_name'],\n",
    "                            'afterclass_name': new_prof['afterclass_name']\n",
    "                        }\n",
    "                        fuzzy_match_found = True\n",
    "                        logger.info(f\"✅ Found in new_professors (100% certain): {prof_name}\")\n",
    "                        break\n",
    "                \n",
    "                if fuzzy_match_found:\n",
    "                    continue\n",
    "                \n",
    "                # Step 4: FIXED: Advanced fuzzy matching against boss_name and afterclass_name\n",
    "                if hasattr(self, 'professor_lookup'):\n",
    "                    best_fuzzy_match = None\n",
    "                    best_fuzzy_score = 0\n",
    "                    \n",
    "                    for lookup_boss_name, lookup_data in self.professor_lookup.items():\n",
    "                        # Get afterclass_name for comparison\n",
    "                        afterclass_candidate = lookup_data.get('afterclass_name', lookup_boss_name)\n",
    "                        \n",
    "                        # Fuzzy match against both boss_name and afterclass_name\n",
    "                        boss_score = self._calculate_fuzzy_score(prof_name, lookup_boss_name)\n",
    "                        afterclass_score = self._calculate_fuzzy_score(prof_name, afterclass_candidate)\n",
    "                        \n",
    "                        max_score = max(boss_score, afterclass_score)\n",
    "                        \n",
    "                        # Only consider very strong matches (85%+ similarity)\n",
    "                        if max_score > 0.85 and max_score > best_fuzzy_score:\n",
    "                            best_fuzzy_match = lookup_data\n",
    "                            best_fuzzy_score = max_score\n",
    "                    \n",
    "                    if best_fuzzy_match:\n",
    "                        # Add to fuzzy matched professors for validation\n",
    "                        fuzzy_matched_professors.append({\n",
    "                            'boss_name': prof_name.upper(),\n",
    "                            'afterclass_name': best_fuzzy_match.get('afterclass_name', prof_name),\n",
    "                            'database_id': best_fuzzy_match['database_id'],\n",
    "                            'method': 'fuzzy_match',\n",
    "                            'confidence_score': f\"{best_fuzzy_score:.2f}\"\n",
    "                        })\n",
    "                        \n",
    "                        # Update lookup\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = best_fuzzy_match\n",
    "                        \n",
    "                        logger.info(f\"🔍 Fuzzy match found: {prof_name} → {best_fuzzy_match.get('afterclass_name')} (score: {best_fuzzy_score:.2f})\")\n",
    "                        continue\n",
    "                \n",
    "                # Step 5: Validate professor name before creating - reject single words\n",
    "                prof_words = prof_name.strip().split()\n",
    "                if len(prof_words) == 1:\n",
    "                    logger.warning(f\"⚠️ Skipping single-word professor name (likely parsing error): '{prof_name}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 6: Create new professor and resolve email\n",
    "                resolved_email = self.resolve_professor_email(afterclass_name)\n",
    "                \n",
    "                # Step 7: Check if resolved email already exists (FIXED: Skip default email)\n",
    "                if (resolved_email and \n",
    "                    resolved_email.lower() != 'enquiry@smu.edu.sg' and \n",
    "                    resolved_email.lower() in email_to_professor):\n",
    "                    # Email already exists - use existing professor\n",
    "                    existing_prof = email_to_professor[resolved_email.lower()]\n",
    "                    if not hasattr(self, 'professor_lookup'):\n",
    "                        self.professor_lookup = {}\n",
    "                    self.professor_lookup[prof_name.upper()] = {\n",
    "                        'database_id': existing_prof['id'],\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': existing_prof.get('name', afterclass_name)\n",
    "                    }\n",
    "                    logger.info(f\"✅ Email duplicate found - using existing professor: {prof_name} → {existing_prof.get('name')} (email: {resolved_email})\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 8: Create new professor only if no match found\n",
    "                professor_id = str(uuid.uuid4())\n",
    "                slug = re.sub(r'[^a-zA-Z0-9]+', '-', afterclass_name.lower()).strip('-')\n",
    "                \n",
    "                new_prof = {\n",
    "                    'id': professor_id,\n",
    "                    'name': afterclass_name,\n",
    "                    'email': resolved_email,\n",
    "                    'slug': slug,\n",
    "                    'photo_url': 'https://smu.edu.sg',\n",
    "                    'profile_url': 'https://smu.edu.sg',\n",
    "                    'belong_to_university': 1,  # SMU\n",
    "                    'created_at': datetime.now().isoformat(),\n",
    "                    'updated_at': datetime.now().isoformat(),\n",
    "                    'boss_name': boss_name,\n",
    "                    'afterclass_name': afterclass_name,\n",
    "                    'original_scraped_name': prof_name\n",
    "                }\n",
    "                \n",
    "                self.new_professors.append(new_prof)\n",
    "                self.stats['professors_created'] += 1\n",
    "                \n",
    "                # Update lookup\n",
    "                if not hasattr(self, 'professor_lookup'):\n",
    "                    self.professor_lookup = {}\n",
    "                self.professor_lookup[prof_name.upper()] = {\n",
    "                    'database_id': professor_id,\n",
    "                    'boss_name': boss_name,\n",
    "                    'afterclass_name': afterclass_name\n",
    "                }\n",
    "                \n",
    "                # Add to email mapping to prevent duplicates within this session (FIXED: Skip default email)\n",
    "                if resolved_email and resolved_email.lower() != 'enquiry@smu.edu.sg':\n",
    "                    email_to_professor[resolved_email.lower()] = new_prof\n",
    "                \n",
    "                logger.info(f\"✅ Created professor: {afterclass_name} with email: {resolved_email}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Error processing professor '{prof_name}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save fuzzy matched professors for validation\n",
    "        if fuzzy_matched_professors:\n",
    "            fuzzy_df = pd.DataFrame(fuzzy_matched_professors)\n",
    "            fuzzy_path = os.path.join(self.verify_dir, 'fuzzy_matched_professors.csv')\n",
    "            fuzzy_df.to_csv(fuzzy_path, index=False)\n",
    "            logger.info(f\"🔍 Saved {len(fuzzy_matched_professors)} fuzzy matched professors for validation\")\n",
    "        \n",
    "        logger.info(f\"✅ Created {self.stats['professors_created']} new professors\")\n",
    "\n",
    "    def _calculate_fuzzy_score(self, name1: str, name2: str) -> float:\n",
    "        \"\"\"Calculate fuzzy similarity score between two names (0-1)\"\"\"\n",
    "        if not name1 or not name2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Normalize names\n",
    "        name1_clean = ' '.join(str(name1).upper().replace(',', ' ').split())\n",
    "        name2_clean = ' '.join(str(name2).upper().replace(',', ' ').split())\n",
    "        \n",
    "        if name1_clean == name2_clean:\n",
    "            return 1.0\n",
    "        \n",
    "        # Check word overlap\n",
    "        words1 = set(name1_clean.split())\n",
    "        words2 = set(name2_clean.split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        \n",
    "        jaccard_score = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        # Boost score if one name is completely contained in the other\n",
    "        if words1.issubset(words2) or words2.issubset(words1):\n",
    "            jaccard_score = min(1.0, jaccard_score + 0.2)\n",
    "        \n",
    "        return jaccard_score\n",
    "\n",
    "    def process_courses(self):\n",
    "        \"\"\"Process courses from standalone sheet with proper change detection for updates\"\"\"\n",
    "        logger.info(\"📚 Processing courses...\")\n",
    "        \n",
    "        # Group by course code to handle duplicates\n",
    "        course_groups = defaultdict(list)\n",
    "        for _, row in self.standalone_data.iterrows():\n",
    "            if pd.notna(row.get('course_code')):\n",
    "                course_groups[row['course_code']].append(row)\n",
    "        \n",
    "        for course_code, rows in course_groups.items():\n",
    "            # Helper function to get sortable key for academic term ordering\n",
    "            def get_sort_key(row):\n",
    "                year_start = row.get('acad_year_start', 0)\n",
    "                year_end = row.get('acad_year_end', 0)\n",
    "                term = str(row.get('term', ''))\n",
    "                \n",
    "                # Convert term to sortable format with proper hierarchy: T1 → T2 → T3A → T3B\n",
    "                term_order = {\n",
    "                    'T1': 1,\n",
    "                    'T2': 2,\n",
    "                    'T3A': 3,\n",
    "                    'T3B': 4,\n",
    "                    '1': 1,    # Handle cases without T prefix\n",
    "                    '2': 2,\n",
    "                    '3A': 3,\n",
    "                    '3B': 4\n",
    "                }\n",
    "                term_value = term_order.get(term.upper(), 0)\n",
    "                return (year_start, year_end, term_value)\n",
    "            \n",
    "            # Sort rows to get the latest one (highest year and term)\n",
    "            sorted_rows = sorted(rows, key=get_sort_key, reverse=True)\n",
    "            latest_row = sorted_rows[0]\n",
    "            \n",
    "            # Check if course exists in cache\n",
    "            if course_code in self.courses_cache:\n",
    "                # Course exists - check for actual changes that need updating\n",
    "                existing = self.courses_cache[course_code]\n",
    "                update_needed = False\n",
    "                update_record = {'id': existing['id'], 'code': course_code}\n",
    "                \n",
    "                # Fields to compare for changes\n",
    "                field_mapping = {\n",
    "                    'name': 'course_name',\n",
    "                    'description': 'course_description', \n",
    "                    'credit_units': 'credit_units',\n",
    "                    'course_area': 'course_area',\n",
    "                    'enrolment_requirements': 'enrolment_requirements'\n",
    "                }\n",
    "                \n",
    "                # Check each field for actual changes\n",
    "                for db_field, raw_field in field_mapping.items():\n",
    "                    new_value = latest_row.get(raw_field)\n",
    "                    old_value = existing.get(db_field)\n",
    "                    \n",
    "                    # Handle different data types properly\n",
    "                    if db_field == 'credit_units':\n",
    "                        # Convert to float for comparison\n",
    "                        new_value = float(new_value) if pd.notna(new_value) else None\n",
    "                        old_value = float(old_value) if pd.notna(old_value) else None\n",
    "                    else:\n",
    "                        # String comparison - handle None/NaN\n",
    "                        if pd.isna(new_value):\n",
    "                            new_value = None\n",
    "                        else:\n",
    "                            new_value = str(new_value).strip()\n",
    "                        \n",
    "                        if pd.isna(old_value):\n",
    "                            old_value = None\n",
    "                        else:\n",
    "                            old_value = str(old_value).strip() if old_value is not None else None\n",
    "                    \n",
    "                    # Only update if there's an actual change\n",
    "                    if new_value != old_value:\n",
    "                        # Skip if new value is empty/None and old value exists (don't overwrite with empty)\n",
    "                        if new_value is None or new_value == '':\n",
    "                            if old_value is not None and old_value != '':\n",
    "                                continue  # Don't overwrite existing data with empty data\n",
    "                        \n",
    "                        update_record[db_field] = new_value\n",
    "                        update_needed = True\n",
    "                        logger.info(f\"📝 Course {course_code}: {db_field} changed from '{old_value}' to '{new_value}'\")\n",
    "                \n",
    "                if update_needed:\n",
    "                    self.update_courses.append(update_record)\n",
    "                    self.stats['courses_updated'] += 1\n",
    "                    \n",
    "                    # Update cache with new values\n",
    "                    for field, value in update_record.items():\n",
    "                        if field not in ['id', 'code']:\n",
    "                            self.courses_cache[course_code][field] = value\n",
    "                            \n",
    "                    logger.info(f\"✅ Course {course_code} marked for update\")\n",
    "                else:\n",
    "                    logger.info(f\"⏭️ Course {course_code} - no changes detected\")\n",
    "            else:\n",
    "                # Create new course\n",
    "                course_id = str(uuid.uuid4())\n",
    "                \n",
    "                new_course = {\n",
    "                    'id': course_id,\n",
    "                    'code': course_code,\n",
    "                    'name': latest_row.get('course_name', 'Unknown Course'),\n",
    "                    'description': latest_row.get('course_description', 'No description available'),\n",
    "                    'credit_units': float(latest_row.get('credit_units', 1.0)) if pd.notna(latest_row.get('credit_units')) else 1.0,\n",
    "                    'belong_to_university': 1,  # SMU\n",
    "                    'belong_to_faculty': None,  # Will be assigned later\n",
    "                    'course_area': latest_row.get('course_area'),\n",
    "                    'enrolment_requirements': latest_row.get('enrolment_requirements')\n",
    "                }\n",
    "                \n",
    "                self.new_courses.append(new_course)\n",
    "                self.stats['courses_created'] += 1\n",
    "                \n",
    "                # Store course info for later faculty assignment\n",
    "                self.courses_needing_faculty.append({\n",
    "                    'course_id': course_id,\n",
    "                    'course_code': course_code,\n",
    "                    'course_name': latest_row.get('course_name', 'Unknown Course'),\n",
    "                    'course_outline_url': latest_row.get('course_outline_url')\n",
    "                })\n",
    "                self.stats['courses_needing_faculty'] += 1\n",
    "                \n",
    "                # Update cache\n",
    "                self.courses_cache[course_code] = new_course\n",
    "        \n",
    "        logger.info(f\"✅ Created {self.stats['courses_created']} new courses\")\n",
    "        logger.info(f\"✅ Updated {self.stats['courses_updated']} existing courses\")\n",
    "        logger.info(f\"⚠️  {self.stats['courses_needing_faculty']} courses need faculty assignment\")\n",
    "\n",
    "    def assign_course_faculties_interactive(self):\n",
    "        \"\"\"Interactive faculty assignment with option to create new faculties\"\"\"\n",
    "        if not self.courses_needing_faculty:\n",
    "            logger.info(\"✅ No courses need faculty assignment\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"🎓 Starting interactive faculty assignment for {len(self.courses_needing_faculty)} courses\")\n",
    "        \n",
    "        # Get current max faculty ID for incrementing\n",
    "        max_faculty_id = max(self.faculties_cache.keys()) if self.faculties_cache else 0\n",
    "        \n",
    "        faculty_assignments = []\n",
    "        \n",
    "        for course_info in self.courses_needing_faculty:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"🎓 FACULTY ASSIGNMENT NEEDED\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Course Code: {course_info['course_code']}\")\n",
    "            print(f\"Course Name: {course_info['course_name']}\")\n",
    "            \n",
    "            # Get the last filepath for this course from multiple sheet\n",
    "            driver = None\n",
    "            course_code = course_info['course_code']\n",
    "            last_filepath = self.get_last_filepath_by_course(course_code)\n",
    "            \n",
    "            if last_filepath:\n",
    "                print(f\"\\nOpening scraped HTML file: {last_filepath}\")\n",
    "                \n",
    "                try:\n",
    "                    # Setup Chrome options\n",
    "                    chrome_options = Options()\n",
    "                    chrome_options.add_argument(\"--new-window\")\n",
    "                    chrome_options.add_argument(\"--start-maximized\")\n",
    "                    \n",
    "                    # Initialize driver\n",
    "                    driver = webdriver.Chrome(options=chrome_options)\n",
    "                    \n",
    "                    # Open the HTML file\n",
    "                    abs_path = os.path.abspath(last_filepath)\n",
    "                    from pathlib import Path\n",
    "                    file_path = Path(abs_path)\n",
    "                    \n",
    "                    if file_path.exists():\n",
    "                        # Use pathlib's as_uri() method for proper file:// URL\n",
    "                        file_url = file_path.as_uri()\n",
    "                        driver.get(file_url)\n",
    "                        print(\"✅ Scraped HTML file opened in browser\")\n",
    "                        print(\"📋 Review the course content to determine the correct faculty\")\n",
    "                    else:\n",
    "                        print(f\"⚠️ HTML file not found: {abs_path}\")\n",
    "                        print(\"📋 Proceeding without file preview\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Could not open HTML file: {e}\")\n",
    "                    print(\"📋 Proceeding without file preview\")\n",
    "            else:\n",
    "                print(f\"⚠️ No scraped HTML file found for course {course_code}\")\n",
    "                print(\"📋 Proceeding without file preview\")\n",
    "            \n",
    "            # Show existing faculties\n",
    "            print(\"\\nExisting Faculty Options:\")\n",
    "            faculty_list = sorted(self.faculties_cache.values(), key=lambda x: x['id'])\n",
    "            for faculty in faculty_list:\n",
    "                print(f\"{faculty['id']}. {faculty['name']} ({faculty['acronym']})\")\n",
    "            \n",
    "            print(f\"\\n0. Skip (will need manual review)\")\n",
    "            print(f\"99. Create new faculty\")\n",
    "            \n",
    "            while True:\n",
    "                choice = input(f\"\\nEnter faculty number (0-{max(f['id'] for f in faculty_list)}, 99): \").strip()\n",
    "                \n",
    "                if choice == '0':\n",
    "                    faculty_id = None\n",
    "                    break\n",
    "                elif choice == '99':\n",
    "                    # Create new faculty\n",
    "                    print(\"\\n📝 Creating new faculty:\")\n",
    "                    faculty_name = input(\"Enter faculty name: \").strip()\n",
    "                    faculty_acronym = input(\"Enter faculty acronym (e.g., SCIS): \").strip().upper()\n",
    "                    faculty_url = input(\"Enter faculty website URL (or press Enter for default): \").strip()\n",
    "                    \n",
    "                    if not faculty_url:\n",
    "                        faculty_url = f\"https://smu.edu.sg/{faculty_acronym.lower()}\"\n",
    "                    \n",
    "                    # Increment faculty ID\n",
    "                    max_faculty_id += 1\n",
    "                    new_faculty = {\n",
    "                        'id': max_faculty_id,\n",
    "                        'name': faculty_name,\n",
    "                        'acronym': faculty_acronym,\n",
    "                        'site_url': faculty_url,\n",
    "                        'belong_to_university': 1,  # SMU\n",
    "                        'created_at': datetime.now().isoformat(),\n",
    "                        'updated_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    # Add to cache\n",
    "                    self.faculties_cache[max_faculty_id] = new_faculty\n",
    "                    self.faculty_acronym_to_id[faculty_acronym] = max_faculty_id\n",
    "                    \n",
    "                    # Save to new_faculties list\n",
    "                    if not hasattr(self, 'new_faculties'):\n",
    "                        self.new_faculties = []\n",
    "                    self.new_faculties.append(new_faculty)\n",
    "                    \n",
    "                    faculty_id = max_faculty_id\n",
    "                    print(f\"✅ Created new faculty: {faculty_name} (ID: {faculty_id})\")\n",
    "                    break\n",
    "                else:\n",
    "                    try:\n",
    "                        faculty_id = int(choice)\n",
    "                        if faculty_id in [f['id'] for f in faculty_list]:\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f\"Invalid choice. Please enter a valid faculty ID.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Invalid input. Please enter a number.\")\n",
    "            \n",
    "            # Close browser after selection\n",
    "            if driver:\n",
    "                try:\n",
    "                    print(\"\\n🔄 Closing browser...\")\n",
    "                    driver.quit()\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error closing browser: {e}\")\n",
    "            \n",
    "            # Store assignment\n",
    "            faculty_assignments.append({\n",
    "                'course_id': course_info['course_id'],\n",
    "                'course_code': course_info['course_code'],\n",
    "                'faculty_id': faculty_id\n",
    "            })\n",
    "        \n",
    "        # Apply assignments\n",
    "        for assignment in faculty_assignments:\n",
    "            if assignment['faculty_id'] is not None:\n",
    "                # Update new_courses\n",
    "                for course in self.new_courses:\n",
    "                    if course['id'] == assignment['course_id']:\n",
    "                        course['belong_to_faculty'] = assignment['faculty_id']\n",
    "                        break\n",
    "                \n",
    "                # Update cache\n",
    "                if assignment['course_code'] in self.courses_cache:\n",
    "                    self.courses_cache[assignment['course_code']]['belong_to_faculty'] = assignment['faculty_id']\n",
    "        \n",
    "        # Save outputs\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Updated new_courses.csv with faculty assignments\")\n",
    "        \n",
    "        if hasattr(self, 'new_faculties') and self.new_faculties:\n",
    "            df = pd.DataFrame(self.new_faculties)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_faculties.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_faculties)} new faculties\")\n",
    "        \n",
    "        logger.info(\"✅ Faculty assignment completed\")\n",
    "\n",
    "    # Also add this as an alias to the existing method name\n",
    "    def assign_course_faculties(self):\n",
    "        \"\"\"Alias for assign_course_faculties_interactive\"\"\"\n",
    "        return self.assign_course_faculties_interactive()\n",
    "\n",
    "    def process_acad_terms(self):\n",
    "        \"\"\"Process academic terms from standalone sheet\"\"\"\n",
    "        logger.info(\"📅 Processing academic terms...\")\n",
    "        \n",
    "        # Group by (acad_year_start, acad_year_end, term)\n",
    "        term_groups = defaultdict(list)\n",
    "        \n",
    "        for _, row in self.standalone_data.iterrows():\n",
    "            # Try to extract from row data first\n",
    "            year_start = row.get('acad_year_start')\n",
    "            year_end = row.get('acad_year_end')\n",
    "            term = row.get('term')\n",
    "            \n",
    "            # If any are missing, try to extract from source file path if available\n",
    "            if pd.isna(year_start) or pd.isna(year_end) or pd.isna(term):\n",
    "                if 'source_file' in row and pd.notna(row['source_file']):\n",
    "                    fallback_term_id = self.extract_acad_term_from_path(row['source_file'])\n",
    "                    if fallback_term_id:\n",
    "                        # Parse the fallback\n",
    "                        match = re.match(r'AY(\\d{4})(\\d{2})T(\\w+)', fallback_term_id)\n",
    "                        if match:\n",
    "                            year_start = int(match.group(1)) if pd.isna(year_start) else year_start\n",
    "                            year_end = int(match.group(2)) if pd.isna(year_end) else year_end\n",
    "                            term = f\"T{match.group(3)}\" if pd.isna(term) else term\n",
    "            \n",
    "            key = (year_start, year_end, term)\n",
    "            if all(pd.notna(v) for v in key):\n",
    "                term_groups[key].append(row)\n",
    "        \n",
    "        # Rest of the function remains the same...\n",
    "        for (year_start, year_end, term), rows in term_groups.items():\n",
    "            # Generate acad_term_id (keep T for ID)\n",
    "            acad_term_id = f\"AY{int(year_start)}{int(year_end) % 100:02d}{term}\"\n",
    "            \n",
    "            # Check if already exists\n",
    "            if acad_term_id in self.acad_term_cache:\n",
    "                continue\n",
    "            \n",
    "            # Find most common period_text and dates\n",
    "            period_counter = Counter()\n",
    "            date_info = {}\n",
    "            \n",
    "            for row in rows:\n",
    "                period_text = row.get('period_text', '')\n",
    "                if pd.notna(period_text):\n",
    "                    period_counter[period_text] += 1\n",
    "                    if period_text not in date_info:\n",
    "                        date_info[period_text] = {\n",
    "                            'start_dt': row.get('start_dt'),\n",
    "                            'end_dt': row.get('end_dt')\n",
    "                        }\n",
    "            \n",
    "            # Get most common period\n",
    "            if period_counter:\n",
    "                most_common_period = period_counter.most_common(1)[0][0]\n",
    "                dates = date_info[most_common_period]\n",
    "            else:\n",
    "                dates = {'start_dt': None, 'end_dt': None}\n",
    "            \n",
    "            # Get boss_id from first row\n",
    "            boss_id = rows[0].get('acad_term_boss_id')\n",
    "            \n",
    "            # Remove T prefix from term field for database storage\n",
    "            clean_term = str(term)[1:] if str(term).startswith('T') else str(term)\n",
    "            \n",
    "            new_term = {\n",
    "                'id': acad_term_id,\n",
    "                'acad_year_start': int(year_start),\n",
    "                'acad_year_end': int(year_end),\n",
    "                'term': clean_term,  # Store without T prefix\n",
    "                'boss_id': int(boss_id) if pd.notna(boss_id) else None,\n",
    "                'start_dt': dates['start_dt'],\n",
    "                'end_dt': dates['end_dt']\n",
    "            }\n",
    "            \n",
    "            self.new_acad_terms.append(new_term)\n",
    "            self.acad_term_cache[acad_term_id] = new_term\n",
    "            \n",
    "            logger.info(f\"✅ Created academic term: {acad_term_id} (term: {clean_term})\")\n",
    "        \n",
    "        logger.info(f\"✅ Created {len(self.new_acad_terms)} new academic terms\")\n",
    "\n",
    "    def process_classes(self, use_db_cache_for_classes=False):\n",
    "        \"\"\"Process classes from standalone sheet with proper field mapping and deduplication\"\"\"\n",
    "        logger.info(\"🏫 Processing classes...\")\n",
    "        logger.info(f\"   Using db_cache for classes: {use_db_cache_for_classes}\")\n",
    "        \n",
    "        # Load existing classes if using cache\n",
    "        if use_db_cache_for_classes:\n",
    "            self.load_existing_classes_cache()\n",
    "        \n",
    "        processed_classes = set()\n",
    "        validation_errors = []\n",
    "        successful_creates = 0\n",
    "        \n",
    "        for idx, row in self.standalone_data.iterrows():\n",
    "            try:\n",
    "                course_code = row.get('course_code')\n",
    "                section = row.get('section')\n",
    "                acad_term_id = row.get('acad_term_id')\n",
    "                boss_id = row.get('class_boss_id')  \n",
    "                record_key = row.get('record_key')\n",
    "                \n",
    "                # Less strict validation - only require essential fields\n",
    "                if pd.isna(course_code) or pd.isna(section) or pd.isna(acad_term_id):\n",
    "                    validation_errors.append({\n",
    "                        'row': idx,\n",
    "                        'errors': ['missing_essential_fields'],\n",
    "                        'course_code': course_code,\n",
    "                        'section': section,\n",
    "                        'acad_term_id': acad_term_id\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                # Enhanced course lookup with new_courses fallback\n",
    "                course_id = None\n",
    "                if course_code in self.courses_cache:\n",
    "                    course_id = self.courses_cache[course_code]['id']\n",
    "                else:\n",
    "                    # Check in newly created courses\n",
    "                    for course in self.new_courses:\n",
    "                        if course['code'] == course_code:\n",
    "                            course_id = course['id']\n",
    "                            break\n",
    "                \n",
    "                if not course_id:\n",
    "                    validation_errors.append({\n",
    "                        'row': idx,\n",
    "                        'errors': ['course_not_found'],\n",
    "                        'course_code': course_code,\n",
    "                        'section': section,\n",
    "                        'acad_term_id': acad_term_id\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                # Check academic term exists\n",
    "                if acad_term_id not in self.acad_term_cache:\n",
    "                    validation_errors.append({\n",
    "                        'row': idx,\n",
    "                        'errors': ['acad_term_not_found'],\n",
    "                        'course_code': course_code,\n",
    "                        'section': section,\n",
    "                        'acad_term_id': acad_term_id\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                # Professor lookup - get list of unique (professor_id, original_name) tuples\n",
    "                professor_mappings = self._find_professors_for_class(record_key) if record_key else []\n",
    "                \n",
    "                # Allow classes without professors (log but continue)\n",
    "                if not professor_mappings:\n",
    "                    logger.warning(f\"⚠️ No professors found for class: {course_code}-{section} - creating class anyway\")\n",
    "                    professor_mappings = [(None, '')]\n",
    "                \n",
    "                # FIXED: Only set warn_inaccuracy=True when multiple DIFFERENT professors teach same course/section/term\n",
    "                unique_professors = set(prof_id for prof_id, _ in professor_mappings if prof_id is not None)\n",
    "                warn_inaccuracy = len(unique_professors) > 1\n",
    "                \n",
    "                # Create class records - one per UNIQUE professor\n",
    "                class_ids_created = []\n",
    "                \n",
    "                for prof_id, prof_name in professor_mappings:\n",
    "                    class_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    new_class = {\n",
    "                        'id': class_id,\n",
    "                        'section': str(section),\n",
    "                        'course_id': course_id,\n",
    "                        'professor_id': prof_id,\n",
    "                        'acad_term_id': acad_term_id,\n",
    "                        'created_at': datetime.now().isoformat(),\n",
    "                        'updated_at': datetime.now().isoformat(),\n",
    "                        'grading_basis': row.get('grading_basis'),\n",
    "                        'course_outline_url': row.get('course_outline_url'),\n",
    "                        'boss_id': int(boss_id) if pd.notna(boss_id) else None,\n",
    "                        'raw_professor_name': prof_name,  # Individual professor name, not the full string\n",
    "                        'warn_inaccuracy': warn_inaccuracy  # Only True when multiple different professors\n",
    "                    }\n",
    "                    \n",
    "                    self.new_classes.append(new_class)\n",
    "                    class_ids_created.append(class_id)\n",
    "                    successful_creates += 1\n",
    "                    self.stats['classes_created'] += 1\n",
    "                \n",
    "                # Update class_id_mapping to store list of class IDs\n",
    "                if record_key and class_ids_created:\n",
    "                    self.class_id_mapping[record_key] = class_ids_created\n",
    "                \n",
    "                # Mark this course/section/term combination as processed\n",
    "                class_key = (str(course_code), str(section), str(acad_term_id))\n",
    "                processed_classes.add(class_key)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                validation_errors.append({\n",
    "                    'row': idx,\n",
    "                    'errors': [f'exception: {str(e)}'],\n",
    "                    'course_code': row.get('course_code'),\n",
    "                    'section': row.get('section'),\n",
    "                    'acad_term_id': row.get('acad_term_id')\n",
    "                })\n",
    "                logger.error(f\"❌ Exception processing row {idx}: {e}\")\n",
    "        \n",
    "        # Enhanced reporting\n",
    "        logger.info(f\"✅ Successfully created {successful_creates} classes\")\n",
    "        logger.info(f\"⚠️ {len(validation_errors)} validation errors encountered\")\n",
    "        \n",
    "        # Save validation errors for analysis\n",
    "        if validation_errors:\n",
    "            error_df = pd.DataFrame(validation_errors)\n",
    "            error_path = os.path.join(self.output_base, 'class_validation_errors.csv')\n",
    "            error_df.to_csv(error_path, index=False)\n",
    "            logger.info(f\"💾 Saved validation errors to {error_path}\")\n",
    "        \n",
    "        return len(self.new_classes) > 0\n",
    "\n",
    "    def _get_original_professor_names(self, record_key: str) -> List[str]:\n",
    "        \"\"\"Extract original professor names from multiple sheet for debugging\"\"\"\n",
    "        rows = self.multiple_lookup.get(record_key, [])\n",
    "        professor_names = []\n",
    "        \n",
    "        for row in rows:\n",
    "            if pd.notna(row.get('professor_name')):\n",
    "                professor_name = str(row['professor_name']).strip()\n",
    "                if professor_name and professor_name not in professor_names:\n",
    "                    professor_names.append(professor_name)\n",
    "        \n",
    "        return professor_names\n",
    "        \n",
    "    def _find_professors_for_class(self, record_key: str) -> List[tuple]:\n",
    "        \"\"\"Find professor IDs for a class and return list of (professor_id, original_name) tuples\n",
    "        Deduplicates by professor_id to avoid creating multiple class records for same professor\"\"\"\n",
    "        if not record_key or pd.isna(record_key):\n",
    "            return []\n",
    "        \n",
    "        rows = self.multiple_lookup.get(record_key, [])\n",
    "        professor_mappings = []\n",
    "        seen_professor_ids = set()  # Track unique professor IDs\n",
    "        \n",
    "        # Ensure professor lookup is loaded\n",
    "        if not hasattr(self, 'professor_lookup_loaded'):\n",
    "            self.load_professor_lookup_csv()\n",
    "        \n",
    "        for row in rows:\n",
    "            prof_name_raw = row.get('professor_name')\n",
    "            \n",
    "            # FIXED: Better handling of NaN values from raw_data.xlsx\n",
    "            if prof_name_raw is None or pd.isna(prof_name_raw):\n",
    "                continue\n",
    "            \n",
    "            # Convert to string and strip - handles float NaN properly\n",
    "            original_prof_name = str(prof_name_raw).strip()\n",
    "            \n",
    "            # Skip empty strings and 'nan' strings\n",
    "            if not original_prof_name or original_prof_name.lower() == 'nan':\n",
    "                continue\n",
    "            \n",
    "            # Split the professor names intelligently\n",
    "            split_professors = self._split_professor_names(original_prof_name)\n",
    "            \n",
    "            # Process each split professor\n",
    "            for prof_name in split_professors:\n",
    "                if prof_name and prof_name.strip():  # Additional check for empty strings\n",
    "                    prof_id = self._lookup_professor_with_fallback(prof_name.strip())\n",
    "                    if prof_id and prof_id not in seen_professor_ids:\n",
    "                        professor_mappings.append((prof_id, prof_name.strip()))\n",
    "                        seen_professor_ids.add(prof_id)\n",
    "        \n",
    "        return professor_mappings\n",
    "\n",
    "    def _split_professor_names(self, prof_name: str) -> List[str]:\n",
    "        \"\"\"Intelligently split professor names with improved comma-based parsing\"\"\"\n",
    "        # Handle None, NaN, and non-string values from raw_data.xlsx\n",
    "        if prof_name is None or pd.isna(prof_name):\n",
    "            return []\n",
    "        \n",
    "        # Ensure it's a string and handle float NaN\n",
    "        prof_name = str(prof_name).strip()\n",
    "        \n",
    "        # Check for empty string or 'nan' string after conversion\n",
    "        if not prof_name or prof_name.lower() == 'nan':\n",
    "            return []\n",
    "        \n",
    "        # Step 1: Check if the entire name exists in professor_lookup (single professor)\n",
    "        prof_name_upper = prof_name.upper()\n",
    "        if hasattr(self, 'professor_lookup') and prof_name_upper in self.professor_lookup:\n",
    "            return [prof_name]\n",
    "        \n",
    "        # Step 2: Check hardcoded multi-instructor combinations first\n",
    "        multi_instructor_combinations = {\n",
    "            \"ERIC YEE SHIN CHONG, MANDY THAM\": [\"ERIC YEE SHIN CHONG\", \"MANDY THAM\"],\n",
    "            \"ZHENG ZHICHAO, DANIEL, TAN KAR WAY\": [\"ZHENG ZHICHAO, DANIEL\", \"TAN KAR WAY\"],\n",
    "            \"KAM WAI WARREN BARTHOLOMEW CHIK, LANX GOH\": [\"KAM WAI WARREN BARTHOLOMEW CHIK\", \"LANX GOH\"],\n",
    "            \"ANDREW MIN HAN CHIN, DANIEL TAN\": [\"ANDREW MIN HAN CHIN\", \"DANIEL TAN\"],\n",
    "            \"PAUL GRIFFIN, TA NGUYEN BINH DUONG\": [\"PAUL GRIFFIN\", \"TA NGUYEN BINH DUONG\"],\n",
    "            \"ANDREW MIN HAN CHIN, JUNJI SUMITANI\": [\"ANDREW MIN HAN CHIN\", \"JUNJI SUMITANI\"],\n",
    "            \"DAVID GOMULYA, LIM CHON PHUNG, AJAY MAKHIJA\": [\"DAVID GOMULYA\", \"LIM CHON PHUNG\", \"AJAY MAKHIJA\"],\n",
    "            \"JACK HONG JIAJUN, ANG SER KENG\": [\"JACK HONG JIAJUN\", \"ANG SER KENG\"],\n",
    "            \"DAVID GOMULYA, DAVID LLEWELYN\": [\"DAVID GOMULYA\", \"DAVID LLEWELYN\"],\n",
    "            \"TERENCE FAN PING-CHING, JONATHAN TEE\": [\"TERENCE FAN PING-CHING\", \"JONATHAN TEE\"],\n",
    "            \"RONG WANG, CHENG QIANG, CHEN XIA, LIANDONG ZHANG, WANG JIWEI, YUE HENG\": [\"RONG WANG\", \"CHENG QIANG\", \"CHEN XIA\", \"LIANDONG ZHANG\", \"WANG JIWEI\", \"YUE HENG\"],\n",
    "            \"PASCALE CRAMA, ARNOUD DE MEYER\": [\"PASCALE CRAMA\", \"ARNOUD DE MEYER\"],\n",
    "            \"TERENCE FAN PING-CHING, WILSON TENG\": [\"TERENCE FAN PING-CHING\", \"WILSON TENG\"],\n",
    "            \"ANDREW MIN HAN CHIN, LI JIN\": [\"ANDREW MIN HAN CHIN\", \"LI JIN\"],\n",
    "            \"ONG, BENJAMIN JOSHUA, EUGENE TAN KHENG BOON\": [\"ONG, BENJAMIN JOSHUA\", \"EUGENE TAN KHENG BOON\"],\n",
    "            \"MANDY THAM, ERIC YEE SHIN CHONG\": [\"MANDY THAM\", \"ERIC YEE SHIN CHONG\"],\n",
    "            \"TERENCE FAN PING-CHING, RUTH CHIANG\": [\"TERENCE FAN PING-CHING\", \"RUTH CHIANG\"],\n",
    "            \"JARED POON JUN KEAT, CHAM YANWEI, DERRICK\": [\"JARED POON JUN KEAT\", \"CHAM YANWEI, DERRICK\"],\n",
    "            \"DAVID GOMULYA, SZE TIAM LIN\": [\"DAVID GOMULYA\", \"SZE TIAM LIN\"],\n",
    "            \"ANDREW MIN HAN CHIN, JAY WONG\": [\"ANDREW MIN HAN CHIN\", \"JAY WONG\"],\n",
    "            \"MARK CHONG YIEW KIM, VICTOR OCAMPO\": [\"MARK CHONG YIEW KIM\", \"VICTOR OCAMPO\"],\n",
    "            \"TSE, JUSTIN K, AIDAN WONG\": [\"TSE, JUSTIN K\", \"AIDAN WONG\"],\n",
    "            \"TANG HONG WEE, GERALD SEAH, MUHAMMED AMEER S/O MOHAMED NOOR, LAU MENG YAN\": [\"TANG HONG WEE\", \"GERALD SEAH\", \"MUHAMMED AMEER S/O MOHAMED NOOR\", \"LAU MENG YAN\"],\n",
    "            \"AURELIO GURREA MARTINEZ, LOH SONG-EN, SAMUEL\": [\"AURELIO GURREA MARTINEZ\", \"LOH SONG-EN, SAMUEL\"],\n",
    "            \"CHNG SHUQI, AMELIA CHUA, MUHAMMED AMEER S/O MOHAMED NOOR\": [\"CHNG SHUQI\", \"AMELIA CHUA\", \"MUHAMMED AMEER S/O MOHAMED NOOR\"]\n",
    "        }\n",
    "        \n",
    "        if prof_name in multi_instructor_combinations:\n",
    "            return multi_instructor_combinations[prof_name]\n",
    "        \n",
    "        # Step 3: FIXED: Intelligent comma-based splitting with progressive matching\n",
    "        # Split by comma first\n",
    "        comma_parts = [part.strip() for part in prof_name.split(',') if part.strip()]\n",
    "        \n",
    "        # If only one part (no commas), treat as single professor\n",
    "        if len(comma_parts) <= 1:\n",
    "            return [prof_name]\n",
    "        \n",
    "        # Get all boss_names from professor_lookup for matching\n",
    "        boss_names = set()\n",
    "        if hasattr(self, 'professor_lookup'):\n",
    "            for key in self.professor_lookup.keys():\n",
    "                if key is not None and not pd.isna(key):\n",
    "                    key_str = str(key).strip()\n",
    "                    if key_str and key_str.lower() != 'nan':\n",
    "                        boss_names.add(key_str.upper())\n",
    "        \n",
    "        professors_found = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(comma_parts):\n",
    "            current_candidate = comma_parts[i]\n",
    "            matched = False\n",
    "            \n",
    "            # Try progressive matching: add more comma parts until we find a match\n",
    "            for j in range(i + 1, len(comma_parts) + 1):\n",
    "                candidate = ', '.join(comma_parts[i:j])\n",
    "                candidate_upper = candidate.upper()\n",
    "                \n",
    "                # Check for exact match\n",
    "                if candidate_upper in boss_names:\n",
    "                    professors_found.append(candidate)\n",
    "                    i = j  # Move past all used parts\n",
    "                    matched = True\n",
    "                    break\n",
    "                \n",
    "                # Check for partial word match (all words in candidate must be in some boss_name)\n",
    "                candidate_words = set(candidate.replace(',', ' ').split())\n",
    "                for boss_name in boss_names:\n",
    "                    boss_words = set(boss_name.replace(',', ' ').split())\n",
    "                    if candidate_words.issubset(boss_words) and len(candidate_words) >= 2:\n",
    "                        professors_found.append(candidate)\n",
    "                        i = j  # Move past all used parts\n",
    "                        matched = True\n",
    "                        break\n",
    "                \n",
    "                if matched:\n",
    "                    break\n",
    "            \n",
    "            # If no match found and we're at a single part, check if it's reasonable\n",
    "            if not matched:\n",
    "                single_part = comma_parts[i]\n",
    "                words_in_part = single_part.split()\n",
    "                \n",
    "                # Only accept if it has at least 2 words (avoid single word professors)\n",
    "                if len(words_in_part) >= 2:\n",
    "                    professors_found.append(single_part)\n",
    "                else:\n",
    "                    # Try to combine with next part if available\n",
    "                    if i + 1 < len(comma_parts):\n",
    "                        combined = f\"{single_part} {comma_parts[i + 1]}\"\n",
    "                        professors_found.append(combined)\n",
    "                        i += 2  # Skip next part too\n",
    "                    else:\n",
    "                        # Last resort: single word, but log warning\n",
    "                        logger.warning(f\"⚠️ Single word professor detected (may be parsing error): '{single_part}' from '{prof_name}'\")\n",
    "                        professors_found.append(single_part)\n",
    "                        i += 1\n",
    "                \n",
    "                if not matched:\n",
    "                    i += 1\n",
    "        \n",
    "        # Final validation: remove any single-word results if there are multi-word alternatives\n",
    "        valid_professors = []\n",
    "        for prof in professors_found:\n",
    "            prof_words = prof.strip().split()\n",
    "            if len(prof_words) >= 2 or len(professors_found) == 1:  # Keep single words only if it's the only result\n",
    "                valid_professors.append(prof)\n",
    "        \n",
    "        # If we couldn't split intelligently, fall back to treating as single professor\n",
    "        if not valid_professors:\n",
    "            return [prof_name]\n",
    "        \n",
    "        return valid_professors\n",
    "\n",
    "    def _lookup_professor_with_fallback(self, prof_name: str) -> Optional[str]:\n",
    "        \"\"\"Enhanced professor lookup with multiple fallback strategies\"\"\"\n",
    "        \n",
    "        # Handle None, NaN, and ensure it's a string\n",
    "        if prof_name is None or pd.isna(prof_name):\n",
    "            return None\n",
    "        \n",
    "        # Ensure prof_name is a string and handle 'nan' strings\n",
    "        prof_name = str(prof_name).strip()\n",
    "        \n",
    "        if not prof_name or prof_name.lower() == 'nan':\n",
    "            return None\n",
    "        \n",
    "        # Strategy 1: Direct boss_name lookup in professor_lookup.csv\n",
    "        normalized_name = prof_name.upper()\n",
    "        if hasattr(self, 'professor_lookup') and normalized_name in self.professor_lookup:\n",
    "            return self.professor_lookup[normalized_name]['database_id']\n",
    "        \n",
    "        # Strategy 2: Try variations of the name\n",
    "        variations = [\n",
    "            prof_name.strip(),  # Original\n",
    "            prof_name.strip().upper(),  # Uppercase\n",
    "            prof_name.replace(',', '').strip().upper(),  # Remove commas\n",
    "            ' '.join(prof_name.replace(',', ' ').split()).upper()  # Normalize spaces\n",
    "        ]\n",
    "        \n",
    "        if hasattr(self, 'professor_lookup'):\n",
    "            for variation in variations:\n",
    "                if variation in self.professor_lookup:\n",
    "                    return self.professor_lookup[variation]['database_id']\n",
    "        \n",
    "        # Strategy 3: Fuzzy matching against boss_name keys (100% certain only)\n",
    "        if hasattr(self, 'professor_lookup'):\n",
    "            for lookup_name in self.professor_lookup.keys():\n",
    "                if self._names_match_fuzzy_exact(normalized_name, lookup_name):\n",
    "                    return self.professor_lookup[lookup_name]['database_id']\n",
    "        \n",
    "        # Strategy 4: Check if professor exists in database cache\n",
    "        if normalized_name in self.professors_cache:\n",
    "            return self.professors_cache[normalized_name]['id']\n",
    "        \n",
    "        # Strategy 5: Create new professor with email duplicate check\n",
    "        logger.warning(f\"⚠️ Professor not found in lookup: {prof_name} - creating new professor\")\n",
    "        \n",
    "        try:\n",
    "            boss_name, afterclass_name = self.normalize_professor_name(prof_name)\n",
    "            \n",
    "            # Check if already created in this session\n",
    "            for new_prof in self.new_professors:\n",
    "                if new_prof['boss_name'] == boss_name:\n",
    "                    return new_prof['id']\n",
    "            \n",
    "            # Create new professor\n",
    "            professor_id = str(uuid.uuid4())\n",
    "            slug = re.sub(r'[^a-zA-Z0-9]+', '-', afterclass_name.lower()).strip('-')\n",
    "            \n",
    "            # Resolve email using Outlook\n",
    "            resolved_email = self.resolve_professor_email(afterclass_name)\n",
    "            \n",
    "            # Check if email already exists in existing professors\n",
    "            if resolved_email:\n",
    "                # Check in professors_cache\n",
    "                for cached_boss_name, cached_prof in self.professors_cache.items():\n",
    "                    if 'email' in cached_prof and cached_prof['email'] and cached_prof['email'].lower() == resolved_email.lower():\n",
    "                        # Email already exists - use existing professor\n",
    "                        logger.info(f\"✅ Email duplicate found during fallback - using existing professor: {prof_name} → {cached_prof.get('name')} (email: {resolved_email})\")\n",
    "                        \n",
    "                        # Update lookup to point to existing professor\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = {\n",
    "                            'database_id': cached_prof['id'],\n",
    "                            'boss_name': boss_name,\n",
    "                            'afterclass_name': cached_prof.get('name', afterclass_name)\n",
    "                        }\n",
    "                        \n",
    "                        # Also add boss_name as lookup key\n",
    "                        self.professor_lookup[boss_name.upper()] = {\n",
    "                            'database_id': cached_prof['id'],\n",
    "                            'boss_name': boss_name,\n",
    "                            'afterclass_name': cached_prof.get('name', afterclass_name)\n",
    "                        }\n",
    "                        \n",
    "                        return cached_prof['id']\n",
    "                \n",
    "                # Also check in already created new professors\n",
    "                for new_prof in self.new_professors:\n",
    "                    if 'email' in new_prof and new_prof['email'] and new_prof['email'].lower() == resolved_email.lower():\n",
    "                        # Email already exists in new professors - use that one\n",
    "                        logger.info(f\"✅ Email duplicate found in new professors - using existing: {prof_name} → {new_prof['name']} (email: {resolved_email})\")\n",
    "                        \n",
    "                        # Update lookup to point to existing professor\n",
    "                        if not hasattr(self, 'professor_lookup'):\n",
    "                            self.professor_lookup = {}\n",
    "                        self.professor_lookup[prof_name.upper()] = {\n",
    "                            'database_id': new_prof['id'],\n",
    "                            'boss_name': boss_name,\n",
    "                            'afterclass_name': new_prof['afterclass_name']\n",
    "                        }\n",
    "                        \n",
    "                        # Also add boss_name as lookup key\n",
    "                        self.professor_lookup[boss_name.upper()] = {\n",
    "                            'database_id': new_prof['id'],\n",
    "                            'boss_name': boss_name,\n",
    "                            'afterclass_name': new_prof['afterclass_name']\n",
    "                        }\n",
    "                        \n",
    "                        return new_prof['id']\n",
    "            \n",
    "            # No email duplicate found - create new professor\n",
    "            new_prof = {\n",
    "                'id': professor_id,\n",
    "                'name': afterclass_name,\n",
    "                'email': resolved_email,\n",
    "                'slug': slug,\n",
    "                'photo_url': 'https://smu.edu.sg',\n",
    "                'profile_url': 'https://smu.edu.sg',\n",
    "                'belong_to_university': 1,  # SMU\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'updated_at': datetime.now().isoformat(),\n",
    "                'boss_name': boss_name,\n",
    "                'afterclass_name': afterclass_name,\n",
    "                'original_scraped_name': prof_name\n",
    "            }\n",
    "            \n",
    "            self.new_professors.append(new_prof)\n",
    "            self.stats['professors_created'] += 1\n",
    "            \n",
    "            # Update lookup\n",
    "            if not hasattr(self, 'professor_lookup'):\n",
    "                self.professor_lookup = {}\n",
    "            self.professor_lookup[prof_name.upper()] = {\n",
    "                'database_id': professor_id,\n",
    "                'boss_name': boss_name,\n",
    "                'afterclass_name': afterclass_name\n",
    "            }\n",
    "            \n",
    "            # Also add boss_name as lookup key\n",
    "            self.professor_lookup[boss_name.upper()] = {\n",
    "                'database_id': professor_id,\n",
    "                'boss_name': boss_name,\n",
    "                'afterclass_name': afterclass_name\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"✨ Created new professor for verification: {afterclass_name} with email: {resolved_email}\")\n",
    "            return professor_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error creating new professor for '{prof_name}': {e}\")\n",
    "            return None\n",
    "\n",
    "    def _names_match_fuzzy(self, name1: str, name2: str) -> bool:\n",
    "        \"\"\"Simple fuzzy matching for names\"\"\"\n",
    "\n",
    "        # Ensure both names are strings\n",
    "        name1 = str(name1) if name1 is not None else \"\"\n",
    "        name2 = str(name2) if name2 is not None else \"\"\n",
    "\n",
    "        # Remove common variations\n",
    "        clean1 = ' '.join(name1.replace(',', ' ').split())\n",
    "        clean2 = ' '.join(name2.replace(',', ' ').split())\n",
    "        \n",
    "        # Check if all words in shorter name appear in longer name\n",
    "        words1 = clean1.split()\n",
    "        words2 = clean2.split()\n",
    "        \n",
    "        if len(words1) <= len(words2):\n",
    "            return all(word in words2 for word in words1)\n",
    "        else:\n",
    "            return all(word in words1 for word in words2)\n",
    "        \n",
    "    def _names_match_fuzzy_exact(self, name1: str, name2: str) -> bool:\n",
    "        \"\"\"Exact fuzzy matching for names - only matches if completely identical after normalization\"\"\"\n",
    "        \n",
    "        # Handle None and non-string values\n",
    "        if name1 is None or name2 is None:\n",
    "            return False\n",
    "        \n",
    "        # Ensure both names are strings\n",
    "        name1 = str(name1) if name1 is not None else \"\"\n",
    "        name2 = str(name2) if name2 is not None else \"\"\n",
    "        \n",
    "        # Remove common variations and normalize\n",
    "        clean1 = ' '.join(name1.replace(',', ' ').replace('.', ' ').split()).upper()\n",
    "        clean2 = ' '.join(name2.replace(',', ' ').replace('.', ' ').split()).upper()\n",
    "        \n",
    "        # Only return True if they are exactly the same after cleaning\n",
    "        return clean1 == clean2\n",
    "\n",
    "    def load_professor_lookup_csv(self):\n",
    "        \"\"\"Load professor lookup CSV once and cache it properly\"\"\"\n",
    "        # Check if already loaded to prevent repeated loading\n",
    "        if hasattr(self, 'professor_lookup_loaded') and self.professor_lookup_loaded:\n",
    "            return\n",
    "        \n",
    "        lookup_file = 'script_input/professor_lookup.csv'\n",
    "        \n",
    "        if not os.path.exists(lookup_file):\n",
    "            logger.warning(\"📋 professor_lookup.csv not found - will use database cache only\")\n",
    "            self.professor_lookup_loaded = True\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            lookup_df = pd.read_csv(lookup_file)\n",
    "            \n",
    "            # Validate required columns exist\n",
    "            required_cols = ['boss_name', 'afterclass_name', 'database_id', 'method']\n",
    "            missing_cols = [col for col in required_cols if col not in lookup_df.columns]\n",
    "            if missing_cols:\n",
    "                logger.error(f\"❌ professor_lookup.csv missing required columns: {missing_cols}\")\n",
    "                self.professor_lookup_loaded = True\n",
    "                return\n",
    "            \n",
    "            # Clear existing lookup and load fresh data\n",
    "            self.professor_lookup = {}\n",
    "            loaded_count = 0\n",
    "            \n",
    "            for _, row in lookup_df.iterrows():\n",
    "                boss_name = row.get('boss_name')\n",
    "                afterclass_name = row.get('afterclass_name')\n",
    "                database_id = row.get('database_id')\n",
    "                \n",
    "                # Skip rows with critical missing values\n",
    "                if pd.isna(boss_name) or pd.isna(database_id):\n",
    "                    continue\n",
    "                    \n",
    "                # Use boss_name as the primary key for lookup (as you specified)\n",
    "                boss_name_key = str(boss_name).strip().upper()\n",
    "                self.professor_lookup[boss_name_key] = {\n",
    "                    'database_id': str(database_id),\n",
    "                    'boss_name': str(boss_name),\n",
    "                    'afterclass_name': str(afterclass_name) if not pd.isna(afterclass_name) else str(boss_name)\n",
    "                }\n",
    "                loaded_count += 1\n",
    "            \n",
    "            logger.info(f\"✅ Loaded {loaded_count} entries from professor_lookup.csv\")\n",
    "            self.professor_lookup_loaded = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error loading professor_lookup.csv: {e}\")\n",
    "            logger.info(\"📋 Continuing with database cache only\")\n",
    "            self.professor_lookup_loaded = True\n",
    "\n",
    "    def _create_new_professor(self, prof_name: str) -> str:\n",
    "        \"\"\"Create a new professor record for verification\"\"\"\n",
    "        boss_name, afterclass_name = self.normalize_professor_name(prof_name)\n",
    "        \n",
    "        # Check if already created in this session\n",
    "        for new_prof in self.new_professors:\n",
    "            if new_prof['boss_name'] == boss_name:\n",
    "                return new_prof['id']\n",
    "        \n",
    "        # Create new professor\n",
    "        professor_id = str(uuid.uuid4())\n",
    "        slug = re.sub(r'[^a-zA-Z0-9]+', '-', afterclass_name.lower()).strip('-')\n",
    "        \n",
    "        # Resolve email using Outlook (same as in process_professors)\n",
    "        resolved_email = self.resolve_professor_email(afterclass_name)\n",
    "        \n",
    "        new_prof = {\n",
    "            'id': professor_id,\n",
    "            'name': afterclass_name,\n",
    "            'email': resolved_email,  # Now using Outlook resolution\n",
    "            'slug': slug,\n",
    "            'photo_url': 'https://smu.edu.sg',\n",
    "            'profile_url': 'https://smu.edu.sg',\n",
    "            'belong_to_university': 1,  # SMU\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'updated_at': datetime.now().isoformat(),\n",
    "            'boss_name': boss_name,\n",
    "            'afterclass_name': afterclass_name,\n",
    "            'original_scraped_name': prof_name\n",
    "        }\n",
    "        \n",
    "        self.new_professors.append(new_prof)\n",
    "        self.stats['professors_created'] += 1\n",
    "        \n",
    "        # Update lookup\n",
    "        self.professor_lookup[boss_name] = {\n",
    "            'database_id': professor_id,\n",
    "            'boss_name': boss_name,\n",
    "            'afterclass_name': afterclass_name\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"✨ Created new professor for verification: {afterclass_name} with email: {resolved_email}\")\n",
    "        return professor_id\n",
    "\n",
    "    def process_timings(self):\n",
    "        \"\"\"Process class timings and exam timings from multiple sheet\"\"\"\n",
    "        logger.info(\"⏰ Processing class timings and exam timings...\")\n",
    "        \n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            record_key = row.get('record_key')\n",
    "            if record_key not in self.class_id_mapping:\n",
    "                continue\n",
    "            \n",
    "            # Get all class IDs for this record_key (now a list)\n",
    "            class_ids = self.class_id_mapping[record_key]\n",
    "            if not isinstance(class_ids, list):\n",
    "                class_ids = [class_ids]  # Ensure it's a list for backward compatibility\n",
    "            \n",
    "            timing_type = row.get('type', 'CLASS')\n",
    "            \n",
    "            # Create timing records for each class ID\n",
    "            for class_id in class_ids:\n",
    "                if timing_type == 'CLASS':\n",
    "                    timing_record = {\n",
    "                        'class_id': class_id,\n",
    "                        'start_date': row.get('start_date'),\n",
    "                        'end_date': row.get('end_date'),\n",
    "                        'day_of_week': row.get('day_of_week'),\n",
    "                        'start_time': row.get('start_time'),\n",
    "                        'end_time': row.get('end_time'),\n",
    "                        'venue': row.get('venue', '')\n",
    "                    }\n",
    "                    self.new_class_timings.append(timing_record)\n",
    "                    self.stats['timings_created'] += 1\n",
    "                \n",
    "                elif timing_type == 'EXAM':\n",
    "                    exam_record = {\n",
    "                        'class_id': class_id,\n",
    "                        'date': row.get('date'),\n",
    "                        'day_of_week': row.get('day_of_week'),\n",
    "                        'start_time': row.get('start_time'),\n",
    "                        'end_time': row.get('end_time'),\n",
    "                        'venue': row.get('venue')\n",
    "                    }\n",
    "                    self.new_class_exam_timings.append(exam_record)\n",
    "                    self.stats['exams_created'] += 1\n",
    "        \n",
    "        logger.info(f\"✅ Created {self.stats['timings_created']} class timings\")\n",
    "        logger.info(f\"✅ Created {self.stats['exams_created']} exam timings\")\n",
    "        \n",
    "    def save_outputs(self):\n",
    "        \"\"\"Save all generated CSV files\"\"\"\n",
    "        logger.info(\"💾 Saving output files...\")\n",
    "        \n",
    "        # In Phase 2, professors have already been saved and corrected\n",
    "        # Only save if we're in Phase 1 or if there are new professors to save\n",
    "        if self.new_professors and not hasattr(self, '_phase2_mode'):\n",
    "            df = pd.DataFrame(self.new_professors)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_professors.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_professors)} new professors\")\n",
    "        \n",
    "        # Save new courses (to verify folder)\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_courses)} new courses\")\n",
    "        \n",
    "        # Save course updates\n",
    "        if self.update_courses:\n",
    "            df = pd.DataFrame(self.update_courses)\n",
    "            df.to_csv(os.path.join(self.output_base, 'update_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.update_courses)} course updates\")\n",
    "        \n",
    "        # Save academic terms\n",
    "        if self.new_acad_terms:\n",
    "            df = pd.DataFrame(self.new_acad_terms)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_acad_term.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_acad_terms)} academic terms\")\n",
    "        \n",
    "        # Save classes\n",
    "        if self.new_classes:\n",
    "            df = pd.DataFrame(self.new_classes)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_classes.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_classes)} classes\")\n",
    "        \n",
    "        # Save class timings\n",
    "        if self.new_class_timings:\n",
    "            df = pd.DataFrame(self.new_class_timings)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_class_timing.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_class_timings)} class timings\")\n",
    "        \n",
    "        # Save exam timings\n",
    "        if self.new_class_exam_timings:\n",
    "            df = pd.DataFrame(self.new_class_exam_timings)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_class_exam_timing.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_class_exam_timings)} exam timings\")\n",
    "        \n",
    "        # Save courses needing faculty assignment\n",
    "        if self.courses_needing_faculty:\n",
    "            df = pd.DataFrame(self.courses_needing_faculty)\n",
    "            df.to_csv(os.path.join(self.output_base, 'courses_needing_faculty.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.courses_needing_faculty)} courses needing faculty assignment\")\n",
    "        \n",
    "        # Create placeholder files only if they don't exist\n",
    "        placeholders = ['new_bid_window.csv', 'new_class_availability.csv', 'new_bid_result.csv']\n",
    "        for filename in placeholders:\n",
    "            filepath = os.path.join(self.output_base, filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                df = pd.DataFrame()\n",
    "                df.to_csv(filepath, index=False)\n",
    "                logger.info(f\"✅ Created placeholder: {filename}\")\n",
    "\n",
    "    def _save_professor_lookup(self):\n",
    "        \"\"\"Save updated professor lookup table\"\"\"\n",
    "        lookup_data = []\n",
    "        \n",
    "        # Add all professors from lookup\n",
    "        for scraped_name, data in self.professor_lookup.items():\n",
    "            lookup_data.append({\n",
    "                'boss_name': data.get('boss_name', scraped_name.upper()),\n",
    "                'afterclass_name': data.get('afterclass_name', scraped_name),\n",
    "                'database_id': data['database_id'],\n",
    "                'method': 'exists' if scraped_name not in [p['original_scraped_name'] for p in self.new_professors] else 'created'\n",
    "            })\n",
    "        \n",
    "        # Sort by scraped_name\n",
    "        lookup_data.sort(key=lambda x: x['scraped_name'])\n",
    "        \n",
    "        # Save to output folder\n",
    "        df = pd.DataFrame(lookup_data)\n",
    "        df.to_csv(os.path.join(self.output_base, 'professor_lookup.csv'), index=False)\n",
    "        logger.info(f\"✅ Saved updated professor lookup with {len(lookup_data)} entries\")\n",
    "\n",
    "    def update_professor_lookup_from_corrected_csv(self):\n",
    "        \"\"\"Update professor lookup from manually corrected new_professors.csv\"\"\"\n",
    "        logger.info(\"🔄 Updating professor lookup from corrected CSV...\")\n",
    "        \n",
    "        # Read corrected new_professors.csv\n",
    "        corrected_csv_path = os.path.join(self.verify_dir, 'new_professors.csv')\n",
    "        if not os.path.exists(corrected_csv_path):\n",
    "            logger.info(f\"📝 No corrected CSV found: {corrected_csv_path} - assuming all professors already exist\")\n",
    "            return True\n",
    "\n",
    "        corrected_df = pd.read_csv(corrected_csv_path)\n",
    "        if corrected_df.empty:\n",
    "            logger.info(f\"📝 Empty corrected CSV - no professors to update\")\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            corrected_df = pd.read_csv(corrected_csv_path)\n",
    "            logger.info(f\"📖 Reading {len(corrected_df)} corrected professor records\")\n",
    "            \n",
    "            # Clear and rebuild the new_professors list with corrected data\n",
    "            self.new_professors = []\n",
    "            \n",
    "            # Update internal professor_lookup and rebuild new_professors\n",
    "            updated_count = 0\n",
    "            for _, row in corrected_df.iterrows():\n",
    "                original_name = row.get('original_scraped_name', '')\n",
    "                corrected_afterclass_name = row.get('name', '')  # This is the corrected name\n",
    "                boss_name = row.get('boss_name', '')  # Keep boss name same\n",
    "                professor_id = row.get('id', '')\n",
    "                \n",
    "                # Rebuild the professor record with corrected data\n",
    "                corrected_prof = {\n",
    "                    'id': professor_id,\n",
    "                    'name': corrected_afterclass_name,  # Use corrected name\n",
    "                    'email': row.get('email', 'enquiry@smu.edu.sg'),\n",
    "                    'slug': row.get('slug', ''),\n",
    "                    'photo_url': row.get('photo_url', 'https://smu.edu.sg'),\n",
    "                    'profile_url': row.get('profile_url', 'https://smu.edu.sg'),\n",
    "                    'belong_to_university': row.get('belong_to_university', 1),\n",
    "                    'created_at': row.get('created_at', datetime.now().isoformat()),\n",
    "                    'updated_at': row.get('updated_at', datetime.now().isoformat()),\n",
    "                    'boss_name': boss_name,\n",
    "                    'afterclass_name': corrected_afterclass_name,\n",
    "                    'original_scraped_name': original_name\n",
    "                }\n",
    "                \n",
    "                # Add to new_professors list\n",
    "                self.new_professors.append(corrected_prof)\n",
    "                \n",
    "                if original_name and professor_id:\n",
    "                    # Update lookup with corrected afterclass name but same boss name\n",
    "                    self.professor_lookup[original_name] = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,  # Keep original boss name\n",
    "                        'afterclass_name': corrected_afterclass_name  # Use corrected name\n",
    "                    }\n",
    "                    updated_count += 1\n",
    "                    \n",
    "                    # Also add the corrected name as a lookup key\n",
    "                    self.professor_lookup[corrected_afterclass_name] = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': corrected_afterclass_name\n",
    "                    }\n",
    "                    \n",
    "                    # Add boss name as lookup key too\n",
    "                    self.professor_lookup[boss_name] = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,\n",
    "                        'afterclass_name': corrected_afterclass_name\n",
    "                    }\n",
    "            \n",
    "            # Save updated professor lookup to CSV\n",
    "            self._save_corrected_professor_lookup()\n",
    "            \n",
    "            logger.info(f\"✅ Updated {updated_count} professor lookup entries\")\n",
    "            logger.info(f\"✅ Rebuilt {len(self.new_professors)} professor records with corrections\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to update professor lookup: {e}\")\n",
    "            return False\n",
    "\n",
    "    def update_professors_with_boss_names(self):\n",
    "        \"\"\"Update professors with missing boss_names using professor_lookup.csv\"\"\"\n",
    "        logger.info(\"👤 Updating professors with missing boss_names...\")\n",
    "        \n",
    "        # Load professor_lookup.csv\n",
    "        lookup_file = 'script_input/professor_lookup.csv'\n",
    "        if not os.path.exists(lookup_file):\n",
    "            logger.info(\"📋 professor_lookup.csv not found - skipping boss_name updates\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            lookup_df = pd.read_csv(lookup_file)\n",
    "            logger.info(f\"📖 Loaded {len(lookup_df)} entries from professor_lookup.csv\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error loading professor_lookup.csv: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Group lookup entries by database_id to handle multiple boss_names\n",
    "        lookup_groups = defaultdict(list)\n",
    "        for _, row in lookup_df.iterrows():\n",
    "            database_id = row.get('database_id')\n",
    "            boss_name = row.get('boss_name')\n",
    "            \n",
    "            if pd.notna(database_id) and pd.notna(boss_name):\n",
    "                lookup_groups[str(database_id)].append(str(boss_name).strip())\n",
    "        \n",
    "        # Find professors with empty boss_name\n",
    "        professors_to_update = []\n",
    "        \n",
    "        for prof_key, prof_data in self.professors_cache.items():\n",
    "            professor_id = prof_data.get('id')\n",
    "            current_boss_name = prof_data.get('boss_name')\n",
    "            \n",
    "            # Check if boss_name is empty/null\n",
    "            if (current_boss_name is None or \n",
    "                pd.isna(current_boss_name) or \n",
    "                str(current_boss_name).strip() == ''):\n",
    "                \n",
    "                # Look for this professor in the lookup\n",
    "                if str(professor_id) in lookup_groups:\n",
    "                    boss_names = lookup_groups[str(professor_id)]\n",
    "                    \n",
    "                    # Remove duplicates while preserving order\n",
    "                    unique_boss_names = []\n",
    "                    seen = set()\n",
    "                    for name in boss_names:\n",
    "                        if name not in seen:\n",
    "                            unique_boss_names.append(name)\n",
    "                            seen.add(name)\n",
    "                    \n",
    "                    # Store as JSON array for CSV compatibility\n",
    "                    import json\n",
    "                    boss_name_json = json.dumps(unique_boss_names)\n",
    "                    \n",
    "                    professors_to_update.append({\n",
    "                        'id': professor_id,\n",
    "                        'boss_name': boss_name_json,  # JSON array string for CSV\n",
    "                        'original_boss_name': current_boss_name,\n",
    "                        'boss_names_found': len(unique_boss_names)\n",
    "                    })\n",
    "                    \n",
    "                    logger.info(f\"✅ Found boss_name(s) for professor {professor_id}: {unique_boss_names}\")\n",
    "        \n",
    "        # Save update_professor.csv\n",
    "        if professors_to_update:\n",
    "            df = pd.DataFrame(professors_to_update)\n",
    "            update_path = os.path.join(self.output_base, 'update_professor.csv')\n",
    "            df.to_csv(update_path, index=False)\n",
    "            logger.info(f\"✅ Saved {len(professors_to_update)} professor updates to update_professor.csv\")\n",
    "            \n",
    "            # Update stats\n",
    "            if not hasattr(self.stats, 'professors_updated'):\n",
    "                self.stats['professors_updated'] = 0\n",
    "            self.stats['professors_updated'] = len(professors_to_update)\n",
    "        else:\n",
    "            logger.info(\"ℹ️ No professors need boss_name updates\")\n",
    "            if not hasattr(self.stats, 'professors_updated'):\n",
    "                self.stats['professors_updated'] = 0\n",
    "\n",
    "    def process_remaining_tables(self):\n",
    "        \"\"\"Process classes and timings after professor lookup is updated\"\"\"\n",
    "        logger.info(\"🏫 Processing remaining tables (classes, timings)...\")\n",
    "        \n",
    "        try:\n",
    "            # Clear any existing data from Phase 1 to avoid duplicates\n",
    "            self.new_classes = []\n",
    "            self.new_class_timings = []\n",
    "            self.new_class_exam_timings = []\n",
    "            self.class_id_mapping = {}\n",
    "            self.stats['classes_created'] = 0\n",
    "            self.stats['timings_created'] = 0\n",
    "            self.stats['exams_created'] = 0\n",
    "            \n",
    "            # Process classes (depends on updated professor lookup)\n",
    "            self.process_classes()\n",
    "            \n",
    "            # Process timings (depends on classes)\n",
    "            self.process_timings()\n",
    "            \n",
    "            logger.info(\"✅ Remaining tables processed successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to process remaining tables: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_corrected_professor_lookup(self):\n",
    "        \"\"\"Save professor lookup with corrected structure: boss_name, afterclass_name, database_id\"\"\"\n",
    "        lookup_data = []\n",
    "        seen_combinations = set()  # To track (boss_name, afterclass_name) for deduplication\n",
    "        \n",
    "        # Collect all unique professor entries\n",
    "        all_entries = {}\n",
    "        \n",
    "        # From professor_lookup\n",
    "        for scraped_name, data in self.professor_lookup.items():\n",
    "            boss_name = data.get('boss_name', scraped_name.upper())\n",
    "            afterclass_name = data.get('afterclass_name', scraped_name)\n",
    "            database_id = data['database_id']\n",
    "            \n",
    "            # Use boss_name as the key (since it's unique and in uppercase)\n",
    "            key = (boss_name, afterclass_name)\n",
    "            if key not in seen_combinations:\n",
    "                all_entries[boss_name] = {\n",
    "                    'boss_name': boss_name,\n",
    "                    'afterclass_name': afterclass_name,\n",
    "                    'database_id': database_id,\n",
    "                    'method': 'created' if any(prof['id'] == database_id for prof in self.new_professors) else 'exists'\n",
    "                }\n",
    "                seen_combinations.add(key)\n",
    "        \n",
    "        # Convert to list and sort\n",
    "        lookup_data = list(all_entries.values())\n",
    "        lookup_data.sort(key=lambda x: x['boss_name'])\n",
    "        \n",
    "        # Save to output folder\n",
    "        df = pd.DataFrame(lookup_data)\n",
    "        df.to_csv(os.path.join(self.output_base, 'professor_lookup.csv'), index=False)\n",
    "        logger.info(f\"✅ Saved updated professor lookup with {len(lookup_data)} unique entries\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"📊 PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"✅ Professors created: {self.stats['professors_created']}\")\n",
    "        print(f\"✅ Courses created: {self.stats['courses_created']}\")\n",
    "        print(f\"✅ Courses updated: {self.stats['courses_updated']}\")\n",
    "        print(f\"⚠️  Courses needing faculty: {self.stats['courses_needing_faculty']}\")\n",
    "        print(f\"✅ Classes created: {self.stats['classes_created']}\")\n",
    "        print(f\"✅ Class timings created: {self.stats['timings_created']}\")\n",
    "        print(f\"✅ Exam timings created: {self.stats['exams_created']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\n📁 OUTPUT FILES:\")\n",
    "        print(f\"   Verify folder: {self.verify_dir}/\")\n",
    "        print(f\"   - new_professors.csv ({self.stats['professors_created']} records)\")\n",
    "        print(f\"   - new_courses.csv ({self.stats['courses_created']} records)\")\n",
    "        print(f\"   Output folder: {self.output_base}/\")\n",
    "        print(f\"   - update_courses.csv ({self.stats['courses_updated']} records)\")\n",
    "        print(f\"   - new_acad_term.csv ({len(self.new_acad_terms)} records)\")\n",
    "        print(f\"   - new_classes.csv ({self.stats['classes_created']} records)\")\n",
    "        print(f\"   - new_class_timing.csv ({self.stats['timings_created']} records)\")\n",
    "        print(f\"   - new_class_exam_timing.csv ({self.stats['exams_created']} records)\")\n",
    "        print(f\"   - professor_lookup.csv (updated)\")\n",
    "        print(f\"   - courses_needing_faculty.csv ({self.stats['courses_needing_faculty']} records)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def run_phase1_professors_and_courses(self):\n",
    "        \"\"\"Phase 1: Process professors and courses with automated faculty mapping\"\"\"\n",
    "        try:\n",
    "            logger.info(\"🚀 Starting Phase 1: Professors and Courses with Automated Faculty Mapping\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Load data\n",
    "            if not self.load_or_cache_data():\n",
    "                logger.error(\"❌ Failed to load database data\")\n",
    "                return False\n",
    "            \n",
    "            if not self.load_raw_data():\n",
    "                logger.error(\"❌ Failed to load raw data\")\n",
    "                return False\n",
    "            \n",
    "            # Process professors (CSV only, no lookup update)\n",
    "            self.process_professors()\n",
    "            \n",
    "            # Process courses\n",
    "            self.process_courses()\n",
    "            \n",
    "            # NEW: Automated faculty mapping using BOSS data\n",
    "            logger.info(\"\\n🎓 Running automated faculty mapping...\")\n",
    "            try:\n",
    "                self.map_courses_to_faculties_from_boss()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Automated faculty mapping failed: {e}\")\n",
    "                logger.info(\"   Continuing with manual faculty assignment...\")\n",
    "            \n",
    "            # Process academic terms\n",
    "            self.process_acad_terms()\n",
    "            \n",
    "            # Save phase 1 outputs\n",
    "            self._save_phase1_outputs()\n",
    "            \n",
    "            # Print faculty mapping summary\n",
    "            if hasattr(self, 'courses_needing_faculty') and self.courses_needing_faculty:\n",
    "                logger.info(f\"\\n📋 Faculty Assignment Summary:\")\n",
    "                logger.info(f\"   • Automated mappings applied to {self.stats['courses_created'] - len(self.courses_needing_faculty)} courses\")\n",
    "                logger.info(f\"   • {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "                \n",
    "                # Show which courses need manual review\n",
    "                if len(self.courses_needing_faculty) <= 10:\n",
    "                    logger.info(f\"   Courses needing manual review:\")\n",
    "                    for course_info in self.courses_needing_faculty:\n",
    "                        logger.info(f\"     - {course_info['course_code']}: {course_info['course_name']}\")\n",
    "            \n",
    "            logger.info(\"✅ Phase 1 completed - Review files in verify/ folder\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Phase 1 failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run_phase2_remaining_tables(self):\n",
    "        \"\"\"Phase 2: Process classes and timings after professor correction\"\"\"\n",
    "        try:\n",
    "            logger.info(\"🚀 Starting Phase 2: Classes and Timings\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Set phase 2 mode to prevent overwriting corrected professors\n",
    "            self._phase2_mode = True\n",
    "            \n",
    "            # Update professor lookup from corrected CSV\n",
    "            if not self.update_professor_lookup_from_corrected_csv():\n",
    "                logger.error(\"❌ Failed to update professor lookup\")\n",
    "                return False\n",
    "            \n",
    "            # Update professors with missing boss_names\n",
    "            self.update_professors_with_boss_names()\n",
    "            \n",
    "            # Process remaining tables\n",
    "            if not self.process_remaining_tables():\n",
    "                logger.error(\"❌ Failed to process remaining tables\")\n",
    "                return False\n",
    "            \n",
    "            # Save all outputs\n",
    "            self.save_outputs()\n",
    "            \n",
    "            # Print summary\n",
    "            self.print_summary()\n",
    "            \n",
    "            logger.info(\"✅ Phase 2 completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Phase 2 failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_phase1_outputs(self):\n",
    "        \"\"\"Save Phase 1 outputs (professors, courses, acad_terms)\"\"\"\n",
    "        # Save new professors (to verify folder for manual correction)\n",
    "        # Always create the file, even if empty\n",
    "        df = pd.DataFrame(self.new_professors) if self.new_professors else pd.DataFrame(columns=['id', 'name', 'boss_name', 'afterclass_name', 'original_scraped_name'])\n",
    "        df.to_csv(os.path.join(self.verify_dir, 'new_professors.csv'), index=False)\n",
    "        if self.new_professors:\n",
    "            logger.info(f\"✅ Saved {len(self.new_professors)} new professors for review\")\n",
    "        else:\n",
    "            logger.info(f\"✅ Created empty new_professors.csv (all professors already exist)\")\n",
    "        \n",
    "        # Save new courses (to verify folder)\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_courses)} new courses\")\n",
    "        \n",
    "        # Save course updates\n",
    "        if self.update_courses:\n",
    "            df = pd.DataFrame(self.update_courses)\n",
    "            df.to_csv(os.path.join(self.output_base, 'update_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.update_courses)} course updates\")\n",
    "        \n",
    "        # Save academic terms\n",
    "        if self.new_acad_terms:\n",
    "            df = pd.DataFrame(self.new_acad_terms)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_acad_term.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_acad_terms)} academic terms\")\n",
    "\n",
    "    def run(self, skip_faculty_assignment=True):\n",
    "        \"\"\"Run the complete table building process\n",
    "        \n",
    "        Args:\n",
    "            skip_faculty_assignment: If True, faculty assignment is deferred\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"🚀 Starting TableBuilder process\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Load or cache database data\n",
    "            if not self.load_or_cache_data():\n",
    "                logger.error(\"❌ Failed to load database data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 2: Load raw data\n",
    "            if not self.load_raw_data():\n",
    "                logger.error(\"❌ Failed to load raw data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Process tables in dependency order\n",
    "            logger.info(\"\\n📋 Processing tables in dependency order...\")\n",
    "            \n",
    "            # 3.1: Process professors first (no dependencies)\n",
    "            self.process_professors()\n",
    "            \n",
    "            # 3.2: Process courses (without faculty assignment)\n",
    "            self.process_courses()\n",
    "            \n",
    "            # 3.3: Process academic terms (no dependencies)\n",
    "            self.process_acad_terms()\n",
    "            \n",
    "            # 3.4: Process classes (depends on courses, professors, acad_terms)\n",
    "            self.process_classes()\n",
    "            \n",
    "            # 3.5: Process timings (depends on classes)\n",
    "            self.process_timings()\n",
    "            \n",
    "            # Step 4: Save all outputs\n",
    "            self.save_outputs()\n",
    "            \n",
    "            # Step 5: Print summary\n",
    "            self.print_summary()\n",
    "            \n",
    "            if self.stats['courses_needing_faculty'] > 0 and not skip_faculty_assignment:\n",
    "                print(\"\\n⚠️  FACULTY ASSIGNMENT REQUIRED\")\n",
    "                print(f\"   {self.stats['courses_needing_faculty']} courses need faculty assignment\")\n",
    "                print(\"   Run builder.assign_course_faculties() to complete assignment\")\n",
    "            \n",
    "            logger.info(\"\\n✅ TableBuilder process completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Process failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def setup_boss_processing(self):\n",
    "        \"\"\"Initialize BOSS results processing with logging and caches\"\"\"\n",
    "        # Setup logging for BOSS processing\n",
    "        self.boss_log_file = os.path.join(self.output_base, 'boss_result_log.txt')\n",
    "        \n",
    "        # Create the log file and write header\n",
    "        try:\n",
    "            with open(self.boss_log_file, 'w') as f:\n",
    "                f.write(f\"BOSS Results Processing Log - {datetime.now().isoformat()}\\n\")\n",
    "                f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            print(f\"📝 Log file created: {self.boss_log_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Warning: Could not create log file {self.boss_log_file}: {e}\")\n",
    "            self.boss_log_file = None\n",
    "        \n",
    "        # Initialize existing classes cache\n",
    "        self.existing_classes_cache = []\n",
    "        \n",
    "        # Data storage for BOSS results\n",
    "        self.boss_data = []\n",
    "        self.failed_mappings = []\n",
    "        \n",
    "        # Output collectors\n",
    "        self.new_bid_windows = []\n",
    "        self.new_class_availability = []\n",
    "        self.new_bid_result = []\n",
    "        \n",
    "        # Caches for deduplication\n",
    "        self.bid_window_cache = {}  # (acad_term_id, round, window) -> bid_window_id\n",
    "        self.bid_window_id_counter = 1\n",
    "        \n",
    "        # Statistics\n",
    "        self.boss_stats = {\n",
    "            'files_processed': 0,\n",
    "            'total_rows': 0,\n",
    "            'bid_windows_created': 0,\n",
    "            'class_availability_created': 0,\n",
    "            'bid_results_created': 0,\n",
    "            'failed_mappings': 0\n",
    "        }\n",
    "        \n",
    "        print(\"🔄 BOSS results processing setup completed\")\n",
    "\n",
    "    def log_boss_activity(self, message, print_to_stdout=True):\n",
    "        \"\"\"Log activity to both file and optionally stdout\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"[{timestamp}] {message}\\n\"\n",
    "        \n",
    "        # Only write to file if boss_log_file exists (after setup_boss_processing is called)\n",
    "        if hasattr(self, 'boss_log_file') and self.boss_log_file:\n",
    "            try:\n",
    "                with open(self.boss_log_file, 'a') as f:\n",
    "                    f.write(log_message)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Warning: Could not write to log file: {e}\")\n",
    "        \n",
    "        if print_to_stdout:\n",
    "            print(f\"📝 {message}\")\n",
    "\n",
    "    def parse_term_to_acad_term_id(self, term_str):\n",
    "        \"\"\"Convert term string to acad_term_id format\n",
    "        \n",
    "        Examples:\n",
    "        \"2021-22 Term 1\" -> \"AY202122T1\"\n",
    "        \"2021-22 Term 3A\" -> \"AY202122T3A\"\n",
    "        \"\"\"\n",
    "        if not term_str or pd.isna(term_str):\n",
    "            return None\n",
    "        \n",
    "        # Clean the string\n",
    "        term_str = str(term_str).strip()\n",
    "        \n",
    "        # Pattern: YYYY-YY Term X[A/B]\n",
    "        pattern = r'(\\d{4})-(\\d{2})\\s+Term\\s+(\\w+)'\n",
    "        match = re.match(pattern, term_str)\n",
    "        \n",
    "        if match:\n",
    "            year_start = match.group(1)\n",
    "            year_end = match.group(2)\n",
    "            term = match.group(3)\n",
    "            return f\"AY{year_start}{year_end}T{term}\"\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def parse_bidding_window(self, bidding_window_str):\n",
    "        \"\"\"Complete parser for bidding window string to extract round and window\n",
    "        \n",
    "        Examples:\n",
    "        \"Round 1 Window 1\" -> (\"1\", 1)\n",
    "        \"Round 1A Window 2\" -> (\"1A\", 2)\n",
    "        \"Round 2A Window 3\" -> (\"2A\", 3)\n",
    "        \"Incoming Exchange Rnd 1C Win 1\" -> (\"1C\", 1)\n",
    "        \"Incoming Freshmen Rnd 1 Win 4\" -> (\"1F\", 4)\n",
    "        \"\"\"\n",
    "        if not bidding_window_str or pd.isna(bidding_window_str):\n",
    "            return None, None\n",
    "        \n",
    "        # Clean the string\n",
    "        bidding_window_str = str(bidding_window_str).strip()\n",
    "        \n",
    "        # Pattern 1: Standard format \"Round X[A/B/C] Window Y\"\n",
    "        pattern1 = r'Round\\s+(\\w+)\\s+Window\\s+(\\d+)'\n",
    "        match1 = re.match(pattern1, bidding_window_str)\n",
    "        if match1:\n",
    "            round_str = match1.group(1)\n",
    "            window_num = int(match1.group(2))\n",
    "            return round_str, window_num\n",
    "        \n",
    "        # Pattern 2: Incoming Exchange format \"Incoming Exchange Rnd X[A/B/C] Win Y\"\n",
    "        # Map to same round but keep distinction if needed\n",
    "        pattern2 = r'Incoming\\s+Exchange\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)'\n",
    "        match2 = re.match(pattern2, bidding_window_str)\n",
    "        if match2:\n",
    "            round_str = match2.group(1)  # Keep original round (1C)\n",
    "            window_num = int(match2.group(2))\n",
    "            return round_str, window_num\n",
    "        \n",
    "        # Pattern 3: Incoming Freshmen format \"Incoming Freshmen Rnd X Win Y\"\n",
    "        # Map Round 1 -> Round 1F for distinction\n",
    "        pattern3 = r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)'\n",
    "        match3 = re.match(pattern3, bidding_window_str)\n",
    "        if match3:\n",
    "            original_round = match3.group(1)\n",
    "            window_num = int(match3.group(2))\n",
    "            # Map Incoming Freshmen Round 1 to Round 1F\n",
    "            if original_round == \"1\":\n",
    "                round_str = \"1F\"\n",
    "            else:\n",
    "                round_str = f\"{original_round}F\"  # For other rounds if they exist\n",
    "            return round_str, window_num\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    def get_window_hierarchy(self, acad_term_id):\n",
    "        \"\"\"Get the expected window hierarchy for a given academic term\n",
    "        Updated to include incoming student rounds\"\"\"\n",
    "        if not acad_term_id:\n",
    "            return []\n",
    "        \n",
    "        # Extract year and term from acad_term_id\n",
    "        pattern = r'AY(\\d{4})(\\d{2})T(\\w+)'\n",
    "        match = re.match(pattern, acad_term_id)\n",
    "        if not match:\n",
    "            return []\n",
    "        \n",
    "        year_start = int(match.group(1))\n",
    "        year_end = int(match.group(2))\n",
    "        term = match.group(3)\n",
    "        \n",
    "        # Determine academic year\n",
    "        full_year_end = 2000 + year_end if year_end > 50 else 2000 + year_end\n",
    "        if year_start > full_year_end:\n",
    "            full_year_end += 100\n",
    "        \n",
    "        # Term 3A and 3B have different hierarchy\n",
    "        if term in ['3A', '3B']:\n",
    "            return [\n",
    "                (\"1\", 1), (\"1\", 2), (\"1\", 3), (\"1\", 4),\n",
    "                (\"2\", 1), (\"2\", 2)\n",
    "            ]\n",
    "        \n",
    "        # Regular terms (T1, T2) - includes incoming student rounds\n",
    "        base_hierarchy = []\n",
    "        \n",
    "        if full_year_end < 2025:  # Before AY2024-25\n",
    "            base_hierarchy = [\n",
    "                (\"1\", 1), (\"1\", 2),\n",
    "                (\"1A\", 1), (\"1A\", 2),\n",
    "                (\"1B\", 1), (\"1B\", 2),\n",
    "                (\"1C\", 1), (\"1C\", 2), (\"1C\", 3),\n",
    "                (\"2\", 1), (\"2\", 2), (\"2\", 3),\n",
    "                (\"2A\", 1), (\"2A\", 2), (\"2A\", 3)\n",
    "            ]\n",
    "        else:  # From AY2024-25 onwards\n",
    "            base_hierarchy = [\n",
    "                (\"1\", 1),\n",
    "                (\"1A\", 1), (\"1A\", 2), (\"1A\", 3),\n",
    "                (\"1B\", 1), (\"1B\", 2),\n",
    "                (\"1C\", 1), (\"1C\", 2), (\"1C\", 3),\n",
    "                (\"2\", 1), (\"2\", 2), (\"2\", 3),\n",
    "                (\"2A\", 1), (\"2A\", 2), (\"2A\", 3)\n",
    "            ]\n",
    "        \n",
    "        # Add incoming student rounds\n",
    "        incoming_rounds = [\n",
    "            (\"1F\", 1), (\"1F\", 2), (\"1F\", 3), (\"1F\", 4)  # Incoming Freshmen\n",
    "        ]\n",
    "        \n",
    "        # Combine hierarchies: regular rounds first, then incoming rounds\n",
    "        return base_hierarchy + incoming_rounds\n",
    "\n",
    "    def load_boss_results(self):\n",
    "        \"\"\"Load all BOSS results XLSX files\"\"\"\n",
    "        self.log_boss_activity(\"🔍 Loading BOSS results files...\")\n",
    "        \n",
    "        input_pattern = os.path.join('script_input', 'overallBossResults', '*.xlsx')\n",
    "        xlsx_files = glob.glob(input_pattern)\n",
    "        \n",
    "        if not xlsx_files:\n",
    "            self.log_boss_activity(f\"❌ No XLSX files found in pattern: {input_pattern}\")\n",
    "            return False\n",
    "        \n",
    "        self.log_boss_activity(f\"📂 Found {len(xlsx_files)} XLSX files\")\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        for file_path in xlsx_files:\n",
    "            try:\n",
    "                self.log_boss_activity(f\"📖 Loading: {os.path.basename(file_path)}\")\n",
    "                df = pd.read_excel(file_path)\n",
    "                \n",
    "                # Add source file for tracking\n",
    "                df['source_file'] = os.path.basename(file_path)\n",
    "                all_data.append(df)\n",
    "                \n",
    "                self.boss_stats['files_processed'] += 1\n",
    "                self.log_boss_activity(f\"✅ Loaded {len(df)} rows from {os.path.basename(file_path)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log_boss_activity(f\"❌ Error loading {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if all_data:\n",
    "            self.boss_data = pd.concat(all_data, ignore_index=True)\n",
    "            self.boss_stats['total_rows'] = len(self.boss_data)\n",
    "            self.log_boss_activity(f\"✅ Combined {self.boss_stats['total_rows']} total rows\")\n",
    "            return True\n",
    "        else:\n",
    "            self.log_boss_activity(\"❌ No data loaded successfully\")\n",
    "            return False\n",
    "\n",
    "    def process_bid_windows(self):\n",
    "        \"\"\"Process and create bid_window entries with dynamic window detection\"\"\"\n",
    "        self.log_boss_activity(\"🪟 Processing bid windows...\")\n",
    "        \n",
    "        if self.boss_data is None or len(self.boss_data) == 0:\n",
    "            self.log_boss_activity(\"❌ No BOSS data loaded\")\n",
    "            return False\n",
    "        \n",
    "        # Track all unique bid windows found in data\n",
    "        found_windows = defaultdict(set)  # acad_term_id -> set of (round, window) tuples\n",
    "        \n",
    "        # First pass: discover all windows that actually exist in the data\n",
    "        for _, row in self.boss_data.iterrows():\n",
    "            term_str = row.get('Term')\n",
    "            bidding_window_str = row.get('Bidding Window')\n",
    "            \n",
    "            if pd.isna(term_str) or pd.isna(bidding_window_str):\n",
    "                continue\n",
    "            \n",
    "            acad_term_id = self.parse_term_to_acad_term_id(term_str)\n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            \n",
    "            if acad_term_id and round_str and window_num:\n",
    "                found_windows[acad_term_id].add((round_str, window_num))\n",
    "        \n",
    "        # Define the expected order for rounds\n",
    "        round_order = {\n",
    "            '1': 1,\n",
    "            '1A': 2,\n",
    "            '1B': 3,\n",
    "            '1C': 4,    # Incoming Exchange\n",
    "            '1F': 5,    # Incoming Freshmen\n",
    "            '2': 6,\n",
    "            '2A': 7\n",
    "        }\n",
    "        \n",
    "        # Create bid windows in proper order\n",
    "        bid_window_id = 1\n",
    "        \n",
    "        # Process each academic term\n",
    "        for acad_term_id in sorted(found_windows.keys()):\n",
    "            windows_for_term = found_windows[acad_term_id]\n",
    "            \n",
    "            # Convert to list and sort by round order first, then window number\n",
    "            sorted_windows = sorted(\n",
    "                windows_for_term,\n",
    "                key=lambda x: (round_order.get(x[0], 99), x[1])\n",
    "            )\n",
    "            \n",
    "            self.log_boss_activity(f\"📅 Processing {acad_term_id}: found {len(sorted_windows)} windows\")\n",
    "            \n",
    "            # Create bid windows in order\n",
    "            for round_str, window_num in sorted_windows:\n",
    "                window_key = (acad_term_id, round_str, window_num)\n",
    "                \n",
    "                # Skip if already created\n",
    "                if window_key in self.bid_window_cache:\n",
    "                    continue\n",
    "                \n",
    "                new_bid_window = {\n",
    "                    'id': bid_window_id,\n",
    "                    'acad_term_id': acad_term_id,\n",
    "                    'round': round_str,\n",
    "                    'window': window_num\n",
    "                }\n",
    "                \n",
    "                self.new_bid_windows.append(new_bid_window)\n",
    "                self.bid_window_cache[window_key] = bid_window_id\n",
    "                self.boss_stats['bid_windows_created'] += 1\n",
    "                \n",
    "                self.log_boss_activity(\n",
    "                    f\"✅ Created bid_window {bid_window_id}: {acad_term_id} Round {round_str} Window {window_num}\"\n",
    "                )\n",
    "                \n",
    "                bid_window_id += 1\n",
    "        \n",
    "        self.bid_window_id_counter = bid_window_id\n",
    "        self.log_boss_activity(f\"✅ Created {self.boss_stats['bid_windows_created']} bid windows\")\n",
    "        return True\n",
    "\n",
    "    def sort_bid_windows_by_hierarchy(self):\n",
    "        \"\"\"Sort bid windows according to the proper hierarchy\"\"\"\n",
    "        self.log_boss_activity(\"🔄 Sorting bid windows by hierarchy...\")\n",
    "        \n",
    "        # Group by acad_term_id\n",
    "        term_groups = defaultdict(list)\n",
    "        for bw in self.new_bid_windows:\n",
    "            term_groups[bw['acad_term_id']].append(bw)\n",
    "        \n",
    "        sorted_windows = []\n",
    "        new_id_mapping = {}  # old_id -> new_id\n",
    "        new_id_counter = 1\n",
    "        \n",
    "        for acad_term_id in sorted(term_groups.keys()):\n",
    "            windows = term_groups[acad_term_id]\n",
    "            hierarchy = self.get_window_hierarchy(acad_term_id)\n",
    "            \n",
    "            # Create a mapping of (round, window) to bid_window for this term\n",
    "            term_window_map = {(bw['round'], bw['window']): bw for bw in windows}\n",
    "            \n",
    "            # Sort according to hierarchy\n",
    "            for round_str, window_num in hierarchy:\n",
    "                if (round_str, window_num) in term_window_map:\n",
    "                    bw = term_window_map[(round_str, window_num)]\n",
    "                    old_id = bw['id']\n",
    "                    new_id = new_id_counter\n",
    "                    new_id_counter += 1\n",
    "                    \n",
    "                    # Update the bid window with new ID\n",
    "                    bw['id'] = new_id\n",
    "                    sorted_windows.append(bw)\n",
    "                    new_id_mapping[old_id] = new_id\n",
    "                    \n",
    "                    # Update cache\n",
    "                    window_key = (acad_term_id, round_str, window_num)\n",
    "                    self.bid_window_cache[window_key] = new_id\n",
    "        \n",
    "        self.new_bid_windows = sorted_windows\n",
    "        self.bid_window_id_counter = new_id_counter\n",
    "        \n",
    "        self.log_boss_activity(f\"✅ Sorted {len(sorted_windows)} bid windows by hierarchy\")\n",
    "\n",
    "\n",
    "    def find_class_id(self, course_code, section, acad_term_id):\n",
    "        \"\"\"Find class_id using course_code, section, and acad_term_id\n",
    "        Robust version that checks multiple sources in order:\n",
    "        1. Memory cache (new_classes)\n",
    "        2. Database cache (existing_classes_cache)\n",
    "        3. new_classes.csv file\n",
    "        4. Direct database query\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, get course_id from course_code\n",
    "        course_id = self.get_course_id(course_code)\n",
    "        if not course_id:\n",
    "            return None\n",
    "        \n",
    "        # Convert section to string for consistent comparison\n",
    "        section_str = str(section)\n",
    "        \n",
    "        # Source 1: Search in newly created classes (memory)\n",
    "        if hasattr(self, 'new_classes') and self.new_classes:\n",
    "            for class_obj in self.new_classes:\n",
    "                if (class_obj['course_id'] == course_id and \n",
    "                    str(class_obj['section']) == section_str and \n",
    "                    class_obj['acad_term_id'] == acad_term_id):\n",
    "                    return class_obj['id']\n",
    "        \n",
    "        # Source 2: Search in existing database cache\n",
    "        if not hasattr(self, 'existing_classes_cache'):\n",
    "            self.load_existing_classes_cache()\n",
    "        \n",
    "        if hasattr(self, 'existing_classes_cache') and self.existing_classes_cache:\n",
    "            for class_obj in self.existing_classes_cache:\n",
    "                if (class_obj['course_id'] == course_id and \n",
    "                    str(class_obj['section']) == section_str and \n",
    "                    class_obj['acad_term_id'] == acad_term_id):\n",
    "                    return class_obj['id']\n",
    "        \n",
    "        # Source 3: Check new_classes.csv file (if cache is empty/stale)\n",
    "        class_id = self.search_new_classes_csv(course_id, section_str, acad_term_id)\n",
    "        if class_id:\n",
    "            return class_id\n",
    "        \n",
    "        # Source 4: Direct database query (last resort)\n",
    "        if self.connection:\n",
    "            class_id = self.search_database_classes(course_id, section_str, acad_term_id)\n",
    "            if class_id:\n",
    "                return class_id\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def get_course_id(self, course_code):\n",
    "        \"\"\"Get course_id from course_code, checking multiple sources\"\"\"\n",
    "        # Check courses cache (from database)\n",
    "        if course_code in self.courses_cache:\n",
    "            return self.courses_cache[course_code]['id']\n",
    "        \n",
    "        # Check in new_courses (newly created)\n",
    "        for course in self.new_courses:\n",
    "            if course['code'] == course_code:\n",
    "                return course['id']\n",
    "        \n",
    "        # Check new_courses.csv file\n",
    "        try:\n",
    "            new_courses_path = os.path.join(self.output_base, 'new_courses.csv')\n",
    "            verify_courses_path = os.path.join(self.verify_dir, 'new_courses.csv')\n",
    "            \n",
    "            for path in [verify_courses_path, new_courses_path]:\n",
    "                if os.path.exists(path):\n",
    "                    df = pd.read_csv(path)\n",
    "                    matching_courses = df[df['code'] == course_code]\n",
    "                    if not matching_courses.empty:\n",
    "                        return matching_courses.iloc[0]['id']\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"⚠️ Error reading new_courses.csv: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def search_new_classes_csv(self, course_id, section_str, acad_term_id):\n",
    "        \"\"\"Search for class in new_classes.csv file\"\"\"\n",
    "        try:\n",
    "            new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "            if os.path.exists(new_classes_path):\n",
    "                df = pd.read_csv(new_classes_path)\n",
    "                matching_classes = df[\n",
    "                    (df['course_id'] == course_id) & \n",
    "                    (df['section'].astype(str) == section_str) & \n",
    "                    (df['acad_term_id'] == acad_term_id)\n",
    "                ]\n",
    "                if not matching_classes.empty:\n",
    "                    return matching_classes.iloc[0]['id']\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"⚠️ Error reading new_classes.csv: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def search_database_classes(self, course_id, section_str, acad_term_id):\n",
    "        \"\"\"Search for class directly in database\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "            SELECT id FROM classes \n",
    "            WHERE course_id = %s AND section = %s AND acad_term_id = %s\n",
    "            LIMIT 1\n",
    "            \"\"\"\n",
    "            cursor = self.connection.cursor()\n",
    "            cursor.execute(query, (course_id, section_str, acad_term_id))\n",
    "            result = cursor.fetchone()\n",
    "            cursor.close()\n",
    "            \n",
    "            if result:\n",
    "                return result[0]\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"⚠️ Error querying database: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def load_existing_classes_cache(self):\n",
    "        \"\"\"Load existing classes from database cache with fallback options\"\"\"\n",
    "        self.existing_classes_cache = []\n",
    "        \n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, 'classes_cache.pkl')\n",
    "            \n",
    "            # Try loading from cache file first\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    classes_df = pd.read_pickle(cache_file)\n",
    "                    if not classes_df.empty:\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        self.log_boss_activity(f\"📚 Loaded {len(self.existing_classes_cache)} existing classes from cache\")\n",
    "                        return\n",
    "                    else:\n",
    "                        self.log_boss_activity(\"⚠️ Cache file exists but is empty\")\n",
    "                except Exception as e:\n",
    "                    self.log_boss_activity(f\"⚠️ Error reading cache file: {e}\")\n",
    "            \n",
    "            # If cache doesn't exist or is empty, try database\n",
    "            if self.connection:\n",
    "                try:\n",
    "                    query = \"SELECT * FROM classes\"\n",
    "                    classes_df = pd.read_sql_query(query, self.connection)\n",
    "                    if not classes_df.empty:\n",
    "                        # Save to cache for future use\n",
    "                        classes_df.to_pickle(cache_file)\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        self.log_boss_activity(f\"📚 Downloaded and cached {len(self.existing_classes_cache)} existing classes\")\n",
    "                        return\n",
    "                    else:\n",
    "                        self.log_boss_activity(\"⚠️ Database classes table is empty\")\n",
    "                except Exception as e:\n",
    "                    self.log_boss_activity(f\"⚠️ Error downloading classes from database: {e}\")\n",
    "            \n",
    "            # If all else fails, try reading from new_classes.csv\n",
    "            try:\n",
    "                new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "                if os.path.exists(new_classes_path):\n",
    "                    classes_df = pd.read_csv(new_classes_path)\n",
    "                    if not classes_df.empty:\n",
    "                        self.existing_classes_cache = classes_df.to_dict('records')\n",
    "                        self.log_boss_activity(f\"📚 Loaded {len(self.existing_classes_cache)} classes from new_classes.csv as fallback\")\n",
    "                        return\n",
    "            except Exception as e:\n",
    "                self.log_boss_activity(f\"⚠️ Error reading new_classes.csv as fallback: {e}\")\n",
    "            \n",
    "            # Final fallback\n",
    "            self.log_boss_activity(\"⚠️ All class loading methods failed - using empty cache\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.existing_classes_cache = []\n",
    "            self.log_boss_activity(f\"⚠️ Critical error in load_existing_classes_cache: {e}\")\n",
    "\n",
    "    def process_class_availability(self):\n",
    "        \"\"\"Process class availability data with support for multi-professor classes\"\"\"\n",
    "        self.log_boss_activity(\"📊 Processing class availability...\")\n",
    "        \n",
    "        processed_count = 0\n",
    "        \n",
    "        for _, row in self.boss_data.iterrows():\n",
    "            # Parse required fields\n",
    "            course_code = row.get('Course Code')\n",
    "            section = row.get('Section')\n",
    "            term_str = row.get('Term')\n",
    "            bidding_window_str = row.get('Bidding Window')\n",
    "            \n",
    "            # Extract availability data\n",
    "            vacancy = row.get('Vacancy')\n",
    "            enrolled_students = row.get('Enrolled Students')\n",
    "            before_process_vacancy = row.get('Before Process Vacancy')\n",
    "            \n",
    "            # Validate required fields\n",
    "            if pd.isna(course_code) or pd.isna(section) or pd.isna(term_str) or pd.isna(bidding_window_str):\n",
    "                continue\n",
    "            \n",
    "            # Parse term and bidding window\n",
    "            acad_term_id = self.parse_term_to_acad_term_id(term_str)\n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            \n",
    "            if not all([acad_term_id, round_str, window_num]):\n",
    "                continue\n",
    "            \n",
    "            # Find ALL class_ids for this course/section/term (handles multi-professor)\n",
    "            class_ids = self.find_all_class_ids(course_code, str(section), acad_term_id)\n",
    "            \n",
    "            if not class_ids:\n",
    "                # Record failed mapping\n",
    "                failed_row = {\n",
    "                    'course_code': course_code,\n",
    "                    'section': section,\n",
    "                    'acad_term_id': acad_term_id,\n",
    "                    'term_str': term_str,\n",
    "                    'bidding_window_str': bidding_window_str,\n",
    "                    'reason': 'class_not_found',\n",
    "                    'source_file': row.get('source_file', 'unknown')\n",
    "                }\n",
    "                self.failed_mappings.append(failed_row)\n",
    "                self.boss_stats['failed_mappings'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Get bid_window_id\n",
    "            window_key = (acad_term_id, round_str, window_num)\n",
    "            bid_window_id = self.bid_window_cache.get(window_key)\n",
    "            if not bid_window_id:\n",
    "                self.log_boss_activity(f\"⚠️ No bid_window_id for {window_key}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate fields\n",
    "            total = int(vacancy) if pd.notna(vacancy) else 0\n",
    "            current_enrolled = int(enrolled_students) if pd.notna(enrolled_students) else 0\n",
    "            available = int(before_process_vacancy) if pd.notna(before_process_vacancy) else 0\n",
    "            reserved = max(0, total - current_enrolled - available)\n",
    "            \n",
    "            # Create class availability record for EACH class_id (multi-professor support)\n",
    "            for class_id in class_ids:\n",
    "                availability_record = {\n",
    "                    'class_id': class_id,\n",
    "                    'bid_window_id': bid_window_id,\n",
    "                    'total': total,\n",
    "                    'current_enrolled': current_enrolled,\n",
    "                    'reserved': reserved,\n",
    "                    'available': available\n",
    "                }\n",
    "                \n",
    "                self.new_class_availability.append(availability_record)\n",
    "                self.boss_stats['class_availability_created'] += 1\n",
    "            \n",
    "            processed_count += len(class_ids)\n",
    "        \n",
    "        self.log_boss_activity(f\"✅ Processed {processed_count} class availability records\")\n",
    "        return True\n",
    "\n",
    "    def process_bid_results(self):\n",
    "        \"\"\"Process bid result data with support for multi-professor classes\"\"\"\n",
    "        self.log_boss_activity(\"📈 Processing bid results...\")\n",
    "        \n",
    "        processed_count = 0\n",
    "        \n",
    "        for _, row in self.boss_data.iterrows():\n",
    "            # Parse required fields\n",
    "            course_code = row.get('Course Code')\n",
    "            section = row.get('Section')\n",
    "            term_str = row.get('Term')\n",
    "            bidding_window_str = row.get('Bidding Window')\n",
    "            \n",
    "            # Extract bid result data\n",
    "            vacancy = row.get('Vacancy')\n",
    "            opening_vacancy = row.get('Opening Vacancy')\n",
    "            before_process_vacancy = row.get('Before Process Vacancy')\n",
    "            dice = row.get('D.I.C.E')\n",
    "            after_process_vacancy = row.get('After Process Vacancy', 0)\n",
    "            enrolled_students = row.get('Enrolled Students')\n",
    "            median_bid = row.get('Median Bid')\n",
    "            min_bid = row.get('Min Bid')\n",
    "            \n",
    "            # Validate required fields\n",
    "            if pd.isna(course_code) or pd.isna(section) or pd.isna(term_str) or pd.isna(bidding_window_str):\n",
    "                continue\n",
    "            \n",
    "            # Parse term and bidding window\n",
    "            acad_term_id = self.parse_term_to_acad_term_id(term_str)\n",
    "            round_str, window_num = self.parse_bidding_window(bidding_window_str)\n",
    "            \n",
    "            if not all([acad_term_id, round_str, window_num]):\n",
    "                continue\n",
    "            \n",
    "            # Find ALL class_ids for this course/section/term (handles multi-professor)\n",
    "            class_ids = self.find_all_class_ids(course_code, str(section), acad_term_id)\n",
    "            \n",
    "            if not class_ids:\n",
    "                # Failed mapping already recorded in process_class_availability\n",
    "                continue\n",
    "            \n",
    "            # Get bid_window_id\n",
    "            window_key = (acad_term_id, round_str, window_num)\n",
    "            bid_window_id = self.bid_window_cache.get(window_key)\n",
    "            if not bid_window_id:\n",
    "                continue\n",
    "            \n",
    "            # Convert numeric fields\n",
    "            def safe_int(val):\n",
    "                return int(val) if pd.notna(val) else 0\n",
    "            \n",
    "            def safe_float(val):\n",
    "                return float(val) if pd.notna(val) else 0.0\n",
    "            \n",
    "            # Create bid result record for EACH class_id (multi-professor support)\n",
    "            for class_id in class_ids:\n",
    "                bid_result_record = {\n",
    "                    'bid_window_id': bid_window_id,\n",
    "                    'class_id': class_id,\n",
    "                    'vacancy': safe_int(vacancy),\n",
    "                    'opening_vacancy': safe_int(opening_vacancy),\n",
    "                    'before_process_vacancy': safe_int(before_process_vacancy),\n",
    "                    'dice': safe_int(dice),\n",
    "                    'after_process_vacancy': safe_int(after_process_vacancy),\n",
    "                    'enrolled_students': safe_int(enrolled_students),\n",
    "                    'bid_actual_median': safe_float(median_bid),\n",
    "                    'bid_actual_min': safe_float(min_bid),\n",
    "                    'bid_predicted_median': 0.0,\n",
    "                    'bid_predicted_min': 0.0\n",
    "                }\n",
    "                \n",
    "                self.new_bid_result.append(bid_result_record)\n",
    "                self.boss_stats['bid_results_created'] += 1\n",
    "            \n",
    "            processed_count += len(class_ids)\n",
    "        \n",
    "        self.log_boss_activity(f\"✅ Processed {processed_count} bid result records\")\n",
    "        return True\n",
    "\n",
    "    def find_all_class_ids(self, course_code, section, acad_term_id):\n",
    "        \"\"\"Find ALL class_ids for a course/section/term combination (handles multi-professor classes)\"\"\"\n",
    "        \n",
    "        # First, get course_id from course_code\n",
    "        course_id = self.get_course_id(course_code)\n",
    "        if not course_id:\n",
    "            return []\n",
    "        \n",
    "        # Convert section to string for consistent comparison\n",
    "        section_str = str(section)\n",
    "        \n",
    "        class_ids = []\n",
    "        \n",
    "        # Source 1: Search in newly created classes (memory)\n",
    "        if hasattr(self, 'new_classes') and self.new_classes:\n",
    "            for class_obj in self.new_classes:\n",
    "                if (class_obj['course_id'] == course_id and \n",
    "                    str(class_obj['section']) == section_str and \n",
    "                    class_obj['acad_term_id'] == acad_term_id):\n",
    "                    class_ids.append(class_obj['id'])\n",
    "        \n",
    "        # Source 2: Search in existing database cache\n",
    "        if not hasattr(self, 'existing_classes_cache'):\n",
    "            self.load_existing_classes_cache()\n",
    "        \n",
    "        if hasattr(self, 'existing_classes_cache') and self.existing_classes_cache:\n",
    "            for class_obj in self.existing_classes_cache:\n",
    "                if (class_obj['course_id'] == course_id and \n",
    "                    str(class_obj['section']) == section_str and \n",
    "                    class_obj['acad_term_id'] == acad_term_id):\n",
    "                    if class_obj['id'] not in class_ids:  # Avoid duplicates\n",
    "                        class_ids.append(class_obj['id'])\n",
    "        \n",
    "        # Source 3: Check new_classes.csv file (if cache is empty/stale)\n",
    "        try:\n",
    "            new_classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "            if os.path.exists(new_classes_path) and not class_ids:\n",
    "                df = pd.read_csv(new_classes_path)\n",
    "                matching_classes = df[\n",
    "                    (df['course_id'] == course_id) & \n",
    "                    (df['section'].astype(str) == section_str) & \n",
    "                    (df['acad_term_id'] == acad_term_id)\n",
    "                ]\n",
    "                for _, row in matching_classes.iterrows():\n",
    "                    if row['id'] not in class_ids:\n",
    "                        class_ids.append(row['id'])\n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"⚠️ Error reading new_classes.csv: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        # Source 4: Direct database query (last resort)\n",
    "        if self.connection and not class_ids:\n",
    "            try:\n",
    "                query = \"\"\"\n",
    "                SELECT id FROM classes \n",
    "                WHERE course_id = %s AND section = %s AND acad_term_id = %s\n",
    "                \"\"\"\n",
    "                cursor = self.connection.cursor()\n",
    "                cursor.execute(query, (course_id, section_str, acad_term_id))\n",
    "                results = cursor.fetchall()\n",
    "                cursor.close()\n",
    "                \n",
    "                for result in results:\n",
    "                    if result[0] not in class_ids:\n",
    "                        class_ids.append(result[0])\n",
    "            except Exception as e:\n",
    "                self.log_boss_activity(f\"⚠️ Error querying database: {e}\", print_to_stdout=False)\n",
    "        \n",
    "        return class_ids\n",
    "\n",
    "    def save_boss_outputs(self):\n",
    "        \"\"\"Save all BOSS-related output files\"\"\"\n",
    "        self.log_boss_activity(\"💾 Saving BOSS output files...\")\n",
    "        \n",
    "        # Save bid windows\n",
    "        if self.new_bid_windows:\n",
    "            df = pd.DataFrame(self.new_bid_windows)\n",
    "            output_path = os.path.join(self.output_base, 'new_bid_window.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"✅ Saved {len(self.new_bid_windows)} bid windows to new_bid_window.csv\")\n",
    "        \n",
    "        # Save class availability\n",
    "        if self.new_class_availability:\n",
    "            df = pd.DataFrame(self.new_class_availability)\n",
    "            output_path = os.path.join(self.output_base, 'new_class_availability.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"✅ Saved {len(self.new_class_availability)} availability records to new_class_availability.csv\")\n",
    "        \n",
    "        # Save bid results\n",
    "        if self.new_bid_result:\n",
    "            df = pd.DataFrame(self.new_bid_result)\n",
    "            output_path = os.path.join(self.output_base, 'new_bid_result.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"✅ Saved {len(self.new_bid_result)} bid results to new_bid_result.csv\")\n",
    "        \n",
    "        # Save failed mappings\n",
    "        if self.failed_mappings:\n",
    "            df = pd.DataFrame(self.failed_mappings)\n",
    "            output_path = os.path.join(self.output_base, 'failed_boss_results_mapping.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            self.log_boss_activity(f\"⚠️ Saved {len(self.failed_mappings)} failed mappings to failed_boss_results_mapping.csv\")\n",
    "        \n",
    "        self.log_boss_activity(\"✅ All BOSS output files saved successfully\")\n",
    "\n",
    "    def print_boss_summary(self):\n",
    "        \"\"\"Print BOSS processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"📊 BOSS RESULTS PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"📂 Files processed: {self.boss_stats['files_processed']}\")\n",
    "        print(f\"📄 Total rows: {self.boss_stats['total_rows']}\")\n",
    "        print(f\"🪟 Bid windows created: {self.boss_stats['bid_windows_created']}\")\n",
    "        print(f\"📊 Class availability records: {self.boss_stats['class_availability_created']}\")\n",
    "        print(f\"📈 Bid result records: {self.boss_stats['bid_results_created']}\")\n",
    "        print(f\"❌ Failed mappings: {self.boss_stats['failed_mappings']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\n📁 OUTPUT FILES:\")\n",
    "        print(f\"   - new_bid_window.csv ({self.boss_stats['bid_windows_created']} records)\")\n",
    "        print(f\"   - new_class_availability.csv ({self.boss_stats['class_availability_created']} records)\")\n",
    "        print(f\"   - new_bid_result.csv ({self.boss_stats['bid_results_created']} records)\")\n",
    "        if self.boss_stats['failed_mappings'] > 0:\n",
    "            print(f\"   - failed_boss_results_mapping.csv ({self.boss_stats['failed_mappings']} records)\")\n",
    "        print(f\"   - boss_result_log.txt (processing log)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def run_phase3_boss_processing(self):\n",
    "        \"\"\"Run the complete BOSS results processing pipeline\"\"\"\n",
    "        try:\n",
    "            self.log_boss_activity(\"🚀 Starting Phase 3: BOSS Results Processing\")\n",
    "            self.log_boss_activity(\"=\"*60)\n",
    "\n",
    "            # Ensure database connection is available\n",
    "            if not hasattr(self, 'connection') or not self.connection:\n",
    "                self.log_boss_activity(\"🔌 Establishing database connection...\")\n",
    "                if not self.connect_database():\n",
    "                    self.log_boss_activity(\"❌ Failed to establish database connection\")\n",
    "                    return False\n",
    "\n",
    "            # Step 1: Setup\n",
    "            self.setup_boss_processing()\n",
    "            \n",
    "            # Step 2: Load BOSS results\n",
    "            if not self.load_boss_results():\n",
    "                self.log_boss_activity(\"❌ Failed to load BOSS results\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Process bid windows\n",
    "            if not self.process_bid_windows():\n",
    "                self.log_boss_activity(\"❌ Failed to process bid windows\")\n",
    "                return False\n",
    "            \n",
    "            # Step 4: Process class availability\n",
    "            if not self.process_class_availability():\n",
    "                self.log_boss_activity(\"❌ Failed to process class availability\")\n",
    "                return False\n",
    "            \n",
    "            # Step 5: Process bid results\n",
    "            if not self.process_bid_results():\n",
    "                self.log_boss_activity(\"❌ Failed to process bid results\")\n",
    "                return False\n",
    "            \n",
    "            # Step 6: Save outputs\n",
    "            self.save_boss_outputs()\n",
    "            \n",
    "            # Step 7: Print summary\n",
    "            self.print_boss_summary()\n",
    "            \n",
    "            self.log_boss_activity(\"✅ Phase 3: BOSS Results Processing completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_boss_activity(f\"❌ Phase 3 failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def load_faculties_cache(self):\n",
    "        \"\"\"Load faculties from database cache for mapping\"\"\"\n",
    "        try:\n",
    "            cache_file = os.path.join(self.cache_dir, 'faculties_cache.pkl')\n",
    "            \n",
    "            # Try loading from cache file first\n",
    "            if os.path.exists(cache_file):\n",
    "                try:\n",
    "                    faculties_df = pd.read_pickle(cache_file)\n",
    "                    if not faculties_df.empty:\n",
    "                        self.faculties_cache = {}\n",
    "                        self.faculty_acronym_to_id = {}\n",
    "                        \n",
    "                        for _, row in faculties_df.iterrows():\n",
    "                            faculty_id = row['id']\n",
    "                            acronym = row['acronym'].upper()\n",
    "                            \n",
    "                            self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                            self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                        \n",
    "                        logger.info(f\"📚 Loaded {len(self.faculties_cache)} faculties from cache\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(\"⚠️ Faculty cache file exists but is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error reading faculty cache file: {e}\")\n",
    "            \n",
    "            # If cache doesn't exist or failed, try database\n",
    "            if self.connection:\n",
    "                try:\n",
    "                    query = \"SELECT * FROM faculties\"\n",
    "                    faculties_df = pd.read_sql_query(query, self.connection)\n",
    "                    if not faculties_df.empty:\n",
    "                        # Save to cache for future use\n",
    "                        faculties_df.to_pickle(cache_file)\n",
    "                        \n",
    "                        # Load into memory\n",
    "                        self.faculties_cache = {}\n",
    "                        self.faculty_acronym_to_id = {}\n",
    "                        \n",
    "                        for _, row in faculties_df.iterrows():\n",
    "                            faculty_id = row['id']\n",
    "                            acronym = row['acronym'].upper()\n",
    "                            \n",
    "                            self.faculties_cache[faculty_id] = row.to_dict()\n",
    "                            self.faculty_acronym_to_id[acronym] = faculty_id\n",
    "                        \n",
    "                        logger.info(f\"📚 Downloaded and cached {len(self.faculties_cache)} faculties from database\")\n",
    "                        return True\n",
    "                    else:\n",
    "                        logger.warning(\"⚠️ Database faculties table is empty\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"⚠️ Error downloading faculties from database: {e}\")\n",
    "            \n",
    "            # Fallback: create basic mapping from known data\n",
    "            logger.warning(\"⚠️ Using fallback faculty mapping\")\n",
    "            self.faculties_cache = {}\n",
    "            self.faculty_acronym_to_id = {\n",
    "                'LKCSB': 1,   # Lee Kong Chian School of Business\n",
    "                'YPHSL': 2,   # Yong Pung How School of Law\n",
    "                'SOE': 3,     # School of Economics\n",
    "                'SCIS': 4,    # School of Computing and Information Systems\n",
    "                'SOSS': 5,    # School of Social Sciences\n",
    "                'SOA': 6,     # School of Accountancy\n",
    "                'CIS': 7,     # College of Integrative Studies\n",
    "                'CEC': 8,      # Center for English Communication\n",
    "                'C4SR': 9,      # Centre for Social Responsibility\n",
    "                'OCS': 10,      # Dato’ Kho Hui Meng Career Centre\n",
    "            }\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Critical error in load_faculties_cache: {e}\")\n",
    "            return False\n",
    "\n",
    "    def map_courses_to_faculties_from_boss(self):\n",
    "        \"\"\"Map courses to faculties using School/Department data from BOSS results\"\"\"\n",
    "        logger.info(\"🎓 Starting automated faculty mapping from BOSS data...\")\n",
    "        \n",
    "        # Load faculties cache first\n",
    "        if not self.load_faculties_cache():\n",
    "            logger.error(\"❌ Failed to load faculties cache\")\n",
    "            return False\n",
    "        \n",
    "        # Department code to faculty acronym mapping\n",
    "        # Maps BOSS School/Department codes to our faculty acronyms\n",
    "        dept_to_faculty_mapping = {\n",
    "            'SOA': 'SOA',         # School of Accountancy\n",
    "            'SOSS': 'SOSS',       # School of Social Sciences  \n",
    "            'LKCSOB': 'LKCSB',    # Lee Kong Chian School of Business (alternative name)\n",
    "            'LKCSB': 'LKCSB',     # Lee Kong Chian School of Business\n",
    "            'SIS': 'SCIS',        # School of Computing and Information Systems (old name)\n",
    "            'SCIS': 'SCIS',       # School of Computing and Information Systems\n",
    "            'OCC': 'CIS',         # College of Integrative Studies (Office of Core Curriculum)\n",
    "            'CIS': 'CIS',         # College of Integrative Studies\n",
    "            'CEC': 'CEC',         # Center for English Communication\n",
    "            'SOL': 'YPHSL',       # Yong Pung How School of Law (School of Law)\n",
    "            'SOLGPO': 'YPHSL',    # Yong Pung How School of Law (alternative)\n",
    "            'SOE': 'SOE',          # School of Economics\n",
    "            'C4SR': 'C4SR',          # Centre for Social Responsibility\n",
    "            'OCS': 'OCS',          # Dato’ Kho Hui Meng Career Centre\n",
    "        }\n",
    "        \n",
    "        # Track new faculties that need to be created\n",
    "        new_faculties_needed = set()\n",
    "        course_faculty_mappings = {}\n",
    "        \n",
    "        # Load BOSS results to extract School/Department mapping\n",
    "        boss_data_pattern = os.path.join('script_input', 'overallBossResults', '*.xlsx')\n",
    "        boss_files = glob.glob(boss_data_pattern)\n",
    "        \n",
    "        if not boss_files:\n",
    "            logger.warning(\"⚠️ No BOSS results files found for faculty mapping\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"📂 Found {len(boss_files)} BOSS files for faculty mapping\")\n",
    "        \n",
    "        # Collect all course-faculty mappings from BOSS data\n",
    "        boss_faculty_data = []\n",
    "        for file_path in boss_files:\n",
    "            try:\n",
    "                df = pd.read_excel(file_path)\n",
    "                if 'Course Code' in df.columns and 'School/Department' in df.columns:\n",
    "                    # Extract unique course-faculty pairs\n",
    "                    course_dept_pairs = df[['Course Code', 'School/Department']].dropna().drop_duplicates()\n",
    "                    boss_faculty_data.append(course_dept_pairs)\n",
    "                    logger.info(f\"✅ Extracted {len(course_dept_pairs)} course-department pairs from {os.path.basename(file_path)}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Could not read {file_path}: {e}\")\n",
    "        \n",
    "        if not boss_faculty_data:\n",
    "            logger.warning(\"⚠️ No valid BOSS faculty data found\")\n",
    "            return False\n",
    "        \n",
    "        # Combine all BOSS faculty data\n",
    "        combined_boss_data = pd.concat(boss_faculty_data, ignore_index=True).drop_duplicates()\n",
    "        logger.info(f\"📋 Combined {len(combined_boss_data)} unique course-department pairs\")\n",
    "        \n",
    "        # Log unique departments found\n",
    "        unique_depts = combined_boss_data['School/Department'].str.strip().str.upper().unique()\n",
    "        logger.info(f\"🏛️ Unique departments found in BOSS data: {sorted(unique_depts)}\")\n",
    "        \n",
    "        # Process each course-department pair\n",
    "        mapped_count = 0\n",
    "        unmapped_depts = set()\n",
    "        \n",
    "        for _, row in combined_boss_data.iterrows():\n",
    "            course_code = row['Course Code']\n",
    "            dept_code = str(row['School/Department']).strip().upper()\n",
    "            \n",
    "            if not course_code or not dept_code:\n",
    "                continue\n",
    "            \n",
    "            # Check if course exists in our courses (new or existing)\n",
    "            course_exists = False\n",
    "            if course_code in self.courses_cache:\n",
    "                course_exists = True\n",
    "            elif any(course['code'] == course_code for course in self.new_courses):\n",
    "                course_exists = True\n",
    "            \n",
    "            if not course_exists:\n",
    "                continue  # Skip courses we don't have\n",
    "            \n",
    "            # Map department code to faculty acronym, then to faculty ID\n",
    "            if dept_code in dept_to_faculty_mapping:\n",
    "                faculty_acronym = dept_to_faculty_mapping[dept_code]\n",
    "                \n",
    "                # Get faculty ID from acronym\n",
    "                if faculty_acronym in self.faculty_acronym_to_id:\n",
    "                    faculty_id = self.faculty_acronym_to_id[faculty_acronym]\n",
    "                    course_faculty_mappings[course_code] = faculty_id\n",
    "                    mapped_count += 1\n",
    "                    logger.debug(f\"✅ Mapped {course_code}: {dept_code} → {faculty_acronym} → ID {faculty_id}\")\n",
    "                else:\n",
    "                    logger.warning(f\"⚠️ Faculty acronym {faculty_acronym} not found in database\")\n",
    "                    unmapped_depts.add(dept_code)\n",
    "            else:\n",
    "                # Track unmapped department for new faculty creation\n",
    "                unmapped_depts.add(dept_code)\n",
    "                logger.info(f\"🆕 Unmapped department: {dept_code} (for course {course_code})\")\n",
    "        \n",
    "        logger.info(f\"✅ Mapped {mapped_count} courses to existing faculties\")\n",
    "        logger.info(f\"🆕 Found {len(unmapped_depts)} unmapped departments: {sorted(unmapped_depts)}\")\n",
    "        \n",
    "        # Create new faculties for unmapped departments\n",
    "        new_faculty_mappings = {}\n",
    "        if unmapped_depts:\n",
    "            new_faculty_mappings = self._create_new_faculties(unmapped_depts)\n",
    "            \n",
    "            # Update our faculty mapping caches\n",
    "            for dept_code, faculty_data in new_faculty_mappings.items():\n",
    "                faculty_id = faculty_data['id']\n",
    "                faculty_acronym = faculty_data['acronym']\n",
    "                \n",
    "                # Update caches\n",
    "                self.faculties_cache[faculty_id] = faculty_data\n",
    "                self.faculty_acronym_to_id[faculty_acronym] = faculty_id\n",
    "                dept_to_faculty_mapping[dept_code] = faculty_acronym\n",
    "            \n",
    "            # Re-process courses with new faculty mappings\n",
    "            for _, row in combined_boss_data.iterrows():\n",
    "                course_code = row['Course Code']\n",
    "                dept_code = str(row['School/Department']).strip().upper()\n",
    "                \n",
    "                if course_code and dept_code and dept_code in new_faculty_mappings:\n",
    "                    # Check if course exists\n",
    "                    course_exists = False\n",
    "                    if course_code in self.courses_cache:\n",
    "                        course_exists = True\n",
    "                    elif any(course['code'] == course_code for course in self.new_courses):\n",
    "                        course_exists = True\n",
    "                    \n",
    "                    if course_exists:\n",
    "                        faculty_id = new_faculty_mappings[dept_code]['id']\n",
    "                        course_faculty_mappings[course_code] = faculty_id\n",
    "                        mapped_count += 1\n",
    "        \n",
    "        # Apply faculty mappings to courses\n",
    "        self._apply_faculty_mappings_to_courses(course_faculty_mappings)\n",
    "        \n",
    "        logger.info(f\"✅ Automated faculty mapping completed:\")\n",
    "        logger.info(f\"   • {mapped_count} courses mapped to faculties\")\n",
    "        logger.info(f\"   • {len(new_faculty_mappings)} new faculties created\")\n",
    "        logger.info(f\"   • {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _create_new_faculties(self, unmapped_dept_codes):\n",
    "        \"\"\"Create new faculties for unmapped department codes\"\"\"\n",
    "        logger.info(f\"🏗️ Creating {len(unmapped_dept_codes)} new faculties...\")\n",
    "        \n",
    "        # Get next available faculty ID (start after 10 since you have 10 existing faculties)\n",
    "        if hasattr(self, 'faculties_cache') and self.faculties_cache:\n",
    "            next_faculty_id = max(self.faculties_cache.keys()) + 1\n",
    "        else:\n",
    "            next_faculty_id = 11  # Start after existing 10 faculties\n",
    "        \n",
    "        # Since all known faculties are already mapped, any new dept codes \n",
    "        # would be truly new faculties that don't exist yet\n",
    "        new_faculties = []\n",
    "        new_faculty_mappings = {}\n",
    "        \n",
    "        for dept_code in sorted(unmapped_dept_codes):\n",
    "            # Generate a reasonable name from the code\n",
    "            # Common patterns for new faculties\n",
    "            if 'CENTRE' in dept_code.upper():\n",
    "                faculty_name = f\"Centre for {dept_code.replace('CENTRE', '').strip()}\"\n",
    "            elif 'CENTER' in dept_code.upper():\n",
    "                faculty_name = f\"Center for {dept_code.replace('CENTER', '').strip()}\"\n",
    "            elif 'INSTITUTE' in dept_code.upper():\n",
    "                faculty_name = f\"Institute of {dept_code.replace('INSTITUTE', '').strip()}\"\n",
    "            elif 'OFFICE' in dept_code.upper():\n",
    "                faculty_name = f\"Office of {dept_code.replace('OFFICE', '').strip()}\"\n",
    "            else:\n",
    "                # Default pattern\n",
    "                faculty_name = f\"SMU {dept_code}\"\n",
    "            \n",
    "            new_faculty = {\n",
    "                'id': next_faculty_id,\n",
    "                'name': faculty_name,\n",
    "                'acronym': dept_code,  # Use the original dept_code as acronym\n",
    "                'site_url': f'https://www.smu.edu.sg/',\n",
    "                'belong_to_university': 1,  # SMU\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'updated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            new_faculties.append(new_faculty)\n",
    "            new_faculty_mappings[dept_code] = new_faculty\n",
    "            next_faculty_id += 1\n",
    "            \n",
    "            logger.info(f\"✅ Created faculty: {faculty_name} ({dept_code}) with ID {new_faculty['id']}\")\n",
    "        \n",
    "        # Save new faculties to verify folder\n",
    "        if new_faculties:\n",
    "            df = pd.DataFrame(new_faculties)\n",
    "            output_path = os.path.join(self.verify_dir, 'new_faculties.csv')\n",
    "            df.to_csv(output_path, index=False)\n",
    "            logger.info(f\"💾 Saved {len(new_faculties)} new faculties to {output_path}\")\n",
    "        \n",
    "        return new_faculty_mappings\n",
    "\n",
    "    def _apply_faculty_mappings_to_courses(self, course_faculty_mappings):\n",
    "        \"\"\"Apply faculty mappings to new courses and update courses needing faculty\"\"\"\n",
    "        logger.info(f\"🔄 Applying faculty mappings to {len(course_faculty_mappings)} courses...\")\n",
    "        \n",
    "        mapped_count = 0\n",
    "        \n",
    "        # Update new_courses\n",
    "        for course in self.new_courses:\n",
    "            course_code = course['code']\n",
    "            if course_code in course_faculty_mappings:\n",
    "                course['belong_to_faculty'] = course_faculty_mappings[course_code]\n",
    "                mapped_count += 1\n",
    "        \n",
    "        # Update courses_cache\n",
    "        for course_code, faculty_id in course_faculty_mappings.items():\n",
    "            if course_code in self.courses_cache:\n",
    "                self.courses_cache[course_code]['belong_to_faculty'] = faculty_id\n",
    "        \n",
    "        # Remove mapped courses from courses_needing_faculty\n",
    "        original_needing_count = len(self.courses_needing_faculty)\n",
    "        self.courses_needing_faculty = [\n",
    "            course_info for course_info in self.courses_needing_faculty\n",
    "            if course_info['course_code'] not in course_faculty_mappings\n",
    "        ]\n",
    "        \n",
    "        removed_count = original_needing_count - len(self.courses_needing_faculty)\n",
    "        \n",
    "        logger.info(f\"✅ Applied faculty mappings:\")\n",
    "        logger.info(f\"   • {mapped_count} courses updated with faculty\")\n",
    "        logger.info(f\"   • {removed_count} courses removed from manual review queue\")\n",
    "        logger.info(f\"   • {len(self.courses_needing_faculty)} courses still need manual review\")\n",
    "\n",
    "    def extract_acad_term_from_path(self, file_path: str) -> Optional[str]:\n",
    "        r\"\"\"Extract acad_term_id from file path as fallback\n",
    "        Examples:\n",
    "        'script_input\\classTimingsFull\\2021-22_T1' -> 'AY202122T1'\n",
    "        'script_input\\classTimingsFull\\2022-23_T3A' -> 'AY202223T3A'\n",
    "        \"\"\"\n",
    "        # Extract the term folder name\n",
    "        path_parts = file_path.replace('/', '\\\\').split('\\\\')\n",
    "        \n",
    "        for part in path_parts:\n",
    "            # Look for pattern like \"2021-22_T1\"\n",
    "            match = re.match(r'(\\d{4})-(\\d{2})_T(\\w+)', part)\n",
    "            if match:\n",
    "                year_start = match.group(1)\n",
    "                year_end = match.group(2)\n",
    "                term = match.group(3)\n",
    "                return f\"AY{year_start}{year_end}T{term}\"\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def open_course_html_files(self, driver, course_code):\n",
    "        \"\"\"Open relevant HTML files for a course from standalone data\"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, 'standalone_data') or self.standalone_data is None:\n",
    "                print(f\"⚠️ No standalone data available\")\n",
    "                return\n",
    "            \n",
    "            # Filter standalone data for the current course\n",
    "            course_files = self.standalone_data[\n",
    "                self.standalone_data['course_code'].str.upper() == course_code.upper()\n",
    "            ]\n",
    "            \n",
    "            if course_files.empty:\n",
    "                print(f\"📝 No HTML files found for {course_code}\")\n",
    "                return\n",
    "            \n",
    "            # Get unique filepaths for this course\n",
    "            unique_filepaths = course_files['filepath'].dropna().unique()\n",
    "            \n",
    "            if len(unique_filepaths) == 0:\n",
    "                print(f\"📝 No valid filepaths found for {course_code}\")\n",
    "                return\n",
    "            \n",
    "            print(f\"📂 Opening {len(unique_filepaths)} HTML files for {course_code}...\")\n",
    "            \n",
    "            # Open each file in a new tab with proper file:// protocol\n",
    "            for filepath in unique_filepaths:\n",
    "                try:\n",
    "                    # Ensure it's an HTML file\n",
    "                    if not str(filepath).lower().endswith('.html'):\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to absolute path and use proper file:// protocol\n",
    "                    abs_path = os.path.abspath(str(filepath))\n",
    "                    \n",
    "                    # Use pathlib for cross-platform compatibility\n",
    "                    from pathlib import Path\n",
    "                    file_path = Path(abs_path)\n",
    "                    \n",
    "                    if file_path.exists():\n",
    "                        # Use pathlib's as_uri() method for proper file:// URL\n",
    "                        file_url = file_path.as_uri()\n",
    "                        \n",
    "                        # Open in new tab\n",
    "                        driver.execute_script(f\"window.open('{file_url}', '_blank');\")\n",
    "                        print(f\"✅ Opened: {file_path.name}\")\n",
    "                    else:\n",
    "                        print(f\"⚠️ File not found: {abs_path}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Could not open {filepath}: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error opening HTML files for {course_code}: {e}\")\n",
    "\n",
    "    def get_last_filepath_by_course(self, course_code):\n",
    "        \"\"\"Direct filepath lookup for course code - bypasses record_key linking\"\"\"\n",
    "        print(f\"🔍 DEBUG: Looking for course {course_code} using direct method\")\n",
    "        \n",
    "        # Check if we have standalone data with filepath column\n",
    "        if hasattr(self, 'standalone_data') and self.standalone_data is not None:\n",
    "            if 'filepath' in self.standalone_data.columns:\n",
    "                print(f\"✅ DEBUG: Found filepath column in standalone_data\")\n",
    "                \n",
    "                course_records = self.standalone_data[\n",
    "                    self.standalone_data['course_code'].str.upper() == course_code.upper()\n",
    "                ].copy()\n",
    "                \n",
    "                print(f\"📊 DEBUG: Found {len(course_records)} records for {course_code}\")\n",
    "                \n",
    "                if not course_records.empty:\n",
    "                    # Get the most recent record (last row)\n",
    "                    last_record = course_records.iloc[-1]\n",
    "                    filepath = last_record.get('filepath')\n",
    "                    \n",
    "                    print(f\"📁 DEBUG: Last record filepath: {filepath}\")\n",
    "                    \n",
    "                    if pd.notna(filepath):\n",
    "                        print(f\"✅ Found filepath for {course_code}: {filepath}\")\n",
    "                        return filepath\n",
    "                    else:\n",
    "                        print(f\"❌ DEBUG: Filepath is NaN for {course_code}\")\n",
    "            else:\n",
    "                print(f\"❌ DEBUG: No 'filepath' column in standalone_data\")\n",
    "                print(f\"Available columns: {list(self.standalone_data.columns)}\")\n",
    "        \n",
    "        # Fallback: check multiple_data if standalone doesn't have filepath\n",
    "        if hasattr(self, 'multiple_data') and self.multiple_data is not None:\n",
    "            if 'filepath' in self.multiple_data.columns and 'course_code' in self.multiple_data.columns:\n",
    "                print(f\"✅ DEBUG: Checking multiple_data as fallback\")\n",
    "                \n",
    "                course_records = self.multiple_data[\n",
    "                    self.multiple_data['course_code'].str.upper() == course_code.upper()\n",
    "                ].copy()\n",
    "                \n",
    "                if not course_records.empty:\n",
    "                    last_record = course_records.iloc[-1]\n",
    "                    filepath = last_record.get('filepath')\n",
    "                    \n",
    "                    if pd.notna(filepath):\n",
    "                        print(f\"✅ Found filepath in multiple_data for {course_code}: {filepath}\")\n",
    "                        return filepath\n",
    "        \n",
    "        print(f\"❌ DEBUG: No filepath found for {course_code}\")\n",
    "        return None\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"Explicitly close database connection\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "            logger.info(\"🔒 Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Phase 1 - Professor and Course Processing with Automated Faculty Mapping**\n",
    "\n",
    "**What This Does:**\n",
    "- Initializes the TableBuilder system and connects to PostgreSQL database for existing data validation\n",
    "- Processes professors from raw data with advanced name normalization handling Asian, Western, and mixed naming patterns\n",
    "- Resolves professor email addresses automatically using Microsoft Outlook integration\n",
    "- Handles hardcoded multi-instructor combinations and prevents duplicate professor creation through multiple validation strategies\n",
    "- Creates new courses from standalone data and automatically maps them to SMU faculties using BOSS department data\n",
    "- Generates academic terms with proper ID formatting and date range extraction\n",
    "- Outputs verification files for manual review: `new_professors.csv` for name corrections and `new_courses.csv` for faculty validation\n",
    "- Provides detailed statistics on professors created, courses processed, and automated faculty mappings applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TableBuilder\n",
    "builder = TableBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 1 (professors, courses, acad_terms)\n",
    "success = builder.run_phase1_professors_and_courses()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 Phase 1 completed successfully!\")\n",
    "    print(\"📝 Next steps:\")\n",
    "    print(\"   1. Review script_output/verify/new_professors.csv\")\n",
    "    print(\"   2. Manually correct any professor names if needed\")\n",
    "    print(\"   3. Run Phase 2 in the next cell\")\n",
    "else:\n",
    "    print(\"\\n❌ Phase 1 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 2: Professor Name Review and Correction Interface**\n",
    "\n",
    "**What This Does:**\n",
    "- Loads the generated `new_professors.csv` file from the verification directory\n",
    "- Displays a comparison table showing four name formats: original scraped name, boss format (ALL CAPS), afterclass format (Title Case), and the final processed name\n",
    "- Provides clear instructions for manual correction focusing only on the 'name' column (afterclass format)\n",
    "- Guides users to preserve the boss_name format while correcting any parsing errors or name formatting issues\n",
    "- Handles empty files gracefully when all professors already exist in the database\n",
    "- Prepares corrected data for Phase 2 processing by maintaining proper name mapping relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display new professors for review\n",
    "new_prof_path = os.path.join('script_output', 'verify', 'new_professors.csv')\n",
    "if os.path.exists(new_prof_path):\n",
    "    df = pd.read_csv(new_prof_path)\n",
    "    if not df.empty:\n",
    "        print(f\"📋 {len(df)} new professors created:\")\n",
    "        print(\"\\n🔍 Review these professor names:\")\n",
    "        display(df[['name', 'boss_name', 'afterclass_name', 'original_scraped_name']])\n",
    "        print(\"\\n📝 If any names need correction, edit the 'name' column in:\")\n",
    "        print(f\"   {new_prof_path}\")\n",
    "        print(\"\\n⚠️  Only edit the 'name' column (afterclass format)\")\n",
    "        print(\"   Keep 'boss_name' unchanged\")\n",
    "    else:\n",
    "        print(\"✅ No new professors created - all professors already exist in database\")\n",
    "else:\n",
    "    print(\"✅ No new professors file - all professors already exist in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 3: Phase 2 - Class and Timing Processing with Corrected Professor Data**\n",
    "\n",
    "**What This Does:**\n",
    "- Reads manually corrected professor names from verification CSV files and updates internal lookup tables\n",
    "- Processes classes from standalone data using corrected professor mappings and established course relationships\n",
    "- Handles complex professor assignments including single professors, JSON arrays for multi-instructor classes, and missing professor scenarios\n",
    "- Generates class timing records (weekly schedules) and exam timing records with proper foreign key relationships\n",
    "- Links all timing data to valid class IDs while maintaining referential integrity\n",
    "- Creates complete set of database-ready CSV files: `new_classes.csv`, `new_class_timing.csv`, `new_class_exam_timing.csv`\n",
    "- Provides comprehensive error reporting for validation issues and successful record creation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Phase 2 (classes, timings) after manual correction\n",
    "success = builder.run_phase2_remaining_tables()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 Phase 2 completed successfully!\")\n",
    "    print(\"📝 All tables generated with corrected professor names\")\n",
    "else:\n",
    "    print(\"\\n❌ Phase 2 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 4: Interactive Faculty Assignment for Unmapped Courses**\n",
    "\n",
    "**What This Does:**\n",
    "- Identifies courses that still require manual faculty assignment after automated BOSS-based mapping\n",
    "- Opens scraped HTML course outline files in web browser for informed faculty assignment decisions  \n",
    "- Presents interactive menu of SMU's schools and centers with options to create new faculties for unmapped departments\n",
    "- Provides course code, name, and content preview to guide proper faculty placement decisions\n",
    "- Updates course records with selected faculty assignments and maintains faculty cache consistency\n",
    "- Allows skipping courses that need additional research while preserving assignment workflow\n",
    "- Re-saves updated CSV files with complete faculty information for database insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run faculty assignment process if needed\n",
    "if hasattr(builder, 'courses_needing_faculty') and builder.courses_needing_faculty:\n",
    "    builder.assign_course_faculties()\n",
    "    print(\"\\n✅ Faculty assignment completed!\")\n",
    "else:\n",
    "    print(\"✅ No courses need faculty assignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 5: BOSS Bidding Results Processing and Integration**\n",
    "\n",
    "**What This Does:**\n",
    "- Scans `script_input/overallBossResults/` directory for Excel files containing SMU's bidding system historical data\n",
    "- Parses academic terms from BOSS format (\"2021-22 Term 1\") to standardized database format (\"AY202122T1\")\n",
    "- Creates hierarchical bid windows following SMU's bidding rules with proper round/window progression and incoming student handling\n",
    "- Maps course codes and sections from BOSS data to existing class records using multiple fallback strategies\n",
    "- Extracts comprehensive bidding metrics: vacancy counts, enrollment numbers, median/minimum bids, D.I.C.E scores, and availability data\n",
    "- Generates three new database tables: `new_bid_window.csv`, `new_class_availability.csv`, `new_bid_result.csv`\n",
    "- Creates detailed processing logs with timestamps, failed mapping analysis, and comprehensive statistics reporting\n",
    "- Handles academic year rule differences (pre/post AY2024-25) and provides extensive error tracking for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete Phase 3 pipeline\n",
    "print(\"🚀 Starting Phase 3: BOSS Results Processing\")\n",
    "success = builder.run_phase3_boss_processing()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 Phase 3 completed successfully!\")\n",
    "    builder.close_connection()\n",
    "else:\n",
    "    print(\"\\n❌ Phase 3 failed. Check logs for details.\")\n",
    "\n",
    "# Check failed mappings (if any)\n",
    "failed_path = os.path.join('script_output', 'failed_boss_results_mapping.csv')\n",
    "if os.path.exists(failed_path):\n",
    "    failed_df = pd.read_csv(failed_path)\n",
    "    print(f\"⚠️ {len(failed_df)} failed mappings found:\")\n",
    "    display(failed_df.head(10))\n",
    "    print(f\"\\n📝 Review failed mappings in: {failed_path}\")\n",
    "else:\n",
    "    print(\"✅ No failed mappings - all BOSS results mapped successfully!\")\n",
    "\n",
    "# Inspect generated data\n",
    "print(\"📋 Generated Data Summary:\")\n",
    "\n",
    "# Check bid windows\n",
    "bid_window_path = os.path.join('script_output', 'new_bid_window.csv')\n",
    "if os.path.exists(bid_window_path):\n",
    "    bw_df = pd.read_csv(bid_window_path)\n",
    "    print(f\"\\n🪟 Bid Windows ({len(bw_df)} records):\")\n",
    "\n",
    "# Check class availability\n",
    "availability_path = os.path.join('script_output', 'new_class_availability.csv')\n",
    "if os.path.exists(availability_path):\n",
    "    av_df = pd.read_csv(availability_path)\n",
    "    print(f\"\\n📊 Class Availability ({len(av_df)} records):\")\n",
    "\n",
    "# Check bid results\n",
    "result_path = os.path.join('script_output', 'new_bid_result.csv')\n",
    "if os.path.exists(result_path):\n",
    "    br_df = pd.read_csv(result_path)\n",
    "    print(f\"\\n📈 Bid Results ({len(br_df)} records):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 6: Comprehensive Data Integrity Validation**\n",
    "\n",
    "**What This Does:**\n",
    "- Validates referential integrity across all generated CSV files by checking foreign key relationships between tables\n",
    "- Loads valid IDs from multiple sources: database cache files, new CSV files, professor lookup tables, and verification files\n",
    "- Performs comprehensive validation of course_id references in classes, professor_id fields (both single UUIDs and JSON arrays), and class_id references in timing tables\n",
    "- Checks UUID format validity and ensures all referenced IDs exist in their respective source tables\n",
    "- Generates detailed error reports with specific row numbers, invalid IDs, and raw professor names for debugging\n",
    "- Creates validation summary statistics including total records checked, error counts by type, and data loading metrics\n",
    "- Provides categorized error analysis for professor ID issues including format errors, missing references, and null assignments\n",
    "- Saves validation results to CSV files: `validation_errors.csv`, `validation_warnings.csv`, and `validation_summary.csv` for comprehensive quality assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataIntegrityValidator:\n",
    "    \"\"\"Validates data integrity across generated CSV files and database cache\"\"\"\n",
    "    \n",
    "    def __init__(self, output_base='script_output', cache_dir='db_cache'):\n",
    "        self.output_base = output_base\n",
    "        self.verify_dir = os.path.join(output_base, 'verify')\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        os.makedirs(self.verify_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Data containers\n",
    "        self.valid_course_ids = set()\n",
    "        self.valid_professor_ids = set()\n",
    "        self.valid_class_ids = set()\n",
    "        \n",
    "        # Professor lookup mapping\n",
    "        self.professor_lookup = {}\n",
    "        \n",
    "        # Validation results\n",
    "        self.validation_errors = []\n",
    "        self.validation_warnings = []\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_classes_checked': 0,\n",
    "            'total_timings_checked': 0,\n",
    "            'total_exam_timings_checked': 0,\n",
    "            'course_id_errors': 0,\n",
    "            'professor_id_errors': 0,\n",
    "            'professor_id_format_errors': 0,\n",
    "            'class_id_errors': 0,\n",
    "            'warnings': 0,\n",
    "            'professors_created': 0,\n",
    "            'professors_updated': 0,\n",
    "            'courses_created': 0,\n",
    "            'courses_updated': 0,\n",
    "            'courses_needing_faculty': 0,\n",
    "            'classes_created': 0,\n",
    "            'timings_created': 0,\n",
    "            'exams_created': 0\n",
    "        }\n",
    "        \n",
    "        # Initialize new_acad_terms for compatibility\n",
    "        self.new_acad_terms = []\n",
    "    \n",
    "    def is_valid_uuid(self, uuid_string):\n",
    "        \"\"\"Check if a string is a valid UUID format\"\"\"\n",
    "        if not uuid_string or pd.isna(uuid_string):\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            uuid_pattern = re.compile(\n",
    "                r'^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$',\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            return bool(uuid_pattern.match(str(uuid_string).strip()))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"UUID validation error for {uuid_string}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def safe_read_csv(self, file_path, required_columns=None):\n",
    "        \"\"\"Safely read CSV file with error handling\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"File not found: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            if os.path.getsize(file_path) == 0:\n",
    "                logger.warning(f\"File is empty: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if df.empty:\n",
    "                logger.warning(f\"CSV file is empty: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            if required_columns:\n",
    "                missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                if missing_columns:\n",
    "                    logger.warning(f\"Missing columns in {file_path}: {missing_columns}\")\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading CSV {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def safe_read_pickle(self, file_path):\n",
    "        \"\"\"Safely read pickle file with error handling\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                logger.warning(f\"Pickle file not found: {file_path}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.read_pickle(file_path)\n",
    "            return df if not df.empty else pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading pickle {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_valid_course_ids(self):\n",
    "        \"\"\"Load valid course IDs from new_courses.csv and database cache\"\"\"\n",
    "        logger.info(\"📚 Loading valid course IDs...\")\n",
    "        \n",
    "        # Load from new_courses.csv (verify folder)\n",
    "        new_courses_path = os.path.join(self.verify_dir, 'new_courses.csv')\n",
    "        df = self.safe_read_csv(new_courses_path, ['id'])\n",
    "        if not df.empty:\n",
    "            new_course_ids = set(df['id'].astype(str))\n",
    "            self.valid_course_ids.update(new_course_ids)\n",
    "            logger.info(f\"   ✅ Loaded {len(new_course_ids)} course IDs from new_courses.csv\")\n",
    "        \n",
    "        # Load from database cache\n",
    "        cache_file = os.path.join(self.cache_dir, 'courses_cache.pkl')\n",
    "        courses_df = self.safe_read_pickle(cache_file)\n",
    "        if not courses_df.empty and 'id' in courses_df.columns:\n",
    "            cache_course_ids = set(courses_df['id'].astype(str))\n",
    "            self.valid_course_ids.update(cache_course_ids)\n",
    "            logger.info(f\"   ✅ Loaded {len(cache_course_ids)} course IDs from database cache\")\n",
    "        \n",
    "        logger.info(f\"   📊 Total valid course IDs: {len(self.valid_course_ids)}\")\n",
    "    \n",
    "    def load_valid_professor_ids(self):\n",
    "        \"\"\"Load valid professor IDs from multiple sources including professor_lookup.csv\"\"\"\n",
    "        logger.info(\"👥 Loading valid professor IDs...\")\n",
    "        \n",
    "        # PRIORITY 1: Load from professor_lookup.csv (most authoritative)\n",
    "        lookup_file = 'script_input/professor_lookup.csv'\n",
    "        if os.path.exists(lookup_file):\n",
    "            lookup_df = self.safe_read_csv(lookup_file, ['database_id'])\n",
    "            if not lookup_df.empty and 'database_id' in lookup_df.columns:\n",
    "                lookup_professor_ids = set(lookup_df['database_id'].astype(str))\n",
    "                self.valid_professor_ids.update(lookup_professor_ids)\n",
    "                logger.info(f\"   ✅ Loaded {len(lookup_professor_ids)} professor IDs from professor_lookup.csv\")\n",
    "                \n",
    "                # Also build lookup mapping for analysis\n",
    "                for _, row in lookup_df.iterrows():\n",
    "                    boss_name = row.get('boss_name')\n",
    "                    database_id = str(row.get('database_id'))\n",
    "                    if pd.notna(boss_name) and pd.notna(database_id):\n",
    "                        self.professor_lookup[boss_name] = database_id\n",
    "        \n",
    "        # PRIORITY 2: Load from database cache (professors table)\n",
    "        cache_file = os.path.join(self.cache_dir, 'professors_cache.pkl')\n",
    "        professors_df = self.safe_read_pickle(cache_file)\n",
    "        if not professors_df.empty and 'id' in professors_df.columns:\n",
    "            cache_professor_ids = set(professors_df['id'].astype(str))\n",
    "            self.valid_professor_ids.update(cache_professor_ids)\n",
    "            logger.info(f\"   ✅ Loaded {len(cache_professor_ids)} professor IDs from database cache\")\n",
    "        \n",
    "        # PRIORITY 3: Load from new_professors.csv (verify folder)\n",
    "        new_professors_path = os.path.join(self.verify_dir, 'new_professors.csv')\n",
    "        df = self.safe_read_csv(new_professors_path, ['id'])\n",
    "        if not df.empty:\n",
    "            new_professor_ids = set(df['id'].astype(str))\n",
    "            self.valid_professor_ids.update(new_professor_ids)\n",
    "            logger.info(f\"   ✅ Loaded {len(new_professor_ids)} professor IDs from new_professors.csv\")\n",
    "        \n",
    "        logger.info(f\"   📊 Total valid professor IDs: {len(self.valid_professor_ids)}\")\n",
    "    \n",
    "    def load_valid_class_ids(self):\n",
    "        \"\"\"Load valid class IDs from new_classes.csv\"\"\"\n",
    "        logger.info(\"🏫 Loading valid class IDs...\")\n",
    "        \n",
    "        classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "        df = self.safe_read_csv(classes_path, ['id'])\n",
    "        if not df.empty:\n",
    "            self.valid_class_ids = set(df['id'].astype(str))\n",
    "            logger.info(f\"   ✅ Loaded {len(self.valid_class_ids)} class IDs from new_classes.csv\")\n",
    "        else:\n",
    "            logger.error(f\"   ❌ Could not load class IDs from {classes_path}\")\n",
    "        \n",
    "        logger.info(f\"   📊 Total valid class IDs: {len(self.valid_class_ids)}\")\n",
    "    \n",
    "    def parse_professor_ids(self, professor_id_field):\n",
    "        \"\"\"Safely parse professor ID field which can be single ID or JSON array\"\"\"\n",
    "        if pd.isna(professor_id_field) or str(professor_id_field).strip() == '':\n",
    "            return []\n",
    "        \n",
    "        professor_id_str = str(professor_id_field).strip()\n",
    "        \n",
    "        # Check if it's a JSON array\n",
    "        if professor_id_str.startswith('[') and professor_id_str.endswith(']'):\n",
    "            try:\n",
    "                # Handle both single and double quotes\n",
    "                normalized_json = professor_id_str.replace(\"'\", '\"')\n",
    "                parsed_ids = json.loads(normalized_json)\n",
    "                \n",
    "                if isinstance(parsed_ids, list):\n",
    "                    return [str(pid).strip() for pid in parsed_ids if pd.notna(pid)]\n",
    "                else:\n",
    "                    return []\n",
    "            except (json.JSONDecodeError, TypeError) as e:\n",
    "                logger.warning(f\"JSON parsing error for professor_id: {professor_id_str} - {e}\")\n",
    "                return []\n",
    "        else:\n",
    "            # Single professor ID\n",
    "            return [professor_id_str] if professor_id_str else []\n",
    "    \n",
    "    def validate_classes(self):\n",
    "        \"\"\"Validate course_id and professor_id references in new_classes.csv\"\"\"\n",
    "        logger.info(\"🔍 Validating new_classes.csv...\")\n",
    "        \n",
    "        classes_path = os.path.join(self.output_base, 'new_classes.csv')\n",
    "        df = self.safe_read_csv(classes_path, ['id', 'course_id'])\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(f\"   ❌ Could not validate classes - file not found or empty\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.stats['total_classes_checked'] = len(df)\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    class_id = str(row['id'])\n",
    "                    course_id = str(row['course_id'])\n",
    "                    professor_id_field = row.get('professor_id')\n",
    "                    raw_professor_name = row.get('raw_professor_name', '')\n",
    "                    \n",
    "                    # Validate course_id\n",
    "                    if course_id not in self.valid_course_ids:\n",
    "                        error = {\n",
    "                            'type': 'course_id_missing',\n",
    "                            'file': 'new_classes.csv',\n",
    "                            'row': idx,\n",
    "                            'class_id': class_id,\n",
    "                            'invalid_id': course_id,\n",
    "                            'field': 'course_id'\n",
    "                        }\n",
    "                        self.validation_errors.append(error)\n",
    "                        self.stats['course_id_errors'] += 1\n",
    "                    \n",
    "                    # Validate professor_id\n",
    "                    professor_ids_to_check = self.parse_professor_ids(professor_id_field)\n",
    "                    \n",
    "                    if professor_ids_to_check:\n",
    "                        for prof_id in professor_ids_to_check:\n",
    "                            prof_id_str = str(prof_id).strip()\n",
    "                            \n",
    "                            # Check UUID format\n",
    "                            if not self.is_valid_uuid(prof_id_str):\n",
    "                                error = {\n",
    "                                    'type': 'professor_id_invalid_uuid',\n",
    "                                    'file': 'new_classes.csv',\n",
    "                                    'row': idx,\n",
    "                                    'class_id': class_id,\n",
    "                                    'invalid_id': prof_id_str,\n",
    "                                    'field': 'professor_id',\n",
    "                                    'raw_professor_name': raw_professor_name,\n",
    "                                    'course_id': course_id\n",
    "                                }\n",
    "                                self.validation_errors.append(error)\n",
    "                                self.stats['professor_id_format_errors'] += 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Check if professor exists\n",
    "                            if prof_id_str not in self.valid_professor_ids:\n",
    "                                error = {\n",
    "                                    'type': 'professor_id_not_found',\n",
    "                                    'file': 'new_classes.csv',\n",
    "                                    'row': idx,\n",
    "                                    'class_id': class_id,\n",
    "                                    'invalid_id': prof_id_str,\n",
    "                                    'field': 'professor_id',\n",
    "                                    'raw_professor_name': raw_professor_name,\n",
    "                                    'course_id': course_id\n",
    "                                }\n",
    "                                self.validation_errors.append(error)\n",
    "                                self.stats['professor_id_errors'] += 1\n",
    "                    else:\n",
    "                        # Warning for missing professor\n",
    "                        warning = {\n",
    "                            'type': 'professor_id_null',\n",
    "                            'file': 'new_classes.csv',\n",
    "                            'row': idx,\n",
    "                            'class_id': class_id,\n",
    "                            'message': 'No professors found for class',\n",
    "                            'raw_professor_name': raw_professor_name,\n",
    "                            'course_id': course_id\n",
    "                        }\n",
    "                        self.validation_warnings.append(warning)\n",
    "                        self.stats['warnings'] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing row {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"   ✅ Validated {len(df)} classes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   ❌ Error validating classes: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def validate_class_timings(self):\n",
    "        \"\"\"Validate class_id references in new_class_timing.csv\"\"\"\n",
    "        logger.info(\"⏰ Validating new_class_timing.csv...\")\n",
    "        \n",
    "        timings_path = os.path.join(self.output_base, 'new_class_timing.csv')\n",
    "        df = self.safe_read_csv(timings_path, ['class_id'])\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(f\"   ⚠️ new_class_timing.csv not found or empty\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.stats['total_timings_checked'] = len(df)\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    class_id = str(row['class_id'])\n",
    "                    \n",
    "                    if class_id not in self.valid_class_ids:\n",
    "                        error = {\n",
    "                            'type': 'class_id_missing',\n",
    "                            'file': 'new_class_timing.csv',\n",
    "                            'row': idx,\n",
    "                            'invalid_id': class_id,\n",
    "                            'field': 'class_id'\n",
    "                        }\n",
    "                        self.validation_errors.append(error)\n",
    "                        self.stats['class_id_errors'] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing timing row {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"   ✅ Validated {len(df)} class timings\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   ❌ Error validating class timings: {e}\")\n",
    "    \n",
    "    def validate_exam_timings(self):\n",
    "        \"\"\"Validate class_id references in new_class_exam_timing.csv\"\"\"\n",
    "        logger.info(\"📝 Validating new_class_exam_timing.csv...\")\n",
    "        \n",
    "        exam_timings_path = os.path.join(self.output_base, 'new_class_exam_timing.csv')\n",
    "        df = self.safe_read_csv(exam_timings_path, ['class_id'])\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(f\"   ⚠️ new_class_exam_timing.csv not found or empty\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            self.stats['total_exam_timings_checked'] = len(df)\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    class_id = str(row['class_id'])\n",
    "                    \n",
    "                    if class_id not in self.valid_class_ids:\n",
    "                        error = {\n",
    "                            'type': 'class_id_missing',\n",
    "                            'file': 'new_class_exam_timing.csv',\n",
    "                            'row': idx,\n",
    "                            'invalid_id': class_id,\n",
    "                            'field': 'class_id'\n",
    "                        }\n",
    "                        self.validation_errors.append(error)\n",
    "                        self.stats['class_id_errors'] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing exam timing row {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            logger.info(f\"   ✅ Validated {len(df)} exam timings\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"   ❌ Error validating exam timings: {e}\")\n",
    "    \n",
    "    def analyze_professor_issues(self):\n",
    "        \"\"\"Analyze professor-related issues in detail\"\"\"\n",
    "        logger.info(\"🔬 Analyzing professor issues...\")\n",
    "        \n",
    "        # Group professor errors by type\n",
    "        error_types = {}\n",
    "        for error in self.validation_errors:\n",
    "            if 'professor_id' in error['type']:\n",
    "                error_type = error['type']\n",
    "                if error_type not in error_types:\n",
    "                    error_types[error_type] = []\n",
    "                error_types[error_type].append(error)\n",
    "        \n",
    "        if error_types:\n",
    "            print(f\"\\n📊 PROFESSOR ID ERROR ANALYSIS:\")\n",
    "            for error_type, errors in error_types.items():\n",
    "                print(f\"\\n   Error Type: {error_type}\")\n",
    "                print(f\"   Count: {len(errors)}\")\n",
    "                \n",
    "                # Show unique invalid IDs for this error type\n",
    "                unique_invalid_ids = set()\n",
    "                for error in errors:\n",
    "                    unique_invalid_ids.add(error['invalid_id'])\n",
    "                \n",
    "                print(f\"   Unique Invalid IDs: {len(unique_invalid_ids)}\")\n",
    "                \n",
    "                # Show sample errors\n",
    "                print(f\"   Sample errors:\")\n",
    "                for i, error in enumerate(errors[:3]):\n",
    "                    print(f\"     {i+1}. Class {error['class_id']} - Raw name: {error.get('raw_professor_name', 'N/A')}\")\n",
    "                    print(f\"        Invalid ID: {error['invalid_id']}\")\n",
    "                \n",
    "                if len(errors) > 3:\n",
    "                    print(f\"     ... and {len(errors) - 3} more\")\n",
    "    \n",
    "    def save_validation_report(self):\n",
    "        \"\"\"Save validation errors and warnings to CSV files\"\"\"\n",
    "        logger.info(\"💾 Saving validation report...\")\n",
    "        \n",
    "        try:\n",
    "            # Save validation errors\n",
    "            if self.validation_errors:\n",
    "                errors_df = pd.DataFrame(self.validation_errors)\n",
    "                errors_path = os.path.join(self.output_base, 'validation_errors.csv')\n",
    "                errors_df.to_csv(errors_path, index=False)\n",
    "                logger.info(f\"   ❌ Saved {len(self.validation_errors)} validation errors to validation_errors.csv\")\n",
    "            \n",
    "            # Save validation warnings\n",
    "            if self.validation_warnings:\n",
    "                warnings_df = pd.DataFrame(self.validation_warnings)\n",
    "                warnings_path = os.path.join(self.output_base, 'validation_warnings.csv')\n",
    "                warnings_df.to_csv(warnings_path, index=False)\n",
    "                logger.info(f\"   ⚠️ Saved {len(self.validation_warnings)} validation warnings to validation_warnings.csv\")\n",
    "            \n",
    "            # Save summary report\n",
    "            summary = {\n",
    "                'validation_timestamp': datetime.now().isoformat(),\n",
    "                'total_classes_checked': self.stats['total_classes_checked'],\n",
    "                'total_timings_checked': self.stats['total_timings_checked'],\n",
    "                'total_exam_timings_checked': self.stats['total_exam_timings_checked'],\n",
    "                'total_errors': len(self.validation_errors),\n",
    "                'total_warnings': len(self.validation_warnings),\n",
    "                'course_id_errors': self.stats['course_id_errors'],\n",
    "                'professor_id_errors': self.stats['professor_id_errors'],\n",
    "                'professor_id_format_errors': self.stats['professor_id_format_errors'],\n",
    "                'class_id_errors': self.stats['class_id_errors'],\n",
    "                'valid_course_ids_loaded': len(self.valid_course_ids),\n",
    "                'valid_professor_ids_loaded': len(self.valid_professor_ids),\n",
    "                'valid_class_ids_loaded': len(self.valid_class_ids)\n",
    "            }\n",
    "            \n",
    "            summary_df = pd.DataFrame([summary])\n",
    "            summary_path = os.path.join(self.output_base, 'validation_summary.csv')\n",
    "            summary_df.to_csv(summary_path, index=False)\n",
    "            logger.info(f\"   📊 Saved validation summary to validation_summary.csv\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving validation report: {e}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"📊 PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"✅ Professors created: {self.stats['professors_created']}\")\n",
    "        print(f\"✅ Professors updated: {self.stats.get('professors_updated', 0)}\")\n",
    "        print(f\"✅ Courses created: {self.stats['courses_created']}\")\n",
    "        print(f\"✅ Courses updated: {self.stats['courses_updated']}\")\n",
    "        print(f\"⚠️  Courses needing faculty: {self.stats['courses_needing_faculty']}\")\n",
    "        print(f\"✅ Classes created: {self.stats['classes_created']}\")\n",
    "        print(f\"✅ Class timings created: {self.stats['timings_created']}\")\n",
    "        print(f\"✅ Exam timings created: {self.stats['exams_created']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\n📁 OUTPUT FILES:\")\n",
    "        print(f\"   Verify folder: {self.verify_dir}/\")\n",
    "        print(f\"   - new_professors.csv ({self.stats['professors_created']} records)\")\n",
    "        print(f\"   - new_courses.csv ({self.stats['courses_created']} records)\")\n",
    "        print(f\"   Output folder: {self.output_base}/\")\n",
    "        print(f\"   - update_courses.csv ({self.stats['courses_updated']} records)\")\n",
    "        print(f\"   - update_professor.csv ({self.stats.get('professors_updated', 0)} records)\")\n",
    "        print(f\"   - new_acad_term.csv ({len(self.new_acad_terms)} records)\")\n",
    "        print(f\"   - new_classes.csv ({self.stats['classes_created']} records)\")\n",
    "        print(f\"   - new_class_timing.csv ({self.stats['timings_created']} records)\")\n",
    "        print(f\"   - new_class_exam_timing.csv ({self.stats['exams_created']} records)\")\n",
    "        print(f\"   - professor_lookup.csv (updated)\")\n",
    "        print(f\"   - courses_needing_faculty.csv ({self.stats['courses_needing_faculty']} records)\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    def run_validation(self):\n",
    "        \"\"\"Run complete data integrity validation\"\"\"\n",
    "        try:\n",
    "            logger.info(\"🚀 Starting Data Integrity Validation\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Load valid IDs from all sources\n",
    "            self.load_valid_course_ids()\n",
    "            self.load_valid_professor_ids()\n",
    "            self.load_valid_class_ids()\n",
    "            \n",
    "            # Step 2: Validate references\n",
    "            self.validate_classes()\n",
    "            self.validate_class_timings()\n",
    "            self.validate_exam_timings()\n",
    "            \n",
    "            # Step 3: Analyze issues\n",
    "            self.analyze_professor_issues()\n",
    "            \n",
    "            # Step 4: Save and display results\n",
    "            self.save_validation_report()\n",
    "            self.print_summary()\n",
    "            \n",
    "            logger.info(\"\\n✅ Data integrity validation completed!\")\n",
    "            \n",
    "            # Return validation status\n",
    "            return len(self.validation_errors) == 0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Validation failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = DataIntegrityValidator()\n",
    "success = validator.run_validation()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 All data integrity checks passed!\")\n",
    "    exit(0)\n",
    "else:\n",
    "    print(\"\\n💥 Data integrity issues found - check error reports!\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_coverage_standalone(output_dir='script_output'):\n",
    "    \"\"\"Standalone function to analyze class coverage and generate detailed report\"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔍 CLASS COVERAGE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load new_classes.csv\n",
    "    classes_path = os.path.join(output_dir, 'new_classes.csv')\n",
    "    if not os.path.exists(classes_path):\n",
    "        print(f\"❌ File not found: {classes_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        classes_df = pd.read_csv(classes_path)\n",
    "        total_classes = len(classes_df)\n",
    "        all_class_ids = set(classes_df['id'].unique())\n",
    "        \n",
    "        # Files to check\n",
    "        files_to_check = {\n",
    "            'new_class_availability.csv': 'class_availability',\n",
    "            'new_class_exam_timing.csv': 'class_exam_timing', \n",
    "            'new_class_timing.csv': 'class_timing',\n",
    "            'new_bid_result.csv': 'bid_result'\n",
    "        }\n",
    "        \n",
    "        coverage_results = {}\n",
    "        orphan_class_ids = defaultdict(list)\n",
    "        \n",
    "        # Load each file and analyze\n",
    "        for filename, table_name in files_to_check.items():\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            if not os.path.exists(file_path):\n",
    "                coverage_results[table_name] = {\n",
    "                    'found_ids': set(),\n",
    "                    'orphan_ids': set()\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'class_id' in df.columns:\n",
    "                found_class_ids = set(df['class_id'].unique())\n",
    "                \n",
    "                # Check for orphan class_ids\n",
    "                orphan_ids = found_class_ids - all_class_ids\n",
    "                if orphan_ids:\n",
    "                    orphan_class_ids[table_name] = orphan_ids\n",
    "                \n",
    "                # Store valid class_ids\n",
    "                valid_class_ids = found_class_ids & all_class_ids\n",
    "                \n",
    "                coverage_results[table_name] = {\n",
    "                    'found_ids': valid_class_ids,\n",
    "                    'orphan_ids': orphan_ids\n",
    "                }\n",
    "        \n",
    "        # Calculate statistics\n",
    "        print(\"\\n📊 STATISTICS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # 1. Total class rows\n",
    "        print(f\"1. Total class rows created: {total_classes}\")\n",
    "        \n",
    "        # 2. Unique course/section/term combinations\n",
    "        unique_combinations = classes_df.groupby(['course_id', 'section', 'acad_term_id']).size()\n",
    "        num_unique_combinations = len(unique_combinations)\n",
    "        print(f\"2. Unique course/section/term combinations: {num_unique_combinations}\")\n",
    "        \n",
    "        # 3. Classes from multiple professors\n",
    "        multi_professor_combinations = unique_combinations[unique_combinations > 1]\n",
    "        total_multi_professor_classes = multi_professor_combinations.sum()\n",
    "        print(f\"3. Class records from multiple professors: {total_multi_professor_classes}\")\n",
    "        \n",
    "        # 4. Unique classes duplicated due to multiple professors\n",
    "        num_duplicated_unique_classes = len(multi_professor_combinations)\n",
    "        print(f\"4. Unique classes duplicated due to multiple professors: {num_duplicated_unique_classes}\")\n",
    "        \n",
    "        # 5. Classes with no BOSS results\n",
    "        no_boss_classes = []\n",
    "        for class_id in all_class_ids:\n",
    "            in_availability = class_id in coverage_results.get('class_availability', {}).get('found_ids', set())\n",
    "            in_bid_result = class_id in coverage_results.get('bid_result', {}).get('found_ids', set())\n",
    "            if not in_availability and not in_bid_result:\n",
    "                no_boss_classes.append(class_id)\n",
    "        print(f\"5. Classes with no BOSS results (no availability/bid_result): {len(no_boss_classes)}\")\n",
    "        \n",
    "        # 6. Classes with no exams but have class timings\n",
    "        has_timing = coverage_results.get('class_timing', {}).get('found_ids', set())\n",
    "        has_exam = coverage_results.get('class_exam_timing', {}).get('found_ids', set())\n",
    "        no_exam_with_timing = has_timing - has_exam\n",
    "        print(f\"6. Classes with class timings but no exams: {len(no_exam_with_timing)}\")\n",
    "        \n",
    "        # 7. Classes with exams but no class timings\n",
    "        exam_no_timing = has_exam - has_timing\n",
    "        print(f\"7. Classes with exams but no class timings: {len(exam_no_timing)}\")\n",
    "        \n",
    "        # 8. Classes with both exams and class timings\n",
    "        both_exam_timing = has_exam & has_timing\n",
    "        print(f\"8. Classes with both exams and class timings: {len(both_exam_timing)}\")\n",
    "        \n",
    "        # 9. Orphan class_ids\n",
    "        total_orphans = sum(len(ids) for ids in orphan_class_ids.values())\n",
    "        print(f\"9. Orphan class_ids (in tables but not in new_classes): {total_orphans}\")\n",
    "        if total_orphans > 0:\n",
    "            for table, ids in orphan_class_ids.items():\n",
    "                print(f\"   - {table}: {len(ids)} orphan IDs\")\n",
    "        \n",
    "        # 10. BOSS records not mapped to scraped data\n",
    "        print(\"\\n📊 Checking BOSS records not mapped to scraped data...\")\n",
    "        \n",
    "        # Load BOSS data to check unmapped records\n",
    "        boss_unmapped_count = 0\n",
    "        try:\n",
    "            import glob\n",
    "            boss_files = glob.glob(os.path.join('script_input', 'overallBossResults', '*.xlsx'))\n",
    "            if boss_files:\n",
    "                # Get unique course/section/term from scraped data\n",
    "                scraped_combinations = set()\n",
    "                for _, row in classes_df.iterrows():\n",
    "                    # Need to get course code from course_id\n",
    "                    course_id = row['course_id']\n",
    "                    section = str(row['section'])\n",
    "                    acad_term_id = row['acad_term_id']\n",
    "                    scraped_combinations.add((course_id, section, acad_term_id))\n",
    "                \n",
    "                # Count unique BOSS combinations not in scraped\n",
    "                boss_unique_combinations = set()\n",
    "                for file_path in boss_files[:1]:  # Sample first file for performance\n",
    "                    boss_df = pd.read_excel(file_path)\n",
    "                    if all(col in boss_df.columns for col in ['Course Code', 'Section', 'Term']):\n",
    "                        for _, row in boss_df.iterrows():\n",
    "                            if pd.notna(row['Course Code']) and pd.notna(row['Section']) and pd.notna(row['Term']):\n",
    "                                course_code = row['Course Code']\n",
    "                                section = str(row['Section'])\n",
    "                                term = row['Term']\n",
    "                                # Convert term to acad_term_id format\n",
    "                                if isinstance(term, str) and '-' in term:\n",
    "                                    import re\n",
    "                                    match = re.match(r'(\\d{4})-(\\d{2})\\s+Term\\s+(\\w+)', term)\n",
    "                                    if match:\n",
    "                                        acad_term_id = f\"AY{match.group(1)}{match.group(2)}T{match.group(3)}\"\n",
    "                                        boss_unique_combinations.add((course_code, section, acad_term_id))\n",
    "                \n",
    "                # Note: This is approximate as we're comparing course_code vs course_id\n",
    "                print(f\"10. Unique BOSS combinations sampled: {len(boss_unique_combinations)}\")\n",
    "                print(\"    (Note: Exact count requires course_code to course_id mapping)\")\n",
    "        except Exception as e:\n",
    "            print(f\"10. Could not analyze BOSS unmapped records: {e}\")\n",
    "        \n",
    "        # Generate detailed report\n",
    "        print(\"\\n💾 Generating detailed report...\")\n",
    "        \n",
    "        missing_report = []\n",
    "        for _, class_row in classes_df.iterrows():\n",
    "            class_id = class_row['id']\n",
    "            \n",
    "            row = {\n",
    "                'class_id': class_id,\n",
    "                'course_id': class_row.get('course_id'),\n",
    "                'section': class_row.get('section'),\n",
    "                'professor_id': class_row.get('professor_id'),\n",
    "                'acad_term_id': class_row.get('acad_term_id'),\n",
    "                'boss_id': class_row.get('boss_id'),\n",
    "                'raw_professor_name': class_row.get('raw_professor_name', ''),\n",
    "                'warn_inaccuracy': class_row.get('warn_inaccuracy', False)\n",
    "            }\n",
    "            \n",
    "            # Check each table\n",
    "            for table_name, result in coverage_results.items():\n",
    "                row[f'in_{table_name}'] = 'Yes' if class_id in result['found_ids'] else 'No'\n",
    "            \n",
    "            # Add summary flags\n",
    "            row['has_boss_data'] = 'Yes' if (\n",
    "                row.get('in_class_availability') == 'Yes' or \n",
    "                row.get('in_bid_result') == 'Yes'\n",
    "            ) else 'No'\n",
    "            \n",
    "            row['has_timing_data'] = 'Yes' if row.get('in_class_timing') == 'Yes' else 'No'\n",
    "            row['has_exam_data'] = 'Yes' if row.get('in_class_exam_timing') == 'Yes' else 'No'\n",
    "            \n",
    "            missing_report.append(row)\n",
    "        \n",
    "        # Save report\n",
    "        report_df = pd.DataFrame(missing_report)\n",
    "        report_path = os.path.join(output_dir, 'class_coverage_detailed_report.csv')\n",
    "        report_df.to_csv(report_path, index=False)\n",
    "        print(f\"✅ Detailed report saved to: {report_path}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"📊 SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total classes analyzed: {total_classes}\")\n",
    "        print(f\"Report generated: class_coverage_detailed_report.csv\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Usage:\n",
    "check_class_coverage_standalone('script_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates_in_tables(output_dir='script_output'):\n",
    "    \"\"\"Check for duplicate records in class_availability, class_exam_timing, class_timing, and bid_result\"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔍 DUPLICATE RECORDS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Tables to check with their unique key combinations\n",
    "    tables_to_check = {\n",
    "        'new_class_availability.csv': {\n",
    "            'name': 'class_availability',\n",
    "            'key_columns': ['class_id', 'bid_window_id'],\n",
    "            'description': 'class_id + bid_window_id'\n",
    "        },\n",
    "        'new_bid_result.csv': {\n",
    "            'name': 'bid_result',\n",
    "            'key_columns': ['bid_window_id', 'class_id'],\n",
    "            'description': 'bid_window_id + class_id'\n",
    "        },\n",
    "        'new_class_timing.csv': {\n",
    "            'name': 'class_timing',\n",
    "            'key_columns': None,  # No composite key, check for exact duplicates\n",
    "            'description': 'all columns (no defined unique key)'\n",
    "        },\n",
    "        'new_class_exam_timing.csv': {\n",
    "            'name': 'class_exam_timing',\n",
    "            'key_columns': None,  # No composite key, check for exact duplicates\n",
    "            'description': 'all columns (no defined unique key)'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_duplicates = {}\n",
    "    \n",
    "    for filename, config in tables_to_check.items():\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        table_name = config['name']\n",
    "        \n",
    "        print(f\"\\n📊 Checking {table_name}...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️  {filename} not found - skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            total_rows = len(df)\n",
    "            print(f\"Total rows: {total_rows}\")\n",
    "            \n",
    "            if total_rows == 0:\n",
    "                print(\"❌ No data in file\")\n",
    "                continue\n",
    "            \n",
    "            duplicates_found = []\n",
    "            \n",
    "            if config['key_columns']:\n",
    "                # Check for duplicates based on composite key\n",
    "                key_cols = config['key_columns']\n",
    "                \n",
    "                # Verify columns exist\n",
    "                missing_cols = [col for col in key_cols if col not in df.columns]\n",
    "                if missing_cols:\n",
    "                    print(f\"❌ Missing required columns: {missing_cols}\")\n",
    "                    continue\n",
    "                \n",
    "                # Find duplicates\n",
    "                duplicated_mask = df.duplicated(subset=key_cols, keep=False)\n",
    "                duplicates = df[duplicated_mask].copy()\n",
    "                \n",
    "                if len(duplicates) > 0:\n",
    "                    # Sort by key columns for better visualization\n",
    "                    duplicates = duplicates.sort_values(by=key_cols)\n",
    "                    \n",
    "                    # Group duplicates\n",
    "                    duplicate_groups = duplicates.groupby(key_cols).size().reset_index(name='count')\n",
    "                    num_duplicate_groups = len(duplicate_groups)\n",
    "                    \n",
    "                    print(f\"❌ Found {len(duplicates)} duplicate rows\")\n",
    "                    print(f\"   Duplicate groups: {num_duplicate_groups}\")\n",
    "                    print(f\"   Unique constraint violated: {config['description']}\")\n",
    "                    \n",
    "                    # Show sample duplicates\n",
    "                    print(\"\\n   Sample duplicate groups:\")\n",
    "                    for idx, group in duplicate_groups.head(5).iterrows():\n",
    "                        key_values = {col: group[col] for col in key_cols}\n",
    "                        print(f\"   • {key_values} appears {group['count']} times\")\n",
    "                    \n",
    "                    if num_duplicate_groups > 5:\n",
    "                        print(f\"   ... and {num_duplicate_groups - 5} more duplicate groups\")\n",
    "                    \n",
    "                    duplicates_found = duplicates\n",
    "                else:\n",
    "                    print(f\"✅ No duplicates found on composite key: {config['description']}\")\n",
    "            \n",
    "            else:\n",
    "                # Check for exact row duplicates (all columns)\n",
    "                duplicated_mask = df.duplicated(keep=False)\n",
    "                duplicates = df[duplicated_mask].copy()\n",
    "                \n",
    "                if len(duplicates) > 0:\n",
    "                    print(f\"❌ Found {len(duplicates)} duplicate rows (exact matches)\")\n",
    "                    \n",
    "                    # Show sample duplicates\n",
    "                    print(\"\\n   Sample duplicate rows:\")\n",
    "                    shown = 0\n",
    "                    for idx, row in duplicates.head(10).iterrows():\n",
    "                        if shown < 5:\n",
    "                            print(f\"   • Row {idx}: class_id={row.get('class_id', 'N/A')}\")\n",
    "                            if 'start_time' in row:\n",
    "                                print(f\"     Timing: {row.get('day_of_week', '')} {row.get('start_time', '')}-{row.get('end_time', '')}\")\n",
    "                            if 'date' in row:\n",
    "                                print(f\"     Exam: {row.get('date', '')} {row.get('start_time', '')}-{row.get('end_time', '')}\")\n",
    "                            shown += 1\n",
    "                    \n",
    "                    duplicates_found = duplicates\n",
    "                else:\n",
    "                    print(f\"✅ No exact duplicate rows found\")\n",
    "            \n",
    "            # Additional checks for timing tables\n",
    "            if table_name == 'class_timing' and len(df) > 0:\n",
    "                # Check for same class with overlapping timings\n",
    "                print(\"\\n🔍 Checking for overlapping class timings...\")\n",
    "                if all(col in df.columns for col in ['class_id', 'day_of_week', 'start_time', 'end_time']):\n",
    "                    overlap_issues = []\n",
    "                    for class_id in df['class_id'].unique():\n",
    "                        class_timings = df[df['class_id'] == class_id]\n",
    "                        if len(class_timings) > 1:\n",
    "                            # Check each pair of timings\n",
    "                            timings_list = class_timings.to_dict('records')\n",
    "                            for i in range(len(timings_list)):\n",
    "                                for j in range(i + 1, len(timings_list)):\n",
    "                                    t1 = timings_list[i]\n",
    "                                    t2 = timings_list[j]\n",
    "                                    if (t1['day_of_week'] == t2['day_of_week'] and \n",
    "                                        pd.notna(t1['day_of_week']) and pd.notna(t2['day_of_week'])):\n",
    "                                        # Same day - check for time overlap\n",
    "                                        overlap_issues.append({\n",
    "                                            'class_id': class_id,\n",
    "                                            'day': t1['day_of_week'],\n",
    "                                            'timing1': f\"{t1['start_time']}-{t1['end_time']}\",\n",
    "                                            'timing2': f\"{t2['start_time']}-{t2['end_time']}\"\n",
    "                                        })\n",
    "                    \n",
    "                    if overlap_issues:\n",
    "                        print(f\"⚠️  Found {len(overlap_issues)} potential timing conflicts\")\n",
    "                        for issue in overlap_issues[:3]:\n",
    "                            print(f\"   • Class {issue['class_id']} on {issue['day']}: {issue['timing1']} and {issue['timing2']}\")\n",
    "                    else:\n",
    "                        print(\"✅ No overlapping timings found\")\n",
    "            \n",
    "            # Store results\n",
    "            if len(duplicates_found) > 0:\n",
    "                all_duplicates[table_name] = {\n",
    "                    'dataframe': duplicates_found,\n",
    "                    'total_duplicates': len(duplicates_found),\n",
    "                    'key_columns': config['key_columns'],\n",
    "                    'description': config['description']\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Export duplicate reports\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"💾 EXPORTING DUPLICATE REPORTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if all_duplicates:\n",
    "        for table_name, dup_info in all_duplicates.items():\n",
    "            output_filename = f\"duplicates_{table_name}.csv\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            # Add duplicate group numbers\n",
    "            df = dup_info['dataframe']\n",
    "            if dup_info['key_columns']:\n",
    "                # Add group number for easier identification\n",
    "                df['duplicate_group'] = df.groupby(dup_info['key_columns']).ngroup() + 1\n",
    "                df = df.sort_values(by=['duplicate_group'] + dup_info['key_columns'])\n",
    "            \n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"✅ Exported {dup_info['total_duplicates']} duplicate records to: {output_filename}\")\n",
    "    else:\n",
    "        print(\"✅ No duplicates found in any table!\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_duplicates = sum(info['total_duplicates'] for info in all_duplicates.values())\n",
    "    print(f\"Total duplicate records found: {total_duplicates}\")\n",
    "    \n",
    "    if all_duplicates:\n",
    "        print(\"\\nDuplicates by table:\")\n",
    "        for table_name, info in all_duplicates.items():\n",
    "            print(f\"  • {table_name}: {info['total_duplicates']} duplicates on {info['description']}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Usage:\n",
    "check_duplicates_in_tables('script_output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bidly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
