{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SMU Course Scraping Using Selenium**\n",
    "\n",
    "<div style=\"background-color:#FFD700; padding:15px; border-radius:5px; border: 2px solid #FF4500;\">\n",
    "    \n",
    "  <h1 style=\"color:#8B0000;\">⚠️🚨 SCRAPE THIS DATA AT YOUR OWN RISK 🚨⚠️</h1>\n",
    "  \n",
    "  <p><strong>📌 If you need the data, please contact me directly.</strong> Only available for **existing students**.</p>\n",
    "\n",
    "  <h3>🔗 📩 How to Get the Data?</h3>\n",
    "  <p>📨 <strong>Reach out to me for access</strong> instead of scraping manually.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:#FFF8DC; padding:12px; border-radius:5px; border: 1px solid #DAA520;\">\n",
    "    \n",
    "  <h2 style=\"color:#8B8000;\">✨ Looking for the Latest Model? Consider V4! ✨</h2>\n",
    "  <p>👉 <a href=\"V4_example_prediction.ipynb\"><strong>Check out V4 Here</strong></a></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Objective**\n",
    "This script is designed to scrape SMU course details from the BOSS system using Selenium. The process involves:\n",
    "1. Logging into the system manually to bypass authentication.\n",
    "2. Iteratively scraping class details for specified academic years and terms.\n",
    "3. Writing the scraped data to structured CSV files.\n",
    "\n",
    "### **Script Structure**\n",
    "1. **Setup**: Import libraries and initialize Selenium WebDriver.\n",
    "2. **Login**: Wait for manual login and authentication.\n",
    "3. **Scraping Logic**:\n",
    "    - `scrape_class_details`: Scrapes course details for a specific class number, academic year, and term.\n",
    "    - `main`: Manages the scraping process for multiple academic years and terms.\n",
    "4. **Execution**: Log in and start scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PGGSSENCMODE'] = 'disable'\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import logging\n",
    "import psycopg2\n",
    "from typing import List, Optional, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "from dotenv import load_dotenv\n",
    "import webbrowser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Scrape all BOSS data**\n",
    "\n",
    "### **BOSS Class Scraper Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `BOSSClassScraper` class automates the extraction of class timing data from SMU's BOSS (Banner Online Self-Service) system with intelligent resume capabilities. It systematically scrapes class details across multiple academic terms and saves them as HTML files for further processing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Automated Web Scraping**: Navigates through BOSS class detail pages using Selenium WebDriver\n",
    "- **Resume Capability**: Automatically detects existing scraped files and continues from the last scraped class number, preventing duplicate work\n",
    "- **Flexible Term Range**: Dynamically derives academic years from input parameters (e.g., '2025-26_T1' to '2028-29_T2') rather than hardcoded lists\n",
    "- **Smart Pagination**: Scans class numbers from 1000-5000 with intelligent termination after 300 consecutive empty records\n",
    "- **Progress Tracking**: Monitors existing files and resumes scraping from the highest class number found for each term\n",
    "- **Data Organization**: Saves HTML files in structured directories by academic term (`script_input/classTimingsFull/`)\n",
    "- **Incremental CSV Updates**: Appends only new valid files to the existing CSV index, avoiding duplicates\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, standard libraries (`os`, `time`, `csv`, `re`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- Network access to SMU's BOSS system\n",
    "\n",
    "**User Requirements:**\n",
    "- **Manual Authentication**: User must manually log in and complete Microsoft Authenticator process when prompted\n",
    "- **SMU Credentials**: Valid access to BOSS system\n",
    "- **Directory Structure**: Code creates `script_input/classTimingsFull/` for HTML files and `script_input/scraped_filepaths.csv` for the file index\n",
    "\n",
    "**Resume Functionality:**\n",
    "- **Interruption Handling**: If scraping stops halfway due to network issues or manual interruption, the next run automatically resumes from the exact point it left off\n",
    "- **Duplicate Prevention**: Existing files are automatically detected and skipped, preventing re-downloading of already scraped data\n",
    "- **Natural Termination**: Uses 300 consecutive empty records threshold to handle BOSS system inconsistencies without hardcoded limits\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "scraper = BOSSClassScraper()\n",
    "# Will automatically resume from previous progress if files exist\n",
    "success = scraper.run_full_scraping_process('2025-26_T1', '2025-26_T3B')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOSSClassScraper:\n",
    "    \"\"\"\n",
    "    A class to scrape class details from BOSS (SMU's online class registration system)\n",
    "    and save them as HTML files for further processing with resume capability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BOSS Class Scraper with configuration parameters.\n",
    "        \"\"\"\n",
    "        self.term_code_map = {'T1': '10', 'T2': '20', 'T3A': '31', 'T3B': '32'}\n",
    "        self.all_terms = ['T1', 'T2', 'T3A', 'T3B']\n",
    "        self.driver = None\n",
    "        self.min_class_number = 1000\n",
    "        self.max_class_number = 5000\n",
    "        self.consecutive_empty_threshold = 300\n",
    "        \n",
    "    def _derive_academic_years(self, start_ay_term, end_ay_term):\n",
    "        \"\"\"\n",
    "        Derive academic years from start and end terms.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting term (e.g., '2025-26_T1')\n",
    "            end_ay_term: Ending term (e.g., '2028-29_T2')\n",
    "            \n",
    "        Returns:\n",
    "            List of academic years in format ['2025-26', '2026-27', ...]\n",
    "        \"\"\"\n",
    "        start_year = int(start_ay_term[:4])\n",
    "        end_year = int(end_ay_term[:4])\n",
    "        \n",
    "        academic_years = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            next_year = (year + 1) % 100\n",
    "            ay = f\"{year}-{next_year:02d}\"\n",
    "            academic_years.append(ay)\n",
    "            \n",
    "        return academic_years\n",
    "    \n",
    "    def _get_existing_files_progress(self, base_dir):\n",
    "        \"\"\"\n",
    "        Check existing files and determine the last scraped position for each term.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory where HTML files are stored\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with term as key and last scraped class number as value\n",
    "        \"\"\"\n",
    "        progress = {}\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            return progress\n",
    "            \n",
    "        for term_folder in os.listdir(base_dir):\n",
    "            term_path = os.path.join(base_dir, term_folder)\n",
    "            if os.path.isdir(term_path):\n",
    "                max_class_num = 0\n",
    "                \n",
    "                for filename in os.listdir(term_path):\n",
    "                    if filename.endswith('.html'):\n",
    "                        # Extract class number from filename\n",
    "                        # Format: SelectedAcadTerm=XXYY&SelectedClassNumber=ZZZZ.html\n",
    "                        match = re.search(r'SelectedClassNumber=(\\d+)\\.html', filename)\n",
    "                        if match:\n",
    "                            class_num = int(match.group(1))\n",
    "                            max_class_num = max(max_class_num, class_num)\n",
    "                \n",
    "                if max_class_num > 0:\n",
    "                    progress[term_folder] = max_class_num\n",
    "                    print(f\"Found existing progress for {term_folder}: last class number {max_class_num}\")\n",
    "        \n",
    "        return progress\n",
    "    \n",
    "    def wait_for_manual_login(self):\n",
    "        \"\"\"\n",
    "        Wait for manual login and Microsoft Authenticator process completion.\n",
    "        \"\"\"\n",
    "        print(\"Please log in manually and complete the Microsoft Authenticator process.\")\n",
    "        print(\"Waiting for BOSS dashboard to load...\")\n",
    "        \n",
    "        wait = WebDriverWait(self.driver, 120)\n",
    "        \n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, \"Label_UserName\")))\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(text(),'Sign out')]\")))\n",
    "            \n",
    "            username = self.driver.find_element(By.ID, \"Label_UserName\").text\n",
    "            print(f\"Login successful! Logged in as {username}\")\n",
    "            \n",
    "        except TimeoutException:\n",
    "            print(\"Login failed or timed out. Could not detect login elements.\")\n",
    "            raise Exception(\"Login failed\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    def scrape_and_save_html(self, start_ay_term='2025-26_T1', end_ay_term='2025-26_T1', base_dir='script_input/classTimingsFull'):\n",
    "        \"\"\"\n",
    "        Scrapes class details from BOSS and saves them as HTML files with resume capability.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting academic year and term (e.g., '2025-26_T1')\n",
    "            end_ay_term: Ending academic year and term (e.g., '2025-26_T1')\n",
    "            base_dir: Base directory to save the HTML files\n",
    "        \"\"\"\n",
    "        # Check existing progress\n",
    "        existing_progress = self._get_existing_files_progress(base_dir)\n",
    "        \n",
    "        # Derive academic years from input terms\n",
    "        all_academic_years = self._derive_academic_years(start_ay_term, end_ay_term)\n",
    "        \n",
    "        # Generate all possible AY_TERM combinations\n",
    "        all_ay_terms = []\n",
    "        for ay in all_academic_years:\n",
    "            for term in self.all_terms:\n",
    "                all_ay_terms.append(f\"{ay}_{term}\")\n",
    "        \n",
    "        # Find the indices of the start and end terms\n",
    "        try:\n",
    "            start_idx = all_ay_terms.index(start_ay_term)\n",
    "            end_idx = all_ay_terms.index(end_ay_term)\n",
    "        except ValueError:\n",
    "            print(\"Invalid start or end term provided. Using full range.\")\n",
    "            start_idx = 0\n",
    "            end_idx = len(all_ay_terms) - 1\n",
    "        \n",
    "        # Select the range to scrape\n",
    "        ay_terms_to_scrape = all_ay_terms[start_idx:end_idx+1]\n",
    "        \n",
    "        # Create base directory if needed\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each AY_TERM\n",
    "        for ay_term in ay_terms_to_scrape:\n",
    "            print(f\"Processing {ay_term}...\")\n",
    "            \n",
    "            # Parse AY_TERM for URL\n",
    "            ay, term = ay_term.split('_')\n",
    "            ay_short = ay[2:4]  # last two digits of first year\n",
    "            term_code = self.term_code_map.get(term, '10')\n",
    "            \n",
    "            # Create folder for AY_TERM\n",
    "            folder_path = os.path.join(base_dir, ay_term)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            # Determine starting class number based on existing progress\n",
    "            start_class_num = self.min_class_number\n",
    "            if ay_term in existing_progress:\n",
    "                start_class_num = existing_progress[ay_term] + 1\n",
    "                print(f\"Resuming {ay_term} from class number {start_class_num}\")\n",
    "            \n",
    "            consecutive_empty = 0\n",
    "            \n",
    "            # Scrape each class number in range\n",
    "            for class_num in range(start_class_num, self.max_class_number + 1):\n",
    "                # Check if file already exists\n",
    "                filename = f\"SelectedAcadTerm={ay_short}{term_code}&SelectedClassNumber={class_num:04}.html\"\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"File already exists: {filepath}, skipping...\")\n",
    "                    consecutive_empty = 0  # Reset counter since we have data\n",
    "                    continue\n",
    "                \n",
    "                url = f\"https://boss.intranet.smu.edu.sg/ClassDetails.aspx?SelectedClassNumber={class_num:04}&SelectedAcadTerm={ay_short}{term_code}&SelectedAcadCareer=UGRD\"\n",
    "                \n",
    "                try:\n",
    "                    self.driver.get(url)\n",
    "                    \n",
    "                    wait = WebDriverWait(self.driver, 15)\n",
    "                    try:\n",
    "                        element = wait.until(EC.any_of(\n",
    "                            EC.presence_of_element_located((By.ID, \"lblClassInfoHeader\")),\n",
    "                            EC.presence_of_element_located((By.ID, \"lblErrorDetails\"))\n",
    "                        ))\n",
    "                        \n",
    "                        error_elements = self.driver.find_elements(By.ID, \"lblErrorDetails\")\n",
    "                        has_data = True\n",
    "                        \n",
    "                        for error in error_elements:\n",
    "                            if \"No record found\" in error.text:\n",
    "                                has_data = False\n",
    "                                break\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"Wait error: {e}\")\n",
    "                        has_data = False\n",
    "                    \n",
    "                    if not has_data:\n",
    "                        consecutive_empty += 1\n",
    "                        print(f\"No record found for {ay_term}, class {class_num:04}. Consecutive empty: {consecutive_empty}\")\n",
    "                        \n",
    "                        if consecutive_empty >= self.consecutive_empty_threshold:\n",
    "                            print(f\"{self.consecutive_empty_threshold} consecutive empty records reached for {ay_term}, moving on.\")\n",
    "                            break\n",
    "                        \n",
    "                        time.sleep(2)\n",
    "                        continue\n",
    "                    \n",
    "                    # Reset consecutive empty counter if data found\n",
    "                    consecutive_empty = 0\n",
    "                    \n",
    "                    # Save HTML file\n",
    "                    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        f.write(self.driver.page_source)\n",
    "                    \n",
    "                    print(f\"Saved {filepath}\")\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {url}: {str(e)}\")\n",
    "                    time.sleep(5)\n",
    "        \n",
    "        print(\"Scraping completed.\")\n",
    "    \n",
    "    def generate_scraped_filepaths_csv(self, base_dir='script_input/classTimingsFull', output_csv='script_input/scraped_filepaths.csv'):\n",
    "        \"\"\"\n",
    "        Generates a CSV file with paths to all valid HTML files (those without \"No record found\").\n",
    "        Updates existing CSV by appending new valid files.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory where HTML files are stored\n",
    "            output_csv: Path to the output CSV file\n",
    "            \n",
    "        Returns:\n",
    "            Path to the generated CSV file or None if error\n",
    "        \"\"\"\n",
    "        # Read existing filepaths if CSV exists\n",
    "        existing_filepaths = set()\n",
    "        if os.path.exists(output_csv):\n",
    "            try:\n",
    "                with open(output_csv, 'r', encoding='utf-8') as csvfile:\n",
    "                    reader = csv.reader(csvfile)\n",
    "                    next(reader)  # Skip header\n",
    "                    for row in reader:\n",
    "                        if row:\n",
    "                            existing_filepaths.add(row[0])\n",
    "                print(f\"Found {len(existing_filepaths)} existing filepaths in CSV\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading existing CSV: {str(e)}\")\n",
    "        \n",
    "        filepaths = []\n",
    "        \n",
    "        if not os.path.exists(base_dir):\n",
    "            print(f\"Directory '{base_dir}' does not exist.\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "        \n",
    "        # Walk through directory structure\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.html'):\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    \n",
    "                    # Skip if already in existing filepaths\n",
    "                    if filepath in existing_filepaths:\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            if 'No record found' not in content:\n",
    "                                filepaths.append(filepath)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {filepath}: {str(e)}\")\n",
    "        \n",
    "        # Append new filepaths to CSV\n",
    "        mode = 'a' if existing_filepaths else 'w'\n",
    "        with open(output_csv, mode, newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not existing_filepaths:  # Write header only if new file\n",
    "                writer.writerow(['Filepath'])\n",
    "            for path in filepaths:\n",
    "                writer.writerow([path])\n",
    "        \n",
    "        total_valid_files = len(existing_filepaths) + len(filepaths)\n",
    "        print(f\"CSV updated with {len(filepaths)} new valid file paths. Total: {total_valid_files} files at {output_csv}\")\n",
    "        return output_csv\n",
    "    \n",
    "    def run_full_scraping_process(self, start_ay_term='2025-26_T1', end_ay_term='2025-26_T1'):\n",
    "        \"\"\"\n",
    "        Run the complete scraping process from login to CSV generation with resume capability.\n",
    "        \n",
    "        Args:\n",
    "            start_ay_term: Starting academic year and term\n",
    "            end_ay_term: Ending academic year and term\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set up WebDriver\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            \n",
    "            # Navigate to login page and wait for manual login\n",
    "            self.driver.get(\"https://boss.intranet.smu.edu.sg/\")\n",
    "            self.wait_for_manual_login()\n",
    "            \n",
    "            # Run the main scraping function\n",
    "            self.scrape_and_save_html(start_ay_term, end_ay_term)\n",
    "            \n",
    "            # Generate CSV with valid file paths\n",
    "            self.generate_scraped_filepaths_csv()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping process: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                self.driver = None\n",
    "            print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 17:13:26,769 - INFO - ====== WebDriver manager ======\n",
      "2025-06-06 17:13:30,049 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-06 17:13:30,290 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-06 17:13:30,530 - INFO - Driver [C:\\Users\\tanzh\\.wdm\\drivers\\chromedriver\\win64\\137.0.7151.68\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in manually and complete the Microsoft Authenticator process.\n",
      "Waiting for BOSS dashboard to load...\n",
      "Login successful! Logged in as Welcome, TAN ZHONG YAN\n",
      "Found existing progress for 2021-22_T1: last class number 2889\n",
      "Found existing progress for 2021-22_T2: last class number 2957\n",
      "Found existing progress for 2021-22_T3A: last class number 1038\n",
      "Found existing progress for 2021-22_T3B: last class number 1033\n",
      "Found existing progress for 2022-23_T1: last class number 2954\n",
      "Found existing progress for 2022-23_T2: last class number 2920\n",
      "Found existing progress for 2022-23_T3A: last class number 1031\n",
      "Found existing progress for 2022-23_T3B: last class number 1027\n",
      "Found existing progress for 2023-24_T1: last class number 2982\n",
      "Found existing progress for 2023-24_T2: last class number 2964\n",
      "Found existing progress for 2023-24_T3A: last class number 1028\n",
      "Found existing progress for 2023-24_T3B: last class number 1033\n",
      "Found existing progress for 2024-25_T1: last class number 2945\n",
      "Found existing progress for 2024-25_T2: last class number 2786\n",
      "Found existing progress for 2024-25_T3A: last class number 1022\n",
      "Found existing progress for 2024-25_T3B: last class number 1028\n",
      "Found existing progress for 2025-26_T1: last class number 1004\n",
      "Processing 2024-25_T3A...\n",
      "Resuming 2024-25_T3A from class number 1023\n",
      "No record found for 2024-25_T3A, class 1023. Consecutive empty: 1\n",
      "No record found for 2024-25_T3A, class 1024. Consecutive empty: 2\n",
      "No record found for 2024-25_T3A, class 1025. Consecutive empty: 3\n",
      "No record found for 2024-25_T3A, class 1026. Consecutive empty: 4\n",
      "No record found for 2024-25_T3A, class 1027. Consecutive empty: 5\n",
      "No record found for 2024-25_T3A, class 1028. Consecutive empty: 6\n",
      "No record found for 2024-25_T3A, class 1029. Consecutive empty: 7\n",
      "No record found for 2024-25_T3A, class 1030. Consecutive empty: 8\n",
      "No record found for 2024-25_T3A, class 1031. Consecutive empty: 9\n",
      "No record found for 2024-25_T3A, class 1032. Consecutive empty: 10\n",
      "No record found for 2024-25_T3A, class 1033. Consecutive empty: 11\n",
      "No record found for 2024-25_T3A, class 1034. Consecutive empty: 12\n",
      "No record found for 2024-25_T3A, class 1035. Consecutive empty: 13\n",
      "No record found for 2024-25_T3A, class 1036. Consecutive empty: 14\n",
      "No record found for 2024-25_T3A, class 1037. Consecutive empty: 15\n",
      "No record found for 2024-25_T3A, class 1038. Consecutive empty: 16\n",
      "No record found for 2024-25_T3A, class 1039. Consecutive empty: 17\n",
      "No record found for 2024-25_T3A, class 1040. Consecutive empty: 18\n",
      "No record found for 2024-25_T3A, class 1041. Consecutive empty: 19\n",
      "No record found for 2024-25_T3A, class 1042. Consecutive empty: 20\n",
      "No record found for 2024-25_T3A, class 1043. Consecutive empty: 21\n",
      "No record found for 2024-25_T3A, class 1044. Consecutive empty: 22\n",
      "No record found for 2024-25_T3A, class 1045. Consecutive empty: 23\n",
      "No record found for 2024-25_T3A, class 1046. Consecutive empty: 24\n",
      "No record found for 2024-25_T3A, class 1047. Consecutive empty: 25\n",
      "No record found for 2024-25_T3A, class 1048. Consecutive empty: 26\n",
      "No record found for 2024-25_T3A, class 1049. Consecutive empty: 27\n",
      "No record found for 2024-25_T3A, class 1050. Consecutive empty: 28\n",
      "No record found for 2024-25_T3A, class 1051. Consecutive empty: 29\n",
      "No record found for 2024-25_T3A, class 1052. Consecutive empty: 30\n",
      "No record found for 2024-25_T3A, class 1053. Consecutive empty: 31\n",
      "No record found for 2024-25_T3A, class 1054. Consecutive empty: 32\n",
      "No record found for 2024-25_T3A, class 1055. Consecutive empty: 33\n",
      "No record found for 2024-25_T3A, class 1056. Consecutive empty: 34\n",
      "No record found for 2024-25_T3A, class 1057. Consecutive empty: 35\n",
      "No record found for 2024-25_T3A, class 1058. Consecutive empty: 36\n",
      "No record found for 2024-25_T3A, class 1059. Consecutive empty: 37\n",
      "No record found for 2024-25_T3A, class 1060. Consecutive empty: 38\n",
      "No record found for 2024-25_T3A, class 1061. Consecutive empty: 39\n",
      "Process completed!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the scraper\u001b[39;00m\n\u001b[32m      2\u001b[39m scraper = BOSSClassScraper()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m success = \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_full_scraping_process\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2024-25_T3A\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2025-26_T1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 307\u001b[39m, in \u001b[36mBOSSClassScraper.run_full_scraping_process\u001b[39m\u001b[34m(self, start_ay_term, end_ay_term)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28mself\u001b[39m.wait_for_manual_login()\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Run the main scraping function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscrape_and_save_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_ay_term\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_ay_term\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Generate CSV with valid file paths\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28mself\u001b[39m.generate_scraped_filepaths_csv()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mBOSSClassScraper.scrape_and_save_html\u001b[39m\u001b[34m(self, start_ay_term, end_ay_term, base_dir)\u001b[39m\n\u001b[32m    194\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.consecutive_empty_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m consecutive empty records reached for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00may_term\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, moving on.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    195\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Reset consecutive empty counter if data found\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the scraper\n",
    "scraper = BOSSClassScraper()\n",
    "success = scraper.run_full_scraping_process('2024-25_T3A', '2025-26_T1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Extract Data from HTML Files**\n",
    "\n",
    "### **HTML Data Extractor Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `HTMLDataExtractor` class processes previously scraped HTML files from SMU's BOSS system and extracts structured data into Excel format. It systematically parses course information, class timings, academic terms, and exam schedules from local HTML files without requiring network access or authentication.\n",
    "\n",
    "**Key Features:**\n",
    "- **Local File Processing**: Uses Selenium WebDriver to parse local HTML files without network connectivity requirements\n",
    "- **Comprehensive Data Extraction**: Extracts course details, academic terms, class timings, exam schedules, grading information, and professor names\n",
    "- **Test-First Approach**: Includes `run_test()` function to validate extraction logic on a small sample before processing all files\n",
    "- **Structured Output**: Organizes extracted data into two Excel sheets - standalone records (one per HTML file) and multiple records (class/exam timings)\n",
    "- **Error Tracking**: Captures and logs parsing errors in a separate sheet for debugging and quality assurance\n",
    "- **Flexible Data Parsing**: Handles multiple academic term naming conventions and date formats used across different years\n",
    "- **Record Linking**: Uses record keys to maintain relationships between standalone and multiple data records\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `selenium`, `webdriver-manager`, `pandas`, `openpyxl`, standard libraries (`os`, `re`, `datetime`, `pathlib`)\n",
    "- Chrome browser and ChromeDriver (auto-managed)\n",
    "- No network access required (processes local files only)\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Scraped HTML Files**: Previously downloaded HTML files from BOSS system stored locally\n",
    "- **File Path CSV**: `script_input/scraped_filepaths.csv` containing paths to valid HTML files\n",
    "- **Directory Structure**: HTML files organized in the expected folder structure (typically `script_input/classTimingsFull/`)\n",
    "\n",
    "**Output Structure:**\n",
    "- **Excel File**: `script_input/raw_data.xlsx` (or custom path) with multiple sheets:\n",
    "  - `standalone`: One record per HTML file with course and class information\n",
    "  - `multiple`: Multiple records for class timings and exam schedules\n",
    "  - `errors`: Parsing errors and problematic files for debugging\n",
    "\n",
    "**Data Extraction Capabilities:**\n",
    "- **Course Information**: Course codes, names, descriptions, credit units, course areas, enrollment requirements\n",
    "- **Academic Terms**: Term IDs, academic years, start/end dates, BOSS IDs\n",
    "- **Class Details**: Sections, grading basis, course outline URLs, professor names\n",
    "- **Timing Data**: Class schedules, exam dates, venues, day-of-week information\n",
    "- **Cross-References**: Maintains linking keys between related records across sheets\n",
    "\n",
    "**Usage in Jupyter Notebook:**\n",
    "```python\n",
    "# Initialize extractor\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Test with sample files first (recommended)\n",
    "test_success = extractor.run_test(test_count=10)\n",
    "\n",
    "if test_success:\n",
    "    # Run full extraction\n",
    "    extractor.run()\n",
    "    \n",
    "# Or run directly without testing\n",
    "extractor.run(\n",
    "    scraped_filepaths_csv='script_input/scraped_filepaths.csv',\n",
    "    output_path='script_input/raw_data.xlsx'\n",
    ")\n",
    "```\n",
    "\n",
    "The class provides a crucial intermediate step between raw HTML scraping and database insertion, creating clean, structured data that can be further processed for database integration or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLDataExtractor:\n",
    "    \"\"\"\n",
    "    Extract raw data from scraped HTML files and save to Excel format using Selenium\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        self.driver = None\n",
    "        \n",
    "    def setup_selenium_driver(self):\n",
    "        \"\"\"Set up Selenium WebDriver for local file access\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--headless')  # Run in headless mode for efficiency\n",
    "            options.add_argument('--disable-gpu')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            print(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Selenium WebDriver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def safe_find_element_text(self, by, value):\n",
    "        \"\"\"Safely find element and return its text\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            return element.text.strip() if element else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def safe_find_element_attribute(self, by, value, attribute):\n",
    "        \"\"\"Safely find element and return its attribute\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            return element.get_attribute(attribute) if element else None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def convert_date_to_timestamp(self, date_str):\n",
    "        \"\"\"Convert DD-Mmm-YYYY to database timestamp format\"\"\"\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%d-%b-%Y')\n",
    "            return date_obj.strftime('%Y-%m-%d 00:00:00.000 +0800')\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def parse_acad_term(self, term_text):\n",
    "        \"\"\"Parse academic term text and return structured data\"\"\"\n",
    "        try:\n",
    "            # Pattern like \"2021-22 Term 2\" or \"2021-22 Session 1\"\n",
    "            pattern = r'(\\d{4})-(\\d{2})\\s+(.*)'\n",
    "            match = re.search(pattern, term_text)\n",
    "            \n",
    "            if not match:\n",
    "                return None, None, None, None\n",
    "            \n",
    "            start_year = int(match.group(1))\n",
    "            end_year_short = int(match.group(2))\n",
    "            term_desc = match.group(3).lower()\n",
    "            \n",
    "            # Convert 2-digit year to 4-digit\n",
    "            if end_year_short < 50:\n",
    "                end_year = 2000 + end_year_short\n",
    "            else:\n",
    "                end_year = 1900 + end_year_short\n",
    "            \n",
    "            # Determine term code\n",
    "            if 'term 1' in term_desc or 'session 1' in term_desc or 'august term' in term_desc:\n",
    "                term_code = 'T1'\n",
    "            elif 'term 2' in term_desc or 'session 2' in term_desc or 'january term' in term_desc:\n",
    "                term_code = 'T2'\n",
    "            elif 'term 3a' in term_desc:\n",
    "                term_code = 'T3A'\n",
    "            elif 'term 3b' in term_desc:\n",
    "                term_code = 'T3B'\n",
    "            else:\n",
    "                return start_year, end_year, None, None\n",
    "            \n",
    "            acad_term_id = f\"AY{start_year}{end_year_short:02d}{term_code}\"\n",
    "            \n",
    "            return start_year, end_year, term_code, acad_term_id\n",
    "        except Exception as e:\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def parse_course_and_section(self, header_text):\n",
    "        \"\"\"Parse course code and section from header text\"\"\"\n",
    "        try:\n",
    "            # Clean the text first\n",
    "            clean_text = re.sub(r'<[^>]+>', '', header_text)\n",
    "            clean_text = re.sub(r'\\s+', ' ', clean_text.strip())\n",
    "            \n",
    "            # Try multiple regex patterns\n",
    "            patterns = [\n",
    "                r'([A-Z0-9_-]+)\\s+-\\s+(.+)',  # Standard format\n",
    "                r'([A-Z]+)\\s+(\\d+[A-Z0-9_]*)\\s+-\\s+(.+)',  # Split format\n",
    "                r'([A-Z0-9_\\s-]+?)\\s*[-–—]\\s*(.+)'  # Fallback\n",
    "            ]\n",
    "            \n",
    "            for i, pattern in enumerate(patterns):\n",
    "                match = re.match(pattern, clean_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    if i == 1:  # Split format\n",
    "                        course_code = match.group(1) + match.group(2)\n",
    "                        section = match.group(3)\n",
    "                    else:\n",
    "                        course_code = match.group(1)\n",
    "                        section = match.group(2)\n",
    "                    \n",
    "                    course_code = re.sub(r'\\s+', '', course_code.upper())\n",
    "                    section = section.strip()\n",
    "                    \n",
    "                    return course_code, section\n",
    "            \n",
    "            return None, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def parse_date_range(self, date_text):\n",
    "        \"\"\"Parse date range text and return start and end timestamps\"\"\"\n",
    "        try:\n",
    "            # Example: \"10-Jan-2022 to 01-May-2022\"\n",
    "            pattern = r'(\\d{1,2}-\\w{3}-\\d{4})\\s+to\\s+(\\d{1,2}-\\w{3}-\\d{4})'\n",
    "            match = re.search(pattern, date_text)\n",
    "            \n",
    "            if not match:\n",
    "                return None, None\n",
    "            \n",
    "            start_date = self.convert_date_to_timestamp(match.group(1))\n",
    "            end_date = self.convert_date_to_timestamp(match.group(2))\n",
    "            \n",
    "            return start_date, end_date\n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_course_areas_list(self):\n",
    "        \"\"\"Extract course areas as comma-separated string using Selenium\"\"\"\n",
    "        try:\n",
    "            course_areas_element = self.driver.find_element(By.ID, 'lblCourseAreas')\n",
    "            course_areas_html = course_areas_element.get_attribute('innerHTML')\n",
    "            \n",
    "            # Extract list items\n",
    "            areas_list = re.findall(r'<li>(.*?)</li>', course_areas_html)\n",
    "            if areas_list:\n",
    "                return ', '.join(areas_list)\n",
    "            else:\n",
    "                # Fallback to text content\n",
    "                return course_areas_element.text.strip()\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def extract_course_outline_url(self):\n",
    "        \"\"\"Extract course outline URL from HTML using Selenium\"\"\"\n",
    "        try:\n",
    "            onclick_attr = self.safe_find_element_attribute(By.ID, 'imgCourseOutline', 'onclick')\n",
    "            if onclick_attr:\n",
    "                url_match = re.search(r\"window\\.open\\('([^']+)'\", onclick_attr)\n",
    "                if url_match:\n",
    "                    return url_match.group(1)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def extract_boss_ids_from_filepath(self, filepath):\n",
    "        \"\"\"Extract BOSS IDs from filepath\"\"\"\n",
    "        try:\n",
    "            filename = os.path.basename(filepath)\n",
    "            acad_term_match = re.search(r'SelectedAcadTerm=(\\d+)', filename)\n",
    "            class_match = re.search(r'SelectedClassNumber=(\\d+)', filename)\n",
    "            \n",
    "            acad_term_boss_id = int(acad_term_match.group(1)) if acad_term_match else None\n",
    "            class_boss_id = int(class_match.group(1)) if class_match else None\n",
    "            \n",
    "            return acad_term_boss_id, class_boss_id\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    \n",
    "    def extract_meeting_information(self, record_key):\n",
    "        \"\"\"Extract class timing and exam timing information using Selenium\"\"\"\n",
    "        try:\n",
    "            meeting_table = self.driver.find_element(By.ID, 'RadGrid_MeetingInfo_ctl00')\n",
    "            tbody = meeting_table.find_element(By.TAG_NAME, 'tbody')\n",
    "            rows = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if len(cells) < 7:\n",
    "                    continue\n",
    "                \n",
    "                meeting_type = cells[0].text.strip()\n",
    "                start_date_text = cells[1].text.strip()\n",
    "                end_date_text = cells[2].text.strip()\n",
    "                day_of_week = cells[3].text.strip()\n",
    "                start_time = cells[4].text.strip()\n",
    "                end_time = cells[5].text.strip()\n",
    "                venue = cells[6].text.strip() if len(cells) > 6 else \"\"\n",
    "                professor_name = cells[7].text.strip() if len(cells) > 7 else \"\"\n",
    "                \n",
    "                # Assume CLASS if meeting_type is empty\n",
    "                if not meeting_type:\n",
    "                    meeting_type = 'CLASS'\n",
    "                \n",
    "                if meeting_type == 'CLASS':\n",
    "                    # Convert dates to timestamp format\n",
    "                    start_date = self.convert_date_to_timestamp(start_date_text)\n",
    "                    end_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    timing_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'CLASS',\n",
    "                        'start_date': start_date,\n",
    "                        'end_date': end_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(timing_record)\n",
    "                \n",
    "                elif meeting_type == 'EXAM':\n",
    "                    # For exams, use the second date (end_date_text) as the exam date\n",
    "                    exam_date = self.convert_date_to_timestamp(end_date_text)\n",
    "                    \n",
    "                    exam_record = {\n",
    "                        'record_key': record_key,\n",
    "                        'type': 'EXAM',\n",
    "                        'date': exam_date,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue,\n",
    "                        'professor_name': professor_name\n",
    "                    }\n",
    "                    self.multiple_data.append(exam_record)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'record_key': record_key,\n",
    "                'error': f'Error extracting meeting information: {str(e)}',\n",
    "                'type': 'parse_error'\n",
    "            })\n",
    "    \n",
    "    def process_html_file(self, filepath):\n",
    "        \"\"\"Process a single HTML file and extract all data using Selenium\"\"\"\n",
    "        try:\n",
    "            # Load HTML file\n",
    "            html_file = Path(filepath).resolve()\n",
    "            file_url = html_file.as_uri()\n",
    "            self.driver.get(file_url)\n",
    "            \n",
    "            # Create unique record key\n",
    "            record_key = f\"{os.path.basename(filepath)}\"\n",
    "            \n",
    "            # Extract basic information\n",
    "            class_header_text = self.safe_find_element_text(By.ID, 'lblClassInfoHeader')\n",
    "            if not class_header_text:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': 'Missing class header',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            course_code, section = self.parse_course_and_section(class_header_text)\n",
    "            \n",
    "            # Extract academic term\n",
    "            term_text = self.safe_find_element_text(By.ID, 'lblClassInfoSubHeader')\n",
    "            acad_year_start, acad_year_end, term, acad_term_id = self.parse_acad_term(term_text) if term_text else (None, None, None, None)\n",
    "            \n",
    "            # Extract course information\n",
    "            course_name = self.safe_find_element_text(By.ID, 'lblClassSection')\n",
    "            course_description = self.safe_find_element_text(By.ID, 'lblCourseDescription')\n",
    "            credit_units_text = self.safe_find_element_text(By.ID, 'lblUnits')\n",
    "            course_areas = self.extract_course_areas_list()\n",
    "            enrolment_requirements = self.safe_find_element_text(By.ID, 'lblEnrolmentRequirements')\n",
    "            \n",
    "            # Process credit units\n",
    "            try:\n",
    "                credit_units = float(credit_units_text) if credit_units_text else None\n",
    "            except (ValueError, TypeError):\n",
    "                credit_units = None\n",
    "            \n",
    "            # Extract grading basis\n",
    "            grading_text = self.safe_find_element_text(By.ID, 'lblGradingBasis')\n",
    "            grading_basis = None\n",
    "            if grading_text:\n",
    "                if grading_text.lower() == 'graded':\n",
    "                    grading_basis = 'Graded'\n",
    "                elif grading_text.lower() in ['pass/fail', 'pass fail']:\n",
    "                    grading_basis = 'Pass/Fail'\n",
    "                else:\n",
    "                    grading_basis = 'NA'\n",
    "            \n",
    "            # Extract course outline URL\n",
    "            course_outline_url = self.extract_course_outline_url()\n",
    "            \n",
    "            # Extract dates\n",
    "            period_text = self.safe_find_element_text(By.ID, 'lblDates')\n",
    "            start_dt, end_dt = self.parse_date_range(period_text) if period_text else (None, None)\n",
    "            \n",
    "            # Extract BOSS IDs\n",
    "            acad_term_boss_id, class_boss_id = self.extract_boss_ids_from_filepath(filepath)\n",
    "            \n",
    "            # Create standalone record\n",
    "            standalone_record = {\n",
    "                'record_key': record_key,\n",
    "                'filepath': filepath,\n",
    "                'course_code': course_code,\n",
    "                'section': section,\n",
    "                'course_name': course_name,\n",
    "                'course_description': course_description,\n",
    "                'credit_units': credit_units,\n",
    "                'course_area': course_areas,\n",
    "                'enrolment_requirements': enrolment_requirements,\n",
    "                'acad_term_id': acad_term_id,\n",
    "                'acad_year_start': acad_year_start,\n",
    "                'acad_year_end': acad_year_end,\n",
    "                'term': term,\n",
    "                'start_dt': start_dt,\n",
    "                'end_dt': end_dt,\n",
    "                'grading_basis': grading_basis,\n",
    "                'course_outline_url': course_outline_url,\n",
    "                'acad_term_boss_id': acad_term_boss_id,\n",
    "                'class_boss_id': class_boss_id,\n",
    "                'term_text': term_text,\n",
    "                'period_text': period_text\n",
    "            }\n",
    "            \n",
    "            self.standalone_data.append(standalone_record)\n",
    "            \n",
    "            # Extract meeting information\n",
    "            self.extract_meeting_information(record_key)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'filepath': filepath,\n",
    "                'error': str(e),\n",
    "                'type': 'processing_error'\n",
    "            })\n",
    "            return False\n",
    "    \n",
    "    def run_test(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv', test_count=10):\n",
    "        \"\"\"Randomly test the extraction on a subset of files\"\"\"\n",
    "        try:\n",
    "            print(f\"Starting test run with {test_count} randomly selected files...\")\n",
    "\n",
    "            # Reset data containers\n",
    "            self.standalone_data = []\n",
    "            self.multiple_data = []\n",
    "            self.errors = []\n",
    "\n",
    "            # Set up Selenium driver\n",
    "            self.setup_selenium_driver()\n",
    "\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "\n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            all_filepaths = df[filepath_column].dropna().tolist()\n",
    "\n",
    "            if len(all_filepaths) == 0:\n",
    "                raise ValueError(\"No valid filepaths found in CSV\")\n",
    "\n",
    "            # Randomly sample filepaths\n",
    "            sample_size = min(test_count, len(all_filepaths))\n",
    "            sampled_filepaths = random.sample(all_filepaths, sample_size)\n",
    "\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "\n",
    "            for i, filepath in enumerate(sampled_filepaths, start=1):\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"Processing test file {i}/{sample_size}: {os.path.basename(filepath)}\")\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "\n",
    "            print(f\"\\nTest run complete: {successful_files}/{processed_files} files successful\")\n",
    "            print(f\"Standalone records extracted: {len(self.standalone_data)}\")\n",
    "            print(f\"Multiple records extracted: {len(self.multiple_data)}\")\n",
    "            if self.errors:\n",
    "                print(f\"Errors encountered: {len(self.errors)}\")\n",
    "                for error in self.errors[:3]:  # Show only the first 3 errors\n",
    "                    print(f\"  - {error['type']}: {error['error']}\")\n",
    "\n",
    "            # Save test results\n",
    "            test_output_path = 'script_input/test_raw_data.xlsx'\n",
    "            self.save_to_excel(test_output_path)\n",
    "\n",
    "            return successful_files > 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in test run: {e}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Test selenium driver closed\")\n",
    "    \n",
    "    def process_all_files(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv'):\n",
    "        \"\"\"Process all files listed in the scraped filepaths CSV\"\"\"\n",
    "        try:\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "            \n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            \n",
    "            total_files = len(df)\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "            \n",
    "            print(f\"Starting to process {total_files} files\")\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                filepath = row[filepath_column]\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                    \n",
    "                    if processed_files % 100 == 0:\n",
    "                        print(f\"Processed {processed_files}/{total_files} files\")\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "            \n",
    "            print(f\"Processing complete: {successful_files}/{processed_files} files successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_all_files: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_to_excel(self, output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Save extracted data to Excel file with two sheets\"\"\"\n",
    "        try:\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "            \n",
    "            # Create DataFrames\n",
    "            standalone_df = pd.DataFrame(self.standalone_data)\n",
    "            multiple_df = pd.DataFrame(self.multiple_data)\n",
    "            \n",
    "            # Save to Excel with multiple sheets\n",
    "            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "                standalone_df.to_excel(writer, sheet_name='standalone', index=False)\n",
    "                multiple_df.to_excel(writer, sheet_name='multiple', index=False)\n",
    "                \n",
    "                # Also save errors if any\n",
    "                if self.errors:\n",
    "                    errors_df = pd.DataFrame(self.errors)\n",
    "                    errors_df.to_excel(writer, sheet_name='errors', index=False)\n",
    "            \n",
    "            print(f\"Data saved to {output_path}\")\n",
    "            print(f\"Standalone records: {len(self.standalone_data)}\")\n",
    "            print(f\"Multiple records: {len(self.multiple_data)}\")\n",
    "            if self.errors:\n",
    "                print(f\"Errors: {len(self.errors)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Excel: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run(self, scraped_filepaths_csv='script_input/scraped_filepaths.csv', output_path='script_input/raw_data.xlsx'):\n",
    "        \"\"\"Run the complete extraction process\"\"\"\n",
    "        print(\"Starting HTML data extraction...\")\n",
    "        \n",
    "        # Reset data containers\n",
    "        self.standalone_data = []\n",
    "        self.multiple_data = []\n",
    "        self.errors = []\n",
    "        \n",
    "        # Set up Selenium driver\n",
    "        self.setup_selenium_driver()\n",
    "        \n",
    "        try:\n",
    "            # Process all files\n",
    "            self.process_all_files(scraped_filepaths_csv)\n",
    "            \n",
    "            # Save to Excel\n",
    "            self.save_to_excel(output_path)\n",
    "            \n",
    "            print(\"HTML data extraction completed!\")\n",
    "            \n",
    "        finally:\n",
    "            if self.driver:\n",
    "                self.driver.quit()\n",
    "                print(\"Selenium driver closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 18:08:34,790 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HTML data extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 18:08:38,277 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-06 18:08:38,298 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-06-06 18:08:38,323 - INFO - Driver [C:\\Users\\tanzh\\.wdm\\drivers\\chromedriver\\win64\\137.0.7151.68\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium WebDriver initialized successfully\n",
      "Starting to process 12976 files\n",
      "Processed 100/12976 files\n",
      "Processed 200/12976 files\n",
      "Processed 300/12976 files\n",
      "Processed 400/12976 files\n",
      "Processed 500/12976 files\n",
      "Processed 600/12976 files\n",
      "Processed 700/12976 files\n",
      "Processed 800/12976 files\n",
      "Processed 900/12976 files\n",
      "Processed 1000/12976 files\n",
      "Processed 1100/12976 files\n",
      "Processed 1200/12976 files\n",
      "Processed 1300/12976 files\n",
      "Processed 1400/12976 files\n",
      "Processed 1500/12976 files\n",
      "Processed 1600/12976 files\n",
      "Processed 1700/12976 files\n",
      "Processed 1800/12976 files\n",
      "Processed 1900/12976 files\n",
      "Processed 2000/12976 files\n",
      "Processed 2100/12976 files\n",
      "Processed 2200/12976 files\n",
      "Processed 2300/12976 files\n",
      "Processed 2400/12976 files\n",
      "Processed 2500/12976 files\n",
      "Processed 2600/12976 files\n",
      "Processed 2700/12976 files\n",
      "Processed 2800/12976 files\n",
      "Processed 2900/12976 files\n",
      "Processed 3000/12976 files\n",
      "Processed 3100/12976 files\n",
      "Processed 3200/12976 files\n",
      "Processed 3300/12976 files\n",
      "Processed 3400/12976 files\n",
      "Processed 3500/12976 files\n",
      "Processed 3600/12976 files\n",
      "Processed 3700/12976 files\n",
      "Processed 3800/12976 files\n",
      "Processed 3900/12976 files\n",
      "Processed 4000/12976 files\n",
      "Processed 4100/12976 files\n",
      "Processed 4200/12976 files\n",
      "Processed 4300/12976 files\n",
      "Processed 4400/12976 files\n",
      "Processed 4500/12976 files\n",
      "Processed 4600/12976 files\n",
      "Processed 4700/12976 files\n",
      "Processed 4800/12976 files\n",
      "Processed 4900/12976 files\n",
      "Processed 5000/12976 files\n",
      "Processed 5100/12976 files\n",
      "Processed 5200/12976 files\n",
      "Processed 5300/12976 files\n",
      "Processed 5400/12976 files\n",
      "Processed 5500/12976 files\n",
      "Processed 5600/12976 files\n",
      "Processed 5700/12976 files\n",
      "Processed 5800/12976 files\n",
      "Processed 5900/12976 files\n",
      "Processed 6000/12976 files\n",
      "Processed 6100/12976 files\n",
      "Processed 6200/12976 files\n",
      "Processed 6300/12976 files\n",
      "Processed 6400/12976 files\n",
      "Processed 6500/12976 files\n",
      "Processed 6600/12976 files\n",
      "Processed 6700/12976 files\n",
      "Processed 6800/12976 files\n",
      "Processed 6900/12976 files\n",
      "Processed 7000/12976 files\n",
      "Processed 7100/12976 files\n",
      "Processed 7200/12976 files\n",
      "Processed 7300/12976 files\n",
      "Processed 7400/12976 files\n",
      "Processed 7500/12976 files\n",
      "Processed 7600/12976 files\n",
      "Processed 7700/12976 files\n",
      "Processed 7800/12976 files\n",
      "Processed 7900/12976 files\n",
      "Processed 8000/12976 files\n",
      "Processed 8100/12976 files\n",
      "Processed 8200/12976 files\n",
      "Processed 8300/12976 files\n",
      "Processed 8400/12976 files\n",
      "Processed 8500/12976 files\n",
      "Processed 8600/12976 files\n",
      "Processed 8700/12976 files\n",
      "Processed 8800/12976 files\n",
      "Processed 8900/12976 files\n",
      "Processed 9000/12976 files\n",
      "Processed 9100/12976 files\n",
      "Processed 9200/12976 files\n",
      "Processed 9300/12976 files\n",
      "Processed 9400/12976 files\n",
      "Processed 9500/12976 files\n",
      "Processed 9600/12976 files\n",
      "Processed 9700/12976 files\n",
      "Processed 9800/12976 files\n",
      "Processed 9900/12976 files\n",
      "Processed 10000/12976 files\n",
      "Processed 10100/12976 files\n",
      "Processed 10200/12976 files\n",
      "Processed 10300/12976 files\n",
      "Processed 10400/12976 files\n",
      "Processed 10500/12976 files\n",
      "Processed 10600/12976 files\n",
      "Processed 10700/12976 files\n",
      "Processed 10800/12976 files\n",
      "Processed 10900/12976 files\n",
      "Processed 11000/12976 files\n",
      "Processed 11100/12976 files\n",
      "Processed 11200/12976 files\n",
      "Processed 11300/12976 files\n",
      "Processed 11400/12976 files\n",
      "Processed 11500/12976 files\n",
      "Processed 11600/12976 files\n",
      "Processed 11700/12976 files\n",
      "Processed 11800/12976 files\n",
      "Processed 11900/12976 files\n",
      "Processed 12000/12976 files\n",
      "Processed 12100/12976 files\n",
      "Processed 12200/12976 files\n",
      "Processed 12300/12976 files\n",
      "Processed 12400/12976 files\n",
      "Processed 12500/12976 files\n",
      "Processed 12600/12976 files\n",
      "Processed 12700/12976 files\n",
      "Processed 12800/12976 files\n",
      "Processed 12900/12976 files\n",
      "Processing complete: 12976/12976 files successful\n",
      "Data saved to script_input/raw_data.xlsx\n",
      "Standalone records: 12976\n",
      "Multiple records: 19988\n",
      "HTML data extraction completed!\n",
      "Selenium driver closed\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "extractor = HTMLDataExtractor()\n",
    "\n",
    "# Run the extraction process\n",
    "extractor.run(scraped_filepaths_csv='script_input/scraped_filepaths.csv', output_path='script_input/raw_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **4. Process Raw Data into Database Tables**\n",
    "\n",
    "### **TableBuilder Summary**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `TableBuilder` class processes structured data from the HTML extractor and transforms it into database-ready CSV files for SMU's class management system. It handles complex data relationships, professor name normalization, duplicate detection, and creates all necessary tables for courses, classes, professors, and timing schedules while maintaining referential integrity.\n",
    "\n",
    "**Key Features:**\n",
    "- **Two-Phase Processing**: Separates professor/course creation from class/timing processing to allow manual review and correction\n",
    "- **Intelligent Professor Matching**: Advanced name normalization and substring matching to prevent duplicate professor creation\n",
    "- **Comprehensive Data Pipeline**: Processes professors, courses, academic terms, classes, class timings, and exam schedules\n",
    "- **Database Cache Integration**: Loads existing data from PostgreSQL to avoid duplicates and maintain consistency\n",
    "- **Manual Review Workflow**: Outputs verification files for human review before final processing\n",
    "- **Asian Name Handling**: Specialized normalization for Asian, Western, and mixed naming conventions common in Singapore\n",
    "- **Faculty Assignment Interface**: Interactive web-based system for assigning courses to appropriate faculties\n",
    "- **Error Recovery**: Robust handling of malformed data and missing information\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `pandas`, `psycopg2`, `openpyxl`, `uuid`, `webbrowser`, standard libraries\n",
    "- PostgreSQL database connection (configured via `.env` file)\n",
    "- Database cache files or live database access for existing data validation\n",
    "\n",
    "**Input Requirements:**\n",
    "- **Raw Data Excel**: `script_input/raw_data.xlsx` from HTML extractor (point 3) with `standalone` and `multiple` sheets\n",
    "- **Database Configuration**: `.env` file with PostgreSQL connection parameters\n",
    "- **Professor Lookup**: `script_input/professor_lookup.csv` for existing professor mappings\n",
    "\n",
    "**Output Structure:**\n",
    "- **Verification Files** (`script_output/verify/`):\n",
    "  - `new_professors.csv`: New professors requiring manual name review\n",
    "  - `new_courses.csv`: New courses for validation\n",
    "- **Database Insert Files** (`script_output/`):\n",
    "  - `new_classes.csv`, `new_class_timing.csv`, `new_class_exam_timing.csv`\n",
    "  - `new_acad_term.csv`, `update_courses.csv`\n",
    "  - `professor_lookup.csv`: Updated professor mapping table\n",
    "\n",
    "**Data Processing Capabilities:**\n",
    "- **Professor Normalization**: Converts names to boss format (ALL CAPS) and afterclass format (Title Case)\n",
    "- **Duplicate Detection**: Substring matching across existing professors, new professors, and cached data\n",
    "- **Course Management**: Creates new courses and updates existing ones with latest information\n",
    "- **Academic Term Generation**: Automatically creates term IDs and manages semester data\n",
    "- **Relationship Mapping**: Maintains foreign key relationships across all generated tables\n",
    "- **Faculty Assignment**: Interactive workflow for assigning courses to SMU's 8 schools/centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TableBuilder:\n",
    "    \"\"\"Comprehensive table builder for university class management system\"\"\"\n",
    "    \n",
    "    def __init__(self, input_file: str = 'script_input/raw_data.xlsx'):\n",
    "        \"\"\"Initialize TableBuilder with database configuration and caching\"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_base = 'script_output'\n",
    "        self.verify_dir = os.path.join(self.output_base, 'verify')\n",
    "        self.cache_dir = 'db_cache'\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(self.output_base, exist_ok=True)\n",
    "        os.makedirs(self.verify_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "        self.db_config = {\n",
    "            'host': os.getenv('DB_HOST'),\n",
    "            'database': os.getenv('DB_NAME'),\n",
    "            'user': os.getenv('DB_USER'),\n",
    "            'password': os.getenv('DB_PASSWORD'),\n",
    "            'port': int(os.getenv('DB_PORT', 5432)),\n",
    "            'gssencmode': 'disable'\n",
    "        }\n",
    "        \n",
    "        # Database connection\n",
    "        self.connection = None\n",
    "        \n",
    "        # Data storage\n",
    "        self.standalone_data = None\n",
    "        self.multiple_data = None\n",
    "        \n",
    "        # Caches\n",
    "        self.professors_cache = {}  # name -> professor data\n",
    "        self.courses_cache = {}     # code -> course data\n",
    "        self.acad_term_cache = {}   # id -> acad_term data\n",
    "        self.professor_lookup = {}  # scraped_name -> database mapping\n",
    "        \n",
    "        # Output data collectors\n",
    "        self.new_professors = []\n",
    "        self.new_courses = []\n",
    "        self.update_courses = []\n",
    "        self.new_acad_terms = []\n",
    "        self.new_classes = []\n",
    "        self.new_class_timings = []\n",
    "        self.new_class_exam_timings = []\n",
    "        \n",
    "        # Class ID mapping for timing tables\n",
    "        self.class_id_mapping = {}  # record_key -> class_id\n",
    "        \n",
    "        # Courses requiring faculty assignment\n",
    "        self.courses_needing_faculty = []\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'professors_created': 0,\n",
    "            'courses_created': 0,\n",
    "            'courses_updated': 0,\n",
    "            'classes_created': 0,\n",
    "            'timings_created': 0,\n",
    "            'exams_created': 0,\n",
    "            'courses_needing_faculty': 0\n",
    "        }\n",
    "        \n",
    "        # Asian surnames database for name normalization\n",
    "        self.asian_surnames = {\n",
    "            'chinese': ['WANG', 'LI', 'ZHANG', 'LIU', 'CHEN', 'YANG', 'HUANG', 'ZHAO', 'WU', 'ZHOU',\n",
    "                       'XU', 'SUN', 'MA', 'ZHU', 'HU', 'GUO', 'HE', 'LIN', 'GAO', 'LUO'],\n",
    "            'singaporean': ['TAN', 'LIM', 'LEE', 'NG', 'ONG', 'WONG', 'GOH', 'CHUA', 'CHAN', 'KOH',\n",
    "                           'TEO', 'AW', 'CHYE', 'YEO', 'SIM', 'CHIA', 'CHONG', 'LAM', 'CHEW', 'TOH'],\n",
    "            'korean': ['KIM', 'LEE', 'PARK', 'CHOI', 'JUNG', 'KANG', 'CHO', 'YUN', 'JANG', 'LIM'],\n",
    "            'vietnamese': ['NGUYEN', 'TRAN', 'LE', 'PHAM', 'HOANG', 'PHAN', 'VU', 'DANG', 'BUI'],\n",
    "            'indian': ['SHARMA', 'SINGH', 'KUMAR', 'GUPTA', 'KOHLI', 'PATEL', 'MAKHIJA']\n",
    "        }\n",
    "        self.all_asian_surnames = set()\n",
    "        for surnames in self.asian_surnames.values():\n",
    "            self.all_asian_surnames.update(surnames)\n",
    "        \n",
    "        # Western given names\n",
    "        self.western_given_names = {\n",
    "            'AARON', 'ADAM', 'ADRIAN', 'ALEXANDER', 'AMANDA', 'ANDREW', 'ANTHONY',\n",
    "            'BENJAMIN', 'CHRISTOPHER', 'DANIEL', 'DAVID', 'EMILY', 'JAMES', 'JENNIFER',\n",
    "            'JOHN', 'MICHAEL', 'PETER', 'ROBERT', 'SARAH', 'THOMAS', 'WILLIAM'\n",
    "        }\n",
    "\n",
    "    def connect_database(self):\n",
    "        \"\"\"Connect to PostgreSQL database\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(**self.db_config)\n",
    "            logger.info(\"✅ Database connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Database connection failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_or_cache_data(self):\n",
    "        \"\"\"Load data from cache or database\"\"\"\n",
    "        # Try loading from cache first\n",
    "        if self._load_from_cache():\n",
    "            logger.info(\"✅ Loaded data from cache\")\n",
    "            return True\n",
    "        \n",
    "        # Connect to database and download\n",
    "        if not self.connect_database():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self._download_and_cache_data()\n",
    "            logger.info(\"✅ Downloaded and cached data from database\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to download data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _load_from_cache(self) -> bool:\n",
    "        \"\"\"Load cached data from files\"\"\"\n",
    "        try:\n",
    "            cache_files = {\n",
    "                'professors': os.path.join(self.cache_dir, 'professors_cache.pkl'),\n",
    "                'courses': os.path.join(self.cache_dir, 'courses_cache.pkl'),\n",
    "                'acad_terms': os.path.join(self.cache_dir, 'acad_terms_cache.pkl')\n",
    "            }\n",
    "            \n",
    "            if all(os.path.exists(f) for f in cache_files.values()):\n",
    "                # Load professors\n",
    "                professors_df = pd.read_pickle(cache_files['professors'])\n",
    "                for _, row in professors_df.iterrows():\n",
    "                    self.professors_cache[row['name'].upper()] = row.to_dict()\n",
    "                \n",
    "                # Load courses\n",
    "                courses_df = pd.read_pickle(cache_files['courses'])\n",
    "                for _, row in courses_df.iterrows():\n",
    "                    self.courses_cache[row['code']] = row.to_dict()\n",
    "                \n",
    "                # Load acad_terms\n",
    "                acad_terms_df = pd.read_pickle(cache_files['acad_terms'])\n",
    "                for _, row in acad_terms_df.iterrows():\n",
    "                    self.acad_term_cache[row['id']] = row.to_dict()\n",
    "                \n",
    "                # Load professor lookup if exists\n",
    "                lookup_file = 'script_input/professor_lookup.csv'\n",
    "                if os.path.exists(lookup_file):\n",
    "                    lookup_df = pd.read_csv(lookup_file)\n",
    "                    for _, row in lookup_df.iterrows():\n",
    "                        self.professor_lookup[row['scraped_name']] = {\n",
    "                            'database_id': row['database_id'],\n",
    "                            'boss_name': row.get('boss_name', row['scraped_name'].upper()),\n",
    "                            'afterclass_name': row.get('afterclass_name', row['scraped_name'])\n",
    "                        }\n",
    "                \n",
    "                return True\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cache loading error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _download_and_cache_data(self):\n",
    "        \"\"\"Download data from database and cache locally\"\"\"\n",
    "        # Download professors\n",
    "        query = \"SELECT * FROM professors\"\n",
    "        professors_df = pd.read_sql_query(query, self.connection)\n",
    "        professors_df.to_pickle(os.path.join(self.cache_dir, 'professors_cache.pkl'))\n",
    "        \n",
    "        # Download courses\n",
    "        query = \"SELECT * FROM courses\"\n",
    "        courses_df = pd.read_sql_query(query, self.connection)\n",
    "        courses_df.to_pickle(os.path.join(self.cache_dir, 'courses_cache.pkl'))\n",
    "        \n",
    "        # Download acad_terms\n",
    "        query = \"SELECT * FROM acad_term\"\n",
    "        acad_terms_df = pd.read_sql_query(query, self.connection)\n",
    "        acad_terms_df.to_pickle(os.path.join(self.cache_dir, 'acad_terms_cache.pkl'))\n",
    "        \n",
    "        # Load into memory\n",
    "        self._load_from_cache()\n",
    "\n",
    "    def load_raw_data(self):\n",
    "        \"\"\"Load raw data from Excel file\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"📂 Loading raw data from {self.input_file}\")\n",
    "            \n",
    "            # Load both sheets\n",
    "            self.standalone_data = pd.read_excel(self.input_file, sheet_name='standalone')\n",
    "            self.multiple_data = pd.read_excel(self.input_file, sheet_name='multiple')\n",
    "            \n",
    "            logger.info(f\"✅ Loaded {len(self.standalone_data)} standalone records\")\n",
    "            logger.info(f\"✅ Loaded {len(self.multiple_data)} multiple records\")\n",
    "            \n",
    "            from collections import defaultdict\n",
    "            \n",
    "            self.multiple_lookup = defaultdict(list)\n",
    "            for _, row in self.multiple_data.iterrows():\n",
    "                key = row.get('record_key')\n",
    "                if pd.notna(key):\n",
    "                    self.multiple_lookup[key].append(row)\n",
    "            \n",
    "            logger.info(f\"✅ Created optimized lookup for {len(self.multiple_lookup)} record keys\")\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load raw data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def normalize_professor_name(self, name: str) -> Tuple[str, str]:\n",
    "        \"\"\"Normalize professor name and return (boss_format, afterclass_format)\"\"\"\n",
    "        if not name or pd.isna(name):\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        # Clean and prepare name\n",
    "        name = str(name).strip()\n",
    "        \n",
    "        # Handle comma-separated names properly\n",
    "        if ',' in name:\n",
    "            comma_count = name.count(',')\n",
    "            if comma_count == 1:\n",
    "                # Single comma - convert \"SURNAME, Given Names\" to \"SURNAME Given Names\"\n",
    "                parts = name.split(',')\n",
    "                if len(parts) == 2:\n",
    "                    surname = parts[0].strip().upper()\n",
    "                    given_names = parts[1].strip()\n",
    "                    # Convert given names to title case\n",
    "                    given_names_title = ' '.join(word.capitalize() for word in given_names.split())\n",
    "                    name = f\"{surname} {given_names_title}\"\n",
    "                # If not exactly 2 parts, keep original\n",
    "            elif comma_count > 1:\n",
    "                # Multiple commas - likely multiple professors, take first\n",
    "                name = name.split(',')[0].strip()\n",
    "\n",
    "        \n",
    "        # Detect naming pattern\n",
    "        words = name.split()\n",
    "        if not words:\n",
    "            return name.upper(), name\n",
    "        \n",
    "        # Detect pattern\n",
    "        pattern = self._detect_name_pattern(words)\n",
    "        \n",
    "        # Format based on pattern\n",
    "        if pattern == 'WESTERN':\n",
    "            # Western: Given SURNAME\n",
    "            boss_name = name.upper()\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == len(words) - 1:  # Last word is surname\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        elif pattern == 'ASIAN':\n",
    "            # Asian: SURNAME Given Given\n",
    "            boss_name = name.upper()\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == 0:  # First word is surname\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        elif pattern == 'SINGAPOREAN':\n",
    "            # Singaporean: Given SURNAME Given\n",
    "            boss_name = name.upper()\n",
    "            surname_idx = self._find_surname_index(words)\n",
    "            afterclass_parts = []\n",
    "            for i, word in enumerate(words):\n",
    "                if i == surname_idx:\n",
    "                    afterclass_parts.append(word.upper())\n",
    "                else:\n",
    "                    afterclass_parts.append(word.capitalize())\n",
    "            afterclass_name = ' '.join(afterclass_parts)\n",
    "        \n",
    "        else:\n",
    "            # Default fallback\n",
    "            boss_name = name.upper()\n",
    "            afterclass_name = ' '.join(word.capitalize() for word in words)\n",
    "        \n",
    "        return boss_name, afterclass_name\n",
    "\n",
    "    def _detect_name_pattern(self, words: List[str]) -> str:\n",
    "        \"\"\"Detect naming pattern: WESTERN, ASIAN, or SINGAPOREAN\"\"\"\n",
    "        if not words:\n",
    "            return 'UNKNOWN'\n",
    "        \n",
    "        # Check for Western pattern\n",
    "        first_upper = words[0].upper()\n",
    "        if first_upper in self.western_given_names:\n",
    "            return 'WESTERN'\n",
    "        \n",
    "        # Check for pure Asian pattern\n",
    "        if first_upper in self.all_asian_surnames:\n",
    "            # Check if no Western names present\n",
    "            has_western = any(w.upper() in self.western_given_names for w in words)\n",
    "            if not has_western:\n",
    "                return 'ASIAN'\n",
    "        \n",
    "        # Check for Singaporean mixed pattern\n",
    "        if len(words) >= 3:\n",
    "            if (words[0].upper() in self.western_given_names and \n",
    "                any(w.upper() in self.all_asian_surnames for w in words[1:])):\n",
    "                return 'SINGAPOREAN'\n",
    "        \n",
    "        # Default to Western if unclear\n",
    "        return 'WESTERN'\n",
    "\n",
    "    def _find_surname_index(self, words: List[str]) -> int:\n",
    "        \"\"\"Find the index of surname in a list of words\"\"\"\n",
    "        for i, word in enumerate(words):\n",
    "            if word.upper() in self.all_asian_surnames:\n",
    "                return i\n",
    "        # Default to last word if no Asian surname found\n",
    "        return len(words) - 1\n",
    "\n",
    "    def process_professors(self):\n",
    "        \"\"\"Process professors from multiple sheet\"\"\"\n",
    "        logger.info(\"👥 Processing professors...\")\n",
    "        \n",
    "        unique_professors = set()\n",
    "        \n",
    "        # Extract unique professor names from multiple sheet\n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            if pd.notna(row.get('professor_name')):\n",
    "                prof_name = str(row['professor_name']).strip()\n",
    "                if prof_name and prof_name.upper() not in ['TBA', 'TO BE ANNOUNCED']:\n",
    "                    # Handle comma-separated names properly during extraction\n",
    "                    comma_count = prof_name.count(',')\n",
    "                    if comma_count == 1:\n",
    "                        parts = prof_name.split(',', 1)\n",
    "                        before_comma = parts[0].strip()\n",
    "                        if len(before_comma.split()) == 1:  # Single-word surname\n",
    "                            prof_name = f\"{before_comma} {parts[1].strip()}\"  # \"SURNAME Given\"\n",
    "                        else:  # Multi-word before comma = multi-instructor\n",
    "                            prof_name = before_comma  # Take first instructor only\n",
    "                    elif comma_count > 1:\n",
    "                        prof_name = prof_name.split(',', 1)[0].strip()  # First instructor\n",
    "                    else:\n",
    "                        prof_name = prof_name  # No commas\n",
    "                    unique_professors.add(prof_name)\n",
    "        \n",
    "        # Process each unique professor\n",
    "        for prof_name in unique_professors:\n",
    "            boss_name, afterclass_name = self.normalize_professor_name(prof_name)\n",
    "            \n",
    "            # Check if professor exists in lookup or cache\n",
    "            if prof_name in self.professor_lookup:\n",
    "                continue\n",
    "            \n",
    "            # Check cache by normalized name\n",
    "            if boss_name in self.professors_cache or afterclass_name.upper() in self.professors_cache:\n",
    "                continue\n",
    "            \n",
    "            # NEW: Add substring matching logic here\n",
    "            duplicate_found = False\n",
    "            \n",
    "            # Check against existing professor_lookup\n",
    "            for existing_scraped_name, prof_data in self.professor_lookup.items():\n",
    "                existing_boss = prof_data.get('boss_name', '')\n",
    "                existing_afterclass = prof_data.get('afterclass_name', '')\n",
    "                \n",
    "                # Check substring matches\n",
    "                if (prof_name.upper() in existing_scraped_name.upper() or \n",
    "                    existing_scraped_name.upper() in prof_name.upper() or\n",
    "                    boss_name.upper() in existing_boss.upper() or\n",
    "                    existing_boss.upper() in boss_name.upper() or\n",
    "                    afterclass_name.upper() in existing_afterclass.upper() or\n",
    "                    existing_afterclass.upper() in afterclass_name.upper()):\n",
    "                    \n",
    "                    # Update lookup to include this variation\n",
    "                    self.professor_lookup[prof_name] = prof_data.copy()\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "            \n",
    "            if duplicate_found:\n",
    "                continue\n",
    "            \n",
    "            # Check against professors_cache\n",
    "            for cached_name, cached_prof in self.professors_cache.items():\n",
    "                cached_boss = cached_prof.get('name', '').upper()\n",
    "                \n",
    "                # Check substring matches with cache\n",
    "                if (prof_name.upper() in cached_name.upper() or \n",
    "                    cached_name.upper() in prof_name.upper() or\n",
    "                    boss_name.upper() in cached_boss or\n",
    "                    cached_boss in boss_name.upper()):\n",
    "                    \n",
    "                    # Update lookup to point to existing professor\n",
    "                    self.professor_lookup[prof_name] = {\n",
    "                        'database_id': cached_prof['id'],\n",
    "                        'boss_name': cached_boss,\n",
    "                        'afterclass_name': cached_prof.get('name', afterclass_name)\n",
    "                    }\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "            \n",
    "            if duplicate_found:\n",
    "                continue\n",
    "            \n",
    "            # Check against new_professors being created in this run\n",
    "            for new_prof in self.new_professors:\n",
    "                new_original = new_prof.get('original_scraped_name', '')\n",
    "                new_boss = new_prof.get('boss_name', '')\n",
    "                new_afterclass = new_prof.get('afterclass_name', '')\n",
    "                \n",
    "                # Check substring matches with new professors\n",
    "                if (prof_name.upper() in new_original.upper() or \n",
    "                    new_original.upper() in prof_name.upper() or\n",
    "                    boss_name.upper() in new_boss.upper() or\n",
    "                    new_boss.upper() in boss_name.upper() or\n",
    "                    afterclass_name.upper() in new_afterclass.upper() or\n",
    "                    new_afterclass.upper() in afterclass_name.upper()):\n",
    "                    \n",
    "                    # Update lookup to point to the new professor\n",
    "                    self.professor_lookup[prof_name] = {\n",
    "                        'database_id': new_prof['id'],\n",
    "                        'boss_name': new_boss,\n",
    "                        'afterclass_name': new_afterclass\n",
    "                    }\n",
    "                    duplicate_found = True\n",
    "                    break\n",
    "            \n",
    "            if duplicate_found:\n",
    "                continue\n",
    "            \n",
    "            # Create new professor (existing code continues here)\n",
    "            professor_id = str(uuid.uuid4())\n",
    "            slug = re.sub(r'[^a-zA-Z0-9]+', '-', afterclass_name.lower()).strip('-')\n",
    "            \n",
    "            new_prof = {\n",
    "                'id': professor_id,\n",
    "                'name': afterclass_name,\n",
    "                'email': 'enquiry@smu.edu.sg',  # Default email\n",
    "                'slug': slug,\n",
    "                'photo_url': 'https://smu.edu.sg',\n",
    "                'profile_url': 'https://smu.edu.sg',\n",
    "                'belong_to_university': 1,  # SMU\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'updated_at': datetime.now().isoformat(),\n",
    "                'boss_name': boss_name,\n",
    "                'afterclass_name': afterclass_name,\n",
    "                'original_scraped_name': prof_name\n",
    "            }\n",
    "            \n",
    "            self.new_professors.append(new_prof)\n",
    "            self.stats['professors_created'] += 1\n",
    "            \n",
    "            # Update lookup\n",
    "            self.professor_lookup[prof_name] = {\n",
    "                'database_id': professor_id,\n",
    "                'boss_name': boss_name,\n",
    "                'afterclass_name': afterclass_name\n",
    "            }\n",
    "        \n",
    "        logger.info(f\"✅ Created {self.stats['professors_created']} new professors\")\n",
    "\n",
    "    def process_courses(self):\n",
    "        \"\"\"Process courses from standalone sheet WITHOUT prompting for faculty\"\"\"\n",
    "        logger.info(\"📚 Processing courses...\")\n",
    "        \n",
    "        # Group by course code to handle duplicates\n",
    "        course_groups = defaultdict(list)\n",
    "        for _, row in self.standalone_data.iterrows():\n",
    "            if pd.notna(row.get('course_code')):\n",
    "                course_groups[row['course_code']].append(row)\n",
    "        \n",
    "        for course_code, rows in course_groups.items():\n",
    "            # Helper function to get sortable key for academic term ordering\n",
    "            def get_sort_key(row):\n",
    "                year_start = row.get('acad_year_start', 0)\n",
    "                year_end = row.get('acad_year_end', 0)\n",
    "                term = str(row.get('term', ''))\n",
    "                \n",
    "                # Convert term to sortable format\n",
    "                term_order = {\n",
    "                    'T1': 1,\n",
    "                    'T2': 2,\n",
    "                    'T3A': 3.1,\n",
    "                    'T3B': 3.2\n",
    "                }\n",
    "                term_value = term_order.get(term.upper(), 0)\n",
    "                return (year_start, year_end, term_value)\n",
    "            \n",
    "            # Sort rows to get the latest one (highest year and term)\n",
    "            sorted_rows = sorted(rows, key=get_sort_key, reverse=True)\n",
    "            latest_row = sorted_rows[0]\n",
    "            \n",
    "            # Check if course exists in cache\n",
    "            if course_code in self.courses_cache:\n",
    "                # Course exists - check for updates\n",
    "                existing = self.courses_cache[course_code]\n",
    "                update_needed = False\n",
    "                update_record = {'id': existing['id'], 'code': course_code}\n",
    "                \n",
    "                # Fields that need comparison for changes\n",
    "                comparison_fields = ['name', 'description', 'credit_units']\n",
    "                \n",
    "                # Fields that always need updating (even if null/empty in existing)\n",
    "                always_update_fields = ['course_area', 'enrolment_requirements']\n",
    "                \n",
    "                # Field mapping from raw data to database columns\n",
    "                field_mapping = {\n",
    "                    'name': 'course_name',\n",
    "                    'description': 'course_description',\n",
    "                    'credit_units': 'credit_units'\n",
    "                }\n",
    "                \n",
    "                # Check comparison fields for changes\n",
    "                for field in comparison_fields:\n",
    "                    raw_field = field_mapping.get(field, field)\n",
    "                    new_value = latest_row.get(raw_field)\n",
    "                    old_value = existing.get(field)\n",
    "                    \n",
    "                    # Convert credit_units to float for proper comparison\n",
    "                    if field == 'credit_units':\n",
    "                        new_value = float(new_value) if pd.notna(new_value) else None\n",
    "                        old_value = float(old_value) if pd.notna(old_value) else None\n",
    "                    \n",
    "                    # Only update if new value exists and differs from old\n",
    "                    if pd.notna(new_value) and new_value != old_value:\n",
    "                        update_record[field] = new_value\n",
    "                        update_needed = True\n",
    "                \n",
    "                # Always update course_area and enrolment_requirements if they have values\n",
    "                for field in always_update_fields:\n",
    "                    new_value = latest_row.get(field)\n",
    "                    if pd.notna(new_value):\n",
    "                        # Always add these fields to update, even if unchanged\n",
    "                        update_record[field] = new_value\n",
    "                        update_needed = True\n",
    "                    elif existing.get(field) is None:\n",
    "                        # If existing has no value and new has no value, no update needed\n",
    "                        pass\n",
    "                    else:\n",
    "                        # If existing has value but new doesn't, keep existing (don't overwrite with null)\n",
    "                        pass\n",
    "                \n",
    "                if update_needed:\n",
    "                    self.update_courses.append(update_record)\n",
    "                    self.stats['courses_updated'] += 1\n",
    "                    \n",
    "                    # Update cache with new values\n",
    "                    for field, value in update_record.items():\n",
    "                        if field != 'id' and field != 'code':\n",
    "                            self.courses_cache[course_code][field] = value\n",
    "            else:\n",
    "                # Create new course WITHOUT faculty assignment\n",
    "                course_id = str(uuid.uuid4())\n",
    "                \n",
    "                new_course = {\n",
    "                    'id': course_id,\n",
    "                    'code': course_code,\n",
    "                    'name': latest_row.get('course_name', 'Unknown Course'),\n",
    "                    'description': latest_row.get('course_description', 'No description available'),\n",
    "                    'credit_units': float(latest_row.get('credit_units', 1.0)) if pd.notna(latest_row.get('credit_units')) else 1.0,\n",
    "                    'belong_to_university': 1,  # SMU\n",
    "                    'belong_to_faculty': None,  # Will be assigned later\n",
    "                    'course_area': latest_row.get('course_area'),\n",
    "                    'enrolment_requirements': latest_row.get('enrolment_requirements')\n",
    "                }\n",
    "                \n",
    "                self.new_courses.append(new_course)\n",
    "                self.stats['courses_created'] += 1\n",
    "                \n",
    "                # Store course info for later faculty assignment\n",
    "                self.courses_needing_faculty.append({\n",
    "                    'course_id': course_id,\n",
    "                    'course_code': course_code,\n",
    "                    'course_name': latest_row.get('course_name', 'Unknown Course'),\n",
    "                    'course_outline_url': latest_row.get('course_outline_url')\n",
    "                })\n",
    "                self.stats['courses_needing_faculty'] += 1\n",
    "                \n",
    "                # Update cache\n",
    "                self.courses_cache[course_code] = new_course\n",
    "        \n",
    "        logger.info(f\"✅ Created {self.stats['courses_created']} new courses\")\n",
    "        logger.info(f\"✅ Updated {self.stats['courses_updated']} existing courses\")\n",
    "        logger.info(f\"⚠️  {self.stats['courses_needing_faculty']} courses need faculty assignment\")\n",
    "\n",
    "    def assign_course_faculties(self):\n",
    "        \"\"\"Separate method to handle faculty assignments for courses\"\"\"\n",
    "        if not self.courses_needing_faculty:\n",
    "            logger.info(\"✅ No courses need faculty assignment\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"🎓 Starting faculty assignment for {len(self.courses_needing_faculty)} courses\")\n",
    "        \n",
    "        faculty_assignments = []\n",
    "        \n",
    "        for course_info in self.courses_needing_faculty:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"🎓 FACULTY ASSIGNMENT NEEDED\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Course Code: {course_info['course_code']}\")\n",
    "            print(f\"Course Name: {course_info['course_name']}\")\n",
    "            \n",
    "            # Open course outline if available\n",
    "            if pd.notna(course_info.get('course_outline_url')):\n",
    "                url = course_info['course_outline_url']\n",
    "                print(f\"Opening course outline: {url}\")\n",
    "                webbrowser.open(url)\n",
    "            \n",
    "            print(\"\\nFaculty Options:\")\n",
    "            print(\"1. Lee Kong Chian School of Business\")\n",
    "            print(\"2. Yong Pung How School of Law\")\n",
    "            print(\"3. School of Economics\")\n",
    "            print(\"4. School of Computing and Information Systems\")\n",
    "            print(\"5. School of Social Sciences\")\n",
    "            print(\"6. School of Accountancy\")\n",
    "            print(\"7. College of Integrative Studies\")\n",
    "            print(\"8. Center for English Communication\")\n",
    "            print(\"0. Skip (will need manual review)\")\n",
    "            \n",
    "            while True:\n",
    "                choice = input(\"\\nEnter faculty number (0-8): \").strip()\n",
    "                if choice == '0':\n",
    "                    faculty_id = None\n",
    "                    break\n",
    "                elif choice in ['1', '2', '3', '4', '5', '6', '7', '8']:\n",
    "                    faculty_id = int(choice)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Invalid choice. Please enter 0-8.\")\n",
    "            \n",
    "            # Store assignment\n",
    "            faculty_assignments.append({\n",
    "                'course_id': course_info['course_id'],\n",
    "                'course_code': course_info['course_code'],\n",
    "                'faculty_id': faculty_id\n",
    "            })\n",
    "        \n",
    "        # Update the new_courses list with faculty assignments\n",
    "        for assignment in faculty_assignments:\n",
    "            if assignment['faculty_id'] is not None:\n",
    "                # Find and update the course in new_courses\n",
    "                for course in self.new_courses:\n",
    "                    if course['id'] == assignment['course_id']:\n",
    "                        course['belong_to_faculty'] = assignment['faculty_id']\n",
    "                        break\n",
    "                \n",
    "                # Update cache\n",
    "                if assignment['course_code'] in self.courses_cache:\n",
    "                    self.courses_cache[assignment['course_code']]['belong_to_faculty'] = assignment['faculty_id']\n",
    "        \n",
    "        # Re-save the new_courses.csv with faculty assignments\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Updated new_courses.csv with faculty assignments\")\n",
    "        \n",
    "        logger.info(\"✅ Faculty assignment completed\")\n",
    "\n",
    "    def process_acad_terms(self):\n",
    "        \"\"\"Process academic terms from standalone sheet\"\"\"\n",
    "        logger.info(\"📅 Processing academic terms...\")\n",
    "        \n",
    "        # Group by (acad_year_start, acad_year_end, term)\n",
    "        term_groups = defaultdict(list)\n",
    "        for _, row in self.standalone_data.iterrows():\n",
    "            key = (\n",
    "                row.get('acad_year_start'),\n",
    "                row.get('acad_year_end'),\n",
    "                row.get('term')\n",
    "            )\n",
    "            if all(pd.notna(v) for v in key):\n",
    "                term_groups[key].append(row)\n",
    "        \n",
    "        for (year_start, year_end, term), rows in term_groups.items():\n",
    "            # Generate acad_term_id\n",
    "            acad_term_id = f\"AY{int(year_start)}{int(year_end) % 100:02d}{term}\"\n",
    "            \n",
    "            # Check if already exists\n",
    "            if acad_term_id in self.acad_term_cache:\n",
    "                continue\n",
    "            \n",
    "            # Find most common period_text and dates\n",
    "            period_counter = Counter()\n",
    "            date_info = {}\n",
    "            \n",
    "            for row in rows:\n",
    "                period_text = row.get('period_text', '')\n",
    "                if pd.notna(period_text):\n",
    "                    period_counter[period_text] += 1\n",
    "                    if period_text not in date_info:\n",
    "                        date_info[period_text] = {\n",
    "                            'start_dt': row.get('start_dt'),\n",
    "                            'end_dt': row.get('end_dt')\n",
    "                        }\n",
    "            \n",
    "            # Get most common period\n",
    "            if period_counter:\n",
    "                most_common_period = period_counter.most_common(1)[0][0]\n",
    "                dates = date_info[most_common_period]\n",
    "            else:\n",
    "                dates = {'start_dt': None, 'end_dt': None}\n",
    "            \n",
    "            # Get boss_id from first row\n",
    "            boss_id = rows[0].get('acad_term_boss_id')\n",
    "            \n",
    "            new_term = {\n",
    "                'id': acad_term_id,\n",
    "                'acad_year_start': int(year_start),\n",
    "                'acad_year_end': int(year_end),\n",
    "                'term': str(term),\n",
    "                'boss_id': int(boss_id) if pd.notna(boss_id) else None,\n",
    "                'start_dt': dates['start_dt'],\n",
    "                'end_dt': dates['end_dt']\n",
    "            }\n",
    "            \n",
    "            self.new_acad_terms.append(new_term)\n",
    "            self.acad_term_cache[acad_term_id] = new_term\n",
    "        \n",
    "        logger.info(f\"✅ Created {len(self.new_acad_terms)} new academic terms\")\n",
    "\n",
    "    def process_classes(self):\n",
    "        \"\"\"Process classes from standalone sheet\"\"\"\n",
    "        logger.info(\"🏫 Processing classes...\")\n",
    "        \n",
    "        try:\n",
    "            for _, row in self.standalone_data.iterrows():\n",
    "                record_key = row.get('record_key')\n",
    "                if pd.notna(record_key):\n",
    "                    # Use optimized professor lookup\n",
    "                    professor_id = self._find_professor_for_class(record_key)\n",
    "                    # Generate class ID\n",
    "                    class_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    # Extract boss_id from record_key\n",
    "                    record_key = row.get('record_key', '')\n",
    "                    boss_id_match = re.search(r'SelectedClassNumber=(\\d+)', record_key)\n",
    "                    boss_id = int(boss_id_match.group(1)) if boss_id_match else None\n",
    "                    \n",
    "                    # Get course_id\n",
    "                    course_code = row.get('course_code')\n",
    "                    course_id = None\n",
    "                    if course_code and course_code in self.courses_cache:\n",
    "                        course_id = self.courses_cache[course_code]['id']\n",
    "                    \n",
    "                    # Get professor_id from multiple sheet\n",
    "                    professor_id = self._find_professor_for_class(record_key)\n",
    "                    \n",
    "                    new_class = {\n",
    "                        'id': class_id,\n",
    "                        'section': row.get('section', ''),\n",
    "                        'course_id': course_id,\n",
    "                        'professor_id': professor_id,\n",
    "                        'acad_term_id': row.get('acad_term_id'),\n",
    "                        'grading_basis': row.get('grading_basis'),\n",
    "                        'course_outline_url': row.get('course_outline_url'),\n",
    "                        'boss_id': boss_id\n",
    "                    }\n",
    "                    \n",
    "                    self.new_classes.append(new_class)\n",
    "                    self.stats['classes_created'] += 1\n",
    "                    \n",
    "                    # Store mapping for timing tables\n",
    "                    self.class_id_mapping[record_key] = class_id\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing classes: {e}\")\n",
    "            raise\n",
    "        logger.info(f\"✅ Created {self.stats['classes_created']} new classes\")\n",
    "\n",
    "    def _find_professor_for_class(self, record_key: str) -> Optional[str]:\n",
    "        \"\"\"Optimised: Find professor ID for a class using pre-indexed multiple_lookup\"\"\"\n",
    "        rows = self.multiple_lookup.get(record_key, [])\n",
    "        for row in rows:\n",
    "            if pd.notna(row.get('professor_name')):\n",
    "                original_prof_name = str(row['professor_name']).strip()\n",
    "\n",
    "                # Step 1: Try full string match first\n",
    "                if original_prof_name in self.professor_lookup:\n",
    "                    return self.professor_lookup[original_prof_name]['database_id']\n",
    "\n",
    "                # Step 2: Parse name by commas\n",
    "                comma_count = original_prof_name.count(',')\n",
    "                if comma_count == 1:\n",
    "                    # One comma - check substring before comma\n",
    "                    parts = original_prof_name.split(',')\n",
    "                    before_comma = parts[0].strip()\n",
    "                    words_before_comma = before_comma.split()\n",
    "                    \n",
    "                    if len(words_before_comma) == 1:\n",
    "                        # Exactly one word before comma - single professor in \"SURNAME, FirstName\" format\n",
    "                        cleaned_name = original_prof_name  # Use full original string\n",
    "                    else:\n",
    "                        # More than one word before comma - multiple professors\n",
    "                        cleaned_name = before_comma  # Use only part before comma\n",
    "                elif comma_count >= 2:\n",
    "                    # Two or more commas - definitely multiple professors\n",
    "                    cleaned_name = original_prof_name.split(',')[0].strip()\n",
    "                else:\n",
    "                    # No commas - single professor\n",
    "                    cleaned_name = original_prof_name\n",
    "\n",
    "                # Step 3: Try cleaned name match\n",
    "                if cleaned_name in self.professor_lookup:\n",
    "                    return self.professor_lookup[cleaned_name]['database_id']\n",
    "\n",
    "                # NEW: Step 3.5: Try substring matching\n",
    "                # Check if any existing professor names are substrings of the original name\n",
    "                # or if the original name is a substring of existing names\n",
    "                for existing_name, prof_data in self.professor_lookup.items():\n",
    "                    # Check if existing name is in the original name\n",
    "                    if existing_name.upper() in original_prof_name.upper():\n",
    "                        return prof_data['database_id']\n",
    "                    # Check if original name is in existing name  \n",
    "                    if original_prof_name.upper() in existing_name.upper():\n",
    "                        return prof_data['database_id']\n",
    "                \n",
    "                # Also check against new professors being created\n",
    "                for new_prof in self.new_professors:\n",
    "                    new_prof_name = new_prof.get('original_scraped_name', '')\n",
    "                    boss_name = new_prof.get('boss_name', '')\n",
    "                    afterclass_name = new_prof.get('afterclass_name', '')\n",
    "                    \n",
    "                    # Check substring matches against various name formats\n",
    "                    names_to_check = [new_prof_name, boss_name, afterclass_name]\n",
    "                    for name in names_to_check:\n",
    "                        if name and (name.upper() in original_prof_name.upper() or \n",
    "                                    original_prof_name.upper() in name.upper()):\n",
    "                            return new_prof['id']\n",
    "\n",
    "                # Step 4: Use normalisation fallback\n",
    "                boss_name, afterclass_name = self.normalize_professor_name(cleaned_name)\n",
    "                if boss_name in self.professors_cache:\n",
    "                    return self.professors_cache[boss_name]['id']\n",
    "                if afterclass_name.upper() in self.professors_cache:\n",
    "                    return self.professors_cache[afterclass_name.upper()]['id']\n",
    "        return None\n",
    "\n",
    "    def process_timings(self):\n",
    "        \"\"\"Process class timings and exam timings from multiple sheet\"\"\"\n",
    "        logger.info(\"⏰ Processing class timings and exam timings...\")\n",
    "        \n",
    "        for _, row in self.multiple_data.iterrows():\n",
    "            record_key = row.get('record_key')\n",
    "            if record_key not in self.class_id_mapping:\n",
    "                continue\n",
    "            \n",
    "            class_id = self.class_id_mapping[record_key]\n",
    "            timing_type = row.get('type', 'CLASS')\n",
    "            \n",
    "            if timing_type == 'CLASS':\n",
    "                timing_record = {\n",
    "                    'class_id': class_id,\n",
    "                    'start_date': row.get('start_date'),\n",
    "                    'end_date': row.get('end_date'),\n",
    "                    'day_of_week': row.get('day_of_week'),\n",
    "                    'start_time': row.get('start_time'),\n",
    "                    'end_time': row.get('end_time'),\n",
    "                    'venue': row.get('venue', '')\n",
    "                }\n",
    "                self.new_class_timings.append(timing_record)\n",
    "                self.stats['timings_created'] += 1\n",
    "            \n",
    "            elif timing_type == 'EXAM':\n",
    "                exam_record = {\n",
    "                    'class_id': class_id,\n",
    "                    'date': row.get('date'),\n",
    "                    'day_of_week': row.get('day_of_week'),\n",
    "                    'start_time': row.get('start_time'),\n",
    "                    'end_time': row.get('end_time'),\n",
    "                    'venue': row.get('venue')\n",
    "                }\n",
    "                self.new_class_exam_timings.append(exam_record)\n",
    "                self.stats['exams_created'] += 1\n",
    "        \n",
    "        logger.info(f\"✅ Created {self.stats['timings_created']} class timings\")\n",
    "        logger.info(f\"✅ Created {self.stats['exams_created']} exam timings\")\n",
    "\n",
    "    def save_outputs(self):\n",
    "        \"\"\"Save all generated CSV files\"\"\"\n",
    "        logger.info(\"💾 Saving output files...\")\n",
    "        \n",
    "        # Save new professors (to verify folder)\n",
    "        if self.new_professors:\n",
    "            df = pd.DataFrame(self.new_professors)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_professors.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_professors)} new professors\")\n",
    "        \n",
    "        # Save new courses (to verify folder)\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_courses)} new courses\")\n",
    "        \n",
    "        # Save course updates\n",
    "        if self.update_courses:\n",
    "            df = pd.DataFrame(self.update_courses)\n",
    "            df.to_csv(os.path.join(self.output_base, 'update_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.update_courses)} course updates\")\n",
    "        \n",
    "        # Save academic terms\n",
    "        if self.new_acad_terms:\n",
    "            df = pd.DataFrame(self.new_acad_terms)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_acad_term.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_acad_terms)} academic terms\")\n",
    "        \n",
    "        # Save classes\n",
    "        if self.new_classes:\n",
    "            df = pd.DataFrame(self.new_classes)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_classes.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_classes)} classes\")\n",
    "        \n",
    "        # Save class timings\n",
    "        if self.new_class_timings:\n",
    "            df = pd.DataFrame(self.new_class_timings)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_class_timing.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_class_timings)} class timings\")\n",
    "        \n",
    "        # Save exam timings\n",
    "        if self.new_class_exam_timings:\n",
    "            df = pd.DataFrame(self.new_class_exam_timings)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_class_exam_timing.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_class_exam_timings)} exam timings\")\n",
    "        \n",
    "        # Save updated professor lookup\n",
    "        # Commented out due to poor surname handling in original code\n",
    "        # if self.new_professors:\n",
    "        #     self._save_professor_lookup()\n",
    "        pass\n",
    "        \n",
    "        # Save courses needing faculty assignment\n",
    "        if self.courses_needing_faculty:\n",
    "            df = pd.DataFrame(self.courses_needing_faculty)\n",
    "            df.to_csv(os.path.join(self.output_base, 'courses_needing_faculty.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.courses_needing_faculty)} courses needing faculty assignment\")\n",
    "        \n",
    "        # Create placeholder files\n",
    "        placeholders = ['new_bid_window.csv', 'new_class_availability.csv', 'new_bid_result.csv']\n",
    "        for filename in placeholders:\n",
    "            df = pd.DataFrame()\n",
    "            df.to_csv(os.path.join(self.output_base, filename), index=False)\n",
    "            logger.info(f\"✅ Created placeholder: {filename}\")\n",
    "\n",
    "    def _save_professor_lookup(self):\n",
    "        \"\"\"Save updated professor lookup table\"\"\"\n",
    "        lookup_data = []\n",
    "        \n",
    "        # Add all professors from lookup\n",
    "        for scraped_name, data in self.professor_lookup.items():\n",
    "            lookup_data.append({\n",
    "                'boss_name': data.get('boss_name', scraped_name.upper()),\n",
    "                'afterclass_name': data.get('afterclass_name', scraped_name),\n",
    "                'database_id': data['database_id'],\n",
    "                'method': 'exists' if scraped_name not in [p['original_scraped_name'] for p in self.new_professors] else 'created'\n",
    "            })\n",
    "        \n",
    "        # Sort by scraped_name\n",
    "        lookup_data.sort(key=lambda x: x['scraped_name'])\n",
    "        \n",
    "        # Save to output folder\n",
    "        df = pd.DataFrame(lookup_data)\n",
    "        df.to_csv(os.path.join(self.output_base, 'professor_lookup.csv'), index=False)\n",
    "        logger.info(f\"✅ Saved updated professor lookup with {len(lookup_data)} entries\")\n",
    "\n",
    "    def update_professor_lookup_from_corrected_csv(self):\n",
    "        \"\"\"Update professor lookup from manually corrected new_professors.csv\"\"\"\n",
    "        logger.info(\"🔄 Updating professor lookup from corrected CSV...\")\n",
    "        \n",
    "        # Read corrected new_professors.csv\n",
    "        corrected_csv_path = os.path.join(self.verify_dir, 'new_professors.csv')\n",
    "        if not os.path.exists(corrected_csv_path):\n",
    "            logger.error(f\"❌ Corrected CSV not found: {corrected_csv_path}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            corrected_df = pd.read_csv(corrected_csv_path)\n",
    "            logger.info(f\"📖 Reading {len(corrected_df)} corrected professor records\")\n",
    "            \n",
    "            # Update internal professor_lookup for new professors\n",
    "            updated_count = 0\n",
    "            for _, row in corrected_df.iterrows():\n",
    "                original_name = row.get('original_scraped_name', '')\n",
    "                corrected_afterclass_name = row.get('name', '')  # This is the corrected name\n",
    "                boss_name = row.get('boss_name', '')  # Keep boss name same\n",
    "                professor_id = row.get('id', '')\n",
    "                \n",
    "                if original_name and professor_id:\n",
    "                    # Update lookup with corrected afterclass name but same boss name\n",
    "                    self.professor_lookup[original_name] = {\n",
    "                        'database_id': professor_id,\n",
    "                        'boss_name': boss_name,  # Keep original boss name\n",
    "                        'afterclass_name': corrected_afterclass_name  # Use corrected name\n",
    "                    }\n",
    "                    updated_count += 1\n",
    "            \n",
    "            # Save updated professor lookup to CSV\n",
    "            self._save_corrected_professor_lookup()\n",
    "            \n",
    "            logger.info(f\"✅ Updated {updated_count} professor lookup entries\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to update professor lookup: {e}\")\n",
    "            return False\n",
    "\n",
    "    def process_remaining_tables(self):\n",
    "        \"\"\"Process classes and timings after professor lookup is updated\"\"\"\n",
    "        logger.info(\"🏫 Processing remaining tables (classes, timings)...\")\n",
    "        \n",
    "        try:\n",
    "            # Process classes (depends on updated professor lookup)\n",
    "            self.process_classes()\n",
    "            \n",
    "            # Process timings (depends on classes)\n",
    "            self.process_timings()\n",
    "            \n",
    "            logger.info(\"✅ Remaining tables processed successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to process remaining tables: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_corrected_professor_lookup(self):\n",
    "        \"\"\"Save professor lookup with corrected names\"\"\"\n",
    "        lookup_data = []\n",
    "        \n",
    "        # Load existing professor lookup if it exists\n",
    "        existing_lookup_path = os.path.join(self.output_base, 'professor_lookup.csv')\n",
    "        if os.path.exists(existing_lookup_path):\n",
    "            existing_df = pd.read_csv(existing_lookup_path)\n",
    "            for _, row in existing_df.iterrows():\n",
    "                lookup_data.append({\n",
    "                    'scraped_name': row.get('scraped_name', ''),\n",
    "                    'boss_name': row.get('boss_name', ''),\n",
    "                    'afterclass_name': row.get('afterclass_name', ''),\n",
    "                    'database_id': row.get('database_id', ''),\n",
    "                    'method': row.get('method', 'exists')\n",
    "                })\n",
    "        \n",
    "        # Add/update with new professor lookup entries\n",
    "        existing_scraped_names = {item['scraped_name'] for item in lookup_data}\n",
    "        \n",
    "        for scraped_name, data in self.professor_lookup.items():\n",
    "            if scraped_name not in existing_scraped_names:\n",
    "                lookup_data.append({\n",
    "                    'scraped_name': scraped_name,\n",
    "                    'boss_name': data.get('boss_name', scraped_name.upper()),\n",
    "                    'afterclass_name': data.get('afterclass_name', scraped_name),\n",
    "                    'database_id': data['database_id'],\n",
    "                    'method': 'created'\n",
    "                })\n",
    "            else:\n",
    "                # Update existing entry with corrected afterclass name\n",
    "                for item in lookup_data:\n",
    "                    if item['scraped_name'] == scraped_name:\n",
    "                        item['afterclass_name'] = data.get('afterclass_name', scraped_name)\n",
    "                        break\n",
    "        \n",
    "        # Sort by scraped_name\n",
    "        lookup_data.sort(key=lambda x: x['scraped_name'])\n",
    "        \n",
    "        # Save to output folder\n",
    "        df = pd.DataFrame(lookup_data)\n",
    "        df.to_csv(os.path.join(self.output_base, 'professor_lookup.csv'), index=False)\n",
    "        logger.info(f\"✅ Saved updated professor lookup with {len(lookup_data)} entries\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"📊 PROCESSING SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"✅ Professors created: {self.stats['professors_created']}\")\n",
    "        print(f\"✅ Courses created: {self.stats['courses_created']}\")\n",
    "        print(f\"✅ Courses updated: {self.stats['courses_updated']}\")\n",
    "        print(f\"⚠️  Courses needing faculty: {self.stats['courses_needing_faculty']}\")\n",
    "        print(f\"✅ Classes created: {self.stats['classes_created']}\")\n",
    "        print(f\"✅ Class timings created: {self.stats['timings_created']}\")\n",
    "        print(f\"✅ Exam timings created: {self.stats['exams_created']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\n📁 OUTPUT FILES:\")\n",
    "        print(f\"   Verify folder: {self.verify_dir}/\")\n",
    "        print(f\"   - new_professors.csv ({self.stats['professors_created']} records)\")\n",
    "        print(f\"   - new_courses.csv ({self.stats['courses_created']} records)\")\n",
    "        print(f\"   Output folder: {self.output_base}/\")\n",
    "        print(f\"   - update_courses.csv ({self.stats['courses_updated']} records)\")\n",
    "        print(f\"   - new_acad_term.csv ({len(self.new_acad_terms)} records)\")\n",
    "        print(f\"   - new_classes.csv ({self.stats['classes_created']} records)\")\n",
    "        print(f\"   - new_class_timing.csv ({self.stats['timings_created']} records)\")\n",
    "        print(f\"   - new_class_exam_timing.csv ({self.stats['exams_created']} records)\")\n",
    "        print(f\"   - professor_lookup.csv (updated)\")\n",
    "        print(f\"   - courses_needing_faculty.csv ({self.stats['courses_needing_faculty']} records)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    def run_phase1_professors_and_courses(self):\n",
    "        \"\"\"Phase 1: Process professors and courses only\"\"\"\n",
    "        try:\n",
    "            logger.info(\"🚀 Starting Phase 1: Professors and Courses\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Load data\n",
    "            if not self.load_or_cache_data():\n",
    "                logger.error(\"❌ Failed to load database data\")\n",
    "                return False\n",
    "            \n",
    "            if not self.load_raw_data():\n",
    "                logger.error(\"❌ Failed to load raw data\")\n",
    "                return False\n",
    "            \n",
    "            # Process professors (CSV only, no lookup update)\n",
    "            self.process_professors()\n",
    "            \n",
    "            # Process courses\n",
    "            self.process_courses()\n",
    "            \n",
    "            # Process academic terms\n",
    "            self.process_acad_terms()\n",
    "            \n",
    "            # Save phase 1 outputs\n",
    "            self._save_phase1_outputs()\n",
    "            \n",
    "            logger.info(\"✅ Phase 1 completed - Review new_professors.csv for manual correction\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Phase 1 failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run_phase2_remaining_tables(self):\n",
    "        \"\"\"Phase 2: Process classes and timings after professor correction\"\"\"\n",
    "        try:\n",
    "            logger.info(\"🚀 Starting Phase 2: Classes and Timings\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Update professor lookup from corrected CSV\n",
    "            if not self.update_professor_lookup_from_corrected_csv():\n",
    "                logger.error(\"❌ Failed to update professor lookup\")\n",
    "                return False\n",
    "            \n",
    "            # Process remaining tables\n",
    "            if not self.process_remaining_tables():\n",
    "                logger.error(\"❌ Failed to process remaining tables\")\n",
    "                return False\n",
    "            \n",
    "            # Save all outputs\n",
    "            self.save_outputs()\n",
    "            \n",
    "            # Print summary\n",
    "            self.print_summary()\n",
    "            \n",
    "            logger.info(\"✅ Phase 2 completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Phase 2 failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _save_phase1_outputs(self):\n",
    "        \"\"\"Save Phase 1 outputs (professors, courses, acad_terms)\"\"\"\n",
    "        # Save new professors (to verify folder for manual correction)\n",
    "        if self.new_professors:\n",
    "            df = pd.DataFrame(self.new_professors)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_professors.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_professors)} new professors for review\")\n",
    "        \n",
    "        # Save new courses (to verify folder)\n",
    "        if self.new_courses:\n",
    "            df = pd.DataFrame(self.new_courses)\n",
    "            df.to_csv(os.path.join(self.verify_dir, 'new_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_courses)} new courses\")\n",
    "        \n",
    "        # Save course updates\n",
    "        if self.update_courses:\n",
    "            df = pd.DataFrame(self.update_courses)\n",
    "            df.to_csv(os.path.join(self.output_base, 'update_courses.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.update_courses)} course updates\")\n",
    "        \n",
    "        # Save academic terms\n",
    "        if self.new_acad_terms:\n",
    "            df = pd.DataFrame(self.new_acad_terms)\n",
    "            df.to_csv(os.path.join(self.output_base, 'new_acad_term.csv'), index=False)\n",
    "            logger.info(f\"✅ Saved {len(self.new_acad_terms)} academic terms\")\n",
    "\n",
    "    def run(self, skip_faculty_assignment=True):\n",
    "        \"\"\"Run the complete table building process\n",
    "        \n",
    "        Args:\n",
    "            skip_faculty_assignment: If True, faculty assignment is deferred\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"🚀 Starting TableBuilder process\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Load or cache database data\n",
    "            if not self.load_or_cache_data():\n",
    "                logger.error(\"❌ Failed to load database data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 2: Load raw data\n",
    "            if not self.load_raw_data():\n",
    "                logger.error(\"❌ Failed to load raw data\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Process tables in dependency order\n",
    "            logger.info(\"\\n📋 Processing tables in dependency order...\")\n",
    "            \n",
    "            # 3.1: Process professors first (no dependencies)\n",
    "            self.process_professors()\n",
    "            \n",
    "            # 3.2: Process courses (without faculty assignment)\n",
    "            self.process_courses()\n",
    "            \n",
    "            # 3.3: Process academic terms (no dependencies)\n",
    "            self.process_acad_terms()\n",
    "            \n",
    "            # 3.4: Process classes (depends on courses, professors, acad_terms)\n",
    "            self.process_classes()\n",
    "            \n",
    "            # 3.5: Process timings (depends on classes)\n",
    "            self.process_timings()\n",
    "            \n",
    "            # Step 4: Save all outputs\n",
    "            self.save_outputs()\n",
    "            \n",
    "            # Step 5: Print summary\n",
    "            self.print_summary()\n",
    "            \n",
    "            if self.stats['courses_needing_faculty'] > 0 and not skip_faculty_assignment:\n",
    "                print(\"\\n⚠️  FACULTY ASSIGNMENT REQUIRED\")\n",
    "                print(f\"   {self.stats['courses_needing_faculty']} courses need faculty assignment\")\n",
    "                print(\"   Run builder.assign_course_faculties() to complete assignment\")\n",
    "            \n",
    "            logger.info(\"\\n✅ TableBuilder process completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Process failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        finally:\n",
    "            # Clean up database connection\n",
    "            if self.connection:\n",
    "                self.connection.close()\n",
    "                logger.info(\"🔒 Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Phase 1 Initialization**\n",
    "```python\n",
    "# Initialize the TableBuilder\n",
    "builder = TableBuilder()\n",
    "\n",
    "# Run Phase 1 (professors, courses, acad_terms)\n",
    "success = builder.run_phase1_professors_and_courses()\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **First-time setup**: Initial processing of new raw data from HTML extractor\n",
    "- **Semester data ingestion**: Beginning of each new academic term data import\n",
    "- **After HTML extraction**: Following successful completion of point 3 data extraction\n",
    "\n",
    "**What It Does:**\n",
    "- Loads existing database cache and raw Excel data\n",
    "- Processes professors with advanced name normalization and duplicate detection\n",
    "- Creates new courses without faculty assignments (deferred for manual review)\n",
    "- Generates academic terms from date ranges and term codes\n",
    "- Outputs verification files for manual review before proceeding\n",
    "\n",
    "**Success Indicators:**\n",
    "- Creates `script_output/verify/new_professors.csv` with properly normalized names\n",
    "- Generates course files ready for faculty assignment\n",
    "- Displays statistics on professors, courses, and terms processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 17:14:23,763 - INFO - 🚀 Starting Phase 1: Professors and Courses\n",
      "2025-06-09 17:14:23,763 - INFO - ============================================================\n",
      "2025-06-09 17:14:24,023 - INFO - ✅ Loaded data from cache\n",
      "2025-06-09 17:14:24,023 - INFO - 📂 Loading raw data from script_input/raw_data.xlsx\n",
      "2025-06-09 17:14:32,059 - INFO - ✅ Loaded 12976 standalone records\n",
      "2025-06-09 17:14:32,059 - INFO - ✅ Loaded 19988 multiple records\n",
      "2025-06-09 17:14:33,078 - INFO - ✅ Created optimized lookup for 11926 record keys\n",
      "2025-06-09 17:14:33,078 - INFO - 👥 Processing professors...\n",
      "2025-06-09 17:14:33,937 - INFO - ✅ Created 11 new professors\n",
      "2025-06-09 17:14:33,938 - INFO - 📚 Processing courses...\n",
      "2025-06-09 17:14:34,838 - INFO - ✅ Created 141 new courses\n",
      "2025-06-09 17:14:34,839 - INFO - ✅ Updated 1158 existing courses\n",
      "2025-06-09 17:14:34,839 - INFO - ⚠️  141 courses need faculty assignment\n",
      "2025-06-09 17:14:34,858 - INFO - 📅 Processing academic terms...\n",
      "2025-06-09 17:14:35,760 - INFO - ✅ Created 17 new academic terms\n",
      "2025-06-09 17:14:35,772 - INFO - ✅ Saved 11 new professors for review\n",
      "2025-06-09 17:14:35,777 - INFO - ✅ Saved 141 new courses\n",
      "2025-06-09 17:14:35,796 - INFO - ✅ Saved 1158 course updates\n",
      "2025-06-09 17:14:35,798 - INFO - ✅ Saved 17 academic terms\n",
      "2025-06-09 17:14:35,798 - INFO - ✅ Phase 1 completed - Review new_professors.csv for manual correction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Phase 1 completed successfully!\n",
      "📝 Next steps:\n",
      "   1. Review script_output/verify/new_professors.csv\n",
      "   2. Manually correct any professor names if needed\n",
      "   3. Run Phase 2 in the next cell\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TableBuilder\n",
    "builder = TableBuilder()\n",
    "\n",
    "# Run Phase 1 (professors, courses, acad_terms)\n",
    "success = builder.run_phase1_professors_and_courses()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 Phase 1 completed successfully!\")\n",
    "    print(\"📝 Next steps:\")\n",
    "    print(\"   1. Review script_output/verify/new_professors.csv\")\n",
    "    print(\"   2. Manually correct any professor names if needed\")\n",
    "    print(\"   3. Run Phase 2 in the next cell\")\n",
    "else:\n",
    "    print(\"\\n❌ Phase 1 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 2: Professor Review Interface**\n",
    "```python\n",
    "# Display new professors for review\n",
    "new_prof_path = os.path.join('script_output', 'verify', 'new_professors.csv')\n",
    "df = pd.read_csv(new_prof_path)\n",
    "display(df[['name', 'boss_name', 'afterclass_name', 'original_scraped_name']])\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **After Phase 1 completion**: Review professor names before final processing\n",
    "- **Quality assurance**: Verify name normalization accuracy for Asian and Western names\n",
    "- **Before database insertion**: Ensure all professor names are correctly formatted\n",
    "\n",
    "**What It Does:**\n",
    "- Displays newly created professors with different name formats\n",
    "- Shows original scraped names vs. normalized versions\n",
    "- Provides clear guidance on which column to edit (name = afterclass format)\n",
    "- Preserves boss_name format for database consistency\n",
    "\n",
    "**Manual Review Process:**\n",
    "- Check `name` column for proper Title Case formatting\n",
    "- Verify Asian surnames are correctly identified and positioned\n",
    "- Correct any obvious parsing errors (e.g., \"TSE, JUSTIN K, AIDAN WONG\" cases)\n",
    "- Save changes directly to the CSV file for Phase 2 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 10 new professors created:\n",
      "\n",
      "🔍 Review these professor names:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>boss_name</th>\n",
       "      <th>afterclass_name</th>\n",
       "      <th>original_scraped_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEE Yun</td>\n",
       "      <td>LEE YUN</td>\n",
       "      <td>LEE Yun</td>\n",
       "      <td>LEE YUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yu QI</td>\n",
       "      <td>YU QI</td>\n",
       "      <td>Yu QI</td>\n",
       "      <td>YU QI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tang TONY</td>\n",
       "      <td>TANG TONY</td>\n",
       "      <td>Tang TONY</td>\n",
       "      <td>TANG TONY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hara KOTARO</td>\n",
       "      <td>HARA KOTARO</td>\n",
       "      <td>Hara KOTARO</td>\n",
       "      <td>HARA KOTARO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HU Naiyuan</td>\n",
       "      <td>HU NAIYUAN</td>\n",
       "      <td>HU Naiyuan</td>\n",
       "      <td>HU NAIYUAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Koh ANDREW</td>\n",
       "      <td>KOH ANDREW</td>\n",
       "      <td>Koh ANDREW</td>\n",
       "      <td>KOH ANDREW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ricks JACOB</td>\n",
       "      <td>RICKS JACOB</td>\n",
       "      <td>Ricks JACOB</td>\n",
       "      <td>RICKS JACOB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ZHANG Ce</td>\n",
       "      <td>ZHANG CE</td>\n",
       "      <td>ZHANG Ce</td>\n",
       "      <td>ZHANG CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zeng QINGLI</td>\n",
       "      <td>ZENG QINGLI</td>\n",
       "      <td>Zeng QINGLI</td>\n",
       "      <td>ZENG QINGLI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pepito NONA</td>\n",
       "      <td>PEPITO NONA</td>\n",
       "      <td>Pepito NONA</td>\n",
       "      <td>PEPITO NONA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name    boss_name afterclass_name original_scraped_name\n",
       "0      LEE Yun      LEE YUN         LEE Yun               LEE YUN\n",
       "1        Yu QI        YU QI           Yu QI                 YU QI\n",
       "2    Tang TONY    TANG TONY       Tang TONY             TANG TONY\n",
       "3  Hara KOTARO  HARA KOTARO     Hara KOTARO           HARA KOTARO\n",
       "4   HU Naiyuan   HU NAIYUAN      HU Naiyuan            HU NAIYUAN\n",
       "5   Koh ANDREW   KOH ANDREW      Koh ANDREW            KOH ANDREW\n",
       "6  Ricks JACOB  RICKS JACOB     Ricks JACOB           RICKS JACOB\n",
       "7     ZHANG Ce     ZHANG CE        ZHANG Ce              ZHANG CE\n",
       "8  Zeng QINGLI  ZENG QINGLI     Zeng QINGLI           ZENG QINGLI\n",
       "9  Pepito NONA  PEPITO NONA     Pepito NONA           PEPITO NONA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 If any names need correction, edit the 'name' column in:\n",
      "   script_output\\verify\\new_professors.csv\n",
      "\n",
      "⚠️  Only edit the 'name' column (afterclass format)\n",
      "   Keep 'boss_name' unchanged\n"
     ]
    }
   ],
   "source": [
    "# Display new professors for review\n",
    "new_prof_path = os.path.join('script_output', 'verify', 'new_professors.csv')\n",
    "if os.path.exists(new_prof_path):\n",
    "    df = pd.read_csv(new_prof_path)\n",
    "    print(f\"📋 {len(df)} new professors created:\")\n",
    "    print(\"\\n🔍 Review these professor names:\")\n",
    "    display(df[['name', 'boss_name', 'afterclass_name', 'original_scraped_name']])\n",
    "    print(\"\\n📝 If any names need correction, edit the 'name' column in:\")\n",
    "    print(f\"   {new_prof_path}\")\n",
    "    print(\"\\n⚠️  Only edit the 'name' column (afterclass format)\")\n",
    "    print(\"   Keep 'boss_name' unchanged\")\n",
    "else:\n",
    "    print(\"❌ new_professors.csv not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 3: Phase 2 Completion**\n",
    "```python\n",
    "# Run Phase 2 (classes, timings) after manual correction\n",
    "success = builder.run_phase2_remaining_tables()\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **After manual professor review**: Following corrections to new_professors.csv\n",
    "- **Final data processing**: Complete the database table generation pipeline\n",
    "- **Before database insertion**: Generate all remaining tables with correct relationships\n",
    "\n",
    "**What It Does:**\n",
    "- Updates internal professor lookup from manually corrected CSV files\n",
    "- Processes classes using corrected professor mappings and course relationships\n",
    "- Generates class timing and exam timing records linked to classes\n",
    "- Creates complete set of database-ready CSV files\n",
    "- Maintains referential integrity across all generated tables\n",
    "\n",
    "**Output Generation:**\n",
    "- Links professors to classes using updated lookup mappings\n",
    "- Ensures all timing records reference valid class IDs\n",
    "- Produces final statistics and file summaries for database insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 17:15:17,815 - INFO - 🚀 Starting Phase 2: Classes and Timings\n",
      "2025-06-09 17:15:17,815 - INFO - ============================================================\n",
      "2025-06-09 17:15:17,816 - INFO - 🔄 Updating professor lookup from corrected CSV...\n",
      "2025-06-09 17:15:17,818 - INFO - 📖 Reading 10 corrected professor records\n",
      "2025-06-09 17:15:17,825 - INFO - ✅ Saved updated professor lookup with 1116 entries\n",
      "2025-06-09 17:15:17,825 - INFO - ✅ Updated 10 professor lookup entries\n",
      "2025-06-09 17:15:17,826 - INFO - 🏫 Processing remaining tables (classes, timings)...\n",
      "2025-06-09 17:15:17,827 - INFO - 🏫 Processing classes...\n",
      "2025-06-09 17:15:19,021 - INFO - ✅ Created 12976 new classes\n",
      "2025-06-09 17:15:19,023 - INFO - ⏰ Processing class timings and exam timings...\n",
      "2025-06-09 17:15:20,118 - INFO - ✅ Created 13084 class timings\n",
      "2025-06-09 17:15:20,119 - INFO - ✅ Created 6904 exam timings\n",
      "2025-06-09 17:15:20,119 - INFO - ✅ Remaining tables processed successfully\n",
      "2025-06-09 17:15:20,120 - INFO - 💾 Saving output files...\n",
      "2025-06-09 17:15:20,122 - INFO - ✅ Saved 11 new professors\n",
      "2025-06-09 17:15:20,128 - INFO - ✅ Saved 141 new courses\n",
      "2025-06-09 17:15:20,146 - INFO - ✅ Saved 1158 course updates\n",
      "2025-06-09 17:15:20,149 - INFO - ✅ Saved 17 academic terms\n",
      "2025-06-09 17:15:20,212 - INFO - ✅ Saved 12976 classes\n",
      "2025-06-09 17:15:20,256 - INFO - ✅ Saved 13084 class timings\n",
      "2025-06-09 17:15:20,279 - INFO - ✅ Saved 6904 exam timings\n",
      "2025-06-09 17:15:20,282 - INFO - ✅ Saved 141 courses needing faculty assignment\n",
      "2025-06-09 17:15:20,284 - INFO - ✅ Created placeholder: new_bid_window.csv\n",
      "2025-06-09 17:15:20,286 - INFO - ✅ Created placeholder: new_class_availability.csv\n",
      "2025-06-09 17:15:20,288 - INFO - ✅ Created placeholder: new_bid_result.csv\n",
      "2025-06-09 17:15:20,289 - INFO - ✅ Phase 2 completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "📊 PROCESSING SUMMARY\n",
      "======================================================================\n",
      "✅ Professors created: 11\n",
      "✅ Courses created: 141\n",
      "✅ Courses updated: 1158\n",
      "⚠️  Courses needing faculty: 141\n",
      "✅ Classes created: 12976\n",
      "✅ Class timings created: 13084\n",
      "✅ Exam timings created: 6904\n",
      "======================================================================\n",
      "\n",
      "📁 OUTPUT FILES:\n",
      "   Verify folder: script_output\\verify/\n",
      "   - new_professors.csv (11 records)\n",
      "   - new_courses.csv (141 records)\n",
      "   Output folder: script_output/\n",
      "   - update_courses.csv (1158 records)\n",
      "   - new_acad_term.csv (17 records)\n",
      "   - new_classes.csv (12976 records)\n",
      "   - new_class_timing.csv (13084 records)\n",
      "   - new_class_exam_timing.csv (6904 records)\n",
      "   - professor_lookup.csv (updated)\n",
      "   - courses_needing_faculty.csv (141 records)\n",
      "======================================================================\n",
      "\n",
      "🎉 Phase 2 completed successfully!\n",
      "📝 All tables generated with corrected professor names\n"
     ]
    }
   ],
   "source": [
    "# Run Phase 2 (classes, timings) after manual correction\n",
    "success = builder.run_phase2_remaining_tables()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n🎉 Phase 2 completed successfully!\")\n",
    "    print(\"📝 All tables generated with corrected professor names\")\n",
    "else:\n",
    "    print(\"\\n❌ Phase 2 failed. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 4: Faculty Assignment (Optional)**\n",
    "```python\n",
    "# Run faculty assignment process if needed\n",
    "if hasattr(builder, 'courses_needing_faculty') and builder.courses_needing_faculty:\n",
    "    builder.assign_course_faculties()\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **New course processing**: When courses lack faculty assignments in manual mapping\n",
    "- **Interactive assignment**: For courses requiring human judgment on faculty placement\n",
    "- **Policy compliance**: Ensuring all courses are properly assigned to SMU schools\n",
    "\n",
    "**What It Does:**\n",
    "- Opens course outline URLs in web browser for informed decision-making\n",
    "- Presents interactive menu of SMU's 8 schools and centers\n",
    "- Updates course records with selected faculty assignments\n",
    "- Re-saves CSV files with complete faculty information\n",
    "\n",
    "**Faculty Options Available:**\n",
    "1. Lee Kong Chian School of Business\n",
    "2. Yong Pung How School of Law  \n",
    "3. School of Economics\n",
    "4. School of Computing and Information Systems\n",
    "5. School of Social Sciences\n",
    "6. School of Accountancy\n",
    "7. College of Integrative Studies\n",
    "8. Center for English Communication\n",
    "\n",
    "**Best Practices:**\n",
    "- Review course outlines before making faculty assignments\n",
    "- Use existing course patterns as reference for similar courses\n",
    "- Skip courses requiring additional research (can be assigned later)\n",
    "- Ensure consistency with SMU's academic structure and course offerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 16:08:40,418 - INFO - 🎓 Starting faculty assignment for 141 courses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎓 FACULTY ASSIGNMENT NEEDED\n",
      "============================================================\n",
      "Course Code: ISFS603\n",
      "Course Name: Corporate Banking and Blockchain\n",
      "Opening course outline: https://courses.smu.edu.sg/sites/courses.smu.edu.sg/files/PGP2022/SCISGPO/2210/ISFS603_ PAUL GRIFFIN.pdf\n",
      "\n",
      "Faculty Options:\n",
      "1. Lee Kong Chian School of Business\n",
      "2. Yong Pung How School of Law\n",
      "3. School of Economics\n",
      "4. School of Computing and Information Systems\n",
      "5. School of Social Sciences\n",
      "6. School of Accountancy\n",
      "7. College of Integrative Studies\n",
      "8. Center for English Communication\n",
      "0. Skip (will need manual review)\n",
      "Invalid choice. Please enter 0-8.\n",
      "Invalid choice. Please enter 0-8.\n",
      "Invalid choice. Please enter 0-8.\n"
     ]
    }
   ],
   "source": [
    "# Run faculty assignment process if needed\n",
    "if hasattr(builder, 'courses_needing_faculty') and builder.courses_needing_faculty:\n",
    "    builder.assign_course_faculties()\n",
    "    print(\"\\n✅ Faculty assignment completed!\")\n",
    "else:\n",
    "    print(\"✅ No courses need faculty assignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting course synchronization...\n",
      "✅ Loaded 99 manually mapped courses\n",
      "✅ Loaded 141 script-generated courses\n",
      "📋 Created mapping for 99 course codes\n",
      "🗑️  Identified 42 script-generated UUIDs to delete (unmapped)\n",
      "🔗 Created UUID mapping for 99 courses (old → new)\n",
      "🔍 Filtered courses: 141 → 99 (42 dropped)\n",
      "💾 Updated script_output\\verify\\new_courses.csv with UUIDs and faculty mapping\n",
      "💾 Updated classes: 12976 → 12695 (281 deleted from unmapped courses)\n",
      "💾 Updated class timings: 13084 → 12910 (174 deleted)\n",
      "💾 Updated exam timings: 6904 → 6893 (11 deleted)\n",
      "\n",
      "✅ Course synchronization completed successfully!\n",
      "📊 Summary:\n",
      "   • Courses: 6893 kept (matched manual mapping)\n",
      "   • 42 unmapped script courses deleted\n",
      "   • UUIDs and faculty assignments updated from manual mapping\n",
      "   • Classes: 12695 kept\n",
      "   • Existing non-script data preserved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sync_courses_with_manual_mapping():\n",
    "    \"\"\"\n",
    "    Sync script-generated courses with manually mapped courses and filter related data\n",
    "    \"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    manual_courses_path = r'extracted_data\\3. new_courses.csv'\n",
    "    script_courses_path = r'script_output\\verify\\new_courses.csv'\n",
    "    \n",
    "    print(\"🔄 Starting course synchronization...\")\n",
    "    \n",
    "    # Load manually mapped courses\n",
    "    try:\n",
    "        manual_courses = pd.read_csv(manual_courses_path)\n",
    "        print(f\"✅ Loaded {len(manual_courses)} manually mapped courses\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Manual courses file not found: {manual_courses_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Load script-generated courses\n",
    "    try:\n",
    "        script_courses = pd.read_csv(script_courses_path)\n",
    "        print(f\"✅ Loaded {len(script_courses)} script-generated courses\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Script courses file not found: {script_courses_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Create mappings from course code to manual UUID and faculty\n",
    "    code_to_manual_uuid = dict(zip(manual_courses['code'], manual_courses['id']))\n",
    "    code_to_faculty = dict(zip(manual_courses['code'], manual_courses['belong_to_faculty']))\n",
    "    print(f\"📋 Created mapping for {len(code_to_manual_uuid)} course codes\")\n",
    "    \n",
    "    # CRITICAL: Identify script-generated UUIDs that should be deleted\n",
    "    # These are the UUIDs from script courses that are NOT mapped to manual courses\n",
    "    script_courses_to_keep = script_courses[script_courses['code'].isin(manual_courses['code'])].copy()\n",
    "    script_courses_to_delete = script_courses[~script_courses['code'].isin(manual_courses['code'])].copy()\n",
    "    \n",
    "    # Get the UUIDs that should be deleted (unmapped script UUIDs)\n",
    "    uuids_to_delete = set(script_courses_to_delete['id'])\n",
    "    print(f\"🗑️  Identified {len(uuids_to_delete)} script-generated UUIDs to delete (unmapped)\")\n",
    "    \n",
    "    # Create old UUID to new UUID mapping for courses that are kept\n",
    "    old_to_new_uuid = {}\n",
    "    for _, row in script_courses_to_keep.iterrows():\n",
    "        old_uuid = row['id']\n",
    "        new_uuid = code_to_manual_uuid[row['code']]\n",
    "        old_to_new_uuid[old_uuid] = new_uuid\n",
    "    \n",
    "    print(f\"🔗 Created UUID mapping for {len(old_to_new_uuid)} courses (old → new)\")\n",
    "    \n",
    "    # Filter script courses to only those in manual courses\n",
    "    original_count = len(script_courses)\n",
    "    filtered_script_courses = script_courses_to_keep.copy()\n",
    "    filtered_count = len(filtered_script_courses)\n",
    "    \n",
    "    print(f\"🔍 Filtered courses: {original_count} → {filtered_count} ({original_count - filtered_count} dropped)\")\n",
    "    \n",
    "    # Update filtered script courses IDs and belong_to_faculty to match manual data\n",
    "    filtered_script_courses['id'] = filtered_script_courses['code'].map(code_to_manual_uuid)\n",
    "    filtered_script_courses['belong_to_faculty'] = filtered_script_courses['code'].map(code_to_faculty)\n",
    "    \n",
    "    # Save updated script courses\n",
    "    filtered_script_courses.to_csv(script_courses_path, index=False)\n",
    "    print(f\"💾 Updated {script_courses_path} with UUIDs and faculty mapping\")\n",
    "    \n",
    "    # Filter and update classes - ONLY delete unmapped script UUIDs\n",
    "    classes_path = r'script_output\\new_classes.csv'\n",
    "    if os.path.exists(classes_path):\n",
    "        classes_df = pd.read_csv(classes_path)\n",
    "        original_classes = len(classes_df)\n",
    "        \n",
    "        # First, remove records with UUIDs that should be deleted\n",
    "        classes_df_cleaned = classes_df[~classes_df['course_id'].isin(uuids_to_delete)].copy()\n",
    "        deleted_classes = original_classes - len(classes_df_cleaned)\n",
    "        \n",
    "        # Then, update course_id from old UUID to new UUID (for mapped courses)\n",
    "        classes_df_cleaned['course_id'] = classes_df_cleaned['course_id'].map(old_to_new_uuid).fillna(classes_df_cleaned['course_id'])\n",
    "        \n",
    "        filtered_classes_count = len(classes_df_cleaned)\n",
    "        \n",
    "        # Save updated classes\n",
    "        classes_df_cleaned.to_csv(classes_path, index=False)\n",
    "        print(f\"💾 Updated classes: {original_classes} → {filtered_classes_count} ({deleted_classes} deleted from unmapped courses)\")\n",
    "        \n",
    "        # Get valid class IDs for timing tables\n",
    "        valid_class_ids = set(classes_df_cleaned['id'])\n",
    "    else:\n",
    "        print(f\"⚠️  Classes file not found: {classes_path}\")\n",
    "        valid_class_ids = set()\n",
    "    \n",
    "    # Filter and update class timings - ONLY delete records linked to deleted classes\n",
    "    timing_files = [\n",
    "        (r'script_output\\new_class_timing.csv', 'class timings'),\n",
    "        (r'script_output\\new_class_exam_timing.csv', 'exam timings')\n",
    "    ]\n",
    "    \n",
    "    for file_path, description in timing_files:\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path)\n",
    "            original_count = len(df)\n",
    "            \n",
    "            # Filter by valid class_ids (keep existing + mapped, remove only deleted class records)\n",
    "            filtered_df = df[df['class_id'].isin(valid_class_ids)].copy()\n",
    "            filtered_count = len(filtered_df)\n",
    "            deleted_count = original_count - filtered_count\n",
    "            \n",
    "            # Save filtered data\n",
    "            filtered_df.to_csv(file_path, index=False)\n",
    "            print(f\"💾 Updated {description}: {original_count} → {filtered_count} ({deleted_count} deleted)\")\n",
    "        else:\n",
    "            print(f\"⚠️  File not found: {file_path}\")\n",
    "    \n",
    "    print(\"\\n✅ Course synchronization completed successfully!\")\n",
    "    print(f\"📊 Summary:\")\n",
    "    print(f\"   • Courses: {filtered_count} kept (matched manual mapping)\")\n",
    "    print(f\"   • {len(uuids_to_delete)} unmapped script courses deleted\")\n",
    "    print(f\"   • UUIDs and faculty assignments updated from manual mapping\")\n",
    "    print(f\"   • Classes: {filtered_classes_count if 'filtered_classes_count' in locals() else 'N/A'} kept\")\n",
    "    print(f\"   • Existing non-script data preserved\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run the synchronization\n",
    "sync_courses_with_manual_mapping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bidly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
