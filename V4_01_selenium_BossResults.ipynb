{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SMU Course Scraping Using Selenium**\n",
    "\n",
    "<div style=\"background-color:#FFD700; padding:15px; border-radius:5px; border: 2px solid #FF4500;\">\n",
    "    \n",
    "  <h1 style=\"color:#8B0000;\">‚ö†Ô∏èüö® SCRAPE THIS DATA AT YOUR OWN RISK üö®‚ö†Ô∏è</h1>\n",
    "  \n",
    "  <p><strong>üìå If you need the data, please contact me directly.</strong> Only available for **existing students**.</p>\n",
    "\n",
    "  <h3>üîó üì© How to Get the Data?</h3>\n",
    "  <p>üì® <strong>Reach out to me for access</strong> instead of scraping manually.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:#FFF8DC; padding:12px; border-radius:5px; border: 1px solid #DAA520;\">\n",
    "    \n",
    "  <h2 style=\"color:#8B8000;\">‚ú® Looking for the Latest Model? Consider V4! ‚ú®</h2>\n",
    "  <p>üëâ <a href=\"V4_example_prediction.ipynb\"><strong>Check out V4 Here</strong></a></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Objective**\n",
    "This script is designed to scrape SMU course details from the BOSS system using Selenium. The process involves:\n",
    "1. Logging into the system manually to bypass authentication.\n",
    "2. Iteratively scraping class details for specified academic years and terms.\n",
    "3. Writing the scraped data to structured CSV files.\n",
    "\n",
    "### **Script Structure**\n",
    "1. **Setup**: Import libraries and initialize Selenium WebDriver.\n",
    "2. **Login**: Wait for manual login and authentication.\n",
    "3. **Scraping Logic**:\n",
    "    - `scrape_class_details`: Scrapes course details for a specific class number, academic year, and term.\n",
    "    - `main`: Manages the scraping process for multiple academic years and terms.\n",
    "4. **Execution**: Log in and start scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import psycopg2\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **2. Login Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_manual_login(driver):\n",
    "    print(\"Please log in manually and complete the Microsoft Authenticator process.\")\n",
    "    print(\"Waiting for BOSS dashboard to load...\")\n",
    "    \n",
    "    # Create a WebDriverWait object with 2-minute timeout\n",
    "    wait = WebDriverWait(driver, 120)\n",
    "    \n",
    "    try:\n",
    "        # Wait for the username label that appears after successful login\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"Label_UserName\")))\n",
    "        \n",
    "        # Verify additional elements to confirm we're fully logged in\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(text(),'Sign out')]\")))\n",
    "        \n",
    "        # Get the username for confirmation\n",
    "        username = driver.find_element(By.ID, \"Label_UserName\").text\n",
    "        print(f\"Login successful! Logged in as {username}\")\n",
    "        \n",
    "    except TimeoutException:\n",
    "        print(\"Login failed or timed out. Could not detect login elements.\")\n",
    "        raise Exception(\"Login failed\")\n",
    "    \n",
    "    # Additional small delay to ensure everything is loaded\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. Scrape data**\n",
    "\n",
    "### **3.1 Scrape all data from BOSS**\n",
    "1. Take all existing AddedInfo files with `SelectedClassNumber` min and max.\n",
    "2. Scrape entire webpage on BOSS, seperate them by AY and Term\n",
    "3. Create an overall scraping logic for future. Past AY2024T3B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_class_number_ranges(directory='classTimings'):\n",
    "    \"\"\"\n",
    "    Read existing CSV files to determine min and max class numbers for each AY_TERM.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory containing CSV files (format: [AY]_[Term]AddedInfo.csv)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping AY_TERM to min/max class numbers\n",
    "    \"\"\"\n",
    "    class_number_ranges = {}\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Regex pattern to match files like 2021-22_T1AddedInfo.csv\n",
    "    pattern = re.compile(r'(\\d{4}-\\d{2}_T\\d[AB]?)AddedInfo\\.csv')\n",
    "    \n",
    "    try:\n",
    "        files_found = False\n",
    "        for filename in os.listdir(directory):\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                files_found = True\n",
    "                # Extract AY_TERM from filename\n",
    "                ay_term = match.group(1)\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                \n",
    "                min_class = None\n",
    "                max_class = None\n",
    "                \n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    reader = csv.DictReader(file)\n",
    "                    for row in reader:\n",
    "                        try:\n",
    "                            class_num = int(row.get('SelectedClassNumber', '').strip())\n",
    "                            if min_class is None or class_num < min_class:\n",
    "                                min_class = class_num\n",
    "                            if max_class is None or class_num > max_class:\n",
    "                                max_class = class_num\n",
    "                        except (ValueError, TypeError):\n",
    "                            continue\n",
    "                \n",
    "                if min_class is not None and max_class is not None:\n",
    "                    class_number_ranges[ay_term] = {'min': min_class, 'max': max_class}\n",
    "                else:\n",
    "                    # Default values if no valid class numbers found\n",
    "                    class_number_ranges[ay_term] = {'min': 1000, 'max': 5000}\n",
    "                    \n",
    "        # If no files found, return empty dictionary\n",
    "        if not files_found:\n",
    "            print(\"No class timing files found. Will use default ranges.\")\n",
    "            return {}\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory '{directory}' not found. Creating it.\")\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    return class_number_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_html(driver, start_ay_term='2021-22_T1', end_ay_term='2024-25_T3B', base_dir='classTimingsFull'):\n",
    "    \"\"\"\n",
    "    Scrapes class details from BOSS and saves them as HTML files\n",
    "    \n",
    "    Args:\n",
    "        driver: WebDriver instance that is already logged in\n",
    "        start_ay_term: Starting academic year and term (e.g., '2021-22_T1')\n",
    "        end_ay_term: Ending academic year and term (e.g., '2024-25_T3B')\n",
    "        base_dir: Base directory to save the HTML files\n",
    "    \"\"\"\n",
    "    # Term code mapping for URL parameters\n",
    "    term_code_map = {'T1': '10', 'T2': '20', 'T3A': '31', 'T3B': '32'}\n",
    "    \n",
    "    # Define all possible terms in order\n",
    "    all_terms = ['T1', 'T2', 'T3A', 'T3B']\n",
    "    \n",
    "    # Define all possible academic years\n",
    "    all_academic_years = ['2021-22', '2022-23', '2023-24', '2024-25', '2025-26', '2026-27']\n",
    "    \n",
    "    # Generate all possible AY_TERM combinations\n",
    "    all_ay_terms = []\n",
    "    for ay in all_academic_years:\n",
    "        for term in all_terms:\n",
    "            all_ay_terms.append(f\"{ay}_{term}\")\n",
    "    \n",
    "    # Find the indices of the start and end terms\n",
    "    try:\n",
    "        start_idx = all_ay_terms.index(start_ay_term)\n",
    "        end_idx = all_ay_terms.index(end_ay_term)\n",
    "    except ValueError:\n",
    "        print(\"Invalid start or end term provided. Using full range.\")\n",
    "        start_idx = 0\n",
    "        end_idx = len(all_ay_terms) - 1\n",
    "    \n",
    "    # Select the range to scrape\n",
    "    ay_terms_to_scrape = all_ay_terms[start_idx:end_idx+1]\n",
    "    \n",
    "    # First, read existing class ranges if available\n",
    "    class_number_ranges = read_class_number_ranges('classTimings')\n",
    "    print(f\"Found class number ranges: {class_number_ranges}\")\n",
    "    \n",
    "    # Create base directory if needed\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each AY_TERM\n",
    "    for ay_term in ay_terms_to_scrape:\n",
    "        print(f\"Processing {ay_term}...\")\n",
    "        \n",
    "        # Parse AY_TERM for URL\n",
    "        ay, term = ay_term.split('_')\n",
    "        ay_short = ay[2:4]  # last two digits of first year\n",
    "        term_code = term_code_map.get(term, '10')\n",
    "        \n",
    "        # Get min/max class numbers or use defaults\n",
    "        ranges = class_number_ranges.get(ay_term, {'min': 1000, 'max': 5000})\n",
    "        min_class = ranges.get('min', 1000)\n",
    "        max_class = ranges.get('max', 5000)\n",
    "        \n",
    "        # Create folder for AY_TERM\n",
    "        folder_path = os.path.join(base_dir, ay_term)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        consecutive_empty = 0\n",
    "        \n",
    "        # Scrape each class number in range\n",
    "        for class_num in range(min_class, max_class + 1):\n",
    "            url = f\"https://boss.intranet.smu.edu.sg/ClassDetails.aspx?SelectedClassNumber={class_num:04}&SelectedAcadTerm={ay_short}{term_code}&SelectedAcadCareer=UGRD\"\n",
    "            \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                \n",
    "                # Wait for EITHER the success element OR the error element to appear\n",
    "                wait = WebDriverWait(driver, 15)\n",
    "                try:\n",
    "                    # Wait for either the class header OR the error details element\n",
    "                    element = wait.until(EC.any_of(\n",
    "                        EC.presence_of_element_located((By.ID, \"lblClassInfoHeader\")),\n",
    "                        EC.presence_of_element_located((By.ID, \"lblErrorDetails\"))\n",
    "                    ))\n",
    "                    \n",
    "                    # Check if \"No record found\" is in the error details\n",
    "                    error_elements = driver.find_elements(By.ID, \"lblErrorDetails\")\n",
    "                    has_data = True\n",
    "                    \n",
    "                    for error in error_elements:\n",
    "                        if \"No record found\" in error.text:\n",
    "                            has_data = False\n",
    "                            break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Wait error: {e}\")\n",
    "                    has_data = False\n",
    "                \n",
    "                if not has_data:\n",
    "                    consecutive_empty += 1\n",
    "                    print(f\"No record found for {ay_term}, class {class_num:04}. Consecutive empty: {consecutive_empty}\")\n",
    "                    \n",
    "                    if consecutive_empty >= 100:\n",
    "                        print(f\"100 consecutive empty records reached for {ay_term}, moving on.\")\n",
    "                        break\n",
    "                    \n",
    "                    # No need to wait 30 seconds if we already know it's empty\n",
    "                    time.sleep(2)  # Small pause before next request\n",
    "                    continue\n",
    "                \n",
    "                # Reset consecutive empty counter if data found\n",
    "                consecutive_empty = 0\n",
    "                \n",
    "                # Save HTML file\n",
    "                filename = f\"SelectedAcadTerm={ay_short}{term_code}&SelectedClassNumber={class_num:04}.html\"\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                \n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(driver.page_source)\n",
    "                \n",
    "                print(f\"Saved {filepath}\")\n",
    "                \n",
    "                # Small pause between requests\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {str(e)}\")\n",
    "                time.sleep(5)  # Wait a bit longer after an error\n",
    "    \n",
    "    print(\"Scraping completed.\")\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scraped_filepaths_csv(base_dir='classTimingsFull', output_csv='scraped_filepaths.csv'):\n",
    "    \"\"\"\n",
    "    Generates a CSV file with paths to all valid HTML files (those without \"No record found\")\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory where HTML files are stored\n",
    "        output_csv: Name of the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Path to the generated CSV file\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    \n",
    "    # Check if base directory exists\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Directory '{base_dir}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Walk through directory structure\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        if 'No record found' not in content:\n",
    "                            filepaths.append(filepath)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {filepath}: {str(e)}\")\n",
    "    \n",
    "    # Write to CSV\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Filepath'])\n",
    "        for path in filepaths:\n",
    "            writer.writerow([path])\n",
    "    \n",
    "    print(f\"Generated CSV file with {len(filepaths)} valid file paths at {output_csv}\")\n",
    "    return output_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in manually and complete the Microsoft Authenticator process.\n",
      "Waiting for BOSS dashboard to load...\n",
      "Login successful! Logged in as Welcome, TAN ZHONG YAN\n",
      "Found class number ranges: {'2021-22_T1': {'min': 1002, 'max': 2889}, '2021-22_T2': {'min': 1002, 'max': 2957}, '2021-22_T3A': {'min': 1002, 'max': 1038}, '2021-22_T3B': {'min': 1002, 'max': 1033}, '2022-23_T1': {'min': 1002, 'max': 2954}, '2022-23_T2': {'min': 1002, 'max': 2920}, '2022-23_T3A': {'min': 1002, 'max': 1031}, '2022-23_T3B': {'min': 1002, 'max': 1027}, '2023-24_T1': {'min': 1002, 'max': 2982}, '2023-24_T2': {'min': 1002, 'max': 2964}, '2023-24_T3A': {'min': 1002, 'max': 1028}, '2023-24_T3B': {'min': 1003, 'max': 1033}, '2024-25_T1': {'min': 1002, 'max': 2945}, '2024-25_T2': {'min': 1002, 'max': 2786}}\n",
      "Processing 2025-26_T1...\n",
      "No record found for 2025-26_T1, class 1000. Consecutive empty: 1\n",
      "No record found for 2025-26_T1, class 1001. Consecutive empty: 2\n",
      "Saved classTimingsFull\\2025-26_T1\\SelectedAcadTerm=2510&SelectedClassNumber=1002.html\n",
      "Saved classTimingsFull\\2025-26_T1\\SelectedAcadTerm=2510&SelectedClassNumber=1003.html\n",
      "Saved classTimingsFull\\2025-26_T1\\SelectedAcadTerm=2510&SelectedClassNumber=1004.html\n",
      "No record found for 2025-26_T1, class 1005. Consecutive empty: 1\n",
      "No record found for 2025-26_T1, class 1006. Consecutive empty: 2\n",
      "No record found for 2025-26_T1, class 1007. Consecutive empty: 3\n",
      "No record found for 2025-26_T1, class 1008. Consecutive empty: 4\n",
      "No record found for 2025-26_T1, class 1009. Consecutive empty: 5\n",
      "No record found for 2025-26_T1, class 1010. Consecutive empty: 6\n",
      "No record found for 2025-26_T1, class 1011. Consecutive empty: 7\n",
      "No record found for 2025-26_T1, class 1012. Consecutive empty: 8\n",
      "No record found for 2025-26_T1, class 1013. Consecutive empty: 9\n",
      "No record found for 2025-26_T1, class 1014. Consecutive empty: 10\n",
      "No record found for 2025-26_T1, class 1015. Consecutive empty: 11\n",
      "No record found for 2025-26_T1, class 1016. Consecutive empty: 12\n",
      "No record found for 2025-26_T1, class 1017. Consecutive empty: 13\n",
      "No record found for 2025-26_T1, class 1018. Consecutive empty: 14\n",
      "No record found for 2025-26_T1, class 1019. Consecutive empty: 15\n",
      "No record found for 2025-26_T1, class 1020. Consecutive empty: 16\n",
      "No record found for 2025-26_T1, class 1021. Consecutive empty: 17\n",
      "No record found for 2025-26_T1, class 1022. Consecutive empty: 18\n",
      "No record found for 2025-26_T1, class 1023. Consecutive empty: 19\n",
      "No record found for 2025-26_T1, class 1024. Consecutive empty: 20\n",
      "No record found for 2025-26_T1, class 1025. Consecutive empty: 21\n",
      "No record found for 2025-26_T1, class 1026. Consecutive empty: 22\n",
      "No record found for 2025-26_T1, class 1027. Consecutive empty: 23\n",
      "No record found for 2025-26_T1, class 1028. Consecutive empty: 24\n",
      "No record found for 2025-26_T1, class 1029. Consecutive empty: 25\n",
      "No record found for 2025-26_T1, class 1030. Consecutive empty: 26\n",
      "No record found for 2025-26_T1, class 1031. Consecutive empty: 27\n",
      "No record found for 2025-26_T1, class 1032. Consecutive empty: 28\n",
      "No record found for 2025-26_T1, class 1033. Consecutive empty: 29\n",
      "No record found for 2025-26_T1, class 1034. Consecutive empty: 30\n",
      "No record found for 2025-26_T1, class 1035. Consecutive empty: 31\n",
      "No record found for 2025-26_T1, class 1036. Consecutive empty: 32\n",
      "No record found for 2025-26_T1, class 1037. Consecutive empty: 33\n",
      "No record found for 2025-26_T1, class 1038. Consecutive empty: 34\n",
      "No record found for 2025-26_T1, class 1039. Consecutive empty: 35\n",
      "No record found for 2025-26_T1, class 1040. Consecutive empty: 36\n",
      "No record found for 2025-26_T1, class 1041. Consecutive empty: 37\n",
      "No record found for 2025-26_T1, class 1042. Consecutive empty: 38\n",
      "No record found for 2025-26_T1, class 1043. Consecutive empty: 39\n",
      "No record found for 2025-26_T1, class 1044. Consecutive empty: 40\n",
      "No record found for 2025-26_T1, class 1045. Consecutive empty: 41\n",
      "No record found for 2025-26_T1, class 1046. Consecutive empty: 42\n",
      "No record found for 2025-26_T1, class 1047. Consecutive empty: 43\n",
      "No record found for 2025-26_T1, class 1048. Consecutive empty: 44\n",
      "No record found for 2025-26_T1, class 1049. Consecutive empty: 45\n",
      "No record found for 2025-26_T1, class 1050. Consecutive empty: 46\n",
      "No record found for 2025-26_T1, class 1051. Consecutive empty: 47\n",
      "No record found for 2025-26_T1, class 1052. Consecutive empty: 48\n",
      "No record found for 2025-26_T1, class 1053. Consecutive empty: 49\n",
      "No record found for 2025-26_T1, class 1054. Consecutive empty: 50\n",
      "No record found for 2025-26_T1, class 1055. Consecutive empty: 51\n",
      "No record found for 2025-26_T1, class 1056. Consecutive empty: 52\n",
      "No record found for 2025-26_T1, class 1057. Consecutive empty: 53\n",
      "No record found for 2025-26_T1, class 1058. Consecutive empty: 54\n",
      "No record found for 2025-26_T1, class 1059. Consecutive empty: 55\n",
      "No record found for 2025-26_T1, class 1060. Consecutive empty: 56\n",
      "No record found for 2025-26_T1, class 1061. Consecutive empty: 57\n",
      "No record found for 2025-26_T1, class 1062. Consecutive empty: 58\n",
      "No record found for 2025-26_T1, class 1063. Consecutive empty: 59\n",
      "No record found for 2025-26_T1, class 1064. Consecutive empty: 60\n",
      "No record found for 2025-26_T1, class 1065. Consecutive empty: 61\n",
      "No record found for 2025-26_T1, class 1066. Consecutive empty: 62\n",
      "No record found for 2025-26_T1, class 1067. Consecutive empty: 63\n",
      "No record found for 2025-26_T1, class 1068. Consecutive empty: 64\n",
      "No record found for 2025-26_T1, class 1069. Consecutive empty: 65\n",
      "No record found for 2025-26_T1, class 1070. Consecutive empty: 66\n",
      "No record found for 2025-26_T1, class 1071. Consecutive empty: 67\n",
      "No record found for 2025-26_T1, class 1072. Consecutive empty: 68\n",
      "No record found for 2025-26_T1, class 1073. Consecutive empty: 69\n",
      "No record found for 2025-26_T1, class 1074. Consecutive empty: 70\n",
      "No record found for 2025-26_T1, class 1075. Consecutive empty: 71\n",
      "No record found for 2025-26_T1, class 1076. Consecutive empty: 72\n",
      "No record found for 2025-26_T1, class 1077. Consecutive empty: 73\n",
      "No record found for 2025-26_T1, class 1078. Consecutive empty: 74\n",
      "No record found for 2025-26_T1, class 1079. Consecutive empty: 75\n",
      "No record found for 2025-26_T1, class 1080. Consecutive empty: 76\n",
      "No record found for 2025-26_T1, class 1081. Consecutive empty: 77\n",
      "No record found for 2025-26_T1, class 1082. Consecutive empty: 78\n",
      "No record found for 2025-26_T1, class 1083. Consecutive empty: 79\n",
      "No record found for 2025-26_T1, class 1084. Consecutive empty: 80\n",
      "No record found for 2025-26_T1, class 1085. Consecutive empty: 81\n",
      "No record found for 2025-26_T1, class 1086. Consecutive empty: 82\n",
      "No record found for 2025-26_T1, class 1087. Consecutive empty: 83\n",
      "No record found for 2025-26_T1, class 1088. Consecutive empty: 84\n",
      "No record found for 2025-26_T1, class 1089. Consecutive empty: 85\n",
      "No record found for 2025-26_T1, class 1090. Consecutive empty: 86\n",
      "No record found for 2025-26_T1, class 1091. Consecutive empty: 87\n",
      "No record found for 2025-26_T1, class 1092. Consecutive empty: 88\n",
      "No record found for 2025-26_T1, class 1093. Consecutive empty: 89\n",
      "No record found for 2025-26_T1, class 1094. Consecutive empty: 90\n",
      "No record found for 2025-26_T1, class 1095. Consecutive empty: 91\n",
      "No record found for 2025-26_T1, class 1096. Consecutive empty: 92\n",
      "No record found for 2025-26_T1, class 1097. Consecutive empty: 93\n",
      "No record found for 2025-26_T1, class 1098. Consecutive empty: 94\n",
      "No record found for 2025-26_T1, class 1099. Consecutive empty: 95\n",
      "No record found for 2025-26_T1, class 1100. Consecutive empty: 96\n",
      "No record found for 2025-26_T1, class 1101. Consecutive empty: 97\n",
      "No record found for 2025-26_T1, class 1102. Consecutive empty: 98\n",
      "No record found for 2025-26_T1, class 1103. Consecutive empty: 99\n",
      "No record found for 2025-26_T1, class 1104. Consecutive empty: 100\n",
      "100 consecutive empty records reached for 2025-26_T1, moving on.\n",
      "Scraping completed.\n",
      "Generated CSV file with 12976 valid file paths at scraped_filepaths.csv\n",
      "Process completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set up WebDriver - REMOVED headless mode to allow manual login\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    \n",
    "    try:\n",
    "        # Initialize the driver\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        \n",
    "        # Step 1: Navigate to login page and wait for manual login\n",
    "        driver.get(\"https://boss.intranet.smu.edu.sg/\")\n",
    "        wait_for_manual_login(driver)\n",
    "        \n",
    "        # Step 2: Now that we're logged in, proceed with scraping\n",
    "        # You can optionally run a test scrape first\n",
    "        # test_scrape_class_details(driver)\n",
    "        \n",
    "        # Step 3: Run the main scraping function with the authenticated driver\n",
    "        scrape_and_save_html(driver, '2025-26_T1', '2025-26_T1', 'classTimingsFull')\n",
    "        \n",
    "        # Step 4: Generate CSV with valid file paths\n",
    "        generate_scraped_filepaths_csv('classTimingsFull', 'scraped_filepaths.csv')\n",
    "        \n",
    "    finally:\n",
    "        # Ensure driver is closed properly\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "        print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Extract needed data from all scraped websites**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 20:36:57,966 - INFO - üöÄ Starting Schema Conformance Tests\n",
      "2025-05-23 20:36:57,966 - INFO - ============================================================\n",
      "2025-05-23 20:36:57,967 - INFO - üìä Testing Database Connectivity...\n",
      "2025-05-23 20:36:57,967 - INFO - Testing database connection...\n",
      "2025-05-23 20:36:58,008 - INFO - ‚úÖ Database connection successful\n",
      "2025-05-23 20:36:58,012 - INFO - ‚úÖ Table 'courses': 0 records accessible\n",
      "2025-05-23 20:36:58,016 - INFO - ‚úÖ Table 'classes': 0 records accessible\n",
      "2025-05-23 20:36:58,019 - INFO - ‚úÖ Table 'acad_term': 0 records accessible\n",
      "2025-05-23 20:36:58,023 - INFO - ‚úÖ Table 'class_timing': 0 records accessible\n",
      "2025-05-23 20:36:58,026 - INFO - ‚úÖ Table 'class_exam_timing': 0 records accessible\n",
      "2025-05-23 20:36:58,027 - INFO - üìÅ Loading test HTML files...\n",
      "2025-05-23 20:36:58,821 - INFO - Found 12976 existing files\n",
      "2025-05-23 20:36:58,823 - INFO - Selected 1000 files for testing\n",
      "2025-05-23 20:36:58,823 - INFO - üîß Testing Schema Conformance...\n",
      "2025-05-23 20:36:58,824 - INFO - Testing AfterClassDataExtractor functionality...\n",
      "2025-05-23 20:36:58,824 - INFO - ‚úÖ Successfully imported AfterClassDataExtractor\n",
      "2025-05-23 20:36:58,825 - INFO - ====== WebDriver manager ======\n",
      "2025-05-23 20:37:02,378 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-23 20:37:02,616 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-23 20:37:02,846 - INFO - Driver [C:\\Users\\tanzh\\.wdm\\drivers\\chromedriver\\win64\\136.0.7103.113\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-05-23 20:37:03,896 - INFO - Selenium WebDriver initialized successfully\n",
      "2025-05-23 20:37:03,896 - INFO - ‚úÖ Selenium WebDriver initialized\n",
      "2025-05-23 20:37:03,944 - INFO - Database connection established\n",
      "2025-05-23 20:37:03,945 - INFO - ‚úÖ Extractor database connection successful\n",
      "2025-05-23 20:37:03,945 - INFO - Testing database caching...\n",
      "2025-05-23 20:37:03,947 - INFO - Loaded cached tables: 0 courses, 0 classes\n",
      "2025-05-23 20:37:03,947 - INFO - ‚úÖ Cached tables loaded successfully\n",
      "2025-05-23 20:37:03,948 - INFO - ‚úÖ Courses cache: 0 records\n",
      "2025-05-23 20:37:03,948 - INFO - ‚úÖ Classes cache: 0 records\n",
      "2025-05-23 20:37:03,949 - INFO - Testing file processing with 1000 files...\n",
      "2025-05-23 20:37:03,949 - INFO - Processing file 1/1000: SelectedAcadTerm=2220&SelectedClassNumber=1606.html\n",
      "2025-05-23 20:37:04,032 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2220&SelectedClassNumber=1606.html\n",
      "2025-05-23 20:37:04,032 - INFO - Processing file 2/1000: SelectedAcadTerm=2410&SelectedClassNumber=1933.html\n",
      "2025-05-23 20:37:04,074 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2410&SelectedClassNumber=1933.html\n",
      "2025-05-23 20:37:04,075 - INFO - Processing file 3/1000: SelectedAcadTerm=2120&SelectedClassNumber=1910.html\n",
      "2025-05-23 20:37:04,156 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2120&SelectedClassNumber=1910.html\n",
      "2025-05-23 20:37:04,156 - INFO - Processing file 4/1000: SelectedAcadTerm=2420&SelectedClassNumber=2670.html\n",
      "2025-05-23 20:37:04,217 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2420&SelectedClassNumber=2670.html\n",
      "2025-05-23 20:37:04,218 - INFO - Processing file 5/1000: SelectedAcadTerm=2420&SelectedClassNumber=2684.html\n",
      "2025-05-23 20:37:04,266 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2420&SelectedClassNumber=2684.html\n",
      "2025-05-23 20:37:04,267 - INFO - Processing file 6/1000: SelectedAcadTerm=2310&SelectedClassNumber=2475.html\n",
      "2025-05-23 20:37:04,302 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2310&SelectedClassNumber=2475.html\n",
      "2025-05-23 20:37:04,303 - INFO - Processing file 7/1000: SelectedAcadTerm=2220&SelectedClassNumber=2772.html\n",
      "2025-05-23 20:37:04,335 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2220&SelectedClassNumber=2772.html\n",
      "2025-05-23 20:37:04,336 - INFO - Processing file 8/1000: SelectedAcadTerm=2110&SelectedClassNumber=2167.html\n",
      "2025-05-23 20:37:04,382 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2110&SelectedClassNumber=2167.html\n",
      "2025-05-23 20:37:04,383 - INFO - Processing file 9/1000: SelectedAcadTerm=2110&SelectedClassNumber=1551.html\n",
      "2025-05-23 20:37:04,416 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2110&SelectedClassNumber=1551.html\n",
      "2025-05-23 20:37:04,416 - INFO - Processing file 10/1000: SelectedAcadTerm=2420&SelectedClassNumber=1289.html\n",
      "2025-05-23 20:37:04,453 - WARNING - ‚ö†Ô∏è Failed to process: SelectedAcadTerm=2420&SelectedClassNumber=1289.html\n",
      "2025-05-23 20:37:08,407 - INFO - Processing file 100/1000: SelectedAcadTerm=2210&SelectedClassNumber=2593.html\n",
      "2025-05-23 20:37:08,462 - INFO - üìä Progress: 100/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:12,323 - INFO - Processing file 200/1000: SelectedAcadTerm=2120&SelectedClassNumber=1168.html\n",
      "2025-05-23 20:37:12,356 - INFO - üìä Progress: 200/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:15,980 - INFO - Processing file 300/1000: SelectedAcadTerm=2210&SelectedClassNumber=1593.html\n",
      "2025-05-23 20:37:16,013 - INFO - üìä Progress: 300/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:20,398 - INFO - Processing file 400/1000: SelectedAcadTerm=2220&SelectedClassNumber=1753.html\n",
      "2025-05-23 20:37:20,433 - INFO - üìä Progress: 400/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:24,832 - INFO - Processing file 500/1000: SelectedAcadTerm=2210&SelectedClassNumber=1692.html\n",
      "2025-05-23 20:37:24,869 - INFO - üìä Progress: 500/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:28,529 - INFO - Processing file 600/1000: SelectedAcadTerm=2420&SelectedClassNumber=1657.html\n",
      "2025-05-23 20:37:28,594 - INFO - üìä Progress: 600/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:32,568 - INFO - Processing file 700/1000: SelectedAcadTerm=2310&SelectedClassNumber=1558.html\n",
      "2025-05-23 20:37:32,625 - INFO - üìä Progress: 700/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:38,719 - INFO - Processing file 800/1000: SelectedAcadTerm=2420&SelectedClassNumber=1044.html\n",
      "2025-05-23 20:37:38,770 - INFO - üìä Progress: 800/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:43,470 - INFO - Processing file 900/1000: SelectedAcadTerm=2310&SelectedClassNumber=1486.html\n",
      "2025-05-23 20:37:43,525 - INFO - üìä Progress: 900/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:49,276 - INFO - Processing file 1000/1000: SelectedAcadTerm=2320&SelectedClassNumber=1228.html\n",
      "2025-05-23 20:37:49,315 - INFO - üìä Progress: 1000/1000 files processed (0.0% success rate)\n",
      "2025-05-23 20:37:49,316 - INFO - Testing CSV generation and schema conformance...\n",
      "2025-05-23 20:37:49,323 - INFO - Saved 1000 error records\n",
      "2025-05-23 20:37:49,324 - WARNING - ‚ö†Ô∏è Expected file not generated: courses_updates.csv\n",
      "2025-05-23 20:37:49,325 - WARNING - ‚ö†Ô∏è Expected file not generated: classes_updates.csv\n",
      "2025-05-23 20:37:49,326 - WARNING - ‚ö†Ô∏è Expected file not generated: acad_term.csv\n",
      "2025-05-23 20:37:49,326 - WARNING - ‚ö†Ô∏è Expected file not generated: class_timing.csv\n",
      "2025-05-23 20:37:49,328 - WARNING - ‚ö†Ô∏è Expected file not generated: class_exam_timing.csv\n",
      "2025-05-23 20:37:49,329 - INFO - ‚úÖ Error logging working: 1000 errors logged\n",
      "2025-05-23 20:37:49,342 - INFO - ‚úÖ Error CSV structure valid: 1000 error records\n",
      "2025-05-23 20:37:51,491 - INFO - Selenium WebDriver closed\n",
      "2025-05-23 20:37:51,492 - INFO - Database connection closed\n",
      "2025-05-23 20:37:51,493 - INFO - ‚úÖ Extractor cleanup completed\n",
      "2025-05-23 20:37:51,494 - INFO - ============================================================\n",
      "2025-05-23 20:37:51,495 - INFO - üìä SCHEMA CONFORMANCE TEST RESULTS\n",
      "2025-05-23 20:37:51,495 - INFO - ============================================================\n",
      "2025-05-23 20:37:51,496 - INFO - üóÑÔ∏è DATABASE CONNECTIVITY:\n",
      "2025-05-23 20:37:51,496 - INFO -   Connection Status: ‚úÖ PASSED\n",
      "2025-05-23 20:37:51,498 - INFO -   courses: ‚úÖ 0 records\n",
      "2025-05-23 20:37:51,498 - INFO -   classes: ‚úÖ 0 records\n",
      "2025-05-23 20:37:51,499 - INFO -   acad_term: ‚úÖ 0 records\n",
      "2025-05-23 20:37:51,500 - INFO -   class_timing: ‚úÖ 0 records\n",
      "2025-05-23 20:37:51,500 - INFO -   class_exam_timing: ‚úÖ 0 records\n",
      "2025-05-23 20:37:51,501 - INFO - \n",
      "üîß FILE PROCESSING:\n",
      "2025-05-23 20:37:51,501 - INFO -   Processed: 0/1000 files (0.0% success)\n",
      "2025-05-23 20:37:51,502 - INFO - \n",
      "üìÑ CSV GENERATION & SCHEMA VALIDATION:\n",
      "2025-05-23 20:37:51,502 - INFO -   courses_updates.csv: ‚ùå Not generated\n",
      "2025-05-23 20:37:51,503 - INFO -   classes_updates.csv: ‚ùå Not generated\n",
      "2025-05-23 20:37:51,503 - INFO -   acad_term.csv: ‚ùå Not generated\n",
      "2025-05-23 20:37:51,504 - INFO -   class_timing.csv: ‚ùå Not generated\n",
      "2025-05-23 20:37:51,505 - INFO -   class_exam_timing.csv: ‚ùå Not generated\n",
      "2025-05-23 20:37:51,505 - INFO - \n",
      "‚úÖ No critical errors encountered!\n",
      "2025-05-23 20:37:51,508 - INFO - \n",
      "üìã RECOMMENDATIONS:\n",
      "2025-05-23 20:37:51,509 - INFO -   ‚Ä¢ Review processing_errors.csv for parsing issues\n",
      "2025-05-23 20:37:51,509 - INFO - ============================================================\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import logging\n",
    "import random\n",
    "import importlib.util\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SchemaConformanceTestRunner:\n",
    "    def __init__(self):\n",
    "        self.test_results = {\n",
    "            'database_connection': False,\n",
    "            'table_access': {},\n",
    "            'scraping_success_count': 0,\n",
    "            'scraping_error_count': 0,\n",
    "            'total_files_tested': 0,\n",
    "            'missing_files_count': 0,\n",
    "            'csv_generation': {},\n",
    "            'schema_validation': {},\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        # Test database configuration for localhost Supabase\n",
    "        self.test_db_config = {\n",
    "            'host': 'localhost',\n",
    "            'database': 'postgres', \n",
    "            'user': 'postgres',\n",
    "            'password': 'changeme',\n",
    "            'port': 5433\n",
    "        }\n",
    "        \n",
    "        # Expected schema structure based on Prisma schema\n",
    "        self.expected_schemas = {\n",
    "            'courses_updates.csv': {\n",
    "                'required_columns': ['code', 'course_area', 'enrolment_requirements'],\n",
    "                'table_name': 'courses'\n",
    "            },\n",
    "            'classes_updates.csv': {\n",
    "                'required_columns': ['course_code', 'section', 'acad_term_id', 'grading_basis', 'course_outline_url'],\n",
    "                'table_name': 'classes'\n",
    "            },\n",
    "            'acad_term.csv': {\n",
    "                'required_columns': ['id', 'acad_year_start', 'acad_year_end', 'term', 'boss_id', 'start_dt', 'end_dt'],\n",
    "                'table_name': 'acad_term'\n",
    "            },\n",
    "            'class_timing.csv': {\n",
    "                'required_columns': ['class_id', 'start_date', 'end_date', 'day_of_week', 'start_time', 'end_time', 'venue'],\n",
    "                'table_name': 'class_timing'\n",
    "            },\n",
    "            'class_exam_timing.csv': {\n",
    "                'required_columns': ['class_id', 'date', 'day_of_week', 'start_time', 'end_time', 'venue'],\n",
    "                'table_name': 'class_exam_timing'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def test_database_connection(self):\n",
    "        \"\"\"Test connection to localhost Supabase database and table access\"\"\"\n",
    "        logger.info(\"Testing database connection...\")\n",
    "        try:\n",
    "            connection = psycopg2.connect(\n",
    "                host=self.test_db_config['host'],\n",
    "                database=self.test_db_config['database'],\n",
    "                user=self.test_db_config['user'],\n",
    "                password=self.test_db_config['password'],\n",
    "                port=self.test_db_config['port']\n",
    "            )\n",
    "            \n",
    "            cursor = connection.cursor()\n",
    "            cursor.execute('SELECT 1')\n",
    "            result = cursor.fetchone()\n",
    "            \n",
    "            if result == (1,):\n",
    "                logger.info(\"‚úÖ Database connection successful\")\n",
    "                self.test_results['database_connection'] = True\n",
    "                self.test_table_access(cursor)\n",
    "                \n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Database connection failed: {e}\")\n",
    "            self.test_results['errors'].append(f\"Database connection error: {e}\")\n",
    "            self.test_results['database_connection'] = False\n",
    "\n",
    "    def test_table_access(self, cursor):\n",
    "        \"\"\"Test access to required database tables\"\"\"\n",
    "        required_tables = ['courses', 'classes', 'acad_term', 'class_timing', 'class_exam_timing']\n",
    "        \n",
    "        for table in required_tables:\n",
    "            try:\n",
    "                cursor.connection.rollback()\n",
    "                cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "                count = cursor.fetchone()[0]\n",
    "                \n",
    "                self.test_results['table_access'][table] = {\n",
    "                    'accessible': True,\n",
    "                    'record_count': count\n",
    "                }\n",
    "                logger.info(f\"‚úÖ Table '{table}': {count} records accessible\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                cursor.connection.rollback()\n",
    "                logger.error(f\"‚ùå Table '{table}' access failed: {e}\")\n",
    "                self.test_results['table_access'][table] = {\n",
    "                    'accessible': False,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "\n",
    "    def load_actual_filepaths(self, sample_size=20):\n",
    "        \"\"\"Load actual filepaths from scraped_filepaths.csv\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists('scraped_filepaths.csv'):\n",
    "                logger.error(\"‚ùå scraped_filepaths.csv not found\")\n",
    "                return []\n",
    "            \n",
    "            filepaths_df = pd.read_csv('scraped_filepaths.csv')\n",
    "            filepath_column = 'Filepath' if 'Filepath' in filepaths_df.columns else 'filepath'\n",
    "            \n",
    "            existing_files = []\n",
    "            for filepath in filepaths_df[filepath_column]:\n",
    "                if pd.notna(filepath) and os.path.exists(str(filepath).strip()):\n",
    "                    existing_files.append(str(filepath).strip())\n",
    "            \n",
    "            logger.info(f\"Found {len(existing_files)} existing files\")\n",
    "            \n",
    "            if len(existing_files) > sample_size:\n",
    "                selected_files = random.sample(existing_files, sample_size)\n",
    "            else:\n",
    "                selected_files = existing_files\n",
    "                \n",
    "            logger.info(f\"Selected {len(selected_files)} files for testing\")\n",
    "            return selected_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading filepaths: {e}\")\n",
    "            return []\n",
    "\n",
    "    def import_extractor_class(self):\n",
    "        \"\"\"Import AfterClassDataExtractor class\"\"\"\n",
    "        try:\n",
    "            # In Jupyter notebook, try to access the globally defined class\n",
    "            if 'AfterClassDataExtractor' in globals():\n",
    "                return globals()['AfterClassDataExtractor']\n",
    "            else:\n",
    "                raise ImportError(\"AfterClassDataExtractor not found in global scope\")\n",
    "        except Exception as e:\n",
    "            raise ImportError(f\"Could not access AfterClassDataExtractor: {e}\")\n",
    "\n",
    "    def test_extractor_functionality(self, test_filepaths):\n",
    "        \"\"\"Test the AfterClassDataExtractor with schema validation\"\"\"\n",
    "        logger.info(\"Testing AfterClassDataExtractor functionality...\")\n",
    "        \n",
    "        try:\n",
    "            AfterClassDataExtractor = self.import_extractor_class()\n",
    "            logger.info(\"‚úÖ Successfully imported AfterClassDataExtractor\")\n",
    "            \n",
    "            extractor = AfterClassDataExtractor(self.test_db_config)\n",
    "            extractor.setup_selenium_driver()\n",
    "            logger.info(\"‚úÖ Selenium WebDriver initialized\")\n",
    "            \n",
    "            extractor.connect_database()\n",
    "            logger.info(\"‚úÖ Extractor database connection successful\")\n",
    "            \n",
    "            # Test caching functionality\n",
    "            self.test_caching_functionality(extractor)\n",
    "            \n",
    "            # Test file processing with schema validation\n",
    "            self.test_file_processing_with_validation(extractor, test_filepaths)\n",
    "            \n",
    "            # Test CSV generation and schema conformance\n",
    "            self.test_csv_generation_and_schema(extractor)\n",
    "            \n",
    "            extractor.cleanup()\n",
    "            logger.info(\"‚úÖ Extractor cleanup completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Extractor functionality test failed: {e}\"\n",
    "            logger.error(f\"‚ùå {error_msg}\")\n",
    "            self.test_results['errors'].append(error_msg)\n",
    "\n",
    "    def test_caching_functionality(self, extractor):\n",
    "        \"\"\"Test the database caching functionality\"\"\"\n",
    "        logger.info(\"Testing database caching...\")\n",
    "        try:\n",
    "            if not extractor.load_cached_tables():\n",
    "                logger.info(\"Cache not found, downloading from database...\")\n",
    "                extractor.download_and_cache_tables()\n",
    "                logger.info(\"‚úÖ Database tables downloaded and cached\")\n",
    "            else:\n",
    "                logger.info(\"‚úÖ Cached tables loaded successfully\")\n",
    "            \n",
    "            if hasattr(extractor, 'courses_df') and extractor.courses_df is not None:\n",
    "                logger.info(f\"‚úÖ Courses cache: {len(extractor.courses_df)} records\")\n",
    "            if hasattr(extractor, 'classes_df') and extractor.classes_df is not None:\n",
    "                logger.info(f\"‚úÖ Classes cache: {len(extractor.classes_df)} records\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Caching functionality failed: {e}\")\n",
    "            self.test_results['errors'].append(f\"Caching error: {e}\")\n",
    "\n",
    "    def test_file_processing_with_validation(self, extractor, test_filepaths):\n",
    "        \"\"\"Test processing files with data validation\"\"\"\n",
    "        logger.info(f\"Testing file processing with {len(test_filepaths)} files...\")\n",
    "        \n",
    "        # Process ALL files instead of just first 5\n",
    "        for i, filepath in enumerate(test_filepaths, 1):\n",
    "            # Log progress every 100 files\n",
    "            if i % 100 == 0 or i <= 10:  # Show first 10 individual files, then every 100\n",
    "                logger.info(f\"Processing file {i}/{len(test_filepaths)}: {os.path.basename(filepath)}\")\n",
    "            \n",
    "            try:\n",
    "                success = extractor.process_html_file(filepath)\n",
    "                if success:\n",
    "                    self.test_results['scraping_success_count'] += 1\n",
    "                    if i <= 10:  # Only log individual success for first 10 files\n",
    "                        logger.info(f\"‚úÖ Successfully processed: {os.path.basename(filepath)}\")\n",
    "                    \n",
    "                    # Validate data extraction (only for first 10 files to avoid spam)\n",
    "                    if i <= 10:\n",
    "                        self.validate_extracted_data(extractor, filepath)\n",
    "                    \n",
    "                else:\n",
    "                    self.test_results['scraping_error_count'] += 1\n",
    "                    if i <= 10:  # Only log individual failures for first 10 files\n",
    "                        logger.warning(f\"‚ö†Ô∏è Failed to process: {os.path.basename(filepath)}\")\n",
    "                    \n",
    "                self.test_results['total_files_tested'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.test_results['scraping_error_count'] += 1\n",
    "                self.test_results['total_files_tested'] += 1\n",
    "                error_msg = f\"Error processing {os.path.basename(filepath)}: {e}\"\n",
    "                if i <= 10:  # Only log individual errors for first 10 files\n",
    "                    logger.error(f\"‚ùå {error_msg}\")\n",
    "                self.test_results['errors'].append(error_msg)\n",
    "            \n",
    "            # Progress update every 100 files\n",
    "            if i % 100 == 0:\n",
    "                success_rate = (self.test_results['scraping_success_count'] / self.test_results['total_files_tested']) * 100\n",
    "                logger.info(f\"üìä Progress: {i}/{len(test_filepaths)} files processed ({success_rate:.1f}% success rate)\")\n",
    "\n",
    "    def validate_extracted_data(self, extractor, filepath):\n",
    "        \"\"\"Validate that extracted data conforms to schema requirements\"\"\"\n",
    "        try:\n",
    "            # Validate academic term parsing\n",
    "            if hasattr(extractor, 'acad_term') and extractor.acad_term:\n",
    "                latest_term = extractor.acad_term[-1]\n",
    "                self.validate_acad_term_structure(latest_term, filepath)\n",
    "            \n",
    "            # Validate grading basis mapping\n",
    "            if hasattr(extractor, 'classes_updates') and extractor.classes_updates:\n",
    "                latest_class = extractor.classes_updates[-1]\n",
    "                self.validate_grading_basis(latest_class, filepath)\n",
    "            \n",
    "            # Validate timing data structure\n",
    "            if hasattr(extractor, 'class_timing') and extractor.class_timing:\n",
    "                latest_timing = extractor.class_timing[-1]\n",
    "                self.validate_timing_structure(latest_timing, filepath)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Data validation failed for {os.path.basename(filepath)}: {e}\")\n",
    "\n",
    "    def validate_acad_term_structure(self, term_data, filepath):\n",
    "        \"\"\"Validate academic term data structure\"\"\"\n",
    "        required_fields = ['id', 'acad_year_start', 'acad_year_end', 'term', 'boss_id', 'start_dt', 'end_dt']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in term_data:\n",
    "                raise ValueError(f\"Missing required field '{field}' in acad_term\")\n",
    "        \n",
    "        # Validate ID format (should be like AY202122T1)\n",
    "        if not re.match(r'^AY\\d{6}T[12]$|^AY\\d{6}T3[AB]$', term_data['id']):\n",
    "            raise ValueError(f\"Invalid acad_term ID format: {term_data['id']}\")\n",
    "        \n",
    "        # Validate term values\n",
    "        if term_data['term'] not in ['1', '2', '3A', '3B']:\n",
    "            raise ValueError(f\"Invalid term value: {term_data['term']}\")\n",
    "        \n",
    "        logger.debug(f\"‚úÖ Academic term validation passed for {os.path.basename(filepath)}\")\n",
    "\n",
    "    def validate_grading_basis(self, class_data, filepath):\n",
    "        \"\"\"Validate grading basis conforms to enum\"\"\"\n",
    "        if 'grading_basis' in class_data and class_data['grading_basis'] is not None:\n",
    "            valid_values = ['GRADED', 'PASS_FAIL', 'NA']\n",
    "            if class_data['grading_basis'] not in valid_values:\n",
    "                raise ValueError(f\"Invalid grading_basis: {class_data['grading_basis']}\")\n",
    "        \n",
    "        logger.debug(f\"‚úÖ Grading basis validation passed for {os.path.basename(filepath)}\")\n",
    "\n",
    "    def validate_timing_structure(self, timing_data, filepath):\n",
    "        \"\"\"Validate timing data structure\"\"\"\n",
    "        required_fields = ['class_id', 'start_date', 'end_date', 'day_of_week', 'start_time', 'end_time', 'venue']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in timing_data:\n",
    "                raise ValueError(f\"Missing required field '{field}' in class_timing\")\n",
    "        \n",
    "        # Validate day_of_week format (should be 3 characters)\n",
    "        if len(str(timing_data['day_of_week'])) > 3:\n",
    "            raise ValueError(f\"day_of_week too long: {timing_data['day_of_week']}\")\n",
    "        \n",
    "        logger.debug(f\"‚úÖ Timing structure validation passed for {os.path.basename(filepath)}\")\n",
    "\n",
    "    def test_csv_generation_and_schema(self, extractor):\n",
    "        \"\"\"Test CSV generation and validate schema conformance\"\"\"\n",
    "        logger.info(\"Testing CSV generation and schema conformance...\")\n",
    "        try:\n",
    "            output_dir = 'test_output'\n",
    "            extractor.save_csv_files(output_dir)\n",
    "            \n",
    "            for csv_file, schema_info in self.expected_schemas.items():\n",
    "                filepath = os.path.join(output_dir, csv_file)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    self.validate_csv_schema(filepath, csv_file, schema_info)\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Expected file not generated: {csv_file}\")\n",
    "                    self.test_results['csv_generation'][csv_file] = {'generated': False}\n",
    "            \n",
    "            # Validate error logging\n",
    "            self.validate_error_logging(extractor, output_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"CSV generation test failed: {e}\"\n",
    "            logger.error(f\"‚ùå {error_msg}\")\n",
    "            self.test_results['errors'].append(error_msg)\n",
    "\n",
    "    def validate_csv_schema(self, filepath, csv_file, schema_info):\n",
    "        \"\"\"Validate CSV file against expected schema\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            # Check required columns\n",
    "            missing_columns = []\n",
    "            for col in schema_info['required_columns']:\n",
    "                if col not in df.columns:\n",
    "                    missing_columns.append(col)\n",
    "            \n",
    "            if missing_columns:\n",
    "                error_msg = f\"{csv_file}: Missing columns {missing_columns}\"\n",
    "                logger.error(f\"‚ùå {error_msg}\")\n",
    "                self.test_results['schema_validation'][csv_file] = {\n",
    "                    'valid': False,\n",
    "                    'error': error_msg\n",
    "                }\n",
    "                return\n",
    "            \n",
    "            # Validate data types and formats\n",
    "            validation_result = self.validate_data_types(df, csv_file)\n",
    "            \n",
    "            self.test_results['csv_generation'][csv_file] = {\n",
    "                'generated': True,\n",
    "                'record_count': len(df),\n",
    "                'columns': list(df.columns)\n",
    "            }\n",
    "            \n",
    "            self.test_results['schema_validation'][csv_file] = validation_result\n",
    "            \n",
    "            if validation_result['valid']:\n",
    "                logger.info(f\"‚úÖ {csv_file}: {len(df)} records, schema valid\")\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è {csv_file}: Schema validation issues - {validation_result['warnings']}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error validating {csv_file}: {e}\")\n",
    "            self.test_results['schema_validation'][csv_file] = {\n",
    "                'valid': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def validate_data_types(self, df, csv_file):\n",
    "        \"\"\"Validate data types in CSV\"\"\"\n",
    "        warnings = []\n",
    "        \n",
    "        if csv_file == 'acad_term.csv':\n",
    "            # Validate acad_term specific formats\n",
    "            for idx, row in df.head(3).iterrows():\n",
    "                if not re.match(r'^AY\\d{6}T[12]$|^AY\\d{6}T3[AB]$', str(row.get('id', ''))):\n",
    "                    warnings.append(f\"Invalid ID format in row {idx}: {row.get('id')}\")\n",
    "                \n",
    "                if row.get('term') not in ['1', '2', '3A', '3B']:\n",
    "                    warnings.append(f\"Invalid term value in row {idx}: {row.get('term')}\")\n",
    "        \n",
    "        elif csv_file == 'classes_updates.csv':\n",
    "            # Validate classes specific formats\n",
    "            for idx, row in df.head(3).iterrows():\n",
    "                grading = row.get('grading_basis')\n",
    "                if pd.notna(grading) and grading not in ['GRADED', 'PASS_FAIL', 'NA']:\n",
    "                    warnings.append(f\"Invalid grading_basis in row {idx}: {grading}\")\n",
    "        \n",
    "        elif csv_file == 'class_timing.csv':\n",
    "            # Validate timing specific formats\n",
    "            for idx, row in df.head(3).iterrows():\n",
    "                day_of_week = str(row.get('day_of_week', ''))\n",
    "                if len(day_of_week) > 3:\n",
    "                    warnings.append(f\"day_of_week too long in row {idx}: {day_of_week}\")\n",
    "        \n",
    "        return {\n",
    "            'valid': len(warnings) == 0,\n",
    "            'warnings': warnings\n",
    "        }\n",
    "\n",
    "    def validate_error_logging(self, extractor, output_dir):\n",
    "        \"\"\"Validate error logging functionality\"\"\"\n",
    "        try:\n",
    "            error_file = os.path.join(output_dir, 'processing_errors.csv')\n",
    "            \n",
    "            if hasattr(extractor, 'errors') and extractor.errors:\n",
    "                logger.info(f\"‚úÖ Error logging working: {len(extractor.errors)} errors logged\")\n",
    "                \n",
    "                if os.path.exists(error_file):\n",
    "                    error_df = pd.read_csv(error_file)\n",
    "                    expected_columns = ['filepath', 'error', 'type']\n",
    "                    \n",
    "                    missing_error_cols = [col for col in expected_columns if col not in error_df.columns]\n",
    "                    if missing_error_cols:\n",
    "                        logger.warning(f\"‚ö†Ô∏è Error CSV missing columns: {missing_error_cols}\")\n",
    "                    else:\n",
    "                        logger.info(f\"‚úÖ Error CSV structure valid: {len(error_df)} error records\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Error validation failed: {e}\")\n",
    "\n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run comprehensive schema conformance test suite\"\"\"\n",
    "        logger.info(\"üöÄ Starting Schema Conformance Tests\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Test 1: Database Connection\n",
    "        logger.info(\"üìä Testing Database Connectivity...\")\n",
    "        self.test_database_connection()\n",
    "        \n",
    "        # Test 2: Load test files\n",
    "        logger.info(\"üìÅ Loading test HTML files...\")\n",
    "        test_filepaths = self.load_actual_filepaths(1000)\n",
    "        \n",
    "        if not test_filepaths:\n",
    "            logger.error(\"‚ùå No valid HTML files found for testing\")\n",
    "            self.generate_test_report()\n",
    "            return\n",
    "        \n",
    "        # Test 3: Schema conformance testing\n",
    "        logger.info(\"üîß Testing Schema Conformance...\")\n",
    "        self.test_extractor_functionality(test_filepaths)\n",
    "        \n",
    "        # Generate report\n",
    "        self.generate_test_report()\n",
    "\n",
    "    def generate_test_report(self):\n",
    "        \"\"\"Generate comprehensive test report\"\"\"\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"üìä SCHEMA CONFORMANCE TEST RESULTS\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Database connectivity\n",
    "        logger.info(\"üóÑÔ∏è DATABASE CONNECTIVITY:\")\n",
    "        db_status = \"‚úÖ PASSED\" if self.test_results['database_connection'] else \"‚ùå FAILED\"\n",
    "        logger.info(f\"  Connection Status: {db_status}\")\n",
    "        \n",
    "        for table, result in self.test_results['table_access'].items():\n",
    "            if result.get('accessible'):\n",
    "                logger.info(f\"  {table}: ‚úÖ {result['record_count']} records\")\n",
    "            else:\n",
    "                logger.info(f\"  {table}: ‚ùå {result.get('error', 'Access failed')}\")\n",
    "        \n",
    "        # File processing\n",
    "        logger.info(\"\\nüîß FILE PROCESSING:\")\n",
    "        total_tested = self.test_results['total_files_tested']\n",
    "        success_count = self.test_results['scraping_success_count']\n",
    "        error_count = self.test_results['scraping_error_count']\n",
    "        \n",
    "        if total_tested > 0:\n",
    "            success_rate = (success_count / total_tested) * 100\n",
    "            logger.info(f\"  Processed: {success_count}/{total_tested} files ({success_rate:.1f}% success)\")\n",
    "        else:\n",
    "            logger.info(\"  No files were processed\")\n",
    "        \n",
    "        # CSV generation and schema validation\n",
    "        logger.info(\"\\nüìÑ CSV GENERATION & SCHEMA VALIDATION:\")\n",
    "        for csv_file in self.expected_schemas.keys():\n",
    "            csv_result = self.test_results['csv_generation'].get(csv_file, {})\n",
    "            schema_result = self.test_results['schema_validation'].get(csv_file, {})\n",
    "            \n",
    "            if csv_result.get('generated'):\n",
    "                record_count = csv_result.get('record_count', 0)\n",
    "                if schema_result.get('valid'):\n",
    "                    logger.info(f\"  {csv_file}: ‚úÖ {record_count} records, schema valid\")\n",
    "                else:\n",
    "                    warnings = schema_result.get('warnings', [])\n",
    "                    logger.warning(f\"  {csv_file}: ‚ö†Ô∏è {record_count} records, schema issues: {len(warnings)} warnings\")\n",
    "            else:\n",
    "                logger.info(f\"  {csv_file}: ‚ùå Not generated\")\n",
    "        \n",
    "        # Error summary\n",
    "        if self.test_results['errors']:\n",
    "            logger.info(f\"\\n‚ö†Ô∏è ERRORS ENCOUNTERED ({len(self.test_results['errors'])}):\")\n",
    "            for i, error in enumerate(self.test_results['errors'][:3], 1):\n",
    "                logger.info(f\"  {i}. {error}\")\n",
    "            if len(self.test_results['errors']) > 3:\n",
    "                logger.info(f\"  ... and {len(self.test_results['errors']) - 3} more errors\")\n",
    "        else:\n",
    "            logger.info(\"\\n‚úÖ No critical errors encountered!\")\n",
    "        \n",
    "        # Recommendations\n",
    "        logger.info(\"\\nüìã RECOMMENDATIONS:\")\n",
    "        \n",
    "        schema_issues = [k for k, v in self.test_results['schema_validation'].items() if not v.get('valid')]\n",
    "        if schema_issues:\n",
    "            logger.info(f\"  ‚Ä¢ Fix schema issues in: {', '.join(schema_issues)}\")\n",
    "            logger.info(\"  ‚Ä¢ Check column names match Prisma schema exactly\")\n",
    "            logger.info(\"  ‚Ä¢ Validate data type conversions and enum mappings\")\n",
    "        \n",
    "        if error_count > 0:\n",
    "            logger.info(\"  ‚Ä¢ Review processing_errors.csv for parsing issues\")\n",
    "        \n",
    "        if success_count > 0:\n",
    "            logger.info(\"  ‚Ä¢ ‚úÖ Schema conformance testing complete!\")\n",
    "            logger.info(\"  ‚Ä¢ Check 'test_output' folder for generated CSV files\")\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the schema conformance test suite\n",
    "    test_runner = SchemaConformanceTestRunner()\n",
    "    test_runner.run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTUAL SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AfterClassDataExtractor:\n",
    "    def __init__(self, db_config: Dict[str, str]):\n",
    "        \"\"\"Initialize with database configuration for Supabase\"\"\"\n",
    "        self.db_config = db_config\n",
    "        self.connection = None\n",
    "        self.driver = None\n",
    "        \n",
    "        # Local cache for courses and classes\n",
    "        self.courses_df = None\n",
    "        self.classes_df = None\n",
    "        self.courses_cache = {}  # Cache for course code to UUID mapping\n",
    "        self.classes_cache = {}  # Cache for class lookups\n",
    "        \n",
    "        # CSV data storage\n",
    "        self.courses_updates = []\n",
    "        self.classes_updates = []\n",
    "        self.acad_term = []\n",
    "        self.class_timing = []\n",
    "        self.class_exam_timing = []\n",
    "        self.errors = []\n",
    "\n",
    "    def setup_selenium_driver(self):\n",
    "        \"\"\"Set up Selenium WebDriver for local file access\"\"\"\n",
    "        try:\n",
    "            options = Options()\n",
    "            options.add_argument('--no-sandbox')\n",
    "            options.add_argument('--disable-dev-shm-usage')\n",
    "            options.add_argument('--headless')  # Run in headless mode for efficiency\n",
    "            options.add_argument('--disable-gpu')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=options)\n",
    "            logger.info(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Selenium WebDriver: {e}\")\n",
    "            raise\n",
    "\n",
    "    def connect_database(self):\n",
    "        \"\"\"Connect to Supabase PostgreSQL database\"\"\"\n",
    "        try:\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=self.db_config['host'],\n",
    "                database=self.db_config['database'],\n",
    "                user=self.db_config['user'],\n",
    "                password=self.db_config['password'],\n",
    "                port=self.db_config.get('port', 5432)\n",
    "            )\n",
    "            logger.info(\"Database connection established\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Database connection failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def download_and_cache_tables(self, cache_dir: str = 'db_cache'):\n",
    "        \"\"\"Download entire courses and classes tables and cache locally\"\"\"\n",
    "        try:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "            \n",
    "            # Download courses table - USE ACTUAL DATABASE COLUMN NAMES from @map\n",
    "            logger.info(\"Downloading courses table...\")\n",
    "            courses_query = \"\"\"\n",
    "                SELECT id, code, name, description, credit_units, \n",
    "                       belong_to_university, belong_to_faculty, \n",
    "                       course_area, enrolment_requirements, created_at, updated_at\n",
    "                FROM courses\n",
    "            \"\"\"\n",
    "            self.courses_df = pd.read_sql_query(courses_query, self.connection)\n",
    "            \n",
    "            # Save to cache files\n",
    "            courses_cache_file = os.path.join(cache_dir, 'courses_cache.pkl')\n",
    "            self.courses_df.to_pickle(courses_cache_file)\n",
    "            self.courses_df.to_csv(os.path.join(cache_dir, 'courses_cache.csv'), index=False)\n",
    "            \n",
    "            logger.info(f\"Downloaded {len(self.courses_df)} courses\")\n",
    "            \n",
    "            # Download classes table - USE ACTUAL DATABASE COLUMN NAMES from @map\n",
    "            logger.info(\"Downloading classes table...\")\n",
    "            classes_query = \"\"\"\n",
    "                SELECT id, section, course_id, professor_id, acad_term_id,\n",
    "                       grading_basis, course_outline_url, boss_id, created_at, updated_at\n",
    "                FROM classes\n",
    "            \"\"\"\n",
    "            self.classes_df = pd.read_sql_query(classes_query, self.connection)\n",
    "            \n",
    "            # Save to cache files\n",
    "            classes_cache_file = os.path.join(cache_dir, 'classes_cache.pkl')\n",
    "            self.classes_df.to_pickle(classes_cache_file)\n",
    "            self.classes_df.to_csv(os.path.join(cache_dir, 'classes_cache.csv'), index=False)\n",
    "            \n",
    "            logger.info(f\"Downloaded {len(self.classes_df)} classes\")\n",
    "            \n",
    "            # Build lookup caches\n",
    "            self._build_lookup_caches()\n",
    "            \n",
    "            logger.info(\"Database tables cached successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading and caching tables: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_cached_tables(self, cache_dir: str = 'db_cache'):\n",
    "        \"\"\"Load cached tables from local files\"\"\"\n",
    "        try:\n",
    "            courses_cache_file = os.path.join(cache_dir, 'courses_cache.pkl')\n",
    "            classes_cache_file = os.path.join(cache_dir, 'classes_cache.pkl')\n",
    "            \n",
    "            if os.path.exists(courses_cache_file) and os.path.exists(classes_cache_file):\n",
    "                self.courses_df = pd.read_pickle(courses_cache_file)\n",
    "                self.classes_df = pd.read_pickle(classes_cache_file)\n",
    "                \n",
    "                self._build_lookup_caches()\n",
    "                \n",
    "                logger.info(f\"Loaded cached tables: {len(self.courses_df)} courses, {len(self.classes_df)} classes\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.info(\"Cache files not found, will download from database\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading cached tables: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _build_lookup_caches(self):\n",
    "        \"\"\"Build lookup caches from DataFrames\"\"\"\n",
    "        # Build course code to ID mapping\n",
    "        for _, row in self.courses_df.iterrows():\n",
    "            self.courses_cache[row['code']] = row['id']\n",
    "        \n",
    "        # Build class lookup cache (course_id + section + acad_term_id -> class_id)\n",
    "        for _, row in self.classes_df.iterrows():\n",
    "            if pd.notna(row['acad_term_id']) and pd.notna(row['section']):\n",
    "                cache_key = f\"{row['course_id']}_{row['section']}_{row['acad_term_id']}\"\n",
    "                self.classes_cache[cache_key] = row['id']\n",
    "\n",
    "    def get_course_id_by_code(self, course_code: str) -> Optional[str]:\n",
    "        \"\"\"Get course UUID by course code using local cache\"\"\"\n",
    "        return self.courses_cache.get(course_code)\n",
    "\n",
    "    def get_class_id(self, course_id: str, section: str, acad_term_id: str) -> Optional[int]:\n",
    "        \"\"\"Get class ID using local cache\"\"\"\n",
    "        cache_key = f\"{course_id}_{section}_{acad_term_id}\"\n",
    "        return self.classes_cache.get(cache_key)\n",
    "\n",
    "    def load_html_file(self, filepath: str) -> bool:\n",
    "        \"\"\"Load HTML file using Selenium\"\"\"\n",
    "        try:\n",
    "            # Convert to absolute path and use file:// protocol\n",
    "            html_file = Path(filepath).resolve()\n",
    "            file_url = html_file.as_uri()\n",
    "            \n",
    "            self.driver.get(file_url)\n",
    "            logger.debug(f\"Loaded HTML file: {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading HTML file {filepath}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def parse_acad_term(self, term_text: str) -> Dict[str, any]:\n",
    "        \"\"\"Parse academic term text and return structured data\"\"\"\n",
    "        try:\n",
    "            # Examples: \"2021-22 August Term\", \"2021-22 Session 1\", \"2021-22 Session 2\"\n",
    "            pattern = r'(\\d{4})-(\\d{2})\\s+(.*)'\n",
    "            match = re.search(pattern, term_text)\n",
    "            \n",
    "            if not match:\n",
    "                raise ValueError(f\"Cannot parse term: {term_text}\")\n",
    "            \n",
    "            start_year = int(match.group(1))\n",
    "            end_year_short = int(match.group(2))\n",
    "            term_desc = match.group(3).lower()\n",
    "            \n",
    "            # Convert 2-digit year to 4-digit\n",
    "            if end_year_short < 50:  # Assuming years after 2000\n",
    "                end_year = 2000 + end_year_short\n",
    "            else:\n",
    "                end_year = 1900 + end_year_short\n",
    "            \n",
    "            # Determine term code - match schema requirements\n",
    "            if 'august' in term_desc or 'session 1' in term_desc or 'term 1' in term_desc:\n",
    "                term_code = '1'\n",
    "            elif 'january' in term_desc or 'session 2' in term_desc or 'term 2' in term_desc:\n",
    "                term_code = '2'\n",
    "            elif '3a' in term_desc:\n",
    "                term_code = '3A'\n",
    "            elif '3b' in term_desc:\n",
    "                term_code = '3B'\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot determine term code from: {term_desc}\")\n",
    "            \n",
    "            acad_term_id = f\"AY{start_year}{end_year_short:02d}T{term_code}\"\n",
    "            \n",
    "            return {\n",
    "                'id': acad_term_id,\n",
    "                'acad_year_start': start_year,\n",
    "                'acad_year_end': end_year,\n",
    "                'term': term_code,\n",
    "                'term_text': term_text\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing academic term '{term_text}': {e}\")\n",
    "            return None\n",
    "\n",
    "    def parse_date_range(self, date_text: str) -> Tuple[Optional[datetime], Optional[datetime]]:\n",
    "        \"\"\"Parse date range text and return start and end dates\"\"\"\n",
    "        try:\n",
    "            # Example: \"23-Aug-2021 to 14-Nov-2021\"\n",
    "            pattern = r'(\\d{1,2}-\\w{3}-\\d{4})\\s+to\\s+(\\d{1,2}-\\w{3}-\\d{4})'\n",
    "            match = re.search(pattern, date_text)\n",
    "            \n",
    "            if not match:\n",
    "                raise ValueError(f\"Cannot parse date range: {date_text}\")\n",
    "            \n",
    "            start_date_str = match.group(1)\n",
    "            end_date_str = match.group(2)\n",
    "            \n",
    "            start_date = datetime.strptime(start_date_str, '%d-%b-%Y')\n",
    "            end_date = datetime.strptime(end_date_str, '%d-%b-%Y')\n",
    "            \n",
    "            return start_date, end_date\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing date range '{date_text}': {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def parse_single_date(self, date_text: str) -> Optional[datetime]:\n",
    "        \"\"\"Parse single date text\"\"\"\n",
    "        try:\n",
    "            return datetime.strptime(date_text, '%d-%b-%Y')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing date '{date_text}': {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_course_outline_url(self) -> Optional[str]:\n",
    "        \"\"\"Extract course outline URL from HTML using Selenium\"\"\"\n",
    "        try:\n",
    "            course_outline_img = self.driver.find_element(By.ID, 'imgCourseOutline')\n",
    "            onclick_text = course_outline_img.get_attribute('onclick')\n",
    "            if onclick_text:\n",
    "                # Extract URL from: window.open('URL','','toolbar=no, width=700, resizable=yes')\n",
    "                url_match = re.search(r\"window\\.open\\('([^']+)'\", onclick_text)\n",
    "                if url_match:\n",
    "                    return url_match.group(1)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Course outline URL not found or error: {e}\")\n",
    "        return None\n",
    "\n",
    "    def extract_boss_id_from_filepath(self, filepath: str) -> Optional[int]:\n",
    "        \"\"\"Extract BOSS ID from filepath\"\"\"\n",
    "        try:\n",
    "            # Example: \"SelectedAcadTerm=2110&SelectedClassNumber=1002.html\"\n",
    "            filename = os.path.basename(filepath)\n",
    "            match = re.search(r'SelectedClassNumber=(\\d+)', filename)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting BOSS ID from '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "    def safe_find_element_text(self, by: By, value: str) -> Optional[str]:\n",
    "        \"\"\"Safely find element and return its text\"\"\"\n",
    "        try:\n",
    "            element = self.driver.find_element(by, value)\n",
    "            return element.text.strip() if element else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def process_html_file(self, filepath: str) -> bool:\n",
    "        \"\"\"Process a single HTML file and extract all data using Selenium\"\"\"\n",
    "        try:\n",
    "            # Load HTML file\n",
    "            if not self.load_html_file(filepath):\n",
    "                return False\n",
    "            \n",
    "            # Extract basic class information\n",
    "            class_header_text = self.safe_find_element_text(By.ID, 'lblClassInfoHeader')\n",
    "            if not class_header_text:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': 'Missing class header',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            # Parse course code and section\n",
    "            course_match = re.match(r'([A-Z0-9_]+)\\s*-\\s*(.+)', class_header_text)\n",
    "            if not course_match:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': f'Cannot parse course code from: {class_header_text}',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            course_code = course_match.group(1)\n",
    "            section = course_match.group(2)\n",
    "            \n",
    "            # Get course ID from local cache\n",
    "            course_id = self.get_course_id_by_code(course_code)\n",
    "            if not course_id:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': f'Course not found in cache: {course_code}',\n",
    "                    'type': 'database_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            # Extract academic term\n",
    "            term_text = self.safe_find_element_text(By.ID, 'lblClassInfoSubHeader')\n",
    "            if not term_text:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': 'Missing academic term',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            term_data = self.parse_acad_term(term_text)\n",
    "            if not term_data:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': f'Cannot parse academic term: {term_text}',\n",
    "                    'type': 'parse_error'\n",
    "                })\n",
    "                return False\n",
    "            \n",
    "            # Extract course areas\n",
    "            course_areas = self.safe_find_element_text(By.ID, 'lblCourseAreas')\n",
    "            if course_areas:\n",
    "                # Clean up HTML tags if any\n",
    "                course_areas = re.sub(r'<[^>]+>', '', course_areas)\n",
    "            \n",
    "            # Extract enrollment requirements\n",
    "            enrolment_req = self.safe_find_element_text(By.ID, 'lblEnrolmentRequirements')\n",
    "            \n",
    "            # Extract grading basis - match Prisma enum exactly\n",
    "            grading_text = self.safe_find_element_text(By.ID, 'lblGradingBasis')\n",
    "            grading_basis = None\n",
    "            if grading_text:\n",
    "                if grading_text.lower() in ['graded']:\n",
    "                    grading_basis = 'GRADED'\n",
    "                elif grading_text.lower() in ['pass/fail', 'pass fail']:\n",
    "                    grading_basis = 'PASS_FAIL'\n",
    "                else:\n",
    "                    grading_basis = 'NA'\n",
    "            \n",
    "            # Extract course outline URL\n",
    "            course_outline_url = self.extract_course_outline_url()\n",
    "            \n",
    "            # Extract period dates\n",
    "            period_text = self.safe_find_element_text(By.ID, 'lblDates')\n",
    "            start_dt, end_dt = None, None\n",
    "            if period_text:\n",
    "                start_dt, end_dt = self.parse_date_range(period_text)\n",
    "            \n",
    "            # Extract BOSS ID\n",
    "            boss_id = self.extract_boss_id_from_filepath(filepath)\n",
    "            \n",
    "            # Add course update record - match database column names exactly\n",
    "            self.courses_updates.append({\n",
    "                'code': course_code,\n",
    "                'course_area': course_areas,\n",
    "                'enrolment_requirements': enrolment_req\n",
    "            })\n",
    "            \n",
    "            # Add academic term record - match database column names exactly\n",
    "            acad_term_record = {\n",
    "                'id': term_data['id'],\n",
    "                'acad_year_start': term_data['acad_year_start'],\n",
    "                'acad_year_end': term_data['acad_year_end'],\n",
    "                'term': term_data['term'],\n",
    "                'boss_id': boss_id,\n",
    "                'start_dt': start_dt.isoformat() if start_dt else None,\n",
    "                'end_dt': end_dt.isoformat() if end_dt else None\n",
    "            }\n",
    "            self.acad_term.append(acad_term_record)\n",
    "            \n",
    "            # Add class update record - match database column names exactly\n",
    "            self.classes_updates.append({\n",
    "                'course_code': course_code,\n",
    "                'section': section,\n",
    "                'acad_term_id': term_data['id'],\n",
    "                'grading_basis': grading_basis,\n",
    "                'course_outline_url': course_outline_url\n",
    "            })\n",
    "            \n",
    "            # Get class ID for timing records\n",
    "            class_id = self.get_class_id(course_id, section, term_data['id'])\n",
    "            if not class_id:\n",
    "                self.errors.append({\n",
    "                    'filepath': filepath,\n",
    "                    'error': f'Class not found: {course_code}-{section} for {term_data[\"id\"]}',\n",
    "                    'type': 'database_error'\n",
    "                })\n",
    "                # Continue processing but won't add timing records\n",
    "            \n",
    "            # Extract meeting information\n",
    "            self.extract_meeting_information(class_id, filepath)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'filepath': filepath,\n",
    "                'error': str(e),\n",
    "                'type': 'processing_error'\n",
    "            })\n",
    "            logger.error(f\"Error processing file {filepath}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def extract_meeting_information(self, class_id: Optional[int], filepath: str):\n",
    "        \"\"\"Extract class timing and exam timing information using Selenium\"\"\"\n",
    "        try:\n",
    "            # Find the meeting information table\n",
    "            meeting_table = self.driver.find_element(By.ID, 'RadGrid_MeetingInfo_ctl00')\n",
    "            \n",
    "            # Find all data rows in tbody (skip header)\n",
    "            tbody = meeting_table.find_element(By.TAG_NAME, 'tbody')\n",
    "            rows = tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if len(cells) < 8:\n",
    "                    continue\n",
    "                \n",
    "                meeting_type = cells[0].text.strip()\n",
    "                start_date_text = cells[1].text.strip()\n",
    "                end_date_text = cells[2].text.strip()\n",
    "                day_of_week = cells[3].text.strip()\n",
    "                start_time = cells[4].text.strip()\n",
    "                end_time = cells[5].text.strip()\n",
    "                venue = cells[6].text.strip()\n",
    "                \n",
    "                if meeting_type == 'CLASS':\n",
    "                    # Parse dates for class timing\n",
    "                    start_date = self.parse_single_date(start_date_text)\n",
    "                    end_date = self.parse_single_date(end_date_text)\n",
    "                    \n",
    "                    # Use database column names exactly as per schema @map\n",
    "                    timing_record = {\n",
    "                        'class_id': class_id,\n",
    "                        'start_date': start_date.isoformat() if start_date else None,\n",
    "                        'end_date': end_date.isoformat() if end_date else None,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': venue\n",
    "                    }\n",
    "                    self.class_timing.append(timing_record)\n",
    "                \n",
    "                elif meeting_type == 'EXAM':\n",
    "                    # Parse date for exam timing\n",
    "                    exam_date = self.parse_single_date(start_date_text)\n",
    "                    \n",
    "                    # Use database column names exactly as per schema @map\n",
    "                    exam_record = {\n",
    "                        'class_id': class_id,\n",
    "                        'date': exam_date.isoformat() if exam_date else None,\n",
    "                        'day_of_week': day_of_week,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'venue': ''  # Leave empty as specified in schema\n",
    "                    }\n",
    "                    self.class_exam_timing.append(exam_record)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.errors.append({\n",
    "                'filepath': filepath,\n",
    "                'error': f'Error extracting meeting information: {str(e)}',\n",
    "                'type': 'parse_error'\n",
    "            })\n",
    "\n",
    "    def save_csv_files(self, output_dir: str = 'extracted_data'):\n",
    "        \"\"\"Save all extracted data to CSV files\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save courses updates\n",
    "        if self.courses_updates:\n",
    "            df = pd.DataFrame(self.courses_updates)\n",
    "            df.to_csv(os.path.join(output_dir, 'courses_updates.csv'), index=False)\n",
    "            logger.info(f\"Saved {len(self.courses_updates)} course update records\")\n",
    "        \n",
    "        # Save classes updates\n",
    "        if self.classes_updates:\n",
    "            df = pd.DataFrame(self.classes_updates)\n",
    "            df.to_csv(os.path.join(output_dir, 'classes_updates.csv'), index=False)\n",
    "            logger.info(f\"Saved {len(self.classes_updates)} class update records\")\n",
    "        \n",
    "        # Save academic terms - use database table name exactly\n",
    "        if self.acad_term:\n",
    "            df = pd.DataFrame(self.acad_term)\n",
    "            # Remove duplicates based on ID\n",
    "            df = df.drop_duplicates(subset=['id'])\n",
    "            df.to_csv(os.path.join(output_dir, 'acad_term.csv'), index=False)\n",
    "            logger.info(f\"Saved {len(df)} academic term records\")\n",
    "        \n",
    "        # Save class timings - use database table name exactly\n",
    "        if self.class_timing:\n",
    "            df = pd.DataFrame(self.class_timing)\n",
    "            df.to_csv(os.path.join(output_dir, 'class_timing.csv'), index=False)\n",
    "            logger.info(f\"Saved {len(self.class_timing)} class timing records\")\n",
    "        \n",
    "        # Save exam timings - use database table name exactly\n",
    "        if self.class_exam_timing:\n",
    "            df = pd.DataFrame(self.class_exam_timing)\n",
    "            df.to_csv(os.path.join(output_dir, 'class_exam_timing.csv'), index=False)\n",
    "            logger.info(f\"Saved {len(self.class_exam_timing)} exam timing records\")\n",
    "        \n",
    "        # Save errors\n",
    "        if self.errors:\n",
    "            df = pd.DataFrame(self.errors)\n",
    "            df.to_csv(os.path.join(output_dir, 'processing_errors.csv'), index=False)\n",
    "            logger.info(f\"Saved {len(self.errors)} error records\")\n",
    "\n",
    "    def process_all_files(self, scraped_filepaths_csv: str, output_dir: str = 'extracted_data'):\n",
    "        \"\"\"Process all files listed in the scraped filepaths CSV\"\"\"\n",
    "        try:\n",
    "            # Read the CSV file with file paths\n",
    "            df = pd.read_csv(scraped_filepaths_csv)\n",
    "            \n",
    "            # Handle both 'Filepath' and 'filepath' column names\n",
    "            filepath_column = 'Filepath' if 'Filepath' in df.columns else 'filepath'\n",
    "            \n",
    "            total_files = len(df)\n",
    "            processed_files = 0\n",
    "            successful_files = 0\n",
    "            \n",
    "            logger.info(f\"Starting to process {total_files} files\")\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                filepath = row[filepath_column]\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    if self.process_html_file(filepath):\n",
    "                        successful_files += 1\n",
    "                    processed_files += 1\n",
    "                    \n",
    "                    if processed_files % 100 == 0:\n",
    "                        logger.info(f\"Processed {processed_files}/{total_files} files\")\n",
    "                else:\n",
    "                    self.errors.append({\n",
    "                        'filepath': filepath,\n",
    "                        'error': 'File not found',\n",
    "                        'type': 'file_error'\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Processing complete: {successful_files}/{processed_files} files successful\")\n",
    "            \n",
    "            # Save all CSV files\n",
    "            self.save_csv_files(output_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in process_all_files: {e}\")\n",
    "            raise\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"Selenium WebDriver closed\")\n",
    "        \n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            logger.info(\"Database connection closed\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data extraction\"\"\"\n",
    "    \n",
    "    # Database configuration - Fill in your Supabase credentials\n",
    "    db_config = {\n",
    "        'host': 'localhost',  # Replace with your Supabase host\n",
    "        'database': 'postgres',\n",
    "        'user': 'postgres',  # Replace with your username  \n",
    "        'password': 'changeme',  # Replace with your password\n",
    "        'port': 5433\n",
    "    }\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = AfterClassDataExtractor(db_config)\n",
    "    \n",
    "    try:\n",
    "        # Set up Selenium WebDriver\n",
    "        extractor.setup_selenium_driver()\n",
    "        \n",
    "        # Connect to database\n",
    "        extractor.connect_database()\n",
    "        \n",
    "        # Try to load cached tables first, if not available download them\n",
    "        if not extractor.load_cached_tables():\n",
    "            logger.info(\"Downloading fresh data from database...\")\n",
    "            extractor.download_and_cache_tables()\n",
    "        \n",
    "        # Process all files\n",
    "        extractor.process_all_files('scraped_filepaths.csv', 'extracted_data')\n",
    "        \n",
    "        print(\"Data extraction completed successfully!\")\n",
    "        print(\"Check the 'extracted_data' folder for CSV files:\")\n",
    "        print(\"- courses_updates.csv: Course area and enrollment requirements updates\")\n",
    "        print(\"- classes_updates.csv: Class grading basis, term, and outline URL updates\")\n",
    "        print(\"- acad_term.csv: Academic term records\")\n",
    "        print(\"- class_timing.csv: Class timing records\")\n",
    "        print(\"- class_exam_timing.csv: Exam timing records\")\n",
    "        print(\"- processing_errors.csv: Any errors encountered during processing\")\n",
    "        print(\"\\nDatabase cache stored in 'db_cache' folder for future runs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main process failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        extractor.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 19:31:33,739 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Set up Selenium WebDriver\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_selenium_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Connect to database\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mAfterClassDataExtractor.setup_selenium_driver\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     42\u001b[39m options.add_argument(\u001b[33m'\u001b[39m\u001b[33m--disable-gpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m service = Service(\u001b[43mChromeDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.driver = webdriver.Chrome(service=service, options=options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\chrome.py:40\u001b[39m, in \u001b[36mChromeDriverManager.install\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     driver_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_driver_binary_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     os.chmod(driver_path, \u001b[32m0o755\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\core\\manager.py:35\u001b[39m, in \u001b[36mDriverManager._get_driver_binary_path\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_driver_binary_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, driver):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     binary_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cache_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary_path:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:107\u001b[39m, in \u001b[36mDriverCacheManager.find_driver\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m driver_version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_cache_key_driver_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m.load_metadata_content()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:154\u001b[39m, in \u001b[36mDriverCacheManager.get_cache_key_driver_version\u001b[39m\u001b[34m(self, driver)\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache_key_driver_version\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_driver_version_to_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\core\\driver.py:48\u001b[39m, in \u001b[36mDriver.get_driver_version_to_download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._driver_version_to_download\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_latest_release_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\drivers\\chrome.py:55\u001b[39m, in \u001b[36mChromeDriver.get_latest_release_version\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_latest_release_version\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     determined_browser_version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_browser_version_from_os\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGet LATEST \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m version for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._browser_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\core\\driver.py:63\u001b[39m, in \u001b[36mDriver.get_browser_version_from_os\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._browser_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28mself\u001b[39m._browser_version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_os_system_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_browser_version_from_os\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_browser_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._browser_version\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\core\\os_manager.py:159\u001b[39m, in \u001b[36mOperationSystemManager.get_browser_version_from_os\u001b[39m\u001b[34m(self, browser_type)\u001b[39m\n\u001b[32m    158\u001b[39m pattern = PATTERN[browser_type]\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m version = \u001b[43mread_version_from_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m version\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\site-packages\\webdriver_manager\\core\\utils.py:46\u001b[39m, in \u001b[36mread_version_from_cmd\u001b[39m\u001b[34m(cmd, pattern)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m subprocess.Popen(\n\u001b[32m     40\u001b[39m         cmd,\n\u001b[32m     41\u001b[39m         stdout=subprocess.PIPE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m         shell=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     45\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     stdout = \u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m].decode()\n\u001b[32m     47\u001b[39m     version = re.search(pattern, stdout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanzh\\anaconda3\\envs\\bidly_env\\Lib\\subprocess.py:1198\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1197\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stdout:\n\u001b[32m-> \u001b[39m\u001b[32m1198\u001b[39m     stdout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;28mself\u001b[39m.stdout.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 563\u001b[39m, in \u001b[36mAfterClassDataExtractor.cleanup\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcleanup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;43;03m\"\"\"Clean up resources\"\"\"\u001b[39;49;00m\u001b[43m[\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m13\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.driver:\n\u001b[32m    565\u001b[39m         \u001b[38;5;28mself\u001b[39m.driver.quit()\n",
      "\u001b[31mIndexError\u001b[39m: string index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to test scraping and CSV saving\n",
    "# def test_scrape_class_details():\n",
    "#     test_url = \"https://boss.intranet.smu.edu.sg/ClassDetails.aspx?SelectedAcadTerm=2420&SelectedClassNumber=1580\"\n",
    "#     csv_filename = \"TestClassDetails.csv\"\n",
    "#     print(\"Starting test scrape...\")\n",
    "\n",
    "#     # Open CSV file for writing\n",
    "#     with open(csv_filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         headers = [\"Term\", \"Course Code\", \"Section\", \"Description\", \"Grading Basis\"]\n",
    "#         for i in range(1, 4):  # Dynamic columns for up to 3 classes\n",
    "#             headers.extend([f\"class{i}_day\", f\"class{i}_starttime\", f\"class{i}_venue\"])\n",
    "#         writer.writerow(headers)\n",
    "\n",
    "#         driver.get(test_url)\n",
    "#         time.sleep(2)  # Allow time for page load\n",
    "\n",
    "#         try:\n",
    "#             # Extract key elements\n",
    "#             wait = WebDriverWait(driver, 10)\n",
    "#             course_header = wait.until(EC.presence_of_element_located((By.ID, \"lblClassInfoHeader\"))).text\n",
    "#             description = driver.find_element(By.ID, \"lblClassSection\").text\n",
    "#             term = driver.find_element(By.ID, \"lblClassInfoSubHeader\").text\n",
    "#             grading_basis = driver.find_element(By.ID, \"lblGradingBasis\").text\n",
    "\n",
    "#             # Split course code and section\n",
    "#             course_code, section = [item.strip() for item in course_header.split('-')]\n",
    "\n",
    "#             # Extract meeting details\n",
    "#             class_details = []\n",
    "#             rows = driver.find_elements(By.CSS_SELECTOR, \"#RadGrid_MeetingInfo_ctl00 tr.rgRow, #RadGrid_MeetingInfo_ctl00 tr.rgAltRow\")\n",
    "#             for row in rows:\n",
    "#                 cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "#                 if cells and cells[0].text == \"CLASS\":\n",
    "#                     class_details.append({\n",
    "#                         \"day\": cells[3].text,\n",
    "#                         \"start_time\": cells[4].text,\n",
    "#                         \"venue\": cells[6].text\n",
    "#                     })\n",
    "\n",
    "#             # Prepare row data\n",
    "#             row_data = [term, course_code, section, description, grading_basis]\n",
    "#             for detail in class_details[:3]:  # Include up to 3 classes\n",
    "#                 row_data.extend([detail[\"day\"], detail[\"start_time\"], detail[\"venue\"]])\n",
    "\n",
    "#             # Pad missing columns\n",
    "#             for _ in range(len(class_details), 3):\n",
    "#                 row_data.extend([\"\", \"\", \"\"])\n",
    "\n",
    "#             # Write to CSV\n",
    "#             writer.writerow(row_data)\n",
    "#             print(f\"Test data successfully written to {csv_filename}!\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main Execution\n",
    "# try:\n",
    "#     # Step 1: Navigate and wait for manual login\n",
    "#     driver.get(\"https://boss.intranet.smu.edu.sg/OverallResults.aspx\")\n",
    "#     wait_for_manual_login()\n",
    "\n",
    "#     # Step 2: Run the test scrape function\n",
    "#     test_scrape_class_details()\n",
    "\n",
    "# finally:\n",
    "#     driver.quit()\n",
    "#     print(\"Test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Scrape Class Details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_class_details(ay, term_code, class_number, csv_writer):\n",
    "#     url = f\"https://boss.intranet.smu.edu.sg/ClassDetails.aspx?SelectedClassNumber={class_number:04}&SelectedAcadTerm={ay}{term_code}&SelectedAcadCareer=UGRD\"\n",
    "#     driver.get(url)\n",
    "\n",
    "#     # Immediately check for \"No record found\" in the raw page source\n",
    "#     if \"No record found\" in driver.page_source:\n",
    "#         return  # Exit early\n",
    "\n",
    "#     try:\n",
    "#         # Extract course details\n",
    "#         course_header = driver.find_element(By.ID, \"lblClassInfoHeader\").text\n",
    "#         description = driver.find_element(By.ID, \"lblClassSection\").text\n",
    "#         term = driver.find_element(By.ID, \"lblClassInfoSubHeader\").text\n",
    "#         grading_basis = driver.find_element(By.ID, \"lblGradingBasis\").text\n",
    "\n",
    "#         # Split course header into Course Code and Section\n",
    "#         course_code, section = [item.strip() for item in course_header.split('-')]\n",
    "\n",
    "#         # Extract meeting and exam details\n",
    "#         class_details = []\n",
    "#         exam_details = {\"exam_startdate\": \"\", \"exam_day\": \"\", \"exam_starttime\": \"\"}\n",
    "\n",
    "#         rows = driver.find_elements(By.CSS_SELECTOR, \"#RadGrid_MeetingInfo_ctl00 tr.rgRow, #RadGrid_MeetingInfo_ctl00 tr.rgAltRow\")\n",
    "#         for row in rows:\n",
    "#             cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "#             if cells:\n",
    "#                 if cells[0].text == \"CLASS\":  # CLASS rows\n",
    "#                     class_details.append({\n",
    "#                         \"day\": cells[3].text,\n",
    "#                         \"start_time\": cells[4].text,\n",
    "#                         \"venue\": cells[6].text\n",
    "#                     })\n",
    "#                 elif cells[0].text == \"EXAM\":  # EXAM row\n",
    "#                     exam_details[\"exam_startdate\"] = cells[1].text\n",
    "#                     exam_details[\"exam_day\"] = cells[3].text\n",
    "#                     exam_details[\"exam_starttime\"] = cells[4].text\n",
    "\n",
    "#         # Prepare row data with SelectedClassNumber and SelectedAcadTerm\n",
    "#         row_data = [class_number, f\"{ay}{term_code}\", term, course_code, section, description, grading_basis]\n",
    "\n",
    "#         # Add class details (up to 3 classes)\n",
    "#         for detail in class_details[:3]:\n",
    "#             row_data.extend([detail[\"day\"], detail[\"start_time\"], detail[\"venue\"]])\n",
    "#         for _ in range(len(class_details), 3):  # Pad missing class details\n",
    "#             row_data.extend([\"\", \"\", \"\"])\n",
    "\n",
    "#         # Add exam details\n",
    "#         row_data.extend([\n",
    "#             exam_details[\"exam_startdate\"],\n",
    "#             exam_details[\"exam_day\"],\n",
    "#             exam_details[\"exam_starttime\"]\n",
    "#         ])\n",
    "\n",
    "#         # Write to CSV\n",
    "#         csv_writer.writerow(row_data)\n",
    "#         print(f\"Scraped: AY{ay}, Term {term_code}, Class Number {class_number:04}\")\n",
    "#         return True\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error scraping Class Number {class_number:04}, AY{ay}, Term {term_code}: {e}\")\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### **3.3 Main Scraping Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     ay_list = range(21, 25)  # AY2021 to AY2024\n",
    "#     term_mapping = {\"10\": \"T1\", \"20\": \"T2\", \"31\": \"T3A\", \"32\": \"T3B\"}\n",
    "\n",
    "#     for ay in ay_list:\n",
    "#         for term_code, term_name in term_mapping.items():\n",
    "\n",
    "#             # # Use this if your code suddenly stops.\n",
    "#             # # Skip AY 2021 Term 1\n",
    "#             # if ay == 21 and term_code == \"10\":\n",
    "#             #     print(f\"Skipping AY{ay}, Term {term_code} as it has already been scraped.\")\n",
    "#             #     continue  # Skip this iteration\n",
    "\n",
    "#             filename = f\"20{ay}-20{ay+1}_{term_name}AddedInfo.csv\"\n",
    "#             print(f\"Starting scraping for file: {filename}\")\n",
    "\n",
    "#             with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "#                 writer = csv.writer(file)\n",
    "#                 headers = [\"SelectedClassNumber\", \"SelectedAcadTerm\", \"Term\", \"Course Code\", \"Section\", \"Description\", \"Grading Basis\"]\n",
    "#                 for i in range(1, 4):\n",
    "#                     headers.extend([f\"class{i}_day\", f\"class{i}_starttime\", f\"class{i}_venue\"])\n",
    "#                 headers.extend([\"exam_startdate\", \"exam_day\", \"exam_starttime\"])\n",
    "#                 writer.writerow(headers)\n",
    "\n",
    "#                 class_number = 1000  # Start from class number 0001\n",
    "#                 no_record_count = 0  # Track consecutive \"No record found\"\n",
    "\n",
    "#                 while True:\n",
    "#                     success = scrape_class_details(ay, term_code, class_number, writer)\n",
    "\n",
    "#                     if not success:  # If no record is found\n",
    "#                         no_record_count += 1\n",
    "#                     else:\n",
    "#                         no_record_count = 0  # Reset the counter if a record is found\n",
    "\n",
    "#                     # Stop if no record is found 300 times in a row\n",
    "#                     if no_record_count >= 300:\n",
    "#                         print(f\"300 consecutive 'No record found' reached. Moving to next term.\")\n",
    "#                         break\n",
    "\n",
    "#                     class_number += 1  # Increment to next class number\n",
    "\n",
    "#     driver.quit()\n",
    "#     print(\"Scraping completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## **4. Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     driver.get(\"https://boss.intranet.smu.edu.sg/OverallResults.aspx\")\n",
    "#     wait_for_manual_login()\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bidly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
