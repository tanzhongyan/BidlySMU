{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101bbc8b",
   "metadata": {},
   "source": [
    "# **SMU Course Bidding Data Preprocessing**\n",
    "\n",
    "<div style=\"background-color:#DFFFD6; padding:12px; border-radius:5px; border: 1px solid #228B22;\">\n",
    "    \n",
    "  <h2 style=\"color:#006400;\">âœ… Looking to Implement This? âœ…</h2>\n",
    "  \n",
    "  <p>ðŸš€ Get started quickly by using <strong><a href=\"example_prediction.ipynb\">example_prediction.ipynb</a></strong>.</p>\n",
    "  \n",
    "  <ul>\n",
    "    <li>ðŸ“Œ **Pre-trained CatBoost model (`.cbm`) available for instant predictions.**</li>\n",
    "    <li>ðŸ”§ Includes **step-by-step instructions** for making predictions.</li>\n",
    "    <li>âš¡ Works **out-of-the-box**â€”just load the model and start predicting!</li>\n",
    "  </ul>\n",
    "\n",
    "  <h3>ðŸ”— ðŸ“Œ Next Steps:</h3>\n",
    "  <p>ðŸ‘‰ <a href=\"example_prediction.ipynb\"><strong>Go to Example Prediction Notebook</strong></a></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Changes in V4**\n",
    "- Feature selection of top variables.\n",
    "- Reusable data transformer.\n",
    "- Extract data from raw_data rather than html directly.\n",
    "- Transform data for training by parsing in extracted data into data transformer.\n",
    "\n",
    "\n",
    "### **Objective**\n",
    "This notebook performs the following steps:\n",
    "1. Extracts data\n",
    "2. Transform data and split into training and test set based on `acad_year_start` and `term`. This is to prevent autocorrelation effects.\n",
    "\n",
    "### **Requirements**\n",
    "- Python 3.x\n",
    "- Pandas, NumPy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764b043",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005ba9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c43e1",
   "metadata": {},
   "source": [
    "### **2. SMU Bidding Data Feature Engineering Transformer**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `SMUBiddingTransformer` class transforms raw SMU course bidding data into machine learning-ready features optimized for CatBoost training. It automatically categorizes features into categorical (for CatBoost's native handling) and numeric types, with special one-hot encoding for multi-valued day combinations.\n",
    "\n",
    "**Output:** A pandas DataFrame with engineered features organized into two categories:\n",
    "- **Categorical Features**: `subject_area`, `catalogue_no`, `round`, `term`, `start_time`, `course_name`, `section`, `instructor`\n",
    "- **Numeric Features**: `window`, `before_process_vacancy`, `acad_year_start`, `has_mon`, `has_tue`, `has_wed`, `has_thu`, `has_fri`, `has_sat`, `has_sun`\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Input Data:** A pandas DataFrame with these required columns:\n",
    "- `course_code`, `course_name`, `section`, `acad_year_start`, `term`\n",
    "- `start_time`, `day_of_week`, `before_process_vacancy`, `bidding_window`, `instructor`\n",
    "\n",
    "**Dependencies:**\n",
    "- Python packages: `pandas`, `numpy`, `pickle`, `os`, `datetime`, `re`\n",
    "\n",
    "**Configuration:**\n",
    "- All previous embedding parameters are deprecated and ignored\n",
    "- CatBoost handles categorical features and missing values natively\n",
    "- Multi-valued days (e.g., \"Mon,Thu\") are one-hot encoded into 7 binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285e2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMUBiddingTransformer:\n",
    "    \"\"\"\n",
    "    A reusable transformer class for processing SMU course bidding data\n",
    "    optimized for CatBoost model.\n",
    "    \n",
    "    Uses categorical encoding for instructors and one-hot encoding for multi-valued days.\n",
    "    \n",
    "    Expected input columns:\n",
    "    - course_code: str (e.g. 'MGMT715', 'COR-COMM175')\n",
    "    - course_name: str\n",
    "    - acad_year_start: int\n",
    "    - term: str ('1', '2', '3A', '3B')\n",
    "    - start_time: str (e.g. '19:30', 'TBA') - preserved as categorical\n",
    "    - day_of_week: str (can be multivalued, e.g. 'Mon,Thu')\n",
    "    - before_process_vacancy: int\n",
    "    - bidding_window: str (e.g. 'Round 1 Window 1', 'Incoming Freshmen Rnd 1 Win 4')\n",
    "    - instructor: str (can be multivalued, e.g. 'JOHN DOE, JANE SMITH')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the transformer for CatBoost optimization.\n",
    "        \n",
    "        Uses categorical encoding for instructors and one-hot encoding for days.\n",
    "        \"\"\"\n",
    "        # Fitted flags\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Lists to track feature types for CatBoost\n",
    "        self.categorical_features = []\n",
    "        self.numeric_features = []\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame) -> 'SMUBiddingTransformer':\n",
    "        \"\"\"\n",
    "        Fit the transformer on training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Training dataframe with all required columns\n",
    "        \"\"\"\n",
    "        # Validate required columns\n",
    "        required_cols = [\n",
    "            'course_code', 'course_name', 'acad_year_start', 'term',\n",
    "            'start_time', 'day_of_week', 'before_process_vacancy',\n",
    "            'bidding_window', 'instructor', 'section'\n",
    "        ]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        print(f\"Fitting transformer on {len(df)} rows...\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform the dataframe to CatBoost-ready format.\n",
    "        \"\"\"\n",
    "        # Try to load existing model if not fitted\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Transformer must be fitted before transform. Call fit() first.\")\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        df_transformed = df.copy()\n",
    "        \n",
    "        # Reset feature tracking\n",
    "        self.categorical_features = []\n",
    "        self.numeric_features = []\n",
    "        all_features = []\n",
    "        \n",
    "        # 1. Extract course components (categorical + numeric)\n",
    "        course_features = self._extract_course_features(df_transformed)\n",
    "        all_features.append(course_features)\n",
    "        \n",
    "        # 2. Process bidding window (categorical + numeric)\n",
    "        round_window_features = self._extract_round_window(df_transformed)\n",
    "        all_features.append(round_window_features)\n",
    "        \n",
    "        # 3. Basic features (preserve categorical nature) + instructor as categorical\n",
    "        basic_features = self._process_basic_features(df_transformed)\n",
    "        all_features.append(basic_features)\n",
    "        \n",
    "        # 4. Create day one-hot encoding\n",
    "        day_features = self._create_day_one_hot_encoding(df_transformed)\n",
    "        all_features.append(day_features)\n",
    "        \n",
    "        # Combine all features\n",
    "        final_df = pd.concat(all_features, axis=1)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fit the transformer and transform the data in one step.\"\"\"\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "    \n",
    "    def get_categorical_features(self) -> List[str]:\n",
    "        \"\"\"Get list of categorical feature names for CatBoost.\"\"\"\n",
    "        return self.categorical_features.copy()\n",
    "    \n",
    "    def get_numeric_features(self) -> List[str]:\n",
    "        \"\"\"Get list of numeric feature names.\"\"\"\n",
    "        return self.numeric_features.copy()\n",
    "    \n",
    "    def _extract_course_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract subject area and catalogue number from course code.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        def split_course_code(code):\n",
    "            \"\"\"Split course code into subject area and catalogue number.\"\"\"\n",
    "            if pd.isna(code):\n",
    "                return None, None\n",
    "            \n",
    "            code = str(code).strip().upper()\n",
    "            \n",
    "            # Handle hyphenated codes like 'COR-COMM175'\n",
    "            if '-' in code:\n",
    "                parts = code.split('-')\n",
    "                if len(parts) >= 2:\n",
    "                    subject = '-'.join(parts[:-1])\n",
    "                    # Extract number from last part\n",
    "                    num_match = re.search(r'(\\d+)', parts[-1])\n",
    "                    if num_match:\n",
    "                        return subject, int(num_match.group(1))\n",
    "                    else:\n",
    "                        # Try extracting from full last part\n",
    "                        num_match = re.search(r'(\\d+)', code)\n",
    "                        if num_match:\n",
    "                            return subject, int(num_match.group(1))\n",
    "            \n",
    "            # Standard format like 'MGMT715'\n",
    "            match = re.match(r'([A-Z\\-]+)(\\d+)', code)\n",
    "            if match:\n",
    "                return match.group(1), int(match.group(2))\n",
    "            \n",
    "            return code, 0\n",
    "        \n",
    "        # Extract components\n",
    "        splits = df['course_code'].apply(split_course_code)\n",
    "        features['subject_area'] = splits.apply(lambda x: x[0] if x else None)\n",
    "        features['catalogue_no'] = splits.apply(lambda x: x[1] if x else 0)\n",
    "        \n",
    "        # subject_area and catalogue_no are categorical for CatBoost\n",
    "        self.categorical_features.extend(['subject_area', 'catalogue_no'])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_round_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract round and window from bidding_window string.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        def parse_bidding_window(window_str):\n",
    "            \"\"\"Parse bidding window string into round and window number.\"\"\"\n",
    "            if pd.isna(window_str):\n",
    "                return None, None\n",
    "            \n",
    "            window_str = str(window_str).strip()\n",
    "            \n",
    "            # Handle patterns from V4_01 notebook\n",
    "            import re\n",
    "            match = re.search(r'Round\\s+(\\d[A-C]?)\\s+Window\\s+(\\d)', window_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1), int(match.group(2))\n",
    "            \n",
    "            match = re.search(r'Rnd\\s+(\\d[A-C]?)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1), int(match.group(2))\n",
    "            \n",
    "            match = re.search(r'(\\d[A-C]?)', window_str)\n",
    "            if match:\n",
    "                win_match = re.search(r'Window\\s+(\\d)|Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
    "                if win_match:\n",
    "                    window_num = int(win_match.group(1) or win_match.group(2))\n",
    "                    return match.group(1), window_num\n",
    "                return match.group(1), 1\n",
    "            \n",
    "            return '1', 1\n",
    "        \n",
    "        # Extract round and window\n",
    "        parsed = df['bidding_window'].apply(parse_bidding_window)\n",
    "        features['round'] = parsed.apply(lambda x: x[0] if x else '1')\n",
    "        features['window'] = parsed.apply(lambda x: x[1] if x else 1)\n",
    "        \n",
    "        # Round as categorical (preserves ordering like 1, 1A, 1B, 2, 2A)\n",
    "        self.categorical_features.append('round')\n",
    "        \n",
    "        # Window as numeric\n",
    "        self.numeric_features.append('window')\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _process_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process basic features, preserving categorical nature where beneficial.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Numeric features\n",
    "        features['before_process_vacancy'] = pd.to_numeric(\n",
    "            df['before_process_vacancy'], errors='coerce'\n",
    "        ).fillna(0)\n",
    "        features['acad_year_start'] = pd.to_numeric(\n",
    "            df['acad_year_start'], errors='coerce'\n",
    "        ).fillna(2025)\n",
    "        \n",
    "        self.numeric_features.extend(['before_process_vacancy', 'acad_year_start'])\n",
    "        \n",
    "        # Categorical features\n",
    "        features['term'] = df['term'].astype(str)\n",
    "        features['start_time'] = df['start_time'].astype(str)\n",
    "        features['course_name'] = df['course_name'].astype(str)\n",
    "        features['section'] = df['section'].astype(str)\n",
    "        \n",
    "        # Process instructor names (remove duplicates, handle comma-separated format)\n",
    "        features['instructor'] = df['instructor'].apply(self._process_instructor_names)\n",
    "\n",
    "        # Replace empty strings with None for proper CatBoost handling\n",
    "        features.loc[features['start_time'].isin(['', 'nan']), 'start_time'] = None\n",
    "        features.loc[features['course_name'].isin(['', 'nan']), 'course_name'] = None\n",
    "        features.loc[features['section'].isin(['', 'nan']), 'section'] = None\n",
    "        \n",
    "        self.categorical_features.extend(['term', 'start_time', 'course_name', 'section', 'instructor'])\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _create_day_one_hot_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create one-hot encoding for days of the week.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Initialize all day columns as 0\n",
    "        day_columns = ['has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun']\n",
    "        for col in day_columns:\n",
    "            features[col] = 0\n",
    "        \n",
    "        # Day mapping\n",
    "        day_abbrev = {\n",
    "            'MONDAY': 'MON', 'TUESDAY': 'TUE', 'WEDNESDAY': 'WED',\n",
    "            'THURSDAY': 'THU', 'FRIDAY': 'FRI', 'SATURDAY': 'SAT', 'SUNDAY': 'SUN',\n",
    "            'MON': 'MON', 'TUE': 'TUE', 'WED': 'WED', 'THU': 'THU',\n",
    "            'FRI': 'FRI', 'SAT': 'SAT', 'SUN': 'SUN'\n",
    "        }\n",
    "        \n",
    "        day_to_column = {\n",
    "            'MON': 'has_mon', 'TUE': 'has_tue', 'WED': 'has_wed', 'THU': 'has_thu',\n",
    "            'FRI': 'has_fri', 'SAT': 'has_sat', 'SUN': 'has_sun'\n",
    "        }\n",
    "        \n",
    "        # Process each row's day_of_week\n",
    "        for idx, days in enumerate(df['day_of_week']):\n",
    "            if pd.isna(days) or str(days).strip() == '':\n",
    "                continue  # Leave all days as 0\n",
    "            \n",
    "            # Handle multiple days separated by comma\n",
    "            for day in str(days).split(','):\n",
    "                day_upper = day.strip().upper()\n",
    "                standardized_day = day_abbrev.get(day_upper, day_upper)\n",
    "                \n",
    "                if standardized_day in day_to_column:\n",
    "                    features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
    "        \n",
    "        # These are numeric binary features (0/1)\n",
    "        self.numeric_features.extend(day_columns)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Get all feature names after transformation.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Transformer must be fitted to get feature names.\")\n",
    "        \n",
    "        return self.categorical_features + self.numeric_features\n",
    "    \n",
    "    def _process_instructor_names(self, instructor_str):\n",
    "        \"\"\"Process instructor names to remove duplicates and handle comma-separated format.\"\"\"\n",
    "        if pd.isna(instructor_str) or str(instructor_str).strip() == '' or str(instructor_str).upper() == 'TBA':\n",
    "            return None\n",
    "        \n",
    "        # Convert to string and clean\n",
    "        instructor_str = str(instructor_str).strip()\n",
    "        \n",
    "        # Split instructor names using pattern: \", \" followed by uppercase letters (start of lastname)\n",
    "        # This handles cases like \"TSE, JUSTIN K, AIDAN WONG, TSE, JUSTIN K\"\n",
    "        import re\n",
    "        parts = re.split(r', (?=[A-Z]+(?:\\s|,|$))', instructor_str)\n",
    "        \n",
    "        # Remove duplicates while preserving order (don't use set() to avoid losing order)\n",
    "        seen = set()\n",
    "        unique_parts = []\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if part and part not in seen:\n",
    "                seen.add(part)\n",
    "                unique_parts.append(part)\n",
    "        \n",
    "        # Join back into single string for categorical feature\n",
    "        return ', '.join(unique_parts) if unique_parts else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfd160",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **3. SMUDataMerger Class**\n",
    "\n",
    "### **SMU Raw Data and BOSS Results Integration Pipeline**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `SMUDataMerger` class combines SMU's raw course data with BOSS bidding results to create a unified dataset for machine learning analysis. It intelligently merges timing information from multiple data sources and creates course-level records suitable for the `SMUBiddingTransformer`.\n",
    "\n",
    "**Output:** Two timestamped CSV files saved to `script_output/model_training/`:\n",
    "- **Classification Dataset**: `classification/classification_model_data_{timestamp}.csv` - All merged records for predicting bidding success\n",
    "- **Regression Dataset**: `regression/regression_model_data_{timestamp}.csv` - Only records with non-zero bids for predicting bid amounts\n",
    "- **Final DataFrame**: Returns the classification dataset with columns: `course_code`, `course_name`, `acad_year_start`, `term`, `section`, `start_time`, `day_of_week`, `before_process_vacancy`, `bidding_window`, `instructor`, `median_bid`, `min_bid`\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Input Data:**\n",
    "- **Raw Data Excel File** (`script_input/raw_data.xlsx`):\n",
    "  - `standalone` sheet: Core course information with `course_code`, `section`, `acad_year_start`, `term`, `record_key`\n",
    "  - `multiple` sheet: Class sessions with `type`, `day_of_week`, `start_time`, `venue`, `professor_name`, `record_key`\n",
    "- **BOSS Results Folder** (`script_input/overallBossResults/`):\n",
    "  - Multiple Excel files with columns: `Course Code`, `Section`, `Term`, `Before Process Vacancy`, `Bidding Window`, `Instructor`, `Median Bid`, `Min Bid`\n",
    "\n",
    "**Dependencies:**\n",
    "- Python packages: `pandas`, `glob`, `os`, `re`, `datetime`, `collections.Counter`\n",
    "- Excel file reading capabilities (openpyxl or xlrd)\n",
    "\n",
    "**Configuration:**\n",
    "- `raw_data_path` (default=\"script_input/raw_data.xlsx\"): Path to raw data Excel file\n",
    "- `boss_results_folder` (default=\"script_input/overallBossResults\"): Folder containing BOSS results Excel files\n",
    "\n",
    "#### **What the User Needs to Do Step by Step**\n",
    "\n",
    "**Step 1: Initialize the Merger**\n",
    "```python\n",
    "from SMUDataMerger import SMUDataMerger\n",
    "\n",
    "# Initialize with default paths\n",
    "merger = SMUDataMerger()\n",
    "\n",
    "# Or with custom paths\n",
    "merger = SMUDataMerger(\n",
    "    raw_data_path=\"custom_path/raw_data.xlsx\",\n",
    "    boss_results_folder=\"custom_path/boss_results\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 2: Execute the Complete Merge Process**\n",
    "```python\n",
    "# Execute merge and get final dataset\n",
    "final_dataset = merger.process_and_merge()\n",
    "\n",
    "# Check results\n",
    "print(f\"Final dataset shape: {final_dataset.shape}\")\n",
    "print(f\"Unique courses: {final_dataset['course_code'].nunique()}\")\n",
    "```\n",
    "\n",
    "**Step 3: Use Output with SMUBiddingTransformer**\n",
    "```python\n",
    "# The output is automatically saved and ready for SMUBiddingTransformer\n",
    "from SMUBiddingTransformer import SMUBiddingTransformer\n",
    "\n",
    "transformer = SMUBiddingTransformer()\n",
    "ml_features = transformer.fit_transform(final_dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f45b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMUDataMerger:\n",
    "    \"\"\"\n",
    "    A class to merge SMU raw data with boss results data for bidding analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, raw_data_path=\"script_input/raw_data.xlsx\", \n",
    "                 boss_results_folder=\"script_input/overallBossResults\"):\n",
    "        self.raw_data_path = raw_data_path\n",
    "        self.boss_results_folder = boss_results_folder\n",
    "        self.output_folder = \"script_output/model_training\"\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "    \n",
    "    def load_raw_data(self):\n",
    "        \"\"\"\n",
    "        Load and process the raw_data.xlsx file with standalone and multiple sheets.\n",
    "        \"\"\"\n",
    "        print(f\"Loading raw data from {self.raw_data_path}\")\n",
    "        \n",
    "        # Load both sheets\n",
    "        standalone_df = pd.read_excel(self.raw_data_path, sheet_name='standalone')\n",
    "        multiple_df = pd.read_excel(self.raw_data_path, sheet_name='multiple')\n",
    "        \n",
    "        print(f\"Standalone sheet: {standalone_df.shape[0]} rows\")\n",
    "        print(f\"Multiple sheet: {multiple_df.shape[0]} rows\")\n",
    "        \n",
    "        # Filter multiple_df to only include CLASS entries (ignore EXAM)\n",
    "        class_df = multiple_df[multiple_df['type'] == 'CLASS'].copy()\n",
    "        print(f\"Class entries in multiple sheet: {class_df.shape[0]} rows\")\n",
    "        \n",
    "        return standalone_df, class_df\n",
    "\n",
    "    def process_class_timings(self, class_df):\n",
    "        \"\"\"\n",
    "        Process class timings by grouping by record_key and aggregating the timing information.\n",
    "        \"\"\"\n",
    "        if class_df.empty:\n",
    "            return pd.DataFrame(columns=['record_key', 'day_of_week', 'start_time'])\n",
    "        \n",
    "        def aggregate_days(days):\n",
    "            # Remove NaN values and convert to set to remove duplicates\n",
    "            valid_days = [day for day in days if pd.notna(day)]\n",
    "            if not valid_days:\n",
    "                return None\n",
    "            unique_days = sorted(set(valid_days))\n",
    "            return ', '.join(unique_days)\n",
    "        \n",
    "        def get_most_common_time(times):\n",
    "            # Remove NaN values\n",
    "            valid_times = [time for time in times if pd.notna(time)]\n",
    "            if not valid_times:\n",
    "                return None\n",
    "            # Get most common time, or first occurrence if tie\n",
    "            time_counts = Counter(valid_times)\n",
    "            return time_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Group by record_key and aggregate\n",
    "        timing_summary = class_df.groupby('record_key').agg({\n",
    "            'day_of_week': aggregate_days,\n",
    "            'start_time': get_most_common_time,\n",
    "            'venue': lambda x: ', '.join([str(v) for v in x if pd.notna(v)]) if any(pd.notna(v) for v in x) else None,\n",
    "            'professor_name': lambda x: ', '.join([str(p) for p in x if pd.notna(p)]) if any(pd.notna(p) for p in x) else None\n",
    "        }).reset_index()\n",
    "        \n",
    "        return timing_summary\n",
    "\n",
    "    def combine_raw_data(self, standalone_df, class_df):\n",
    "        \"\"\"\n",
    "        Combine standalone and multiple (class) data into one flat dataset.\n",
    "        \"\"\"\n",
    "        print(\"Combining standalone and class timing data...\")\n",
    "        \n",
    "        # Process class timings first\n",
    "        timing_summary = self.process_class_timings(class_df)\n",
    "        \n",
    "        # Merge standalone with timing summary\n",
    "        if not timing_summary.empty:\n",
    "            combined_df = pd.merge(\n",
    "                standalone_df,\n",
    "                timing_summary,\n",
    "                on='record_key',\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            combined_df = standalone_df.copy()\n",
    "            combined_df['day_of_week'] = None\n",
    "            combined_df['start_time'] = None\n",
    "            combined_df['venue'] = None\n",
    "            combined_df['professor_name'] = None\n",
    "        \n",
    "        # Create boss-compatible term format: \"2021-22 Term 1\"\n",
    "        def create_boss_term_format(row):\n",
    "            if pd.notna(row['acad_year_start']) and pd.notna(row['acad_year_end']) and pd.notna(row['term']):\n",
    "                year_start = int(row['acad_year_start'])\n",
    "                year_end = str(int(row['acad_year_end']))[-2:]  # Last 2 digits\n",
    "                term = str(row['term']).strip()\n",
    "                if term.startswith('T'):\n",
    "                    term = term[1:]  # Remove T prefix\n",
    "                return f\"{year_start}-{year_end} Term {term}\"\n",
    "            return None\n",
    "        \n",
    "        # Add boss_term_format column for matching\n",
    "        combined_df['boss_term_format'] = combined_df.apply(create_boss_term_format, axis=1)\n",
    "        \n",
    "        print(f\"Combined raw data shape: {combined_df.shape}\")\n",
    "        print(f\"Sample boss term formats created: {combined_df['boss_term_format'].value_counts().head()}\")\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "    def standardize_term_format(self, term_str):\n",
    "        \"\"\"\n",
    "        Convert term formats - remove T prefix if present.\n",
    "        \"\"\"\n",
    "        if pd.isna(term_str):\n",
    "            return None\n",
    "        \n",
    "        term_str = str(term_str).strip().upper()\n",
    "        \n",
    "        # If starts with 'T', remove it\n",
    "        if term_str.startswith('T'):\n",
    "            return term_str[1:]\n",
    "        \n",
    "        return term_str\n",
    "    \n",
    "    def clean_text_encoding(self, text):\n",
    "        \"\"\"\n",
    "        Clean text encoding issues from web scraping.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Common encoding fixes\n",
    "        replacements = {\n",
    "            'Ã¢â‚¬\"': 'â€“',  # en dash\n",
    "            'Ã¢â‚¬â„¢': \"'\",  # apostrophe\n",
    "            'Ã¢â‚¬Å“': '\"',  # left quote\n",
    "            'Ã¢â‚¬': '\"',   # right quote\n",
    "            'ÃƒÂ©': 'Ã©',   # e acute\n",
    "            'ÃƒÂ¨': 'Ã¨',   # e grave\n",
    "            'Ãƒ ': 'Ã ',   # a grave\n",
    "            'ÃƒÂ¢': 'Ã¢',   # a circumflex\n",
    "            'ÃƒÂ®': 'Ã®',   # i circumflex\n",
    "            'ÃƒÂ´': 'Ã´',   # o circumflex\n",
    "            'ÃƒÂ»': 'Ã»',   # u circumflex\n",
    "            'ÃƒÂ§': 'Ã§',   # c cedilla\n",
    "            'Ã¢â‚¬Â¦': '...',  # ellipsis\n",
    "            'Ã¢â‚¬â€°': ' ',   # thin space\n",
    "            'Ã‚': '',      # non-breaking space artifact\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Remove any remaining non-ASCII characters that might cause issues\n",
    "        # But keep common accented characters\n",
    "        import unicodedata\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def create_course_key(self, course_code, section, acad_year_start, term):\n",
    "        \"\"\"\n",
    "        Create a standardized key for matching courses.\n",
    "        Format: COURSECODE_SECTION_YEAR_TERM\n",
    "        \"\"\"\n",
    "        if pd.isna(course_code) or pd.isna(section):\n",
    "            return None\n",
    "        \n",
    "        # Clean course code and section\n",
    "        course_code_clean = str(course_code).strip().upper()\n",
    "        section_clean = str(section).strip().upper()\n",
    "        \n",
    "        # Handle academic year - ensure it's an integer\n",
    "        if pd.notna(acad_year_start):\n",
    "            acad_year = int(float(acad_year_start))\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # Standardize term format (remove T prefix if present)\n",
    "        if pd.notna(term):\n",
    "            term_clean = self.standardize_term_format(term)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        key = f\"{course_code_clean}_{section_clean}_{acad_year}_{term_clean}\"\n",
    "        return key\n",
    "\n",
    "    def load_boss_results(self):\n",
    "        \"\"\"\n",
    "        Load and combine all Excel files from the overallBossResults folder.\n",
    "        \"\"\"\n",
    "        print(f\"Loading boss results from {self.boss_results_folder}\")\n",
    "        \n",
    "        # Find all Excel files in the folder\n",
    "        excel_files = glob.glob(os.path.join(self.boss_results_folder, \"*.xlsx\"))\n",
    "        \n",
    "        if not excel_files:\n",
    "            print(\"No Excel files found in the boss results folder!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Found {len(excel_files)} Excel files\")\n",
    "        \n",
    "        all_boss_data = []\n",
    "        \n",
    "        for file_path in excel_files:\n",
    "            try:\n",
    "                # Extract academic year and term from filename\n",
    "                filename = os.path.basename(file_path)\n",
    "                print(f\"Processing file: {filename}\")\n",
    "                \n",
    "                # Load the Excel file\n",
    "                df = pd.read_excel(file_path)\n",
    "                \n",
    "                # Add source filename for tracking\n",
    "                df['source_file'] = filename\n",
    "                \n",
    "                all_boss_data.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_boss_data:\n",
    "            print(\"No valid data found in boss results files!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine all dataframes\n",
    "        combined_boss_df = pd.concat(all_boss_data, ignore_index=True)\n",
    "        print(f\"Combined boss results: {combined_boss_df.shape[0]} rows\")\n",
    "        \n",
    "        return combined_boss_df\n",
    "\n",
    "    def parse_term_info(self, term_str):\n",
    "        \"\"\"\n",
    "        Parse term string like \"2021-22 Term 2\" to extract academic year start and term.\n",
    "        \"\"\"\n",
    "        if pd.isna(term_str):\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            # Pattern: \"YYYY-YY Term X\"\n",
    "            match = re.match(r'(\\d{4})-\\d{2}\\s+Term\\s+(.+)', str(term_str).strip())\n",
    "            if match:\n",
    "                acad_year_start = int(match.group(1))\n",
    "                term = match.group(2).strip()  # This will be \"1\", \"2\", \"3A\", \"3B\"\n",
    "                return acad_year_start, term\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing term '{term_str}': {e}\")\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    def merge_data(self, combined_raw_df, boss_df):\n",
    "        \"\"\"\n",
    "        Merge the combined raw data with boss results data.\n",
    "        \"\"\"\n",
    "        print(\"Starting data merge process...\")\n",
    "        \n",
    "        # Filter out rows with missing terms to avoid duplicates\n",
    "        print(f\"\\nRows before filtering missing terms: {len(combined_raw_df)}\")\n",
    "        combined_raw_df = combined_raw_df[combined_raw_df['term'].notna()].copy()\n",
    "        print(f\"Rows after filtering missing terms: {len(combined_raw_df)}\")\n",
    "        \n",
    "        # Clean boss results - remove unnamed columns\n",
    "        boss_columns_to_keep = ['Term', 'Session', 'Bidding Window', 'Course Code', 'Description', \n",
    "                            'Section', 'Vacancy', 'Opening Vacancy', 'Before Process Vacancy', \n",
    "                            'D.I.C.E', 'After Process Vacancy', 'Enrolled Students', \n",
    "                            'Median Bid', 'Min Bid', 'Instructor', 'School/Department', 'source_file']\n",
    "        \n",
    "        # Filter boss_df to only keep valid columns\n",
    "        boss_df_clean = boss_df[[col for col in boss_columns_to_keep if col in boss_df.columns]].copy()\n",
    "        \n",
    "        # Parse boss results term information to extract year and term\n",
    "        boss_df_clean[['boss_acad_year_start', 'boss_term']] = boss_df_clean['Term'].apply(\n",
    "            lambda x: pd.Series(self.parse_term_info(x))\n",
    "        )\n",
    "        \n",
    "        # Create course keys for raw data\n",
    "        combined_raw_df['course_key'] = combined_raw_df.apply(\n",
    "            lambda row: self.create_course_key(\n",
    "                row['course_code'], \n",
    "                row['section'], \n",
    "                row['acad_year_start'], \n",
    "                row['term']  # This is the original term like T1, T2\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Create course keys for boss data\n",
    "        boss_df_clean['course_key'] = boss_df_clean.apply(\n",
    "            lambda row: self.create_course_key(\n",
    "                row['Course Code'], \n",
    "                row['Section'], \n",
    "                row['boss_acad_year_start'], \n",
    "                row['boss_term']  # This should already be in format like \"1\", \"2\", \"3A\"\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Debug: Show sample keys\n",
    "        print(f\"\\nRaw data course keys (first 5):\")\n",
    "        print(combined_raw_df[['course_code', 'section', 'term', 'course_key']].dropna().head())\n",
    "        print(f\"\\nBoss data course keys (first 5):\")\n",
    "        print(boss_df_clean[['Course Code', 'Section', 'Term', 'boss_term', 'course_key']].dropna().head())\n",
    "        \n",
    "        print(f\"\\nRaw data valid keys: {combined_raw_df['course_key'].notna().sum()} out of {len(combined_raw_df)}\")\n",
    "        print(f\"Boss data valid keys: {boss_df_clean['course_key'].notna().sum()} out of {len(boss_df_clean)}\")\n",
    "        \n",
    "        # Find common keys\n",
    "        raw_keys = set(combined_raw_df['course_key'].dropna())\n",
    "        boss_keys = set(boss_df_clean['course_key'].dropna())\n",
    "        common_keys = raw_keys.intersection(boss_keys)\n",
    "        print(f\"\\nCommon keys found: {len(common_keys)}\")\n",
    "        \n",
    "        if len(common_keys) == 0:\n",
    "            print(\"\\nNo matching keys found. Checking for mismatches...\")\n",
    "            print(\"Sample raw keys:\", list(raw_keys)[:5])\n",
    "            print(\"Sample boss keys:\", list(boss_keys)[:5])\n",
    "        \n",
    "        # Perform the merge\n",
    "        merged_df = pd.merge(\n",
    "            combined_raw_df,\n",
    "            boss_df_clean,\n",
    "            on='course_key',\n",
    "            how='inner',\n",
    "            suffixes=('_raw', '_boss')\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nMerged data: {merged_df.shape[0]} rows\")\n",
    "        \n",
    "        if merged_df.empty:\n",
    "            print(\"No matching records found between raw data and boss results.\")\n",
    "            return pd.DataFrame(), merged_df\n",
    "        \n",
    "        # Create the final dataframe with only required columns\n",
    "        final_df = pd.DataFrame()\n",
    "        \n",
    "        # Map columns to match SMUBiddingTransformer expected input\n",
    "        column_mapping = {\n",
    "            'course_code': 'course_code',\n",
    "            'course_name': 'course_name',\n",
    "            'acad_year_start': 'acad_year_start',\n",
    "            'term': 'term',  # Original term format\n",
    "            'start_time': 'start_time',\n",
    "            'day_of_week': 'day_of_week',\n",
    "            'before_process_vacancy': 'Before Process Vacancy',\n",
    "            'bidding_window': 'Bidding Window',\n",
    "            'instructor': 'professor_name',  # Prefer professor_name from class data\n",
    "            'median_bid': 'Median Bid',\n",
    "            'min_bid': 'Min Bid',\n",
    "            'section': 'section'\n",
    "            # REMOVED: 'vacancy' and 'grading_basis'\n",
    "        }\n",
    "        \n",
    "        # If professor_name is empty, use Instructor from boss results\n",
    "        for new_col, source_col in column_mapping.items():\n",
    "            if source_col in merged_df.columns:\n",
    "                final_df[new_col] = merged_df[source_col]\n",
    "            else:\n",
    "                print(f\"Warning: Column {source_col} not found in merged data\")\n",
    "                final_df[new_col] = None\n",
    "        \n",
    "        # Special handling for instructor - use boss Instructor if professor_name is empty\n",
    "        if 'Instructor' in merged_df.columns:\n",
    "            mask = final_df['instructor'].isna() | (final_df['instructor'] == '')\n",
    "            final_df.loc[mask, 'instructor'] = merged_df.loc[mask, 'Instructor']\n",
    "        \n",
    "        # Add course description from boss results if course_name is missing\n",
    "        if 'Description' in merged_df.columns:\n",
    "            mask = final_df['course_name'].isna() | (final_df['course_name'] == '')\n",
    "            final_df.loc[mask, 'course_name'] = merged_df.loc[mask, 'Description']\n",
    "        \n",
    "        # Clean course names and instructor names for encoding issues\n",
    "        final_df['course_name'] = final_df['course_name'].apply(self.clean_text_encoding)\n",
    "        final_df['instructor'] = final_df['instructor'].apply(self.clean_text_encoding)\n",
    "        \n",
    "        print(f\"\\nFinal dataframe columns: {list(final_df.columns)}\")\n",
    "        print(f\"Final dataframe shape: {final_df.shape}\")\n",
    "        \n",
    "        return final_df, merged_df\n",
    "    \n",
    "    def process_and_merge(self):\n",
    "        \"\"\"\n",
    "        Main method to execute the data merging process and save results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Load raw data\n",
    "            standalone_df, class_df = self.load_raw_data()\n",
    "            \n",
    "            # Step 2: Combine standalone and class data into one flat file\n",
    "            combined_raw_df = self.combine_raw_data(standalone_df, class_df)\n",
    "            \n",
    "            # Step 3: Load boss results\n",
    "            boss_df = self.load_boss_results()\n",
    "            \n",
    "            if boss_df.empty:\n",
    "                print(\"No boss results data found. Exiting.\")\n",
    "                return None\n",
    "            \n",
    "            # Step 4: Merge the combined raw data with boss results\n",
    "            final_df, detailed_df = self.merge_data(combined_raw_df, boss_df)\n",
    "            \n",
    "            if final_df.empty:\n",
    "                print(\"No matching records found between raw data and boss results.\")\n",
    "                return None\n",
    "            \n",
    "            # Clean text encoding issues in course_name and instructor\n",
    "            print(\"\\nCleaning text encoding issues...\")\n",
    "            final_df['course_name'] = final_df['course_name'].apply(self.clean_text_encoding)\n",
    "            final_df['instructor'] = final_df['instructor'].apply(self.clean_text_encoding)\n",
    "            \n",
    "            # Step 5: Save the results with timestamp\n",
    "            timestamp = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "\n",
    "            # Create folder structure\n",
    "            classification_folder = os.path.join(self.output_folder, \"classification\")\n",
    "            regression_folder = os.path.join(self.output_folder, \"regression\")\n",
    "            os.makedirs(classification_folder, exist_ok=True)\n",
    "            os.makedirs(regression_folder, exist_ok=True)\n",
    "\n",
    "            # Save classification model data (all data)\n",
    "            classification_filename = f\"classification_model_data_{timestamp}.csv\"\n",
    "            classification_path = os.path.join(classification_folder, classification_filename)\n",
    "            final_df.to_csv(classification_path, index=False)\n",
    "\n",
    "            print(f\"\\nClassification model data saved to: {classification_path}\")\n",
    "            print(f\"Classification dataset shape: {final_df.shape}\")\n",
    "\n",
    "            # Create and save regression model data (non-zero bids only)\n",
    "            regression_df = final_df[(final_df['median_bid'] > 0) & (final_df['min_bid'] > 0)].copy()\n",
    "            regression_filename = f\"regression_model_data_{timestamp}.csv\"\n",
    "            regression_path = os.path.join(regression_folder, regression_filename)\n",
    "            regression_df.to_csv(regression_path, index=False)\n",
    "\n",
    "            print(f\"\\nRegression model data saved to: {regression_path}\")\n",
    "            print(f\"Regression dataset shape: {regression_df.shape}\")\n",
    "            \n",
    "            # Display summary statistics\n",
    "            print(f\"\\nSummary Statistics (Classification Data):\")\n",
    "            print(f\"- Total merged records: {final_df.shape[0]}\")\n",
    "            print(f\"- Unique courses: {final_df['course_code'].nunique()}\")\n",
    "            print(f\"- Unique sections: {final_df['section'].nunique()}\")\n",
    "            print(f\"- Academic years covered: {final_df['acad_year_start'].min()} - {final_df['acad_year_start'].max()}\")\n",
    "            print(f\"- Terms covered: {sorted(final_df['term'].unique())}\")\n",
    "            \n",
    "            # Check for missing critical values\n",
    "            critical_cols = ['course_code', 'section', 'before_process_vacancy', 'median_bid', 'min_bid']\n",
    "            for col in critical_cols:\n",
    "                if col in final_df.columns:\n",
    "                    missing_count = final_df[col].isna().sum()\n",
    "                    if missing_count > 0:\n",
    "                        print(f\"- Missing values in {col}: {missing_count}\")\n",
    "            \n",
    "            # Return the CatBoost dataframe as the primary output\n",
    "            return final_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during merge process: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e19e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data from script_input/raw_data.xlsx\n",
      "Standalone sheet: 12973 rows\n",
      "Multiple sheet: 19986 rows\n",
      "Class entries in multiple sheet: 13082 rows\n",
      "Combining standalone and class timing data...\n",
      "Combined raw data shape: (12973, 26)\n",
      "Sample boss term formats created: boss_term_format\n",
      "2023-24 Term 2    1664\n",
      "2023-24 Term 1    1659\n",
      "2024-25 Term 1    1647\n",
      "2021-22 Term 2    1631\n",
      "2022-23 Term 1    1614\n",
      "Name: count, dtype: int64\n",
      "Loading boss results from script_input/overallBossResults\n",
      "Found 14 Excel files\n",
      "Processing file: 2021-22_T2.xlsx\n",
      "Processing file: 2021-22_T3B.xlsx\n",
      "Processing file: 2022-23_T1.xlsx\n",
      "Processing file: 2022-23_T2.xlsx\n",
      "Processing file: 2022-23_T3A.xlsx\n",
      "Processing file: 2022-23_T3B.xlsx\n",
      "Processing file: 2023-24_T1.xlsx\n",
      "Processing file: 2023-24_T2.xlsx\n",
      "Processing file: 2023-24_T3A.xlsx\n",
      "Processing file: 2023-24_T3B.xlsx\n",
      "Processing file: 2024-25_T1.xlsx\n",
      "Processing file: 2024-25_T2.xlsx\n",
      "Processing file: 2024-25_T3A.xlsx\n",
      "Processing file: 2024-25_T3B.xlsx\n",
      "Combined boss results: 121340 rows\n",
      "Starting data merge process...\n",
      "\n",
      "Rows before filtering missing terms: 12973\n",
      "Rows after filtering missing terms: 12973\n",
      "\n",
      "Raw data course keys (first 5):\n",
      "  course_code section term          course_key\n",
      "0     MGMT715      G1   T1   MGMT715_G1_2021_1\n",
      "1    LGST700A      G1   T1  LGST700A_G1_2021_1\n",
      "2    STAT701A      G1   T1  STAT701A_G1_2021_1\n",
      "3     ACCT666      G1   T1   ACCT666_G1_2021_1\n",
      "4     ACCT635      G1   T1   ACCT635_G1_2021_1\n",
      "\n",
      "Boss data course keys (first 5):\n",
      "  Course Code Section            Term boss_term          course_key\n",
      "0     ACCT001      G1  2021-22 Term 2         2   ACCT001_G1_2021_2\n",
      "1     ACCT009      G1  2021-22 Term 2         2   ACCT009_G1_2021_2\n",
      "2     ACCT101      G1  2021-22 Term 2         2   ACCT101_G1_2021_2\n",
      "3     ACCT101     G10  2021-22 Term 2         2  ACCT101_G10_2021_2\n",
      "4     ACCT101     G11  2021-22 Term 2         2  ACCT101_G11_2021_2\n",
      "\n",
      "Raw data valid keys: 12973 out of 12973\n",
      "Boss data valid keys: 121298 out of 121340\n",
      "\n",
      "Common keys found: 7934\n",
      "\n",
      "Merged data: 121172 rows\n",
      "\n",
      "Final dataframe columns: ['course_code', 'course_name', 'acad_year_start', 'term', 'start_time', 'day_of_week', 'before_process_vacancy', 'bidding_window', 'instructor', 'median_bid', 'min_bid', 'section']\n",
      "Final dataframe shape: (121172, 12)\n",
      "\n",
      "Cleaning text encoding issues...\n",
      "\n",
      "Classification model data saved to: script_output/model_training\\classification\\classification_model_data_180625173049.csv\n",
      "Classification dataset shape: (121172, 12)\n",
      "\n",
      "Regression model data saved to: script_output/model_training\\regression\\regression_model_data_180625173049.csv\n",
      "Regression dataset shape: (36667, 12)\n",
      "\n",
      "Summary Statistics (Classification Data):\n",
      "- Total merged records: 121172\n",
      "- Unique courses: 701\n",
      "- Unique sections: 75\n",
      "- Academic years covered: 2021 - 2024\n",
      "- Terms covered: ['T1', 'T2', 'T3A', 'T3B']\n"
     ]
    }
   ],
   "source": [
    "# Extract data required for model\n",
    "merger = SMUDataMerger()\n",
    "result_df = merger.process_and_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feeae42",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **4. Training Data Preparation Pipeline for SMU Bidding Models**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `SMUBiddingPipeline` class creates model-ready training datasets from SMUDataMerger output using SMUBiddingTransformer. It handles the complete workflow from merged data to timestamped training/test datasets with temporal splits to prevent data leakage.\n",
    "\n",
    "**Output:** Three types of timestamped CSV datasets saved to `script_output/model_training/`:\n",
    "- **Classification Dataset**: `classification/classification_train/test_{timestamp}.csv` - Predicts bidding success with `bids` target (True when median_bid or min_bid equals zero)\n",
    "- **Median Regression Dataset**: `regression/regression_median_train/test_{timestamp}.csv` - Predicts median bid amounts with `target_median_bid` column (filtered for non-zero bids only)  \n",
    "- **Min Regression Dataset**: `regression/regression_min_train/test_{timestamp}.csv` - Predicts minimum bid amounts with `target_min_bid` column (filtered for non-zero bids only)\n",
    "- **Temporal Split**: Training set (pre-2024 T2), Test set (2024 T2/T3A/T3B terms)\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Input Data:**\n",
    "- **Merged CSV File** (from SMUDataMerger output) with columns: `course_code`, `course_name`, `acad_year_start`, `term`, `section`, `start_time`, `day_of_week`, `before_process_vacancy`, `bidding_window`, `instructor`, `median_bid`, `min_bid`\n",
    "\n",
    "**Dependencies:**\n",
    "- Python packages: `pandas`, `pickle`, `os`, `datetime`\n",
    "- Custom modules: `SMUBiddingTransformer` class  \n",
    "- Directory structure: `script_output/model_training/` (auto-created)\n",
    "\n",
    "**Configuration:**\n",
    "- All previous embedding parameters are deprecated and ignored\n",
    "- CatBoost handles categorical features and missing values natively\n",
    "- Multi-valued days (e.g., \"Mon,Thu\") are one-hot encoded into 7 binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebfad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMUBiddingPipeline:\n",
    "    \"\"\"\n",
    "    A comprehensive pipeline class that manages the entire workflow from \n",
    "    SMUDataMerger output to model-ready features using SMUBiddingTransformer.\n",
    "    \n",
    "    This class handles:\n",
    "    1. Loading merged data from SMUDataMerger\n",
    "    2. Preparing training data with SMUBiddingTransformer\n",
    "    3. Transforming user inputs for predictions\n",
    "    4. Saving/loading fitted transformers\n",
    "    \n",
    "    Uses categorical encoding for instructors and one-hot encoding for multi-valued days.\n",
    "    Optimized for CatBoost models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline for CatBoost-optimized feature engineering.\n",
    "        \n",
    "        Uses categorical encoding for instructors and one-hot encoding for days.\n",
    "        \"\"\"\n",
    "        # Initialize transformer\n",
    "        self.transformer = None\n",
    "        \n",
    "        # Required columns for the transformer\n",
    "        self.required_columns = [\n",
    "            'course_code', 'course_name', 'acad_year_start', 'term',\n",
    "            'start_time', 'day_of_week', 'before_process_vacancy',\n",
    "            'bidding_window', 'instructor', 'section'\n",
    "        ]\n",
    "\n",
    "        # Initialize fitted status\n",
    "        self.is_fitted = False\n",
    "        self.training_columns = []\n",
    "    \n",
    "    def load_merged_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load the merged data CSV from SMUDataMerger.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : str\n",
    "            Path to the CSV file created by SMUDataMerger\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Loaded and validated dataframe\n",
    "        \"\"\"\n",
    "        print(f\"Loading merged data from: {file_path}\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Loaded data shape: {df.shape}\")\n",
    "        \n",
    "        # Validate required columns\n",
    "        missing_cols = [col for col in self.required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing columns will be created with default values: {missing_cols}\")\n",
    "            \n",
    "            # Create missing columns with appropriate defaults\n",
    "            for col in missing_cols:\n",
    "                if col == 'section':\n",
    "                    df[col] = 'SEC1'  # Default section\n",
    "                elif col == 'bidding_window':\n",
    "                    df[col] = 'Round 1 Window 1'  # Default bidding window\n",
    "                else:\n",
    "                    df[col] = None\n",
    "        \n",
    "        # Data quality checks\n",
    "        print(\"\\nData Quality Summary:\")\n",
    "        print(f\"- Total records: {len(df)}\")\n",
    "        print(f\"- Unique courses: {df['course_code'].nunique()}\")\n",
    "        print(f\"- Date range: {df['acad_year_start'].min()} - {df['acad_year_start'].max()}\")\n",
    "        print(f\"- Terms: {sorted(df['term'].unique())}\")\n",
    "        \n",
    "        # Check for missing values in critical columns\n",
    "        for col in ['course_code', 'before_process_vacancy']:\n",
    "            missing_count = df[col].isna().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"- Missing {col}: {missing_count} records\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_classification_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Prepare classification training data with temporal split and save as CSV files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Input dataframe with merged SMU data\n",
    "        \"\"\"\n",
    "        print(\"Preparing classification training data with temporal split...\")\n",
    "        \n",
    "        self.transformer = SMUBiddingTransformer()\n",
    "        \n",
    "        print(\"Fitting SMUBiddingTransformer on all data...\")\n",
    "        X_transformed = self.transformer.fit_transform(df)\n",
    "        \n",
    "        # Create bids target column for classification  \n",
    "        # True when median_bid or min_bid equals zero, False otherwise\n",
    "        X_transformed['bids'] = (df['median_bid'] > 0) | (df['min_bid'] > 0)\n",
    "\n",
    "        # Create temporal split\n",
    "        # Test: 2024-25 T2, T3A, T3B\n",
    "        test_mask = (df['acad_year_start'] == 2024) & (df['term'].isin(['T2', 'T3A', 'T3B', '2', '3A', '3B']))\n",
    "        train_mask = ~test_mask\n",
    "        \n",
    "        X_train = X_transformed[train_mask].copy()\n",
    "        X_test = X_transformed[test_mask].copy()\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Save datasets\n",
    "        timestamp = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        classification_folder = \"script_output/model_training/classification\"\n",
    "        os.makedirs(classification_folder, exist_ok=True)\n",
    "        \n",
    "        train_path = os.path.join(classification_folder, f\"classification_train_{timestamp}.csv\")\n",
    "        test_path = os.path.join(classification_folder, f\"classification_test_{timestamp}.csv\")\n",
    "        \n",
    "        X_train.to_csv(train_path, index=False)\n",
    "        X_test.to_csv(test_path, index=False)\n",
    "        \n",
    "        print(f\"Classification training data saved to: {train_path}\")\n",
    "        print(f\"Classification test data saved to: {test_path}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        self.training_columns = list(X_transformed.columns)\n",
    "\n",
    "    def prepare_regression_data_median(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Prepare regression training data for median_bid prediction with temporal split.\n",
    "        \"\"\"\n",
    "        print(\"Preparing regression training data for median_bid prediction...\")\n",
    "        \n",
    "        # Filter out zero/missing median bids\n",
    "        df_clean = df[(df['median_bid'] > 0) & (df['median_bid'].notna())].copy()\n",
    "        print(f\"Records after filtering median_bid: {len(df_clean)}\")\n",
    "        \n",
    "        # Initialize transformer if not already fitted        \n",
    "        self.transformer = SMUBiddingTransformer()\n",
    "        \n",
    "        print(\"Fitting SMUBiddingTransformer...\")\n",
    "        X_transformed = self.transformer.fit_transform(df_clean)\n",
    "        self.is_fitted = True\n",
    "        self.training_columns = list(X_transformed.columns)\n",
    "        \n",
    "        # Create temporal split\n",
    "        test_mask = (df_clean['acad_year_start'] == 2024) & (df_clean['term'].isin(['T2', 'T3A', 'T3B', '2', '3A', '3B']))\n",
    "        train_mask = ~test_mask\n",
    "        \n",
    "        X_train = X_transformed[train_mask].copy()\n",
    "        X_test = X_transformed[test_mask].copy()\n",
    "        y_train = df_clean[train_mask]['median_bid'].copy()\n",
    "        y_test = df_clean[test_mask]['median_bid'].copy()\n",
    "        \n",
    "        # Add target column to features for saving\n",
    "        X_train['target_median_bid'] = y_train.values\n",
    "        X_test['target_median_bid'] = y_test.values\n",
    "        \n",
    "        print(f\"Median regression - Training: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Save datasets\n",
    "        timestamp = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        regression_folder = \"script_output/model_training/regression\"\n",
    "        os.makedirs(regression_folder, exist_ok=True)\n",
    "        \n",
    "        train_path = os.path.join(regression_folder, f\"regression_median_train_{timestamp}.csv\")\n",
    "        test_path = os.path.join(regression_folder, f\"regression_median_test_{timestamp}.csv\")\n",
    "        \n",
    "        X_train.to_csv(train_path, index=False)\n",
    "        X_test.to_csv(test_path, index=False)\n",
    "        \n",
    "        print(f\"Median regression training data saved to: {train_path}\")\n",
    "        print(f\"Median regression test data saved to: {test_path}\")\n",
    "\n",
    "    def prepare_regression_data_min(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Prepare regression training data for min_bid prediction with temporal split.\n",
    "        \"\"\"\n",
    "        print(\"Preparing regression training data for min_bid prediction...\")\n",
    "        \n",
    "        # Filter out zero/missing min bids\n",
    "        df_clean = df[(df['min_bid'] > 0) & (df['min_bid'].notna())].copy()\n",
    "        print(f\"Records after filtering min_bid: {len(df_clean)}\")\n",
    "        \n",
    "        # Initialize transformer if not already fitted\n",
    "        if not self.is_fitted:\n",
    "            \n",
    "            self.transformer = SMUBiddingTransformer()\n",
    "            \n",
    "            print(\"Fitting SMUBiddingTransformer...\")\n",
    "            X_transformed = self.transformer.fit_transform(df_clean)\n",
    "            self.is_fitted = True\n",
    "            self.training_columns = list(X_transformed.columns)\n",
    "        else:\n",
    "            X_transformed = self.transformer.transform(df_clean)\n",
    "        \n",
    "        # Create temporal split\n",
    "        test_mask = (df_clean['acad_year_start'] == 2024) & (df_clean['term'].isin(['T2', 'T3A', 'T3B', '2', '3A', '3B']))\n",
    "        train_mask = ~test_mask\n",
    "        \n",
    "        X_train = X_transformed[train_mask].copy()\n",
    "        X_test = X_transformed[test_mask].copy()\n",
    "        y_train = df_clean[train_mask]['min_bid'].copy()\n",
    "        y_test = df_clean[test_mask]['min_bid'].copy()\n",
    "        \n",
    "        # Add target column to features for saving\n",
    "        X_train['target_min_bid'] = y_train.values\n",
    "        X_test['target_min_bid'] = y_test.values\n",
    "        \n",
    "        print(f\"Min regression - Training: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Save datasets\n",
    "        timestamp = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        regression_folder = \"script_output/model_training/regression\"\n",
    "        os.makedirs(regression_folder, exist_ok=True)\n",
    "        \n",
    "        train_path = os.path.join(regression_folder, f\"regression_min_train_{timestamp}.csv\")\n",
    "        test_path = os.path.join(regression_folder, f\"regression_min_test_{timestamp}.csv\")\n",
    "        \n",
    "        X_train.to_csv(train_path, index=False)\n",
    "        X_test.to_csv(test_path, index=False)\n",
    "        \n",
    "        print(f\"Min regression training data saved to: {train_path}\")\n",
    "        print(f\"Min regression test data saved to: {test_path}\")\n",
    "\n",
    "    def prepare_all_datasets(self, classification_df: pd.DataFrame, regression_df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        OPTIMIZED: Transform data once and create all datasets from the same transformation\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ OPTIMIZED: Preparing all datasets with single transformation...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # STEP 1: Initialize and fit transformer ONCE on classification data\n",
    "        print(\"Fitting transformer on classification data...\")\n",
    "        self.transformer = SMUBiddingTransformer()\n",
    "        \n",
    "        X_transformed_full = self.transformer.fit_transform(classification_df)\n",
    "        self.is_fitted = True\n",
    "        self.training_columns = list(X_transformed_full.columns)\n",
    "        \n",
    "        print(f\"âœ… Single transformation complete: {X_transformed_full.shape}\")\n",
    "        \n",
    "        # STEP 2: Create all datasets from the same transformation\n",
    "        timestamp = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        \n",
    "        # Classification data with temporal split\n",
    "        print(\"\\nðŸ“Š Creating classification datasets...\")\n",
    "        test_mask = (classification_df['acad_year_start'] == 2024) & (classification_df['term'].isin(['T2', 'T3A', 'T3B', '2', '3A', '3B']))\n",
    "        \n",
    "        # Add classification target\n",
    "        X_transformed_full['bids'] = (classification_df['median_bid'] > 0) | (classification_df['min_bid'] > 0)\n",
    "        \n",
    "        X_class_train = X_transformed_full[~test_mask].copy()\n",
    "        X_class_test = X_transformed_full[test_mask].copy()\n",
    "        \n",
    "        # Save classification data\n",
    "        classification_folder = \"script_output/model_training/classification\"\n",
    "        os.makedirs(classification_folder, exist_ok=True)\n",
    "        \n",
    "        X_class_train.to_csv(f\"{classification_folder}/classification_train_{timestamp}.csv\", index=False)\n",
    "        X_class_test.to_csv(f\"{classification_folder}/classification_test_{timestamp}.csv\", index=False)\n",
    "        \n",
    "        print(f\"ðŸ“ Classification: Train={X_class_train.shape[0]}, Test={X_class_test.shape[0]}\")\n",
    "        \n",
    "        # STEP 3: Transform regression data (reuse fitted transformer)\n",
    "        print(\"\\nðŸ“ˆ Creating regression datasets...\")\n",
    "        \n",
    "        # Filter for non-zero bids\n",
    "        median_mask = (regression_df['median_bid'] > 0) & (regression_df['median_bid'].notna())\n",
    "        min_mask = (regression_df['min_bid'] > 0) & (regression_df['min_bid'].notna())\n",
    "        \n",
    "        # Transform regression data using fitted transformer\n",
    "        X_reg_median = self.transformer.transform(regression_df[median_mask])\n",
    "        X_reg_min = self.transformer.transform(regression_df[min_mask])\n",
    "        \n",
    "        # Add targets\n",
    "        X_reg_median['target_median_bid'] = regression_df[median_mask]['median_bid'].values\n",
    "        X_reg_min['target_min_bid'] = regression_df[min_mask]['min_bid'].values\n",
    "        \n",
    "        # Temporal splits for regression\n",
    "        reg_median_test_mask = (regression_df[median_mask]['acad_year_start'] == 2024) & (regression_df[median_mask]['term'].isin(['T2', 'T3A', 'T3B', '2', '3A', '3B']))\n",
    "        reg_min_test_mask = (regression_df[min_mask]['acad_year_start'] == 2024) & (regression_df[min_mask]['term'].isin(['T2', 'T3A', 'T3B', '2', '3A', '3B']))\n",
    "        \n",
    "        # Split and save regression datasets\n",
    "        regression_folder = \"script_output/model_training/regression\"\n",
    "        os.makedirs(regression_folder, exist_ok=True)\n",
    "        \n",
    "        # Median regression\n",
    "        X_reg_median[~reg_median_test_mask].to_csv(f\"{regression_folder}/regression_median_train_{timestamp}.csv\", index=False)\n",
    "        X_reg_median[reg_median_test_mask].to_csv(f\"{regression_folder}/regression_median_test_{timestamp}.csv\", index=False)\n",
    "        \n",
    "        # Min regression  \n",
    "        X_reg_min[~reg_min_test_mask].to_csv(f\"{regression_folder}/regression_min_train_{timestamp}.csv\", index=False)\n",
    "        X_reg_min[reg_min_test_mask].to_csv(f\"{regression_folder}/regression_min_test_{timestamp}.csv\", index=False)\n",
    "        \n",
    "        print(f\"ðŸ“ Median Regression: Train={(~reg_median_test_mask).sum()}, Test={reg_median_test_mask.sum()}\")\n",
    "        print(f\"ðŸ“ Min Regression: Train={(~reg_min_test_mask).sum()}, Test={reg_min_test_mask.sum()}\")        \n",
    "        print(f\"\\nðŸŽ¯ OPTIMIZATION COMPLETE!\")\n",
    "        print(f\"âš¡ Single transformation instead of 3 separate ones\")\n",
    "        print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27ebbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged data from: script_output/model_training/classification/classification_model_data_160625133429.csv\n",
      "Loaded data shape: (121172, 12)\n",
      "\n",
      "Data Quality Summary:\n",
      "- Total records: 121172\n",
      "- Unique courses: 701\n",
      "- Date range: 2021 - 2024\n",
      "- Terms: ['T1', 'T2', 'T3A', 'T3B']\n",
      "Loading merged data from: script_output/model_training/regression/regression_model_data_160625133429.csv\n",
      "Loaded data shape: (36667, 12)\n",
      "\n",
      "Data Quality Summary:\n",
      "- Total records: 36667\n",
      "- Unique courses: 626\n",
      "- Date range: 2021 - 2024\n",
      "- Terms: ['T1', 'T2', 'T3A', 'T3B']\n",
      "ðŸš€ OPTIMIZED: Preparing all datasets with single transformation...\n",
      "============================================================\n",
      "Fitting transformer on classification data...\n",
      "Fitting transformer on 121172 rows...\n",
      "âœ… Single transformation complete: (121172, 18)\n",
      "\n",
      "ðŸ“Š Creating classification datasets...\n",
      "ðŸ“ Classification: Train=114854, Test=6318\n",
      "\n",
      "ðŸ“ˆ Creating regression datasets...\n",
      "ðŸ“ Median Regression: Train=33351, Test=3316\n",
      "ðŸ“ Min Regression: Train=33351, Test=3316\n",
      "\n",
      "ðŸŽ¯ OPTIMIZATION COMPLETE!\n",
      "âš¡ Single transformation instead of 3 separate ones\n",
      "============================================================\n",
      "Pipeline setup complete!\n",
      "Transformer fitted: True\n",
      "Feature columns: 18\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "classification_data_path = \"script_output/model_training/classification/classification_model_data_160625133429.csv\"\n",
    "regression_data_path = \"script_output/model_training/regression/regression_model_data_160625133429.csv\"\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = SMUBiddingPipeline()\n",
    "\n",
    "# Load both datasets\n",
    "df_classification = pipeline.load_merged_data(classification_data_path)\n",
    "df_regression = pipeline.load_merged_data(regression_data_path)\n",
    "\n",
    "# Prepare all datasets with both dataframes\n",
    "pipeline.prepare_all_datasets(df_classification, df_regression)\n",
    "\n",
    "print(\"Pipeline setup complete!\")\n",
    "print(f\"Transformer fitted: {pipeline.is_fitted}\")\n",
    "print(f\"Feature columns: {len(pipeline.training_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f231d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing user input transformation...\n",
      "Input shape: (1, 10)\n",
      "\n",
      "User input:\n",
      "  course_code           course_name  acad_year_start term start_time day_of_week  before_process_vacancy    bidding_window instructor section\n",
      "0     MGMT715  Strategic Management             2025    1      19:30     Mon,Thu                      15  Round 1 Window 1   JOHN DOE      G1\n",
      "\n",
      "âœ… SUCCESS! Transformed shape: (1, 18)\n",
      "ðŸ“Š Feature columns: ['subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy', 'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor', 'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun']\n",
      "ðŸ”¢ Sample values:\n",
      "subject_area                              MGMT\n",
      "catalogue_no                               715\n",
      "round                                        1\n",
      "window                                       1\n",
      "before_process_vacancy                      15\n",
      "acad_year_start                           2025\n",
      "term                                         1\n",
      "start_time                               19:30\n",
      "course_name               Strategic Management\n",
      "section                                     G1\n",
      "Name: 0, dtype: object\n",
      "\n",
      "ðŸ“‹ Feature breakdown:\n",
      "   Categorical: 8\n",
      "   Numeric: 10\n",
      "   Total features: 18\n"
     ]
    }
   ],
   "source": [
    "# Create a sample user input (what a student might want to bid on)\n",
    "user_input = pd.DataFrame({\n",
    "    'course_code': ['MGMT715'],\n",
    "    'course_name': ['Strategic Management'],\n",
    "    'acad_year_start': [2025],\n",
    "    'term': ['1'],\n",
    "    'start_time': ['19:30'],\n",
    "    'day_of_week': ['Mon,Thu'],\n",
    "    'before_process_vacancy': [15],\n",
    "    'bidding_window': ['Round 1 Window 1'],\n",
    "    'instructor': ['JOHN DOE'],\n",
    "    'section': ['G1']\n",
    "})\n",
    "\n",
    "print(\"ðŸ” Testing user input transformation...\")\n",
    "print(f\"Input shape: {user_input.shape}\")\n",
    "print(\"\\nUser input:\")\n",
    "print(user_input.to_string())\n",
    "\n",
    "try:\n",
    "    # Use the existing fitted transformer\n",
    "    if 'pipeline' in locals() and pipeline.transformer is not None and pipeline.is_fitted:\n",
    "        transformed_features = pipeline.transformer.transform(user_input)\n",
    "        print(f\"\\nâœ… SUCCESS! Transformed shape: {transformed_features.shape}\")\n",
    "        print(f\"ðŸ“Š Feature columns: {list(transformed_features.columns)}\")\n",
    "        print(f\"ðŸ”¢ Sample values:\\n{transformed_features.iloc[0].head(10)}\")\n",
    "        \n",
    "        # Show feature types\n",
    "        print(f\"\\nðŸ“‹ Feature breakdown:\")\n",
    "        print(f\"   Categorical: {len(pipeline.transformer.get_categorical_features())}\")\n",
    "        print(f\"   Numeric: {len(pipeline.transformer.get_numeric_features())}\")\n",
    "        print(f\"   Total features: {len(pipeline.transformer.get_feature_names())}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No fitted transformer found. Run the pipeline first!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"ðŸ’¡ Make sure you've run the pipeline training first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bidly_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
