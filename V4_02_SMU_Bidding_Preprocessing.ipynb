{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101bbc8b",
   "metadata": {},
   "source": [
    "# **SMU Course Bidding Data Preprocessing**\n",
    "\n",
    "<div style=\"background-color:#DFFFD6; padding:12px; border-radius:5px; border: 1px solid #228B22;\">\n",
    "    \n",
    "  <h2 style=\"color:#006400;\">âœ… Looking to Implement This? âœ…</h2>\n",
    "  \n",
    "  <p>ðŸš€ Get started quickly by using <strong><a href=\"example_prediction.ipynb\">example_prediction.ipynb</a></strong>.</p>\n",
    "  \n",
    "  <ul>\n",
    "    <li>ðŸ“Œ **Pre-trained CatBoost model (`.cbm`) available for instant predictions.**</li>\n",
    "    <li>ðŸ”§ Includes **step-by-step instructions** for making predictions.</li>\n",
    "    <li>âš¡ Works **out-of-the-box**â€”just load the model and start predicting!</li>\n",
    "  </ul>\n",
    "\n",
    "  <h3>ðŸ”— ðŸ“Œ Next Steps:</h3>\n",
    "  <p>ðŸ‘‰ <a href=\"example_prediction.ipynb\"><strong>Go to Example Prediction Notebook</strong></a></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "### **Changes in V4**\n",
    "- Replaced `BidderCount` with `Before Process Vacancy` due to future dependent results like `After Process Vacancy` which is not available at prediction time.\n",
    "- Development of two models, one for `Median Bid Price` and `Min Bid Price`.\n",
    "- Refined model input to make ingesting data for prediction easier. No label encoding done for `Term` or `Round`.\n",
    "\n",
    "### **Objective**\n",
    "This notebook performs the following steps:\n",
    "1. **Data Cleaning** - Handle redundant columns and remove unwanted data.\n",
    "2. **Feature Engineering** - Create derived features.\n",
    "3. **Exploratory Data Analysis (EDA)** - Analyze key features and outlier cleaning.\n",
    "4. **Save Processed Data** - Save the data into a csv that is useable for other ML models\n",
    "\n",
    "### **Requirements**\n",
    "- Python 3.x\n",
    "- TensorFlow, Pandas, NumPy, Matplotlib, Seaborn, Sklearn\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764b043",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "005ba9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c43e1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **2. SMUBiddingTransformer Class**\n",
    "\n",
    "### **SMU Bidding Data Feature Engineering Transformer**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `SMUBiddingTransformer` class is a comprehensive feature engineering pipeline designed specifically for Singapore Management University (SMU) course bidding data and optimized for CatBoost model training. It transforms raw tabular bidding data into machine learning-ready features while preserving categorical features where beneficial for CatBoost and creating embeddings only for high-cardinality features.\n",
    "\n",
    "**Key Features:**\n",
    "- **CatBoost-Optimized**: Preserves categorical features in their natural form for CatBoost's superior categorical handling while creating embeddings only for high-cardinality features (instructors, day combinations)\n",
    "- **Feature Type Tracking**: Automatically categorizes features into categorical, numeric, and embedding types with dedicated getter methods for each\n",
    "- **Smart Missing Value Handling**: Configurable approach to missing values - either let CatBoost handle them natively or create embeddings for missing data\n",
    "- **High-Cardinality Embeddings**: Creates dense vector representations of instructor names (1000+ unique) and day-of-week combinations using TF-IDF + SVD\n",
    "- **Course Code Intelligence**: Parses various course code formats including hyphenated codes ('COR-COMM175') and standard formats ('MGMT715')\n",
    "- **Bidding Window Parsing**: Extracts round numbers (including 1A, 1B, 2A formats) and window numbers from complex bidding window strings\n",
    "- **Categorical Preservation**: Keeps start_time, term, course_name, subject_area as categorical for CatBoost's target encoding\n",
    "- **Sklearn-Compatible**: Standard transformer interface with `fit()`, `transform()`, and `fit_transform()` methods\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Input Data Format:**\n",
    "The transformer expects a pandas DataFrame with these **required columns**:\n",
    "- `course_code` (str): Course identifier (e.g., 'MGMT715', 'COR-COMM175')\n",
    "- `course_name` (str): Full course name\n",
    "- `acad_year_start` (int): Academic year start (e.g., 2025)\n",
    "- `term` (str): Academic term ('1', '2', '3A', '3B')\n",
    "- `start_time` (str): Class start time (e.g., '19:30', 'TBA') - preserved as categorical\n",
    "- `day_of_week` (str): Days of week, can be comma-separated (e.g., 'Mon,Thu')\n",
    "- `before_process_vacancy` (int): Number of available vacancies\n",
    "- `bidding_window` (str): Bidding window descriptor (e.g., 'Round 1 Window 1', 'Incoming Freshmen Rnd 1 Win 4')\n",
    "- `instructor` (str): Instructor names, can be comma-separated (e.g., 'JOHN DOE, JANE SMITH')\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `pandas`, `numpy`, `sklearn` (TfidfVectorizer, TruncatedSVD)\n",
    "- Standard libraries: `typing`, `warnings`, `re`\n",
    "\n",
    "**Configuration Parameters:**\n",
    "- `n_instructor_components` (int, default=50): Embedding dimensions for instructor names (expected 1000+ unique instructors)\n",
    "- `n_day_components` (int, default=20): Embedding dimensions for day-of-week combinations (handles 7! combinations)\n",
    "- `use_embeddings_for_missing` (bool, default=False): If True, missing values get embeddings. If False (recommended), missing values remain as None/NaN for CatBoost to handle natively\n",
    "\n",
    "#### **Output Format**\n",
    "The transformer produces a pandas DataFrame with engineered features organized into three categories:\n",
    "\n",
    "**Categorical Features** (for CatBoost's `cat_features` parameter):\n",
    "- `subject_area`, `catalogue_no`, `round`, `term`, `start_time`, `course_name`\n",
    "\n",
    "**Numeric Features:**\n",
    "- `window`, `before_process_vacancy`, `acad_year_start`\n",
    "\n",
    "**Embedding Features:**\n",
    "- `instructor_embed_0` through `instructor_embed_{n_instructor_components-1}` (50 by default)\n",
    "- `day_embed_0` through `day_embed_{n_day_components-1}` (20 by default)\n",
    "\n",
    "#### **Usage in Jupyter Notebook**\n",
    "\n",
    "**Basic Usage:**\n",
    "```python\n",
    "from your_module import SMUBiddingTransformer\n",
    "\n",
    "# Initialize transformer\n",
    "transformer = SMUBiddingTransformer(\n",
    "    n_instructor_components=50,     # Instructor embedding size\n",
    "    n_day_components=20,            # Day-of-week embedding size  \n",
    "    use_embeddings_for_missing=False # Let CatBoost handle missing values\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train = transformer.fit_transform(training_dataframe)\n",
    "\n",
    "# Transform new data (after fitting)\n",
    "X_test = transformer.transform(test_dataframe)\n",
    "\n",
    "# Get feature lists for CatBoost\n",
    "categorical_features = transformer.get_categorical_features()\n",
    "numeric_features = transformer.get_numeric_features()\n",
    "embedding_features = transformer.get_embedding_features()\n",
    "```\n",
    "\n",
    "**CatBoost Integration:**\n",
    "```python\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Initialize transformer and prepare data\n",
    "transformer = SMUBiddingTransformer(use_embeddings_for_missing=False)\n",
    "X_train = transformer.fit_transform(training_dataframe)\n",
    "X_test = transformer.transform(test_dataframe)\n",
    "\n",
    "# Use transformer's feature categorization\n",
    "model = CatBoostRegressor(\n",
    "    cat_features=transformer.get_categorical_features(),\n",
    "    iterations=1000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    one_hot_max_size=10  # CatBoost will use target encoding for larger categories\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Feature Inspection:**\n",
    "```python\n",
    "# Check what features were generated\n",
    "print(\"Transformed shape:\", X_train.shape)\n",
    "print(\"\\nCategorical features for CatBoost:\")\n",
    "print(transformer.get_categorical_features())\n",
    "print(f\"\\nNumeric features ({len(transformer.get_numeric_features())}):\")\n",
    "print(transformer.get_numeric_features())\n",
    "print(f\"\\nEmbedding features ({len(transformer.get_embedding_features())}):\")\n",
    "print(transformer.get_embedding_features()[:10])  # First 10\n",
    "```\n",
    "\n",
    "**Resume Capability:**\n",
    "- **Stateful Transformer**: Once fitted, maintains vectorizers and SVD components for consistent transformations\n",
    "- **Missing Value Strategy**: Configurable handling - either create embeddings for missing data or let CatBoost handle them natively (recommended)\n",
    "- **Feature Consistency**: Maintains consistent categorical/numeric/embedding separation across different datasets\n",
    "- **Validation**: Validates input columns and provides clear error messages for missing required fields\n",
    "\n",
    "**Notes:**\n",
    "- Optimized specifically for CatBoost's categorical feature handling capabilities\n",
    "- High-cardinality features (instructors, day combinations) get embeddings while low-cardinality features remain categorical\n",
    "- The `use_embeddings_for_missing=False` default lets CatBoost handle missing values with its built-in missing value support\n",
    "- Feature type tracking enables easy CatBoost configuration without manual feature specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285e2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMUBiddingTransformer:\n",
    "    \"\"\"\n",
    "    A reusable transformer class for processing SMU course bidding data\n",
    "    optimized for CatBoost model.\n",
    "    \n",
    "    This transformer preserves categorical features where beneficial for CatBoost\n",
    "    while creating embeddings for high-cardinality features (instructors, day combinations).\n",
    "    \n",
    "    Expected input columns:\n",
    "    - course_code: str (e.g. 'MGMT715', 'COR-COMM175')\n",
    "    - course_name: str\n",
    "    - acad_year_start: int\n",
    "    - term: str ('1', '2', '3A', '3B')\n",
    "    - start_time: str (e.g. '19:30', 'TBA') - preserved as categorical\n",
    "    - day_of_week: str (can be multivalued, e.g. 'Mon,Thu')\n",
    "    - before_process_vacancy: int\n",
    "    - bidding_window: str (e.g. 'Round 1 Window 1', 'Incoming Freshmen Rnd 1 Win 4')\n",
    "    - instructor: str (can be multivalued, e.g. 'JOHN DOE, JANE SMITH')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_instructor_components: int = 50, \n",
    "                 n_day_components: int = 20,\n",
    "                 use_embeddings_for_missing: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the transformer with embedding dimensions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_instructor_components : int, default=50\n",
    "            Number of dimensions for instructor embeddings (expected 1000+ unique instructors)\n",
    "        n_day_components : int, default=20\n",
    "            Number of dimensions for day_of_week embeddings (handles 7! combinations)\n",
    "        use_embeddings_for_missing : bool, default=False\n",
    "            If True, missing values get embeddings. If False, they remain as None/NaN\n",
    "            for CatBoost to handle natively\n",
    "        \"\"\"\n",
    "        self.n_instructor_components = n_instructor_components\n",
    "        self.n_day_components = n_day_components\n",
    "        self.use_embeddings_for_missing = use_embeddings_for_missing\n",
    "        \n",
    "        # Vectorizers for high-cardinality features only\n",
    "        self.instructor_vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            ngram_range=(1, 2)  # Capture name variations\n",
    "        )\n",
    "        self.instructor_svd = TruncatedSVD(\n",
    "            n_components=n_instructor_components, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Day combination vectorizer\n",
    "        self.day_vectorizer = TfidfVectorizer(\n",
    "            max_features=150,\n",
    "            token_pattern=r'\\b\\w+\\b'\n",
    "        )\n",
    "        self.day_svd = TruncatedSVD(\n",
    "            n_components=n_day_components,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Fitted flags\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Lists to track categorical features for CatBoost\n",
    "        self.categorical_features = []\n",
    "        self.numeric_features = []\n",
    "        self.embedding_features = []\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame) -> 'SMUBiddingTransformer':\n",
    "        \"\"\"\n",
    "        Fit the transformer on training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Training dataframe with all required columns\n",
    "        \"\"\"\n",
    "        # Validate required columns\n",
    "        required_cols = [\n",
    "            'course_code', 'course_name', 'acad_year_start', 'term',\n",
    "            'start_time', 'day_of_week', 'before_process_vacancy',\n",
    "            'bidding_window', 'instructor'\n",
    "        ]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Process instructors for embeddings (high cardinality)\n",
    "        instructor_texts = self._process_instructor_for_embedding(df['instructor'])\n",
    "        if self.use_embeddings_for_missing or any(text != '' for text in instructor_texts):\n",
    "            instructor_tfidf = self.instructor_vectorizer.fit_transform(instructor_texts)\n",
    "            self.instructor_svd.fit(instructor_tfidf)\n",
    "        \n",
    "        # Process day combinations for embeddings (high cardinality)\n",
    "        day_texts = self._process_day_for_embedding(df['day_of_week'])\n",
    "        if self.use_embeddings_for_missing or any(text != '' for text in day_texts):\n",
    "            day_tfidf = self.day_vectorizer.fit_transform(day_texts)\n",
    "            self.day_svd.fit(day_tfidf)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform the dataframe to CatBoost-ready format.\n",
    "        \n",
    "        Returns both the transformed dataframe and lists of categorical feature indices\n",
    "        for CatBoost's cat_features parameter.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Transformer must be fitted before transform. Call fit() first.\")\n",
    "        \n",
    "        # Create a copy to avoid modifying original\n",
    "        df_transformed = df.copy()\n",
    "        \n",
    "        # Reset feature tracking\n",
    "        self.categorical_features = []\n",
    "        self.numeric_features = []\n",
    "        self.embedding_features = []\n",
    "        all_features = []\n",
    "        \n",
    "        # 1. Extract course components (categorical + numeric)\n",
    "        course_features = self._extract_course_features(df_transformed)\n",
    "        all_features.append(course_features)\n",
    "        \n",
    "        # 2. Process bidding window (categorical + numeric)\n",
    "        round_window_features = self._extract_round_window(df_transformed)\n",
    "        all_features.append(round_window_features)\n",
    "        \n",
    "        # 3. Basic features (preserve categorical nature)\n",
    "        basic_features = self._process_basic_features(df_transformed)\n",
    "        all_features.append(basic_features)\n",
    "        \n",
    "        # 4. Create instructor embeddings (only for non-missing)\n",
    "        instructor_embeddings = self._create_instructor_embeddings(df_transformed)\n",
    "        if instructor_embeddings is not None:\n",
    "            all_features.append(instructor_embeddings)\n",
    "        \n",
    "        # 5. Create day embeddings (only for non-missing)\n",
    "        day_embeddings = self._create_day_embeddings(df_transformed)\n",
    "        if day_embeddings is not None:\n",
    "            all_features.append(day_embeddings)\n",
    "        \n",
    "        # Combine all features\n",
    "        final_df = pd.concat(all_features, axis=1)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fit the transformer and transform the data in one step.\"\"\"\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "    \n",
    "    def get_categorical_features(self) -> List[str]:\n",
    "        \"\"\"Get list of categorical feature names for CatBoost.\"\"\"\n",
    "        return self.categorical_features.copy()\n",
    "    \n",
    "    def get_numeric_features(self) -> List[str]:\n",
    "        \"\"\"Get list of numeric feature names.\"\"\"\n",
    "        return self.numeric_features.copy()\n",
    "    \n",
    "    def get_embedding_features(self) -> List[str]:\n",
    "        \"\"\"Get list of embedding feature names.\"\"\"\n",
    "        return self.embedding_features.copy()\n",
    "    \n",
    "    def _extract_course_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract subject area and catalogue number from course code.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        def split_course_code(code):\n",
    "            \"\"\"Split course code into subject area and catalogue number.\"\"\"\n",
    "            if pd.isna(code):\n",
    "                return None, None\n",
    "            \n",
    "            code = str(code).strip().upper()\n",
    "            \n",
    "            # Handle hyphenated codes like 'COR-COMM175'\n",
    "            if '-' in code:\n",
    "                parts = code.split('-')\n",
    "                if len(parts) >= 2:\n",
    "                    subject = '-'.join(parts[:-1])\n",
    "                    # Extract number from last part\n",
    "                    num_match = re.search(r'(\\d+)', parts[-1])\n",
    "                    if num_match:\n",
    "                        return subject, int(num_match.group(1))\n",
    "                    else:\n",
    "                        # Try extracting from full last part\n",
    "                        num_match = re.search(r'(\\d+)', code)\n",
    "                        if num_match:\n",
    "                            return subject, int(num_match.group(1))\n",
    "            \n",
    "            # Standard format like 'MGMT715'\n",
    "            match = re.match(r'([A-Z\\-]+)(\\d+)', code)\n",
    "            if match:\n",
    "                return match.group(1), int(match.group(2))\n",
    "            \n",
    "            return code, 0\n",
    "        \n",
    "        # Extract components\n",
    "        splits = df['course_code'].apply(split_course_code)\n",
    "        features['subject_area'] = splits.apply(lambda x: x[0] if x else None)\n",
    "        features['catalogue_no'] = splits.apply(lambda x: x[1] if x else 0)\n",
    "        \n",
    "        # subject_area and catalogue_no are categorical for CatBoost\n",
    "        self.categorical_features.extend(['subject_area', 'catalogue_no'])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_round_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract round and window from bidding_window string.\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        def parse_bidding_window(window_str):\n",
    "            \"\"\"Parse bidding window string into round and window number.\"\"\"\n",
    "            if pd.isna(window_str):\n",
    "                return None, None\n",
    "            \n",
    "            window_str = str(window_str).strip()\n",
    "            \n",
    "            # Handle patterns from V4_01 notebook\n",
    "            import re\n",
    "            match = re.search(r'Round\\s+(\\d[A-C]?)\\s+Window\\s+(\\d)', window_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1), int(match.group(2))\n",
    "            \n",
    "            match = re.search(r'Rnd\\s+(\\d[A-C]?)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1), int(match.group(2))\n",
    "            \n",
    "            match = re.search(r'(\\d[A-C]?)', window_str)\n",
    "            if match:\n",
    "                win_match = re.search(r'Window\\s+(\\d)|Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
    "                if win_match:\n",
    "                    window_num = int(win_match.group(1) or win_match.group(2))\n",
    "                    return match.group(1), window_num\n",
    "                return match.group(1), 1\n",
    "            \n",
    "            return '1', 1\n",
    "        \n",
    "        # Extract round and window\n",
    "        parsed = df['bidding_window'].apply(parse_bidding_window)\n",
    "        features['round'] = parsed.apply(lambda x: x[0] if x else '1')\n",
    "        features['window'] = parsed.apply(lambda x: x[1] if x else 1)\n",
    "        \n",
    "        # Round as categorical (preserves ordering like 1, 1A, 1B, 2, 2A)\n",
    "        self.categorical_features.append('round')\n",
    "        \n",
    "        # Window as numeric\n",
    "        self.numeric_features.append('window')\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _process_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "            \"\"\"Process basic features, preserving categorical nature where beneficial.\"\"\"\n",
    "            features = pd.DataFrame(index=df.index)\n",
    "            \n",
    "            # Numeric features\n",
    "            features['before_process_vacancy'] = pd.to_numeric(\n",
    "                df['before_process_vacancy'], errors='coerce'\n",
    "            ).fillna(0)\n",
    "            features['acad_year_start'] = pd.to_numeric(\n",
    "                df['acad_year_start'], errors='coerce'\n",
    "            ).fillna(2025)\n",
    "            \n",
    "            self.numeric_features.extend(['before_process_vacancy', 'acad_year_start'])\n",
    "            \n",
    "            # Categorical features\n",
    "            features['term'] = df['term'].astype(str)\n",
    "            features['start_time'] = df['start_time'].astype(str)\n",
    "            features['course_name'] = df['course_name'].astype(str)\n",
    "            features['section'] = df['section'].astype(str)  # Add section as categorical\n",
    "            \n",
    "            # Replace empty strings with None for proper CatBoost handling\n",
    "            features.loc[features['start_time'].isin(['', 'nan']), 'start_time'] = None\n",
    "            features.loc[features['course_name'].isin(['', 'nan']), 'course_name'] = None\n",
    "            features.loc[features['section'].isin(['', 'nan']), 'section'] = None\n",
    "            \n",
    "            self.categorical_features.extend(['term', 'start_time', 'course_name', 'section'])\n",
    "            \n",
    "            return features\n",
    "        \n",
    "    def _process_instructor_for_embedding(self, instructor_series: pd.Series) -> List[str]:\n",
    "        \"\"\"Process instructor names for embedding - only non-missing values.\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for instructor in instructor_series:\n",
    "            if pd.isna(instructor) or str(instructor).strip() == '' or str(instructor).upper() == 'TBA':\n",
    "                if self.use_embeddings_for_missing:\n",
    "                    processed.append('MISSING_INSTRUCTOR')\n",
    "                else:\n",
    "                    processed.append('')  # Will result in zero embeddings\n",
    "            else:\n",
    "                # Combine multiple instructors\n",
    "                names = [name.strip().upper() for name in str(instructor).split(',')]\n",
    "                processed.append(' '.join(names))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _process_day_for_embedding(self, day_series: pd.Series) -> List[str]:\n",
    "        \"\"\"Process day combinations for embedding - only non-missing values.\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        day_abbrev = {\n",
    "            'MONDAY': 'MON', 'TUESDAY': 'TUE', 'WEDNESDAY': 'WED',\n",
    "            'THURSDAY': 'THU', 'FRIDAY': 'FRI', 'SATURDAY': 'SAT', 'SUNDAY': 'SUN',\n",
    "            'MON': 'MON', 'TUE': 'TUE', 'WED': 'WED', 'THU': 'THU',\n",
    "            'FRI': 'FRI', 'SAT': 'SAT', 'SUN': 'SUN'\n",
    "        }\n",
    "        \n",
    "        for days in day_series:\n",
    "            if pd.isna(days) or str(days).strip() == '':\n",
    "                if self.use_embeddings_for_missing:\n",
    "                    processed.append('MISSING_DAY')\n",
    "                else:\n",
    "                    processed.append('')  # Will result in zero embeddings\n",
    "            else:\n",
    "                # Handle multiple days\n",
    "                day_list = []\n",
    "                for day in str(days).split(','):\n",
    "                    day_upper = day.strip().upper()\n",
    "                    day_list.append(day_abbrev.get(day_upper, day_upper))\n",
    "                # Sort for consistency\n",
    "                day_list.sort()\n",
    "                processed.append('_'.join(day_list))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _create_instructor_embeddings(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Create instructor embeddings only for non-missing values.\"\"\"\n",
    "        instructor_texts = self._process_instructor_for_embedding(df['instructor'])\n",
    "        \n",
    "        # Check if we have any non-empty texts\n",
    "        if not self.use_embeddings_for_missing and all(text == '' for text in instructor_texts):\n",
    "            return None\n",
    "        \n",
    "        # Transform using fitted vectorizer\n",
    "        instructor_tfidf = self.instructor_vectorizer.transform(instructor_texts)\n",
    "        instructor_embeddings = self.instructor_svd.transform(instructor_tfidf)\n",
    "        \n",
    "        # Create dataframe with embedding columns\n",
    "        embedding_cols = [f'instructor_embed_{i}' for i in range(self.n_instructor_components)]\n",
    "        instructor_df = pd.DataFrame(\n",
    "            instructor_embeddings,\n",
    "            columns=embedding_cols,\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        # Set embeddings to 0 for missing instructors if not using embeddings for missing\n",
    "        if not self.use_embeddings_for_missing:\n",
    "            mask = [text == '' for text in instructor_texts]\n",
    "            instructor_df.loc[mask] = 0\n",
    "        \n",
    "        self.embedding_features.extend(embedding_cols)\n",
    "        \n",
    "        return instructor_df\n",
    "    \n",
    "    def _create_day_embeddings(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Create day combination embeddings only for non-missing values.\"\"\"\n",
    "        day_texts = self._process_day_for_embedding(df['day_of_week'])\n",
    "        \n",
    "        # Check if we have any non-empty texts\n",
    "        if not self.use_embeddings_for_missing and all(text == '' for text in day_texts):\n",
    "            return None\n",
    "        \n",
    "        # Transform using fitted vectorizer\n",
    "        day_tfidf = self.day_vectorizer.transform(day_texts)\n",
    "        day_embeddings = self.day_svd.transform(day_tfidf)\n",
    "        \n",
    "        # Create dataframe with embedding columns\n",
    "        embedding_cols = [f'day_embed_{i}' for i in range(self.n_day_components)]\n",
    "        day_df = pd.DataFrame(\n",
    "            day_embeddings,\n",
    "            columns=embedding_cols,\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        # Set embeddings to 0 for missing days if not using embeddings for missing\n",
    "        if not self.use_embeddings_for_missing:\n",
    "            mask = [text == '' for text in day_texts]\n",
    "            day_df.loc[mask] = 0\n",
    "        \n",
    "        self.embedding_features.extend(embedding_cols)\n",
    "        \n",
    "        return day_df\n",
    "\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Get all feature names after transformation.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Transformer must be fitted to get feature names.\")\n",
    "        \n",
    "        return self.categorical_features + self.numeric_features + self.embedding_features\n",
    "\n",
    "# # Example usage with CatBoost\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create sample data\n",
    "#     sample_data = pd.DataFrame({\n",
    "#         'course_code': ['MGMT715', 'COR-COMM175', 'ECON101', 'STAT201'],\n",
    "#         'course_name': ['Strategic Management', 'Business Communication', 'Principles of Economics', 'Applied Statistics'],\n",
    "#         'acad_year_start': [2025, 2025, 2025, 2025],\n",
    "#         'term': ['1', '2', '3A', '1'],\n",
    "#         'start_time': ['19:30', '14:00', 'TBA', '10:00'],\n",
    "#         'day_of_week': ['Mon,Thu', 'Tue', '', 'Mon,Wed,Fri'],\n",
    "#         'before_process_vacancy': [10, 5, 15, 8],\n",
    "#         'bidding_window': ['Round 1 Window 1', 'Round 2A Window 3', 'Incoming Freshmen Rnd 1 Win 2', 'Round 1B Window 2'],\n",
    "#         'instructor': ['JOHN DOE, JANE SMITH', 'ROBERT LEE', 'TBA', '']\n",
    "#     })\n",
    "    \n",
    "#     # Initialize transformer\n",
    "#     transformer = SMUBiddingTransformer(\n",
    "#         n_instructor_components=30,\n",
    "#         n_day_components=15,\n",
    "#         use_embeddings_for_missing=False  # Let CatBoost handle missing values\n",
    "#     )\n",
    "    \n",
    "#     # Fit and transform\n",
    "#     X_train = transformer.fit_transform(sample_data)\n",
    "    \n",
    "#     print(\"Transformed shape:\", X_train.shape)\n",
    "#     print(\"\\nCategorical features for CatBoost:\")\n",
    "#     print(transformer.get_categorical_features())\n",
    "#     print(\"\\nNumeric features:\")\n",
    "#     print(transformer.get_numeric_features()[:10])  # First 10\n",
    "#     print(\"\\nEmbedding features:\")\n",
    "#     print(transformer.get_embedding_features()[:10])  # First 10\n",
    "    \n",
    "#     # Example CatBoost integration\n",
    "#     print(\"\\n# CatBoost Usage Example:\")\n",
    "#     print(\"from catboost import CatBoostRegressor\")\n",
    "#     print(\"model = CatBoostRegressor(\")\n",
    "#     print(f\"    cat_features={transformer.get_categorical_features()},\")\n",
    "#     print(\"    iterations=1000,\")\n",
    "#     print(\"    learning_rate=0.03,\")\n",
    "#     print(\"    depth=6\")\n",
    "#     print(\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfd160",
   "metadata": {},
   "source": [
    "## **3. SMUDataMerger Class**\n",
    "\n",
    "### **SMU Raw Data and BOSS Results Integration Pipeline**\n",
    "\n",
    "#### **What This Code Does**\n",
    "The `SMUDataMerger` class is a comprehensive data integration pipeline that combines SMU's raw course data with BOSS (Banner Online Self-Service) bidding results to create a unified dataset for machine learning analysis. It intelligently merges timing information from multiple data sources and creates course-level records suitable for the `SMUBiddingTransformer`.\n",
    "\n",
    "**Key Features:**\n",
    "- **Multi-Source Data Integration**: Combines Excel-based raw data (standalone + multiple sheets) with BOSS results from multiple Excel files\n",
    "- **Intelligent Class Timing Aggregation**: Groups class sessions by `record_key` and consolidates timing information (days, times, venues, instructors) for courses with multiple sessions\n",
    "- **Flexible BOSS Results Loading**: Automatically discovers and processes all Excel files in the `overallBossResults` folder with filename-based metadata extraction\n",
    "- **Smart Course Matching**: Creates standardized course keys (`course_code_section_year_term`) for precise matching between raw data and BOSS results\n",
    "- **Data Quality Handling**: Manages missing values, duplicate timings, and inconsistent formats across different data sources\n",
    "- **Output Standardization**: Produces a clean dataset with columns mapped to `SMUBiddingTransformer` requirements\n",
    "- **Comprehensive Logging**: Tracks merge statistics, missing data, and data quality issues throughout the process\n",
    "\n",
    "#### **What Is Required**\n",
    "\n",
    "**Input Data Structure:**\n",
    "- **Raw Data Excel File** (`script_input/raw_data.xlsx`):\n",
    "  - `standalone` sheet: Core course information with columns like `course_code`, `section`, `acad_year_start`, `term`, `record_key`\n",
    "  - `multiple` sheet: Detailed class sessions with `type`, `day_of_week`, `start_time`, `venue`, `professor_name`, `record_key`\n",
    "- **BOSS Results Folder** (`script_input/overallBossResults/`):\n",
    "  - Multiple Excel files containing bidding results with columns: `Course Code`, `Section`, `Term`, `Before Process Vacancy`, `Bidding Window`, `Instructor`, `Median Bid`, `Min Bid`, `Vacancy`\n",
    "\n",
    "**Technical Dependencies:**\n",
    "- Python packages: `pandas`, `glob`, `os`, `re`, `datetime`, `collections.Counter`\n",
    "- Excel file reading capabilities (openpyxl or xlrd)\n",
    "\n",
    "**Directory Structure:**\n",
    "- Input: `script_input/raw_data.xlsx` and `script_input/overallBossResults/*.xlsx`\n",
    "- Output: `script_output/model_training/model_data_{timestamp}.csv`\n",
    "\n",
    "**Configuration Parameters:**\n",
    "- `raw_data_path` (str, default=\"script_input/raw_data.xlsx\"): Path to raw data Excel file\n",
    "- `boss_results_folder` (str, default=\"script_input/overallBossResults\"): Folder containing BOSS results Excel files\n",
    "\n",
    "#### **Output Format**\n",
    "The merger produces a timestamped CSV file with columns mapped to `SMUBiddingTransformer` requirements:\n",
    "\n",
    "**Primary Columns:**\n",
    "- `course_code`, `course_name`, `acad_year_start`, `term`, `section`\n",
    "- `start_time`, `day_of_week` (aggregated from multiple class sessions)\n",
    "- `before_process_vacancy`, `bidding_window`, `instructor`\n",
    "- `median_bid`, `min_bid`, `vacancy`, `grading_basis`\n",
    "\n",
    "**Data Aggregation Logic:**\n",
    "- **Days**: Comma-separated unique days (e.g., \"Mon, Wed, Fri\")\n",
    "- **Start Time**: Most common start time across sessions\n",
    "- **Venue**: Comma-separated venue list\n",
    "- **Instructors**: Comma-separated instructor names\n",
    "\n",
    "#### **Usage in Jupyter Notebook**\n",
    "\n",
    "**Basic Usage:**\n",
    "```python\n",
    "from your_module import SMUDataMerger\n",
    "\n",
    "# Initialize merger with default paths\n",
    "merger = SMUDataMerger()\n",
    "\n",
    "# Execute the complete merge process\n",
    "final_dataset = merger.process_and_merge()\n",
    "\n",
    "# Check results\n",
    "print(f\"Final dataset shape: {final_dataset.shape}\")\n",
    "print(f\"Unique courses: {final_dataset['course_code'].nunique()}\")\n",
    "```\n",
    "\n",
    "**Custom Configuration:**\n",
    "```python\n",
    "# Custom paths\n",
    "merger = SMUDataMerger(\n",
    "    raw_data_path=\"custom_path/raw_data.xlsx\",\n",
    "    boss_results_folder=\"custom_path/boss_results\"\n",
    ")\n",
    "\n",
    "# Execute merge\n",
    "final_dataset = merger.process_and_merge()\n",
    "```\n",
    "\n",
    "**Step-by-Step Processing:**\n",
    "```python\n",
    "# For debugging or custom processing\n",
    "merger = SMUDataMerger()\n",
    "\n",
    "# Step 1: Load raw data\n",
    "standalone_df, class_df = merger.load_raw_data()\n",
    "\n",
    "# Step 2: Process timing aggregation\n",
    "timing_summary = merger.process_class_timings(class_df)\n",
    "\n",
    "# Step 3: Load BOSS results\n",
    "boss_df = merger.load_boss_results()\n",
    "\n",
    "# Step 4: Perform merge\n",
    "final_df, detailed_df = merger.merge_data(standalone_df, timing_summary, boss_df)\n",
    "```\n",
    "\n",
    "**Integration with SMUBiddingTransformer:**\n",
    "```python\n",
    "# Complete pipeline from raw data to ML features\n",
    "merger = SMUDataMerger()\n",
    "transformer = SMUBiddingTransformer()\n",
    "\n",
    "# Step 1: Merge raw data sources\n",
    "merged_data = merger.process_and_merge()\n",
    "\n",
    "# Step 2: Transform to ML features\n",
    "ml_features = transformer.fit_transform(merged_data)\n",
    "\n",
    "# Ready for CatBoost training\n",
    "print(f\"ML-ready features: {ml_features.shape}\")\n",
    "```\n",
    "\n",
    "**Resume Capability:**\n",
    "- **Automatic Output Management**: Creates timestamped output files to avoid overwriting previous merges\n",
    "- **Data Validation**: Validates merge quality and reports statistics on successful matches\n",
    "- **Error Handling**: Gracefully handles missing files, corrupted data, and merge failures\n",
    "- **Logging**: Comprehensive logging of merge statistics, missing data patterns, and data quality issues\n",
    "\n",
    "**Notes:**\n",
    "- Designed specifically for SMU's data structure with `record_key` linking between sheets\n",
    "- Handles complex class scheduling where courses have multiple sessions (lectures, tutorials, labs)\n",
    "- Inner join strategy ensures only courses with complete raw data + BOSS results are included\n",
    "- Output format directly compatible with `SMUBiddingTransformer` input requirements\n",
    "- Automatic timestamp-based file naming prevents accidental data overwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f45b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class SMUDataMerger:\n",
    "    \"\"\"\n",
    "    A class to merge SMU raw data with boss results data for bidding analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, raw_data_path=\"script_input/raw_data.xlsx\", \n",
    "                 boss_results_folder=\"script_input/overallBossResults\"):\n",
    "        self.raw_data_path = raw_data_path\n",
    "        self.boss_results_folder = boss_results_folder\n",
    "        self.output_folder = \"script_output/model_training\"\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "    \n",
    "    def load_raw_data(self):\n",
    "        \"\"\"\n",
    "        Load and process the raw_data.xlsx file with standalone and multiple sheets.\n",
    "        \"\"\"\n",
    "        print(f\"Loading raw data from {self.raw_data_path}\")\n",
    "        \n",
    "        # Load both sheets\n",
    "        standalone_df = pd.read_excel(self.raw_data_path, sheet_name='standalone')\n",
    "        multiple_df = pd.read_excel(self.raw_data_path, sheet_name='multiple')\n",
    "        \n",
    "        print(f\"Standalone sheet: {standalone_df.shape[0]} rows\")\n",
    "        print(f\"Multiple sheet: {multiple_df.shape[0]} rows\")\n",
    "        \n",
    "        # Filter multiple_df to only include CLASS entries (ignore EXAM)\n",
    "        class_df = multiple_df[multiple_df['type'] == 'CLASS'].copy()\n",
    "        print(f\"Class entries in multiple sheet: {class_df.shape[0]} rows\")\n",
    "        \n",
    "        return standalone_df, class_df\n",
    "\n",
    "    def process_class_timings(self, class_df):\n",
    "        \"\"\"\n",
    "        Process class timings by grouping by record_key and aggregating the timing information.\n",
    "        \"\"\"\n",
    "        if class_df.empty:\n",
    "            return pd.DataFrame(columns=['record_key', 'day_of_week', 'start_time'])\n",
    "        \n",
    "        def aggregate_days(days):\n",
    "            # Remove NaN values and convert to set to remove duplicates\n",
    "            valid_days = [day for day in days if pd.notna(day)]\n",
    "            if not valid_days:\n",
    "                return None\n",
    "            unique_days = sorted(set(valid_days))\n",
    "            return ', '.join(unique_days)\n",
    "        \n",
    "        def get_most_common_time(times):\n",
    "            # Remove NaN values\n",
    "            valid_times = [time for time in times if pd.notna(time)]\n",
    "            if not valid_times:\n",
    "                return None\n",
    "            # Get most common time, or first occurrence if tie\n",
    "            time_counts = Counter(valid_times)\n",
    "            return time_counts.most_common(1)[0][0]\n",
    "        \n",
    "        # Group by record_key and aggregate\n",
    "        timing_summary = class_df.groupby('record_key').agg({\n",
    "            'day_of_week': aggregate_days,\n",
    "            'start_time': get_most_common_time,\n",
    "            'venue': lambda x: ', '.join([str(v) for v in x if pd.notna(v)]) if any(pd.notna(v) for v in x) else None,\n",
    "            'professor_name': lambda x: ', '.join([str(p) for p in x if pd.notna(p)]) if any(pd.notna(p) for p in x) else None\n",
    "        }).reset_index()\n",
    "        \n",
    "        return timing_summary\n",
    "\n",
    "    def combine_raw_data(self, standalone_df, class_df):\n",
    "        \"\"\"\n",
    "        Combine standalone and multiple (class) data into one flat dataset.\n",
    "        \"\"\"\n",
    "        print(\"Combining standalone and class timing data...\")\n",
    "        \n",
    "        # Process class timings first\n",
    "        timing_summary = self.process_class_timings(class_df)\n",
    "        \n",
    "        # Merge standalone with timing summary\n",
    "        if not timing_summary.empty:\n",
    "            combined_df = pd.merge(\n",
    "                standalone_df,\n",
    "                timing_summary,\n",
    "                on='record_key',\n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            combined_df = standalone_df.copy()\n",
    "            combined_df['day_of_week'] = None\n",
    "            combined_df['start_time'] = None\n",
    "            combined_df['venue'] = None\n",
    "            combined_df['professor_name'] = None\n",
    "        \n",
    "        # Create boss-compatible term format: \"2021-22 Term 1\"\n",
    "        def create_boss_term_format(row):\n",
    "            if pd.notna(row['acad_year_start']) and pd.notna(row['acad_year_end']) and pd.notna(row['term']):\n",
    "                year_start = int(row['acad_year_start'])\n",
    "                year_end = str(int(row['acad_year_end']))[-2:]  # Last 2 digits\n",
    "                term = str(row['term']).strip()\n",
    "                if term.startswith('T'):\n",
    "                    term = term[1:]  # Remove T prefix\n",
    "                return f\"{year_start}-{year_end} Term {term}\"\n",
    "            return None\n",
    "        \n",
    "        # Add boss_term_format column for matching\n",
    "        combined_df['boss_term_format'] = combined_df.apply(create_boss_term_format, axis=1)\n",
    "        \n",
    "        print(f\"Combined raw data shape: {combined_df.shape}\")\n",
    "        print(f\"Sample boss term formats created: {combined_df['boss_term_format'].value_counts().head()}\")\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "    def standardize_term_format(self, term_str):\n",
    "        \"\"\"\n",
    "        Convert term formats - remove T prefix if present.\n",
    "        \"\"\"\n",
    "        if pd.isna(term_str):\n",
    "            return None\n",
    "        \n",
    "        term_str = str(term_str).strip().upper()\n",
    "        \n",
    "        # If starts with 'T', remove it\n",
    "        if term_str.startswith('T'):\n",
    "            return term_str[1:]\n",
    "        \n",
    "        return term_str\n",
    "    \n",
    "    def clean_text_encoding(self, text):\n",
    "        \"\"\"\n",
    "        Clean text encoding issues from web scraping.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        \n",
    "        text = str(text)\n",
    "        \n",
    "        # Common encoding fixes\n",
    "        replacements = {\n",
    "            'Ã¢â‚¬\"': 'â€“',  # en dash\n",
    "            'Ã¢â‚¬â„¢': \"'\",  # apostrophe\n",
    "            'Ã¢â‚¬Å“': '\"',  # left quote\n",
    "            'Ã¢â‚¬': '\"',   # right quote\n",
    "            'ÃƒÂ©': 'Ã©',   # e acute\n",
    "            'ÃƒÂ¨': 'Ã¨',   # e grave\n",
    "            'Ãƒ ': 'Ã ',   # a grave\n",
    "            'ÃƒÂ¢': 'Ã¢',   # a circumflex\n",
    "            'ÃƒÂ®': 'Ã®',   # i circumflex\n",
    "            'ÃƒÂ´': 'Ã´',   # o circumflex\n",
    "            'ÃƒÂ»': 'Ã»',   # u circumflex\n",
    "            'ÃƒÂ§': 'Ã§',   # c cedilla\n",
    "            'Ã¢â‚¬Â¦': '...',  # ellipsis\n",
    "            'Ã¢â‚¬â€°': ' ',   # thin space\n",
    "            'Ã‚': '',      # non-breaking space artifact\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Remove any remaining non-ASCII characters that might cause issues\n",
    "        # But keep common accented characters\n",
    "        import unicodedata\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def create_course_key(self, course_code, section, acad_year_start, term):\n",
    "        \"\"\"\n",
    "        Create a standardized key for matching courses.\n",
    "        Format: COURSECODE_SECTION_YEAR_TERM\n",
    "        \"\"\"\n",
    "        if pd.isna(course_code) or pd.isna(section):\n",
    "            return None\n",
    "        \n",
    "        # Clean course code and section\n",
    "        course_code_clean = str(course_code).strip().upper()\n",
    "        section_clean = str(section).strip().upper()\n",
    "        \n",
    "        # Handle academic year - ensure it's an integer\n",
    "        if pd.notna(acad_year_start):\n",
    "            acad_year = int(float(acad_year_start))\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # Standardize term format (remove T prefix if present)\n",
    "        if pd.notna(term):\n",
    "            term_clean = self.standardize_term_format(term)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        key = f\"{course_code_clean}_{section_clean}_{acad_year}_{term_clean}\"\n",
    "        return key\n",
    "\n",
    "    def load_boss_results(self):\n",
    "        \"\"\"\n",
    "        Load and combine all Excel files from the overallBossResults folder.\n",
    "        \"\"\"\n",
    "        print(f\"Loading boss results from {self.boss_results_folder}\")\n",
    "        \n",
    "        # Find all Excel files in the folder\n",
    "        excel_files = glob.glob(os.path.join(self.boss_results_folder, \"*.xlsx\"))\n",
    "        \n",
    "        if not excel_files:\n",
    "            print(\"No Excel files found in the boss results folder!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Found {len(excel_files)} Excel files\")\n",
    "        \n",
    "        all_boss_data = []\n",
    "        \n",
    "        for file_path in excel_files:\n",
    "            try:\n",
    "                # Extract academic year and term from filename\n",
    "                filename = os.path.basename(file_path)\n",
    "                print(f\"Processing file: {filename}\")\n",
    "                \n",
    "                # Load the Excel file\n",
    "                df = pd.read_excel(file_path)\n",
    "                \n",
    "                # Add source filename for tracking\n",
    "                df['source_file'] = filename\n",
    "                \n",
    "                all_boss_data.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_boss_data:\n",
    "            print(\"No valid data found in boss results files!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine all dataframes\n",
    "        combined_boss_df = pd.concat(all_boss_data, ignore_index=True)\n",
    "        print(f\"Combined boss results: {combined_boss_df.shape[0]} rows\")\n",
    "        \n",
    "        return combined_boss_df\n",
    "\n",
    "    def parse_term_info(self, term_str):\n",
    "        \"\"\"\n",
    "        Parse term string like \"2021-22 Term 2\" to extract academic year start and term.\n",
    "        \"\"\"\n",
    "        if pd.isna(term_str):\n",
    "            return None, None\n",
    "        \n",
    "        try:\n",
    "            # Pattern: \"YYYY-YY Term X\"\n",
    "            match = re.match(r'(\\d{4})-\\d{2}\\s+Term\\s+(.+)', str(term_str).strip())\n",
    "            if match:\n",
    "                acad_year_start = int(match.group(1))\n",
    "                term = match.group(2).strip()  # This will be \"1\", \"2\", \"3A\", \"3B\"\n",
    "                return acad_year_start, term\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing term '{term_str}': {e}\")\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    def merge_data(self, combined_raw_df, boss_df):\n",
    "        \"\"\"\n",
    "        Merge the combined raw data with boss results data.\n",
    "        \"\"\"\n",
    "        print(\"Starting data merge process...\")\n",
    "        \n",
    "        # Filter out rows with missing terms to avoid duplicates\n",
    "        print(f\"\\nRows before filtering missing terms: {len(combined_raw_df)}\")\n",
    "        combined_raw_df = combined_raw_df[combined_raw_df['term'].notna()].copy()\n",
    "        print(f\"Rows after filtering missing terms: {len(combined_raw_df)}\")\n",
    "        \n",
    "        # Clean boss results - remove unnamed columns\n",
    "        boss_columns_to_keep = ['Term', 'Session', 'Bidding Window', 'Course Code', 'Description', \n",
    "                            'Section', 'Vacancy', 'Opening Vacancy', 'Before Process Vacancy', \n",
    "                            'D.I.C.E', 'After Process Vacancy', 'Enrolled Students', \n",
    "                            'Median Bid', 'Min Bid', 'Instructor', 'School/Department', 'source_file']\n",
    "        \n",
    "        # Filter boss_df to only keep valid columns\n",
    "        boss_df_clean = boss_df[[col for col in boss_columns_to_keep if col in boss_df.columns]].copy()\n",
    "        \n",
    "        # Parse boss results term information to extract year and term\n",
    "        boss_df_clean[['boss_acad_year_start', 'boss_term']] = boss_df_clean['Term'].apply(\n",
    "            lambda x: pd.Series(self.parse_term_info(x))\n",
    "        )\n",
    "        \n",
    "        # Create course keys for raw data\n",
    "        combined_raw_df['course_key'] = combined_raw_df.apply(\n",
    "            lambda row: self.create_course_key(\n",
    "                row['course_code'], \n",
    "                row['section'], \n",
    "                row['acad_year_start'], \n",
    "                row['term']  # This is the original term like T1, T2\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Create course keys for boss data\n",
    "        boss_df_clean['course_key'] = boss_df_clean.apply(\n",
    "            lambda row: self.create_course_key(\n",
    "                row['Course Code'], \n",
    "                row['Section'], \n",
    "                row['boss_acad_year_start'], \n",
    "                row['boss_term']  # This should already be in format like \"1\", \"2\", \"3A\"\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Debug: Show sample keys\n",
    "        print(f\"\\nRaw data course keys (first 5):\")\n",
    "        print(combined_raw_df[['course_code', 'section', 'term', 'course_key']].dropna().head())\n",
    "        print(f\"\\nBoss data course keys (first 5):\")\n",
    "        print(boss_df_clean[['Course Code', 'Section', 'Term', 'boss_term', 'course_key']].dropna().head())\n",
    "        \n",
    "        print(f\"\\nRaw data valid keys: {combined_raw_df['course_key'].notna().sum()} out of {len(combined_raw_df)}\")\n",
    "        print(f\"Boss data valid keys: {boss_df_clean['course_key'].notna().sum()} out of {len(boss_df_clean)}\")\n",
    "        \n",
    "        # Find common keys\n",
    "        raw_keys = set(combined_raw_df['course_key'].dropna())\n",
    "        boss_keys = set(boss_df_clean['course_key'].dropna())\n",
    "        common_keys = raw_keys.intersection(boss_keys)\n",
    "        print(f\"\\nCommon keys found: {len(common_keys)}\")\n",
    "        \n",
    "        if len(common_keys) == 0:\n",
    "            print(\"\\nNo matching keys found. Checking for mismatches...\")\n",
    "            print(\"Sample raw keys:\", list(raw_keys)[:5])\n",
    "            print(\"Sample boss keys:\", list(boss_keys)[:5])\n",
    "        \n",
    "        # Perform the merge\n",
    "        merged_df = pd.merge(\n",
    "            combined_raw_df,\n",
    "            boss_df_clean,\n",
    "            on='course_key',\n",
    "            how='inner',\n",
    "            suffixes=('_raw', '_boss')\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nMerged data: {merged_df.shape[0]} rows\")\n",
    "        \n",
    "        if merged_df.empty:\n",
    "            print(\"No matching records found between raw data and boss results.\")\n",
    "            return pd.DataFrame(), merged_df\n",
    "        \n",
    "        # Create the final dataframe with only required columns\n",
    "        final_df = pd.DataFrame()\n",
    "        \n",
    "        # Map columns to match SMUBiddingTransformer expected input\n",
    "        column_mapping = {\n",
    "            'course_code': 'course_code',\n",
    "            'course_name': 'course_name',\n",
    "            'acad_year_start': 'acad_year_start',\n",
    "            'term': 'term',  # Original term format\n",
    "            'start_time': 'start_time',\n",
    "            'day_of_week': 'day_of_week',\n",
    "            'before_process_vacancy': 'Before Process Vacancy',\n",
    "            'bidding_window': 'Bidding Window',\n",
    "            'instructor': 'professor_name',  # Prefer professor_name from class data\n",
    "            'median_bid': 'Median Bid',\n",
    "            'min_bid': 'Min Bid',\n",
    "            'section': 'section'\n",
    "            # REMOVED: 'vacancy' and 'grading_basis'\n",
    "        }\n",
    "        \n",
    "        # If professor_name is empty, use Instructor from boss results\n",
    "        for new_col, source_col in column_mapping.items():\n",
    "            if source_col in merged_df.columns:\n",
    "                final_df[new_col] = merged_df[source_col]\n",
    "            else:\n",
    "                print(f\"Warning: Column {source_col} not found in merged data\")\n",
    "                final_df[new_col] = None\n",
    "        \n",
    "        # Special handling for instructor - use boss Instructor if professor_name is empty\n",
    "        if 'Instructor' in merged_df.columns:\n",
    "            mask = final_df['instructor'].isna() | (final_df['instructor'] == '')\n",
    "            final_df.loc[mask, 'instructor'] = merged_df.loc[mask, 'Instructor']\n",
    "        \n",
    "        # Add course description from boss results if course_name is missing\n",
    "        if 'Description' in merged_df.columns:\n",
    "            mask = final_df['course_name'].isna() | (final_df['course_name'] == '')\n",
    "            final_df.loc[mask, 'course_name'] = merged_df.loc[mask, 'Description']\n",
    "        \n",
    "        # Clean course names and instructor names for encoding issues\n",
    "        final_df['course_name'] = final_df['course_name'].apply(self.clean_text_encoding)\n",
    "        final_df['instructor'] = final_df['instructor'].apply(self.clean_text_encoding)\n",
    "        \n",
    "        print(f\"\\nFinal dataframe columns: {list(final_df.columns)}\")\n",
    "        print(f\"Final dataframe shape: {final_df.shape}\")\n",
    "        \n",
    "        return final_df, merged_df\n",
    "    \n",
    "    def process_and_merge(self):\n",
    "        \"\"\"\n",
    "        Main method to execute the data merging process and save results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Load raw data\n",
    "            standalone_df, class_df = self.load_raw_data()\n",
    "            \n",
    "            # Step 2: Combine standalone and class data into one flat file\n",
    "            combined_raw_df = self.combine_raw_data(standalone_df, class_df)\n",
    "            \n",
    "            # Step 3: Load boss results\n",
    "            boss_df = self.load_boss_results()\n",
    "            \n",
    "            if boss_df.empty:\n",
    "                print(\"No boss results data found. Exiting.\")\n",
    "                return None\n",
    "            \n",
    "            # Step 4: Merge the combined raw data with boss results\n",
    "            final_df, detailed_df = self.merge_data(combined_raw_df, boss_df)\n",
    "            \n",
    "            if final_df.empty:\n",
    "                print(\"No matching records found between raw data and boss results.\")\n",
    "                return None\n",
    "            \n",
    "            # Clean text encoding issues in course_name and instructor\n",
    "            print(\"\\nCleaning text encoding issues...\")\n",
    "            final_df['course_name'] = final_df['course_name'].apply(self.clean_text_encoding)\n",
    "            final_df['instructor'] = final_df['instructor'].apply(self.clean_text_encoding)\n",
    "            \n",
    "            # Step 5: Save the results with timestamp\n",
    "            timestamp = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "            \n",
    "            # Save classification model data (all data)\n",
    "            classification_filename = f\"classification_model_data_{timestamp}.csv\"\n",
    "            classification_path = os.path.join(self.output_folder, classification_filename)\n",
    "            final_df.to_csv(classification_path, index=False)\n",
    "            \n",
    "            print(f\"\\nClassification model data saved to: {classification_path}\")\n",
    "            print(f\"Classification dataset shape: {final_df.shape}\")\n",
    "            \n",
    "            # Create and save CatBoost model data (non-zero bids only)\n",
    "            catboost_df = final_df[(final_df['median_bid'] > 0) & (final_df['min_bid'] > 0)].copy()\n",
    "            catboost_filename = f\"catboost_model_data_{timestamp}.csv\"\n",
    "            catboost_path = os.path.join(self.output_folder, catboost_filename)\n",
    "            catboost_df.to_csv(catboost_path, index=False)\n",
    "            \n",
    "            print(f\"\\nCatBoost model data saved to: {catboost_path}\")\n",
    "            print(f\"CatBoost dataset shape: {catboost_df.shape}\")\n",
    "            print(f\"Removed {len(final_df) - len(catboost_df)} rows with zero bids\")\n",
    "            \n",
    "            # Display summary statistics\n",
    "            print(f\"\\nSummary Statistics (Classification Data):\")\n",
    "            print(f\"- Total merged records: {final_df.shape[0]}\")\n",
    "            print(f\"- Unique courses: {final_df['course_code'].nunique()}\")\n",
    "            print(f\"- Unique sections: {final_df['section'].nunique()}\")\n",
    "            print(f\"- Academic years covered: {final_df['acad_year_start'].min()} - {final_df['acad_year_start'].max()}\")\n",
    "            print(f\"- Terms covered: {sorted(final_df['term'].unique())}\")\n",
    "            \n",
    "            # Check for missing critical values\n",
    "            critical_cols = ['course_code', 'section', 'before_process_vacancy', 'median_bid', 'min_bid']\n",
    "            for col in critical_cols:\n",
    "                if col in final_df.columns:\n",
    "                    missing_count = final_df[col].isna().sum()\n",
    "                    if missing_count > 0:\n",
    "                        print(f\"- Missing values in {col}: {missing_count}\")\n",
    "            \n",
    "            # Return the CatBoost dataframe as the primary output\n",
    "            return catboost_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during merge process: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e19e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data from script_input/raw_data.xlsx\n",
      "Standalone sheet: 12973 rows\n",
      "Multiple sheet: 19986 rows\n",
      "Class entries in multiple sheet: 13082 rows\n",
      "Combining standalone and class timing data...\n",
      "Combined raw data shape: (12973, 26)\n",
      "Sample boss term formats created: boss_term_format\n",
      "2023-24 Term 2    1664\n",
      "2023-24 Term 1    1659\n",
      "2024-25 Term 1    1647\n",
      "2021-22 Term 2    1631\n",
      "2022-23 Term 1    1614\n",
      "Name: count, dtype: int64\n",
      "Loading boss results from script_input/overallBossResults\n",
      "Found 14 Excel files\n",
      "Processing file: 2021-22_T2.xlsx\n",
      "Processing file: 2021-22_T3B.xlsx\n",
      "Processing file: 2022-23_T1.xlsx\n",
      "Processing file: 2022-23_T2.xlsx\n",
      "Processing file: 2022-23_T3A.xlsx\n",
      "Processing file: 2022-23_T3B.xlsx\n",
      "Processing file: 2023-24_T1.xlsx\n",
      "Processing file: 2023-24_T2.xlsx\n",
      "Processing file: 2023-24_T3A.xlsx\n",
      "Processing file: 2023-24_T3B.xlsx\n",
      "Processing file: 2024-25_T1.xlsx\n",
      "Processing file: 2024-25_T2.xlsx\n",
      "Processing file: 2024-25_T3A.xlsx\n",
      "Processing file: 2024-25_T3B.xlsx\n",
      "Combined boss results: 121340 rows\n",
      "Starting data merge process...\n",
      "\n",
      "Rows before filtering missing terms: 12973\n",
      "Rows after filtering missing terms: 12973\n",
      "\n",
      "Raw data course keys (first 5):\n",
      "  course_code section term          course_key\n",
      "0     MGMT715      G1   T1   MGMT715_G1_2021_1\n",
      "1    LGST700A      G1   T1  LGST700A_G1_2021_1\n",
      "2    STAT701A      G1   T1  STAT701A_G1_2021_1\n",
      "3     ACCT666      G1   T1   ACCT666_G1_2021_1\n",
      "4     ACCT635      G1   T1   ACCT635_G1_2021_1\n",
      "\n",
      "Boss data course keys (first 5):\n",
      "  Course Code Section            Term boss_term          course_key\n",
      "0     ACCT001      G1  2021-22 Term 2         2   ACCT001_G1_2021_2\n",
      "1     ACCT009      G1  2021-22 Term 2         2   ACCT009_G1_2021_2\n",
      "2     ACCT101      G1  2021-22 Term 2         2   ACCT101_G1_2021_2\n",
      "3     ACCT101     G10  2021-22 Term 2         2  ACCT101_G10_2021_2\n",
      "4     ACCT101     G11  2021-22 Term 2         2  ACCT101_G11_2021_2\n",
      "\n",
      "Raw data valid keys: 12973 out of 12973\n",
      "Boss data valid keys: 121298 out of 121340\n",
      "\n",
      "Common keys found: 7934\n",
      "\n",
      "Merged data: 121172 rows\n",
      "\n",
      "Final dataframe columns: ['course_code', 'course_name', 'acad_year_start', 'term', 'start_time', 'day_of_week', 'before_process_vacancy', 'bidding_window', 'instructor', 'median_bid', 'min_bid', 'section']\n",
      "Final dataframe shape: (121172, 12)\n",
      "\n",
      "Cleaning text encoding issues...\n",
      "\n",
      "Classification model data saved to: script_output/model_training\\classification_model_data_140625093742.csv\n",
      "Classification dataset shape: (121172, 12)\n",
      "\n",
      "CatBoost model data saved to: script_output/model_training\\catboost_model_data_140625093742.csv\n",
      "CatBoost dataset shape: (36667, 12)\n",
      "Removed 84505 rows with zero bids\n",
      "\n",
      "Summary Statistics (Classification Data):\n",
      "- Total merged records: 121172\n",
      "- Unique courses: 701\n",
      "- Unique sections: 75\n",
      "- Academic years covered: 2021 - 2024\n",
      "- Terms covered: ['T1', 'T2', 'T3A', 'T3B']\n"
     ]
    }
   ],
   "source": [
    "# Extract data required for model\n",
    "merger = SMUDataMerger()\n",
    "result_df = merger.process_and_merge()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bidlysmu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
