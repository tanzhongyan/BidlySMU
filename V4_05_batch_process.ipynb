{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1cfa91a",
      "metadata": {},
      "source": [
        "# **SMU Course Bidding Prediction Using CatBoost V4**\n",
        "\n",
        "<div style=\"background-color:#DFFFD6; padding:12px; border-radius:5px; border: 1px solid #228B22;\">\n",
        "   <h2 style=\"color:#006400;\">‚úÖ Looking to Implement This? ‚úÖ</h2>\n",
        "   <p>üöÄ **Get started quickly by using** <strong><a href=\"example_prediction.ipynb\">example_prediction.ipynb</a></strong>.</p> \n",
        "   <ul> \n",
        "      <li>üìå **Three pre-trained CatBoost models (`.cbm`) available for instant predictions.**</li>\n",
        "      <li>üîß Includes **step-by-step instructions** for making predictions with uncertainty quantification.</li>\n",
        "      <li>‚ö° Works **out-of-the-box**‚Äîjust load the models and start predicting!</li>\n",
        "   </ul>\n",
        "   <h3>üîó üìå Next Steps:</h3>\n",
        "   <p>üëâ <a href=\"example_prediction.ipynb\"><strong>Go to Example Prediction Notebook</strong></a></p>\n",
        "</div> \n",
        "<h2><span style=\"color:red\">NOTE: use at your own discretion.</span></h2>\n",
        "\n",
        "### **Changes in V4**\n",
        "- **Three-model architecture**: Added a classification model to predict whether a course will receive bids, complementing the existing median and min bid regression models\n",
        "- **Advanced uncertainty quantification**: Implemented entropy-based confidence scoring for classification and bootstrap-based confidence intervals for regression models\n",
        "- **Enhanced feature engineering**: Incorporated day-of-week boolean flags (`has_mon`, `has_tue`, etc.) for better temporal pattern recognition\n",
        "- **Asymmetric loss function**: Custom loss that penalizes under-predictions more heavily than over-predictions, crucial for bidding strategy\n",
        "- **Comprehensive evaluation suite**: Added confidence interval coverage analysis, residual analysis with emphasis on under-predictions, and cross-model feature importance comparison\n",
        "\n",
        "### **Objective**\n",
        "This notebook predicts bidding outcomes for courses in the SMU bidding system using **three specialized CatBoost models**. Building on insights from **V1, V2, and V3**, this version introduces a comprehensive **multi-model approach** with advanced uncertainty quantification:\n",
        "\n",
        "1. **Classification Model**: Predicts whether a course will receive bids (optimized for high recall)\n",
        "2. **Median Bid Regression Model**: Predicts the median bid price with confidence intervals\n",
        "3. **Min Bid Regression Model**: Predicts the minimum bid price with confidence intervals\n",
        "\n",
        "### **Key Enhancements in V4**\n",
        "\n",
        "**Learning from V3:**\n",
        "   - V3 focused on two regression models for median and min bid prediction\n",
        "   - V4 adds a **classification component** to identify courses that will receive bidding activity\n",
        "   - Enhanced with **probabilistic predictions** and **confidence scoring**\n",
        "\n",
        "**New V4 Features:**\n",
        "   - **Entropy-based confidence scoring** for classification predictions with five confidence levels (Very Low to Very High)\n",
        "   - **Bootstrap sampling** (100 iterations) for robust confidence interval estimation\n",
        "   - **Asymmetric loss function** (Œ±=2.0) that heavily penalizes dangerous under-predictions\n",
        "   - **Comprehensive uncertainty analysis** including interval width and coverage metrics\n",
        "\n",
        "### **Three-Model Architecture**\n",
        "\n",
        "| **Model Type** | **Purpose** | **Output** | **Uncertainty Measure** |\n",
        "|----------------|-------------|------------|-------------------------|\n",
        "| **Classification** | Predict bid courses | Probability + Confidence Level | Entropy-based confidence score |\n",
        "| **Median Bid Regression** | Predict median bid price | Price + 95% CI | Bootstrap confidence intervals |\n",
        "| **Min Bid Regression** | Predict minimum bid price | Price + 95% CI | Bootstrap confidence intervals |\n",
        "\n",
        "### **Updated Dataset Features**\n",
        "\n",
        "| **Feature Name** | **Type** | **Description** |\n",
        "|------------------|----------|-----------------|\n",
        "| **`subject_area`** | Categorical | Subject area (IS, ECON, etc.) |\n",
        "| **`catalogue_no`** | Categorical | Course number |\n",
        "| **`round`** | Categorical | Bidding round (1, 1A, 1B, 1C, 2, 2A) |\n",
        "| **`window`** | Numerical | Bidding window (1-5) |\n",
        "| **`before_process_vacancy`** | Numerical | Available spots before bidding |\n",
        "| **`acad_year_start`** | Numerical | Academic year start |\n",
        "| **`term`** | Categorical | Academic term (1, 2, 3A, 3B) |\n",
        "| **`start_time`** | Categorical | Class start time |\n",
        "| **`course_name`** | Categorical | Course name/description |\n",
        "| **`section`** | Categorical | Course section |\n",
        "| **`instructor`** | Categorical | Instructor name |\n",
        "| **`has_mon`** - **`has_sun`** | Boolean | Day-of-week indicators |\n",
        "| **üéØ Target Variables üéØ** | | **Model outputs** |\n",
        "| **`bids`** | Binary | Whether course receives bids |\n",
        "| **`target_median_bid`** | Numerical | Median bid price |\n",
        "| **`target_min_bid`** | Numerical | Minimum bid price |\n",
        "\n",
        "### **Advanced Uncertainty Quantification**\n",
        "\n",
        "**Classification Confidence:**\n",
        "- **Entropy-based scoring**: Measures prediction certainty using information entropy\n",
        "- **Five confidence levels**: Very Low, Low, Medium, High, Very High\n",
        "- **Probability outputs**: Separate probabilities for bid/non-bid outcomes\n",
        "\n",
        "**Regression Confidence Intervals:**\n",
        "- **Bootstrap sampling**: 100 model iterations for robust uncertainty estimation\n",
        "- **95% confidence intervals**: Upper and lower bounds for each prediction\n",
        "- **Interval width analysis**: Wider intervals indicate higher uncertainty\n",
        "\n",
        "### **Methodology**\n",
        "The notebook follows this enhanced structure:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Loading separate datasets for classification and regression tasks\n",
        "   - Feature standardization and categorical encoding\n",
        "   - Train-test splitting with consistent random seeds\n",
        "\n",
        "2. **Three-Model Training**:\n",
        "   - **Classification**: CatBoost with recall optimization for bid opportunity detection\n",
        "   - **Median Regression**: CatBoost with bootstrap uncertainty quantification\n",
        "   - **Min Regression**: CatBoost with asymmetric loss for under-prediction penalties\n",
        "\n",
        "3. **Advanced Evaluation**:\n",
        "   - **Classification**: Recall (maximizing true positives for bid detection), confusion matrix, entropy-based confidence analysis\n",
        "   - **Regression**: MSE, MAE, R¬≤, asymmetric MSE, confidence interval coverage\n",
        "   - **Cross-model feature importance comparison**\n",
        "\n",
        "4. **Comprehensive Visualization**:\n",
        "   - Confidence distribution plots and uncertainty analysis\n",
        "   - Residual analysis with under-prediction emphasis\n",
        "   - Feature importance rankings across all three models\n",
        "\n",
        "5. **Model Persistence and Reporting**:\n",
        "   - All models saved as `.cbm` files for deployment\n",
        "   - Detailed results exported to CSV format\n",
        "   - Comprehensive summary report generation\n",
        "\n",
        "### **Key Metrics and Performance**\n",
        "\n",
        "**Classification Model:**\n",
        "- **Primary metric**: Recall (optimized for capturing all bidding opportunities - maximizing true positives)\n",
        "- **Confidence analysis**: Distribution of entropy-based confidence scores  \n",
        "- **Output**: Probabilities for bid/no-bid outcomes, confidence levels, and entropy values\n",
        "\n",
        "**Regression Models:**\n",
        "- **Standard metrics**: MSE, MAE, R¬≤ for model accuracy\n",
        "- **Asymmetric MSE**: Custom metric penalizing under-predictions (Œ±=2.0)\n",
        "- **Uncertainty metrics**: Mean confidence interval width and coverage percentage\n",
        "- **Safety analysis**: Percentage of dangerous under-predictions\n",
        "\n",
        "### **Classification Strategy - Maximizing Bidding Opportunities**\n",
        "\n",
        "**Recall-Optimized Approach:**\n",
        "- **Target**: Predict courses that will receive bids (positive class = 1)\n",
        "- **Primary Goal**: Maximize recall to capture all potential bidding opportunities\n",
        "- **Business Logic**: Missing a course that will receive bids (False Negative) is more costly than incorrectly predicting a course will receive bids (False Positive)\n",
        "- **Optimization**: Model trained to minimize missed bidding opportunities while maintaining reasonable precision\n",
        "\n",
        "### **Implementation Notes**\n",
        "To run this V4 notebook:\n",
        "- Install required packages: `pip install catboost pandas numpy matplotlib seaborn scikit-learn scipy`\n",
        "- Ensure you have the three required datasets:\n",
        "  - Classification training/test data\n",
        "  - Median bid regression training/test data\n",
        "  - Min bid regression training/test data\n",
        "- Models automatically save to `script_output_model_training/mode/` directory\n",
        "\n",
        "### **V4 Advantages**\n",
        "- **Comprehensive coverage**: Handles both bid opportunity detection and price prediction\n",
        "- **Risk-aware predictions**: Asymmetric loss prevents dangerous under-bidding\n",
        "- **Confidence-calibrated**: Provides uncertainty measures for better decision-making\n",
        "- **Feature-rich analysis**: Cross-model feature importance for strategic insights\n",
        "- **Production-ready**: All models saved with consistent interfaces for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8520404",
      "metadata": {},
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "359f5072",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import os\n",
        "import psycopg2\n",
        "from dotenv import load_dotenv\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import json\n",
        "import hashlib\n",
        "from typing import List\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add database configuration\n",
        "load_dotenv()\n",
        "db_config = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'database': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': int(os.getenv('DB_PORT', 5432)),\n",
        "    'gssencmode': 'disable'\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "output_dir = Path('script_output/predictions')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "cache_dir = Path('db_cache')\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309e5d9e",
      "metadata": {},
      "source": [
        "## **2. SMUBiddingTransformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ccf08c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SMUBiddingTransformer:\n",
        "    \"\"\"\n",
        "    A reusable transformer class for processing SMU course bidding data\n",
        "    optimized for CatBoost model.\n",
        "    \n",
        "    Uses categorical encoding for instructors and one-hot encoding for multi-valued days.\n",
        "    \n",
        "    Expected input columns:\n",
        "    - course_code: str (e.g. 'MGMT715', 'COR-COMM175')\n",
        "    - course_name: str\n",
        "    - acad_year_start: int\n",
        "    - term: str ('1', '2', '3A', '3B')\n",
        "    - start_time: str (e.g. '19:30', 'TBA') - preserved as categorical\n",
        "    - day_of_week: str (can be multivalued, e.g. 'Mon,Thu')\n",
        "    - before_process_vacancy: int\n",
        "    - bidding_window: str (e.g. 'Round 1 Window 1', 'Incoming Freshmen Rnd 1 Win 4')\n",
        "    - instructor: str (can be multivalued, e.g. 'JOHN DOE, JANE SMITH')\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the transformer for CatBoost optimization.\n",
        "        \n",
        "        Uses categorical encoding for instructors and one-hot encoding for days.\n",
        "        \"\"\"\n",
        "        # Fitted flags\n",
        "        self.is_fitted = False\n",
        "        \n",
        "        # Lists to track feature types for CatBoost\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "    def fit(self, df: pd.DataFrame) -> 'SMUBiddingTransformer':\n",
        "        \"\"\"\n",
        "        Fit the transformer on training data.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Training dataframe with all required columns\n",
        "        \"\"\"\n",
        "        # Validate required columns\n",
        "        required_cols = [\n",
        "            'course_code', 'course_name', 'acad_year_start', 'term',\n",
        "            'start_time', 'day_of_week', 'before_process_vacancy',\n",
        "            'bidding_window', 'instructor', 'section'\n",
        "        ]\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "        \n",
        "        print(f\"Fitting transformer on {len(df)} rows...\")\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "    \n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Transform the dataframe to CatBoost-ready format.\n",
        "        \"\"\"\n",
        "        # Try to load existing model if not fitted\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted before transform. Call fit() first.\")\n",
        "        \n",
        "        # Create a copy to avoid modifying original\n",
        "        df_transformed = df.copy()\n",
        "        \n",
        "        # Reset feature tracking\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "        # 1. Extract course components (categorical + numeric)\n",
        "        course_features = self._extract_course_features(df_transformed)\n",
        "        \n",
        "        # 2. Process bidding window (categorical + numeric)\n",
        "        round_window_features = self._extract_round_window(df_transformed)\n",
        "        \n",
        "        # 3. Basic features (preserve categorical nature) + instructor as categorical\n",
        "        basic_features = self._process_basic_features(df_transformed)\n",
        "        \n",
        "        # 4. Create day one-hot encoding\n",
        "        day_features = self._create_day_one_hot_encoding(df_transformed)\n",
        "        \n",
        "        # Combine all features - FIXED: Ensure proper concatenation\n",
        "        feature_dfs = [course_features, round_window_features, basic_features, day_features]\n",
        "        \n",
        "        # Filter out any empty DataFrames\n",
        "        feature_dfs = [df for df in feature_dfs if not df.empty]\n",
        "        \n",
        "        if not feature_dfs:\n",
        "            raise ValueError(\"No features were extracted\")\n",
        "        \n",
        "        # Concatenate all features\n",
        "        final_df = pd.concat(feature_dfs, axis=1)\n",
        "        \n",
        "        # Verify all expected features are present\n",
        "        expected_features = self.categorical_features + self.numeric_features\n",
        "        missing_features = [f for f in expected_features if f not in final_df.columns]\n",
        "        \n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing features in final dataframe: {missing_features}\")\n",
        "            print(f\"Available columns: {list(final_df.columns)}\")\n",
        "        \n",
        "        # Debug: Print feature summary\n",
        "        print(f\"Transformed data shape: {final_df.shape}\")\n",
        "        print(f\"Features included: {list(final_df.columns)[:10]}...\")  # Show first 10\n",
        "        \n",
        "        return final_df\n",
        "        \n",
        "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Fit the transformer and transform the data in one step.\"\"\"\n",
        "        self.fit(df)\n",
        "        return self.transform(df)\n",
        "    \n",
        "    def get_categorical_features(self) -> List[str]:\n",
        "        \"\"\"Get list of categorical feature names for CatBoost.\"\"\"\n",
        "        return self.categorical_features.copy()\n",
        "    \n",
        "    def get_numeric_features(self) -> List[str]:\n",
        "        \"\"\"Get list of numeric feature names.\"\"\"\n",
        "        return self.numeric_features.copy()\n",
        "    \n",
        "    def _extract_course_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract subject area and catalogue number from course code.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def split_course_code(code):\n",
        "            \"\"\"Split course code into subject area and catalogue number.\"\"\"\n",
        "            if pd.isna(code):\n",
        "                return None, None\n",
        "            \n",
        "            code = str(code).strip().upper()\n",
        "            \n",
        "            # Handle hyphenated codes like 'COR-COMM175'\n",
        "            if '-' in code:\n",
        "                parts = code.split('-')\n",
        "                if len(parts) >= 2:\n",
        "                    subject = '-'.join(parts[:-1])\n",
        "                    # Extract number from last part\n",
        "                    num_match = re.search(r'(\\d+)', parts[-1])\n",
        "                    if num_match:\n",
        "                        return subject, int(num_match.group(1))\n",
        "                    else:\n",
        "                        # Try extracting from full last part\n",
        "                        num_match = re.search(r'(\\d+)', code)\n",
        "                        if num_match:\n",
        "                            return subject, int(num_match.group(1))\n",
        "            \n",
        "            # Standard format like 'MGMT715'\n",
        "            match = re.match(r'([A-Z\\-]+)(\\d+)', code)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            return code, 0\n",
        "        \n",
        "        # Extract components\n",
        "        splits = df['course_code'].apply(split_course_code)\n",
        "        features['subject_area'] = splits.apply(lambda x: x[0] if x else None)\n",
        "        features['catalogue_no'] = splits.apply(lambda x: x[1] if x else 0)\n",
        "\n",
        "        # Debug: Verify extraction\n",
        "        print(f\"Extracted course features: {features.shape}\")\n",
        "        print(f\"Sample subject_area values: {features['subject_area'].head()}\")\n",
        "        print(f\"Sample catalogue_no values: {features['catalogue_no'].head()}\")\n",
        "\n",
        "        # subject_area and catalogue_no are categorical for CatBoost\n",
        "        self.categorical_features.extend(['subject_area', 'catalogue_no'])\n",
        "\n",
        "        return features\n",
        "    \n",
        "    def _extract_round_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract round and window from bidding_window string.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def parse_bidding_window(window_str):\n",
        "            \"\"\"Parse bidding window string into round and window number.\"\"\"\n",
        "            if pd.isna(window_str):\n",
        "                return None, None\n",
        "            \n",
        "            window_str = str(window_str).strip()\n",
        "            # Check for Incoming Freshmen FIRST (before other patterns)\n",
        "            if 'Incoming Freshmen' in window_str:\n",
        "                match = re.search(r'Rnd\\s+(\\d)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if match:\n",
        "                    # Add F suffix to distinguish from regular rounds\n",
        "                    return f\"{match.group(1)}F\", int(match.group(2))     \n",
        "            \n",
        "            # Pattern 1: Standard format\n",
        "            match = re.search(r'Round\\s+(\\d[A-C]?)\\s+Window\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 2: Abbreviated format\n",
        "            match = re.search(r'Rnd\\s+(\\d[A-C]?)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 3: Incoming Exchange format (keeps original round)\n",
        "            match = re.search(r'Incoming\\s+Exchange\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 4: Incoming Freshmen format (adds F suffix)\n",
        "            match = re.search(r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                original_round = match.group(1)\n",
        "                window_num = int(match.group(2))\n",
        "                # Map Incoming Freshmen Round 1 to Round 1F\n",
        "                if original_round == \"1\":\n",
        "                    round_str = \"1F\"\n",
        "                else:\n",
        "                    round_str = f\"{original_round}F\"\n",
        "                return round_str, window_num\n",
        "            \n",
        "            # Fallback patterns...\n",
        "            match = re.search(r'(\\d[A-C]?)', window_str)\n",
        "            if match:\n",
        "                win_match = re.search(r'Window\\s+(\\d)|Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if win_match:\n",
        "                    window_num = int(win_match.group(1) or win_match.group(2))\n",
        "                    return match.group(1), window_num\n",
        "                return match.group(1), 1\n",
        "            \n",
        "            return '1', 1\n",
        "        \n",
        "        # Extract round and window\n",
        "        parsed = df['bidding_window'].apply(parse_bidding_window)\n",
        "        features['round'] = parsed.apply(lambda x: x[0] if x else '1')\n",
        "        features['window'] = parsed.apply(lambda x: x[1] if x else 1)\n",
        "        \n",
        "        # Round as categorical (preserves ordering like 1, 1A, 1B, 2, 2A)\n",
        "        self.categorical_features.append('round')\n",
        "        \n",
        "        # Window as numeric\n",
        "        self.numeric_features.append('window')\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def _process_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process basic features, preserving categorical nature where beneficial.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Numeric features\n",
        "        features['before_process_vacancy'] = pd.to_numeric(\n",
        "            df['before_process_vacancy'], errors='coerce'\n",
        "        ).fillna(0)\n",
        "        features['acad_year_start'] = pd.to_numeric(\n",
        "            df['acad_year_start'], errors='coerce'\n",
        "        ).fillna(2025)\n",
        "        \n",
        "        self.numeric_features.extend(['before_process_vacancy', 'acad_year_start'])\n",
        "        \n",
        "        # Categorical features\n",
        "        features['term'] = df['term'].astype(str)\n",
        "        features['start_time'] = df['start_time'].astype(str)\n",
        "        features['course_name'] = df['course_name'].astype(str)\n",
        "        features['section'] = df['section'].astype(str)\n",
        "        \n",
        "        # Process instructor names (remove duplicates, handle comma-separated format)\n",
        "        features['instructor'] = df['instructor'].apply(self._process_instructor_names)\n",
        "\n",
        "        # Replace empty strings with None for proper CatBoost handling\n",
        "        features.loc[features['start_time'].isin(['', 'nan']), 'start_time'] = None\n",
        "        features.loc[features['course_name'].isin(['', 'nan']), 'course_name'] = None\n",
        "        features.loc[features['section'].isin(['', 'nan']), 'section'] = None\n",
        "        \n",
        "        self.categorical_features.extend(['term', 'start_time', 'course_name', 'section', 'instructor'])\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def _create_day_one_hot_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create one-hot encoding for days of the week.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Initialize all day columns as 0\n",
        "        day_columns = ['has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun']\n",
        "        for col in day_columns:\n",
        "            features[col] = 0\n",
        "        \n",
        "        # Day mapping\n",
        "        day_abbrev = {\n",
        "            'MONDAY': 'MON', 'TUESDAY': 'TUE', 'WEDNESDAY': 'WED',\n",
        "            'THURSDAY': 'THU', 'FRIDAY': 'FRI', 'SATURDAY': 'SAT', 'SUNDAY': 'SUN',\n",
        "            'MON': 'MON', 'TUE': 'TUE', 'WED': 'WED', 'THU': 'THU',\n",
        "            'FRI': 'FRI', 'SAT': 'SAT', 'SUN': 'SUN'\n",
        "        }\n",
        "        \n",
        "        day_to_column = {\n",
        "            'MON': 'has_mon', 'TUE': 'has_tue', 'WED': 'has_wed', 'THU': 'has_thu',\n",
        "            'FRI': 'has_fri', 'SAT': 'has_sat', 'SUN': 'has_sun'\n",
        "        }\n",
        "        \n",
        "        # Process each row's day_of_week\n",
        "        for idx, days_value in enumerate(df['day_of_week']):\n",
        "            if pd.isna(days_value) or str(days_value).strip() == '':\n",
        "                continue  # Leave all days as 0\n",
        "            \n",
        "            days_str = str(days_value).strip()\n",
        "            \n",
        "            # Handle JSON array format\n",
        "            if days_str.startswith('[') and days_str.endswith(']'):\n",
        "                try:\n",
        "                    import json\n",
        "                    days_list = json.loads(days_str)\n",
        "                    if isinstance(days_list, list):\n",
        "                        for day in days_list:\n",
        "                            day_upper = str(day).strip().upper()\n",
        "                            standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                            \n",
        "                            if standardized_day in day_to_column:\n",
        "                                features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try comma-separated format as fallback\n",
        "                    pass\n",
        "            else:\n",
        "                # Handle comma-separated format (legacy support)\n",
        "                for day in days_str.split(','):\n",
        "                    day_upper = day.strip().upper()\n",
        "                    standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                    \n",
        "                    if standardized_day in day_to_column:\n",
        "                        features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "        \n",
        "        # These are numeric binary features (0/1)\n",
        "        self.numeric_features.extend(day_columns)\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def get_feature_names(self) -> List[str]:\n",
        "        \"\"\"Get all feature names after transformation.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted to get feature names.\")\n",
        "        \n",
        "        return self.categorical_features + self.numeric_features\n",
        "    \n",
        "    def _process_instructor_names(self, instructor_input):\n",
        "        \"\"\"Process instructor names to ensure consistent JSON array format as categorical string.\"\"\"\n",
        "        # Handle list/array input\n",
        "        if isinstance(instructor_input, (list, np.ndarray)):\n",
        "            if len(instructor_input) == 0:\n",
        "                return None\n",
        "            # Convert list to JSON string format\n",
        "            unique_instructors = []\n",
        "            seen = set()\n",
        "            for instructor in instructor_input:\n",
        "                if pd.notna(instructor) and str(instructor).strip() and str(instructor).upper() != 'TBA':\n",
        "                    instructor_clean = str(instructor).strip()\n",
        "                    if instructor_clean not in seen:\n",
        "                        seen.add(instructor_clean)\n",
        "                        unique_instructors.append(instructor_clean)\n",
        "            \n",
        "            if unique_instructors:\n",
        "                unique_instructors.sort()\n",
        "                import json\n",
        "                return json.dumps(unique_instructors)\n",
        "            else:\n",
        "                return None\n",
        "        \n",
        "        # Handle string input (original logic)\n",
        "        if pd.isna(instructor_input) or str(instructor_input).strip() == '' or str(instructor_input).upper() == 'TBA':\n",
        "            return None\n",
        "        \n",
        "        instructor_str = str(instructor_input).strip()\n",
        "        \n",
        "        # Check if it's already a JSON array\n",
        "        if instructor_str.startswith('[') and instructor_str.endswith(']'):\n",
        "            try:\n",
        "                import json\n",
        "                # Parse JSON array\n",
        "                instructors = json.loads(instructor_str)\n",
        "                if isinstance(instructors, list) and instructors:\n",
        "                    # Remove duplicates and sort alphabetically\n",
        "                    unique_instructors = []\n",
        "                    seen = set()\n",
        "                    for instructor in instructors:\n",
        "                        instructor_clean = str(instructor).strip()\n",
        "                        if instructor_clean and instructor_clean not in seen:\n",
        "                            seen.add(instructor_clean)\n",
        "                            unique_instructors.append(instructor_clean)\n",
        "                    \n",
        "                    unique_instructors.sort()  # Alphabetical order\n",
        "                    \n",
        "                    # Return as JSON string for categorical feature\n",
        "                    return json.dumps(unique_instructors)\n",
        "                else:\n",
        "                    return None\n",
        "            except json.JSONDecodeError:\n",
        "                # If JSON parsing fails, treat as regular string\n",
        "                pass\n",
        "        \n",
        "        # If not JSON format, treat as comma-separated string and convert to JSON\n",
        "        import re\n",
        "        parts = re.split(r', (?=[A-Z]+(?:\\s|,|$))', instructor_str)\n",
        "        \n",
        "        seen = set()\n",
        "        unique_parts = []\n",
        "        for part in parts:\n",
        "            part = part.strip()\n",
        "            if part and part not in seen:\n",
        "                seen.add(part)\n",
        "                unique_parts.append(part)\n",
        "        \n",
        "        if unique_parts:\n",
        "            unique_parts.sort()  # Alphabetical order\n",
        "            import json\n",
        "            return json.dumps(unique_parts)\n",
        "        else:\n",
        "            return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2ec324",
      "metadata": {},
      "source": [
        "## **3. Database Helper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fa4dac95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def connect_database():\n",
        "    \"\"\"Connect to PostgreSQL database\"\"\"\n",
        "    load_dotenv()\n",
        "    db_config = {\n",
        "        'host': os.getenv('DB_HOST'),\n",
        "        'database': os.getenv('DB_NAME'),\n",
        "        'user': os.getenv('DB_USER'),\n",
        "        'password': os.getenv('DB_PASSWORD'),\n",
        "        'port': int(os.getenv('DB_PORT', 5432)),\n",
        "        'gssencmode': 'disable'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        connection = psycopg2.connect(**db_config)\n",
        "        print(\"‚úÖ Database connection established\")\n",
        "        return connection\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Database connection failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_or_cache_data(connection, cache_dir):\n",
        "    \"\"\"Load data from cache or database\"\"\"\n",
        "    cache_files = {\n",
        "        'courses': cache_dir / 'courses_cache.pkl',\n",
        "        'classes': cache_dir / 'classes_cache.pkl',\n",
        "        'acad_terms': cache_dir / 'acad_terms_cache.pkl',\n",
        "        'professors': cache_dir / 'professors_cache.pkl'\n",
        "    }\n",
        "    \n",
        "    data_cache = {}\n",
        "    \n",
        "    # Try loading from cache first\n",
        "    if all(f.exists() for f in cache_files.values()):\n",
        "        print(\"‚úÖ Loading from cache...\")\n",
        "        for key, file in cache_files.items():\n",
        "            data_cache[key] = pd.read_pickle(file)\n",
        "    else:\n",
        "        print(\"üì• Downloading from database...\")\n",
        "        queries = {\n",
        "            'courses': \"SELECT * FROM courses\",\n",
        "            'classes': \"SELECT * FROM classes\",\n",
        "            'acad_terms': \"SELECT * FROM acad_term\",\n",
        "            'professors': \"SELECT * FROM professors\"\n",
        "        }\n",
        "        \n",
        "        for key, query in queries.items():\n",
        "            df = pd.read_sql_query(query, connection)\n",
        "            df.to_pickle(cache_files[key])\n",
        "            data_cache[key] = df\n",
        "    \n",
        "    return data_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "815cf568",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Preparation Functions\n",
        "def prepare_prediction_data(raw_data_path='script_input/raw_data.xlsx'):\n",
        "    \"\"\"Prepare data for prediction from raw_data.xlsx\"\"\"\n",
        "    print(\"üìÇ Loading raw data...\")\n",
        "    \n",
        "    # Load sheets\n",
        "    standalone_df = pd.read_excel(raw_data_path, sheet_name='standalone')\n",
        "    multiple_df = pd.read_excel(raw_data_path, sheet_name='multiple')\n",
        "    \n",
        "    # Filter for classes with bidding data\n",
        "    bidding_data = standalone_df[\n",
        "        standalone_df['bidding_window'].notna() & \n",
        "        standalone_df['total'].notna()\n",
        "    ].copy()\n",
        "    \n",
        "    # Calculate before_process_vacancy\n",
        "    bidding_data['before_process_vacancy'] = bidding_data['total'] - bidding_data['current_enrolled']\n",
        "    \n",
        "    # Extract round and window from bidding_window\n",
        "    def parse_bidding_window(window_str):\n",
        "        if pd.isna(window_str):\n",
        "            return None, None\n",
        "        \n",
        "        import re\n",
        "        # Handle various formats\n",
        "        patterns = [\n",
        "            (r'Round\\s+(\\w+)\\s+Window\\s+(\\d+)', lambda m: (m.group(1), int(m.group(2)))),\n",
        "            (r'Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', lambda m: (m.group(1), int(m.group(2)))),\n",
        "            (r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', lambda m: (f\"{m.group(1)}F\", int(m.group(2))))\n",
        "        ]\n",
        "        \n",
        "        for pattern, extractor in patterns:\n",
        "            match = re.search(pattern, str(window_str), re.IGNORECASE)\n",
        "            if match:\n",
        "                return extractor(match)\n",
        "        return '1', 1\n",
        "    \n",
        "    bidding_data[['round', 'window']] = bidding_data['bidding_window'].apply(\n",
        "        lambda x: pd.Series(parse_bidding_window(x))\n",
        "    )\n",
        "    \n",
        "    # Get instructor information from multiple sheet\n",
        "    instructor_map = {}\n",
        "    for record_key, group in multiple_df.groupby('record_key'):\n",
        "        professors = group['professor_name'].dropna().unique()\n",
        "        if len(professors) > 0:\n",
        "            instructor_map[record_key] = professors.tolist()\n",
        "    \n",
        "    # Map instructors to bidding data\n",
        "    bidding_data['instructor'] = bidding_data['record_key'].map(\n",
        "        lambda x: instructor_map.get(x, [])\n",
        "    )\n",
        "    \n",
        "    # Get day of week information\n",
        "    day_map = {}\n",
        "    for record_key, group in multiple_df[multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        days = group['day_of_week'].dropna().unique()\n",
        "        if len(days) > 0:\n",
        "            day_map[record_key] = ', '.join(days)\n",
        "    \n",
        "    bidding_data['day_of_week'] = bidding_data['record_key'].map(\n",
        "        lambda x: day_map.get(x, '')\n",
        "    )\n",
        "    \n",
        "    # Get start time\n",
        "    time_map = {}\n",
        "    for record_key, group in multiple_df[multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        times = group['start_time'].dropna()\n",
        "        if len(times) > 0:\n",
        "            time_map[record_key] = times.iloc[0]\n",
        "    \n",
        "    bidding_data['start_time'] = bidding_data['record_key'].map(\n",
        "        lambda x: time_map.get(x, '')\n",
        "    )\n",
        "    \n",
        "    return bidding_data, standalone_df, multiple_df\n",
        "\n",
        "def map_classes_to_predictions(bidding_data, data_cache):\n",
        "    \"\"\"Map predictions to class IDs - checks both database cache and new_classes.csv\"\"\"\n",
        "    courses_df = data_cache['courses']\n",
        "    classes_df = data_cache['classes']\n",
        "    \n",
        "    # Create course code to ID mapping from both sources\n",
        "    course_id_map = dict(zip(courses_df['code'], courses_df['id']))\n",
        "    \n",
        "    # Also check new_courses.csv for courses not in database yet\n",
        "    new_courses_path = Path('script_output/new_courses.csv')\n",
        "    if new_courses_path.exists():\n",
        "        try:\n",
        "            new_courses_df = pd.read_csv(new_courses_path)\n",
        "            for _, row in new_courses_df.iterrows():\n",
        "                if row['code'] not in course_id_map:\n",
        "                    course_id_map[row['code']] = row['id']\n",
        "            print(f\"üìö Added {len(new_courses_df)} courses from new_courses.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load new_courses.csv: {e}\")\n",
        "    \n",
        "    # Also check verify folder for new courses\n",
        "    verify_courses_path = Path('script_output/verify/new_courses.csv')\n",
        "    if verify_courses_path.exists():\n",
        "        try:\n",
        "            verify_courses_df = pd.read_csv(verify_courses_path)\n",
        "            for _, row in verify_courses_df.iterrows():\n",
        "                if row['code'] not in course_id_map:\n",
        "                    course_id_map[row['code']] = row['id']\n",
        "            print(f\"üìö Added {len(verify_courses_df)} courses from verify/new_courses.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load verify/new_courses.csv: {e}\")\n",
        "    \n",
        "    # Load new_classes.csv to find classes not in database yet\n",
        "    new_classes_df = None\n",
        "    new_classes_path = Path('script_output/new_classes.csv')\n",
        "    if new_classes_path.exists():\n",
        "        try:\n",
        "            new_classes_df = pd.read_csv(new_classes_path)\n",
        "            print(f\"üìö Loaded {len(new_classes_df)} classes from new_classes.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load new_classes.csv: {e}\")\n",
        "    \n",
        "    # Map each row to class IDs\n",
        "    class_mappings = []\n",
        "    unmapped_courses = set()\n",
        "    \n",
        "    for idx, row in bidding_data.iterrows():\n",
        "        course_code = row['course_code']\n",
        "        section = str(row['section'])\n",
        "        acad_term_id = row['acad_term_id']\n",
        "        record_key = row.get('record_key', '')\n",
        "        \n",
        "        # Get course ID\n",
        "        course_id = course_id_map.get(course_code)\n",
        "        if not course_id:\n",
        "            unmapped_courses.add(course_code)\n",
        "            continue\n",
        "        \n",
        "        found_in_db = False\n",
        "        found_in_new = False\n",
        "        \n",
        "        # First, try to find in database cache\n",
        "        matching_classes = classes_df[\n",
        "            (classes_df['course_id'] == course_id) & \n",
        "            (classes_df['section'] == section) & \n",
        "            (classes_df['acad_term_id'] == acad_term_id)\n",
        "        ]\n",
        "        \n",
        "        if not matching_classes.empty:\n",
        "            found_in_db = True\n",
        "            for _, class_row in matching_classes.iterrows():\n",
        "                mapping = {\n",
        "                    'prediction_idx': idx,\n",
        "                    'class_id': class_row['id'],\n",
        "                    'professor_id': class_row.get('professor_id'),\n",
        "                    'course_code': course_code,\n",
        "                    'section': section,\n",
        "                    'acad_term_id': acad_term_id,\n",
        "                    'record_key': record_key,\n",
        "                    'source': 'database'\n",
        "                }\n",
        "                class_mappings.append(mapping)\n",
        "        \n",
        "        # If not found in database, check new_classes.csv\n",
        "        if not found_in_db and new_classes_df is not None:\n",
        "            # Try matching by course_id, section, and acad_term_id\n",
        "            new_matching = new_classes_df[\n",
        "                (new_classes_df['course_id'] == course_id) & \n",
        "                (new_classes_df['section'] == section) & \n",
        "                (new_classes_df['acad_term_id'] == acad_term_id)\n",
        "            ]\n",
        "            \n",
        "            if not new_matching.empty:\n",
        "                found_in_new = True\n",
        "                for _, class_row in new_matching.iterrows():\n",
        "                    mapping = {\n",
        "                        'prediction_idx': idx,\n",
        "                        'class_id': class_row['id'],\n",
        "                        'professor_id': class_row.get('professor_id'),\n",
        "                        'course_code': course_code,\n",
        "                        'section': section,\n",
        "                        'acad_term_id': acad_term_id,\n",
        "                        'record_key': record_key,\n",
        "                        'source': 'new_classes'\n",
        "                    }\n",
        "                    class_mappings.append(mapping)\n",
        "        \n",
        "        # If still not found anywhere\n",
        "        if not found_in_db and not found_in_new:\n",
        "            # Create a placeholder mapping\n",
        "            mapping = {\n",
        "                'prediction_idx': idx,\n",
        "                'class_id': f\"PENDING_{course_code}_{section}_{acad_term_id}\",\n",
        "                'professor_id': None,\n",
        "                'course_code': course_code,\n",
        "                'section': section,\n",
        "                'acad_term_id': acad_term_id,\n",
        "                'record_key': record_key,\n",
        "                'source': 'not_found'\n",
        "            }\n",
        "            class_mappings.append(mapping)\n",
        "    \n",
        "    # Create summary\n",
        "    mappings_df = pd.DataFrame(class_mappings)\n",
        "    \n",
        "    if not mappings_df.empty:\n",
        "        print(f\"\\nüìä Mapping Summary:\")\n",
        "        print(f\"   Total mappings: {len(mappings_df)}\")\n",
        "        print(f\"   Unique predictions mapped: {mappings_df['prediction_idx'].nunique()}\")\n",
        "        source_counts = mappings_df['source'].value_counts()\n",
        "        for source, count in source_counts.items():\n",
        "            print(f\"   From {source}: {count}\")\n",
        "        \n",
        "        if unmapped_courses:\n",
        "            print(f\"\\n‚ö†Ô∏è Courses without IDs: {len(unmapped_courses)}\")\n",
        "            print(f\"   Sample: {list(unmapped_courses)[:5]}\")\n",
        "    \n",
        "    return mappings_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b904ae42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATA LOADING AND PREPARATION\n",
            "============================================================\n",
            "‚úÖ Database connection established\n",
            "‚úÖ Loading from cache...\n",
            "üìÇ Loading raw data...\n",
            "üìä Prepared 1165 records for prediction\n",
            "\n",
            "Sample bidding data:\n",
            "                                          record_key  \\\n",
            "0  SelectedAcadTerm=2510&SelectedClassNumber=1002...   \n",
            "1  SelectedAcadTerm=2510&SelectedClassNumber=1003...   \n",
            "2  SelectedAcadTerm=2510&SelectedClassNumber=1004...   \n",
            "3  SelectedAcadTerm=2510&SelectedClassNumber=1005...   \n",
            "4  SelectedAcadTerm=2510&SelectedClassNumber=1006...   \n",
            "\n",
            "                                            filepath course_code section  \\\n",
            "0  script_input/classTimingsFull\\2025-26_T1\\Selec...     THES720      G1   \n",
            "1  script_input/classTimingsFull\\2025-26_T1\\Selec...     OBHR701      G1   \n",
            "2  script_input/classTimingsFull\\2025-26_T1\\Selec...     FNCE710      G1   \n",
            "3  script_input/classTimingsFull\\2025-26_T1\\Selec...  LAW103_603      G1   \n",
            "4  script_input/classTimingsFull\\2025-26_T1\\Selec...  LAW103_603     G61   \n",
            "\n",
            "            course_name                                 course_description  \\\n",
            "0          Dissertation                                       Dissertation   \n",
            "1       Decision Making  This course provides an introduction, at the P...   \n",
            "2  Corporate Finance II  This is a Ph.D. seminar class in corporate fin...   \n",
            "3          Criminal Law  This course will introduce students to the bas...   \n",
            "4          Criminal Law  This course will introduce students to the bas...   \n",
            "\n",
            "   credit_units                                        course_area  \\\n",
            "0         28.00                                     Not Applicable   \n",
            "1          1.00                         GPGM Programme Core (OBHR)   \n",
            "2          1.00                          GPGM Programme Core (Fin)   \n",
            "3          1.25  Legal Studies Electives, Business Options, Eco...   \n",
            "4          1.25  Legal Studies Electives, Business Options, Eco...   \n",
            "\n",
            "                              enrolment_requirements acad_term_id  ...  \\\n",
            "0                      Information is not available.   AY202526T1  ...   \n",
            "1                      Information is not available.   AY202526T1  ...   \n",
            "2                      Information is not available.   AY202526T1  ...   \n",
            "3  Note*: This course is applicable to SMU Underg...   AY202526T1  ...   \n",
            "4  Note*: This course is applicable to SMU Underg...   AY202526T1  ...   \n",
            "\n",
            "   reserved  available       date_extracted    bidding_window  \\\n",
            "0       100          0  2025-06-27 15:28:20  Round 1 Window 1   \n",
            "1        45          0  2025-06-27 15:28:20  Round 1 Window 1   \n",
            "2        45          0  2025-06-27 15:28:21  Round 1 Window 1   \n",
            "3        47          1  2025-06-27 15:28:21  Round 1 Window 1   \n",
            "4         0         48  2025-06-27 15:28:21  Round 1 Window 1   \n",
            "\n",
            "  before_process_vacancy round window          instructor  \\\n",
            "0                     75     1      1                  []   \n",
            "1                     37     1      1        [JOCHEN REB]   \n",
            "2                     36     1      1         [HAO LIANG]   \n",
            "3                     48     1      1       [WALTER WOON]   \n",
            "4                     48     1      1  [CHAN WING CHEONG]   \n",
            "\n",
            "                    day_of_week start_time  \n",
            "0                                           \n",
            "1  Mon, Tue, Wed, Thu, Fri, Sat      09:00  \n",
            "2  Mon, Tue, Wed, Thu, Fri, Sat      14:00  \n",
            "3                      Thu, Fri      09:15  \n",
            "4                      Tue, Thu      12:00  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Load and Prepare Data\n",
        "print(\"=\"*60)\n",
        "print(\"DATA LOADING AND PREPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Connect to database\n",
        "connection = connect_database()\n",
        "if not connection:\n",
        "    raise Exception(\"Failed to connect to database\")\n",
        "\n",
        "# Load cache data\n",
        "data_cache = load_or_cache_data(connection, cache_dir)\n",
        "\n",
        "# Prepare prediction data\n",
        "bidding_data, standalone_df, multiple_df = prepare_prediction_data()\n",
        "print(f\"üìä Prepared {len(bidding_data)} records for prediction\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample bidding data:\")\n",
        "print(bidding_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1fb7a7d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE TRANSFORMATION\n",
            "============================================================\n",
            "Fitting transformer on 1165 rows...\n",
            "Extracted course features: (1165, 2)\n",
            "Sample subject_area values: 0    THES\n",
            "1    OBHR\n",
            "2    FNCE\n",
            "3     LAW\n",
            "4     LAW\n",
            "Name: subject_area, dtype: object\n",
            "Sample catalogue_no values: 0    720\n",
            "1    701\n",
            "2    710\n",
            "3    103\n",
            "4    103\n",
            "Name: catalogue_no, dtype: int64\n",
            "Transformed data shape: (1165, 18)\n",
            "Features included: ['subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy', 'acad_year_start', 'term', 'start_time', 'course_name', 'section']...\n",
            "‚úÖ Transformed data shape: (1165, 18)\n",
            "üìã Features: ['subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy', 'acad_year_start', 'term', 'start_time', 'course_name', 'section']...\n",
            "\n",
            "üíæ Transformed features saved to: script_output\\predictions\\transformed_features_20250628_101742.csv\n",
            "   Total columns: 21\n",
            "   - Identifier columns: 4\n",
            "   - Feature columns: 17\n",
            "   - Categorical features: 8\n",
            "   - Numeric features: 10\n",
            "üìã Metadata saved to: script_output\\predictions\\transformation_metadata_20250628_101742.json\n",
            "\n",
            "üîç Sample of transformed data:\n",
            "                                          record_key course_code section  \\\n",
            "0  SelectedAcadTerm=2510&SelectedClassNumber=1002...     THES720      G1   \n",
            "1  SelectedAcadTerm=2510&SelectedClassNumber=1003...     OBHR701      G1   \n",
            "2  SelectedAcadTerm=2510&SelectedClassNumber=1004...     FNCE710      G1   \n",
            "3  SelectedAcadTerm=2510&SelectedClassNumber=1005...  LAW103_603      G1   \n",
            "4  SelectedAcadTerm=2510&SelectedClassNumber=1006...  LAW103_603     G61   \n",
            "\n",
            "  acad_term_id subject_area catalogue_no round  window  \\\n",
            "0   AY202526T1         THES          720     1       1   \n",
            "1   AY202526T1         OBHR          701     1       1   \n",
            "2   AY202526T1         FNCE          710     1       1   \n",
            "3   AY202526T1          LAW          103     1       1   \n",
            "4   AY202526T1          LAW          103     1       1   \n",
            "\n",
            "   before_process_vacancy  acad_year_start  ... start_time  \\\n",
            "0                      75             2025  ...       None   \n",
            "1                      37             2025  ...      09:00   \n",
            "2                      36             2025  ...      14:00   \n",
            "3                      48             2025  ...      09:15   \n",
            "4                      48             2025  ...      12:00   \n",
            "\n",
            "            course_name            instructor has_mon  has_tue  has_wed  \\\n",
            "0          Dissertation                  None       0        0        0   \n",
            "1       Decision Making        [\"JOCHEN REB\"]       1        1        1   \n",
            "2  Corporate Finance II         [\"HAO LIANG\"]       1        1        1   \n",
            "3          Criminal Law       [\"WALTER WOON\"]       0        0        0   \n",
            "4          Criminal Law  [\"CHAN WING CHEONG\"]       0        1        0   \n",
            "\n",
            "   has_thu  has_fri  has_sat  has_sun  \n",
            "0        0        0        0        0  \n",
            "1        1        1        1        0  \n",
            "2        1        1        1        0  \n",
            "3        1        1        0        0  \n",
            "4        1        0        0        0  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Transform Data (FIXED)\n",
        "def transform_bidding_data(bidding_data, output_dir):\n",
        "    print(\"=\"*60)\n",
        "    print(\"FEATURE TRANSFORMATION\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Initialize transformer\n",
        "    transformer = SMUBiddingTransformer()\n",
        "    transformer.fit(bidding_data)\n",
        "    \n",
        "    # Transform data\n",
        "    X_transformed = transformer.transform(bidding_data)\n",
        "    print(f\"‚úÖ Transformed data shape: {X_transformed.shape}\")\n",
        "    print(f\"üìã Features: {list(X_transformed.columns)[:10]}...\")\n",
        "    \n",
        "    # Ensure all categorical features have __NA__ for null values\n",
        "    categorical_features = transformer.get_categorical_features()\n",
        "    for col in categorical_features:\n",
        "        if col in X_transformed.columns:\n",
        "            # Convert to string type first\n",
        "            X_transformed[col] = X_transformed[col].astype(str)\n",
        "            # Replace 'nan' strings with a consistent placeholder\n",
        "            X_transformed[col] = X_transformed[col].replace('nan', '__NA__')\n",
        "            # Also handle any remaining NaN values\n",
        "            X_transformed[col] = X_transformed[col].fillna('__NA__')\n",
        "            # Handle empty strings\n",
        "            X_transformed[col] = X_transformed[col].replace('', '__NA__')\n",
        "    \n",
        "    # Save transformed data to CSV\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    transformed_output_path = output_dir / f'transformed_features_{timestamp}.csv'\n",
        "    \n",
        "    # Add the original identifiers to help with tracking\n",
        "    X_transformed_with_ids = X_transformed.copy()\n",
        "    X_transformed_with_ids['course_code'] = bidding_data['course_code'].values\n",
        "    X_transformed_with_ids['section'] = bidding_data['section'].values\n",
        "    X_transformed_with_ids['acad_term_id'] = bidding_data['acad_term_id'].values\n",
        "    X_transformed_with_ids['record_key'] = bidding_data['record_key'].values\n",
        "    \n",
        "    # Reorder columns to put identifiers first\n",
        "    id_cols = ['record_key', 'course_code', 'section', 'acad_term_id']\n",
        "    feature_cols = [col for col in X_transformed.columns if col not in id_cols]\n",
        "    X_transformed_with_ids = X_transformed_with_ids[id_cols + feature_cols]\n",
        "    \n",
        "    # Save to CSV\n",
        "    X_transformed_with_ids.to_csv(transformed_output_path, index=False)\n",
        "    print(f\"\\nüíæ Transformed features saved to: {transformed_output_path}\")\n",
        "    print(f\"   Total columns: {len(X_transformed_with_ids.columns)}\")\n",
        "    print(f\"   - Identifier columns: {len(id_cols)}\")\n",
        "    print(f\"   - Feature columns: {len(feature_cols)}\")\n",
        "    print(f\"   - Categorical features: {len(transformer.get_categorical_features())}\")\n",
        "    print(f\"   - Numeric features: {len(transformer.get_numeric_features())}\")\n",
        "    \n",
        "    # Also save a metadata file with feature information\n",
        "    metadata = {\n",
        "        'timestamp': timestamp,\n",
        "        'total_rows': len(X_transformed),\n",
        "        'total_features': len(feature_cols),\n",
        "        'categorical_features': transformer.get_categorical_features(),\n",
        "        'numeric_features': transformer.get_numeric_features(),\n",
        "        'identifier_columns': id_cols,\n",
        "        'transformation_date': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    metadata_path = output_dir / f'transformation_metadata_{timestamp}.json'\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"üìã Metadata saved to: {metadata_path}\")\n",
        "    \n",
        "    # Display sample of transformed data\n",
        "    print(\"\\nüîç Sample of transformed data:\")\n",
        "    print(X_transformed_with_ids.head())\n",
        "    \n",
        "    return X_transformed, transformer\n",
        "\n",
        "# Run Transformation\n",
        "X_transformed, transformer = transform_bidding_data(bidding_data, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a4bf5880",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLASS MAPPING\n",
            "============================================================\n",
            "üìö Added 19 courses from verify/new_courses.csv\n",
            "üìö Loaded 1205 classes from new_classes.csv\n",
            "\n",
            "üìä Mapping Summary:\n",
            "   Total mappings: 1205\n",
            "   Unique predictions mapped: 1165\n",
            "   From new_classes: 1205\n",
            "üîó Mapped to 1205 class instances\n",
            "\n",
            "Sample class mappings:\n",
            "   prediction_idx                              class_id  \\\n",
            "0               0  570e6d3c-111e-4f4b-a624-ec3b154af0b8   \n",
            "1               1  72d2235b-48ec-4c20-abca-e6465d12fe64   \n",
            "2               2  2b9599aa-27ab-44b9-bb7c-301e258316ac   \n",
            "3               3  07841c5b-5d9b-4ea6-858e-c6026b052fe5   \n",
            "4               4  8c31aac8-00de-4663-a025-84894b676116   \n",
            "\n",
            "                           professor_id course_code section acad_term_id  \\\n",
            "0                                   NaN     THES720      G1   AY202526T1   \n",
            "1  80e3f253-ef1f-46e2-93bf-356749da74bc     OBHR701      G1   AY202526T1   \n",
            "2  2344a6fb-c450-4362-ae49-89ddad3fe6ee     FNCE710      G1   AY202526T1   \n",
            "3  c680436e-1fa6-49d5-a325-da3deafa8dcb  LAW103_603      G1   AY202526T1   \n",
            "4  dcca0ef0-0089-436a-954d-a71ede176cfa  LAW103_603     G61   AY202526T1   \n",
            "\n",
            "                                          record_key       source  \n",
            "0  SelectedAcadTerm=2510&SelectedClassNumber=1002...  new_classes  \n",
            "1  SelectedAcadTerm=2510&SelectedClassNumber=1003...  new_classes  \n",
            "2  SelectedAcadTerm=2510&SelectedClassNumber=1004...  new_classes  \n",
            "3  SelectedAcadTerm=2510&SelectedClassNumber=1005...  new_classes  \n",
            "4  SelectedAcadTerm=2510&SelectedClassNumber=1006...  new_classes  \n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Map to Classes\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS MAPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Map to classes\n",
        "class_mappings = map_classes_to_predictions(bidding_data, data_cache)\n",
        "print(f\"üîó Mapped to {len(class_mappings)} class instances\")\n",
        "\n",
        "# Display sample mappings\n",
        "print(\"\\nSample class mappings:\")\n",
        "print(class_mappings.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1fa3cc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MODEL PREDICTIONS\n",
            "============================================================\n",
            "‚úÖ Loaded classification model\n",
            "‚úÖ Loaded median model\n",
            "‚úÖ Loaded min model\n",
            "üìä Using 18 features for prediction\n",
            "üîÆ Generating predictions for 1165 records...\n",
            "‚úÖ Generated predictions for 1165 records\n",
            "\n",
            "üìà Prediction Summary:\n",
            "   Classification predictions: 1165\n",
            "   Median predictions range: 3.93 - 99.52\n",
            "   Min predictions range: -2.32 - 66.04\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Load Models and Generate Predictions (FIXED)\n",
        "def load_models_and_predict(X_transformed, bidding_data):\n",
        "    print(\"=\"*60)\n",
        "    print(\"MODEL PREDICTIONS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Load models\n",
        "    models = {\n",
        "        'classification': CatBoostClassifier(),\n",
        "        'median': CatBoostRegressor(),\n",
        "        'min': CatBoostRegressor()\n",
        "    }\n",
        "    \n",
        "    model_paths = {\n",
        "        'classification': 'script_output/models/classification/production_classification_model.cbm',\n",
        "        'median': 'script_output/models/regression_median/production_regression_median_model.cbm',\n",
        "        'min': 'script_output/models/regression_min/production_regression_min_model.cbm'\n",
        "    }\n",
        "    \n",
        "    # Load each model\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            model.load_model(model_paths[name])\n",
        "            print(f\"‚úÖ Loaded {name} model\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {name} model: {e}\")\n",
        "            return None\n",
        "    \n",
        "    # Verify data format matches model expectations\n",
        "    expected_features = [\n",
        "        'subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy',\n",
        "        'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor',\n",
        "        'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun'\n",
        "    ]\n",
        "    \n",
        "    # Create prediction dataset with only the features expected by models\n",
        "    prediction_data = X_transformed.copy()\n",
        "    \n",
        "    # Ensure all expected features are present\n",
        "    missing_features = set(expected_features) - set(prediction_data.columns)\n",
        "    if missing_features:\n",
        "        print(f\"‚ö†Ô∏è Warning: Missing features: {missing_features}\")\n",
        "    \n",
        "    # Select only the features that exist and are expected\n",
        "    available_features = [col for col in expected_features if col in prediction_data.columns]\n",
        "    prediction_data = prediction_data[available_features]\n",
        "    \n",
        "    print(f\"üìä Using {len(available_features)} features for prediction\")\n",
        "    print(f\"üîÆ Generating predictions for {len(prediction_data)} records...\")\n",
        "    \n",
        "    try:\n",
        "        # Classification predictions\n",
        "        clf_pred = models['classification'].predict(prediction_data)\n",
        "        clf_proba = models['classification'].predict_proba(prediction_data)\n",
        "        \n",
        "        # Regression predictions  \n",
        "        median_pred = models['median'].predict(prediction_data)\n",
        "        min_pred = models['min'].predict(prediction_data)\n",
        "        \n",
        "        print(f\"‚úÖ Generated predictions for {len(prediction_data)} records\")\n",
        "        \n",
        "        # Create results dataframe\n",
        "        results = {\n",
        "            'classification_prediction': clf_pred,\n",
        "            'classification_probabilities': clf_proba,\n",
        "            'median_prediction': median_pred,\n",
        "            'min_prediction': min_pred\n",
        "        }\n",
        "        \n",
        "        return results, models\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during prediction: {e}\")\n",
        "        print(f\"   Data shape: {prediction_data.shape}\")\n",
        "        print(f\"   Data types: {prediction_data.dtypes}\")\n",
        "        return None, models\n",
        "    \n",
        "# Load Models and Generate Predictions\n",
        "prediction_results, loaded_models = load_models_and_predict(X_transformed, bidding_data)\n",
        "if prediction_results:\n",
        "    clf_pred = prediction_results['classification_prediction']\n",
        "    clf_proba = prediction_results['classification_probabilities'] \n",
        "    median_pred = prediction_results['median_prediction']\n",
        "    min_pred = prediction_results['min_prediction']\n",
        "    \n",
        "    print(f\"\\nüìà Prediction Summary:\")\n",
        "    print(f\"   Classification predictions: {len(clf_pred)}\")\n",
        "    print(f\"   Median predictions range: {min(median_pred):.2f} - {max(median_pred):.2f}\")\n",
        "    print(f\"   Min predictions range: {min(min_pred):.2f} - {max(min_pred):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6c3fd5b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "UNCERTAINTY QUANTIFICATION\n",
            "============================================================\n",
            "‚úÖ Calculated prediction uncertainties\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Calculate Uncertainties and Confidence\n",
        "print(\"=\"*60)\n",
        "print(\"UNCERTAINTY QUANTIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def calculate_entropy_confidence(probabilities):\n",
        "    \"\"\"Calculate entropy-based confidence scores\"\"\"\n",
        "    epsilon = 1e-10\n",
        "    entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
        "    max_entropy = -np.log(1/probabilities.shape[1])\n",
        "    confidence_score = 1 - (entropy / max_entropy)\n",
        "    \n",
        "    confidence_levels = np.where(\n",
        "        confidence_score >= 0.9, 'Very High',\n",
        "        np.where(confidence_score >= 0.7, 'High',\n",
        "                np.where(confidence_score >= 0.5, 'Medium',\n",
        "                        np.where(confidence_score >= 0.3, 'Low', 'Very Low')))\n",
        "    )\n",
        "    return confidence_score, confidence_levels\n",
        "\n",
        "# Calculate classification confidence\n",
        "confidence_scores, confidence_levels = calculate_entropy_confidence(clf_proba)\n",
        "\n",
        "# Calculate regression uncertainties using virtual ensembles\n",
        "uncertainties = {}\n",
        "for model_name in ['median', 'min']:\n",
        "    model = loaded_models[model_name]\n",
        "    n_trees = model.tree_count_\n",
        "    n_subsets = 10\n",
        "    trees_per_subset = max(1, n_trees // n_subsets)\n",
        "    \n",
        "    subset_predictions = []\n",
        "    for i in range(n_subsets):\n",
        "        tree_start = i * trees_per_subset\n",
        "        tree_end = min((i + 1) * trees_per_subset, n_trees)\n",
        "        if tree_start < n_trees:\n",
        "            partial_pred = model.predict(X_transformed, \n",
        "                                       ntree_start=tree_start, \n",
        "                                       ntree_end=tree_end)\n",
        "            subset_predictions.append(partial_pred)\n",
        "    \n",
        "    uncertainties[model_name] = np.std(subset_predictions, axis=0)\n",
        "\n",
        "print(\"‚úÖ Calculated prediction uncertainties\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "23065c79",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAFETY FACTOR APPLICATION\n",
            "============================================================\n",
            "üìä Optimal safety factors:\n",
            "   Median: 0.70\n",
            "   Min: 0.70\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Apply Safety Factors\n",
        "print(\"=\"*60)\n",
        "print(\"SAFETY FACTOR APPLICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load safety factor tables\n",
        "median_sf_df = pd.read_csv('script_output/models/regression_median/median_bid_safety_factor_analysis.csv')\n",
        "min_sf_df = pd.read_csv('script_output/models/regression_min/min_bid_safety_factor_analysis.csv')\n",
        "\n",
        "# Find optimal safety factors (example: SF with TPR > 0.9)\n",
        "median_optimal_idx = median_sf_df[median_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "min_optimal_idx = min_sf_df[min_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "\n",
        "median_optimal_sf = median_sf_df.iloc[median_optimal_idx]['safety_factor']\n",
        "min_optimal_sf = min_sf_df.iloc[min_optimal_idx]['safety_factor']\n",
        "\n",
        "print(f\"üìä Optimal safety factors:\")\n",
        "print(f\"   Median: {median_optimal_sf:.2f}\")\n",
        "print(f\"   Min: {min_optimal_sf:.2f}\")\n",
        "\n",
        "# Apply safety factors\n",
        "median_recommended = median_pred * (1 + median_optimal_sf)\n",
        "min_recommended = min_pred * (1 + min_optimal_sf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8bd0ece3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CREATING OUTPUT TABLES\n",
            "============================================================\n",
            "‚úÖ Created 1165 prediction results\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: Create Output DataFrames\n",
        "print(\"=\"*60)\n",
        "print(\"CREATING OUTPUT TABLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create PredictionResult entries\n",
        "prediction_results = []\n",
        "\n",
        "for idx in range(len(bidding_data)):\n",
        "    # Create input hash for deduplication\n",
        "    input_data = bidding_data.iloc[idx]\n",
        "    input_str = f\"{input_data['course_code']}_{input_data['term']}_{input_data['round']}_{input_data['window']}\"\n",
        "    input_hash = hashlib.sha256(input_str.encode()).hexdigest()\n",
        "    \n",
        "    prediction_result = {\n",
        "        'id': str(uuid.uuid4()),\n",
        "        'input_hash': input_hash,\n",
        "        'model_version': 'v4.0',\n",
        "        'clf_predicted': bool(clf_pred[idx]),\n",
        "        'clf_prob_no_bid': float(clf_proba[idx, 0]),\n",
        "        'clf_prob_bid': float(clf_proba[idx, 1]),\n",
        "        'clf_confidence_score': float(confidence_scores[idx]),\n",
        "        'clf_confidence_level': confidence_levels[idx],\n",
        "        'median_predicted': float(median_pred[idx]),\n",
        "        'median_lower_95ci': float(median_pred[idx] - 1.96 * uncertainties['median'][idx]),\n",
        "        'median_upper_95ci': float(median_pred[idx] + 1.96 * uncertainties['median'][idx]),\n",
        "        'median_uncertainty': float(uncertainties['median'][idx]),\n",
        "        'median_recommended': float(median_recommended[idx]),\n",
        "        'min_predicted': float(min_pred[idx]),\n",
        "        'min_lower_95ci': float(min_pred[idx] - 1.96 * uncertainties['min'][idx]),\n",
        "        'min_upper_95ci': float(min_pred[idx] + 1.96 * uncertainties['min'][idx]),\n",
        "        'min_uncertainty': float(uncertainties['min'][idx]),\n",
        "        'min_recommended': float(min_recommended[idx]),\n",
        "        'safety_factor_median': float(median_optimal_sf),\n",
        "        'safety_factor_min': float(min_optimal_sf),\n",
        "        'recommendations': json.dumps({\n",
        "            'action': 'bid' if clf_pred[idx] else 'skip',\n",
        "            'suggested_bid': float(median_recommended[idx]),\n",
        "            'minimum_safe_bid': float(min_recommended[idx])\n",
        "        }),\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'updated_at': datetime.now().isoformat()\n",
        "    }\n",
        "    prediction_results.append(prediction_result)\n",
        "\n",
        "prediction_results_df = pd.DataFrame(prediction_results)\n",
        "print(f\"‚úÖ Created {len(prediction_results_df)} prediction results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e359bbf0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLASS-PREDICTION MAPPING\n",
            "============================================================\n",
            "‚úÖ Created 1205 class-prediction mappings\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: Map Predictions to Classes\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS-PREDICTION MAPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create class-prediction mappings\n",
        "class_predictions = []\n",
        "\n",
        "for _, mapping in class_mappings.iterrows():\n",
        "    pred_idx = mapping['prediction_idx']\n",
        "    if pred_idx < len(prediction_results_df):\n",
        "        pred_result = prediction_results_df.iloc[pred_idx]\n",
        "        \n",
        "        class_pred = {\n",
        "            'class_id': mapping['class_id'],\n",
        "            'prediction_result_id': pred_result['id'],\n",
        "            'course_code': mapping['course_code'],\n",
        "            'section': mapping['section'],\n",
        "            'acad_term_id': mapping['acad_term_id'],\n",
        "            'professor_id': mapping.get('professor_id')\n",
        "        }\n",
        "        class_predictions.append(class_pred)\n",
        "\n",
        "class_predictions_df = pd.DataFrame(class_predictions)\n",
        "print(f\"‚úÖ Created {len(class_predictions_df)} class-prediction mappings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "258dcc86",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAFETY FACTOR TABLE\n",
            "============================================================\n",
            "‚úÖ Created 22 safety factor entries\n"
          ]
        }
      ],
      "source": [
        "# Cell 13: Create Safety Factor Table\n",
        "print(\"=\"*60)\n",
        "print(\"SAFETY FACTOR TABLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "safety_factor_entries = []\n",
        "\n",
        "# Process median safety factors\n",
        "for _, row in median_sf_df.iterrows():\n",
        "    entry = {\n",
        "        'id': str(uuid.uuid4()),\n",
        "        'model_version': 'v4.0',\n",
        "        'prediction_type': 'median',\n",
        "        'safety_factor': float(row['safety_factor']),\n",
        "        'tpr': float(row['tpr']),\n",
        "        'mean_loss': float(row['mean_loss']),\n",
        "        'under_prediction_rate': None,\n",
        "        'mae': float(row['mae']),\n",
        "        'mse': float(row['mse']),\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    safety_factor_entries.append(entry)\n",
        "\n",
        "# Process min safety factors\n",
        "for _, row in min_sf_df.iterrows():\n",
        "    entry = {\n",
        "        'id': str(uuid.uuid4()),\n",
        "        'model_version': 'v4.0',\n",
        "        'prediction_type': 'min',\n",
        "        'safety_factor': float(row['safety_factor']),\n",
        "        'tpr': float(row['tpr']),\n",
        "        'mean_loss': float(row['mean_loss']),\n",
        "        'under_prediction_rate': float(row.get('under_prediction_rate', 0)),\n",
        "        'mae': float(row['mae']),\n",
        "        'mse': float(row['mse']),\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    safety_factor_entries.append(entry)\n",
        "\n",
        "safety_factor_df = pd.DataFrame(safety_factor_entries)\n",
        "print(f\"‚úÖ Created {len(safety_factor_df)} safety factor entries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6f598032",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAVING RESULTS\n",
            "============================================================\n",
            "\n",
            "‚úÖ Batch prediction completed!\n",
            "üìÅ Results saved to script_output\\predictions\n",
            "   - Predictions: 1165 records\n",
            "   - Class mappings: 1205 records\n",
            "   - Safety factors: 22 entries\n",
            "\n",
            "üìä Summary:\n",
            "   - Courses with bids: 1057\n",
            "   - Courses without bids: 108\n",
            "   - Median bid range: 4 - 100\n",
            "\n",
            "üîí Database connection closed\n"
          ]
        }
      ],
      "source": [
        "# Cell 14: Save Results\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save all outputs\n",
        "prediction_results_df.to_csv(output_dir / f'prediction_results_{timestamp}.csv', index=False)\n",
        "class_predictions_df.to_csv(output_dir / f'class_predictions_{timestamp}.csv', index=False)\n",
        "safety_factor_df.to_csv(output_dir / f'safety_factor_table_{timestamp}.csv', index=False)\n",
        "\n",
        "# Create summary report\n",
        "summary = {\n",
        "    'timestamp': timestamp,\n",
        "    'total_predictions': len(prediction_results_df),\n",
        "    'total_class_mappings': len(class_predictions_df),\n",
        "    'unique_courses': bidding_data['course_code'].nunique(),\n",
        "    'unique_terms': bidding_data['acad_term_id'].nunique(),\n",
        "    'predictions_with_bids': int(clf_pred.sum()),\n",
        "    'predictions_without_bids': int(len(clf_pred) - clf_pred.sum()),\n",
        "    'median_bid_range': {\n",
        "        'min': float(median_pred.min()),\n",
        "        'max': float(median_pred.max()),\n",
        "        'mean': float(median_pred.mean())\n",
        "    },\n",
        "    'min_bid_range': {\n",
        "        'min': float(min_pred.min()),\n",
        "        'max': float(min_pred.max()),\n",
        "        'mean': float(min_pred.mean())\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(output_dir / f'prediction_summary_{timestamp}.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Batch prediction completed!\")\n",
        "print(f\"üìÅ Results saved to {output_dir}\")\n",
        "print(f\"   - Predictions: {len(prediction_results_df)} records\")\n",
        "print(f\"   - Class mappings: {len(class_predictions_df)} records\")\n",
        "print(f\"   - Safety factors: {len(safety_factor_df)} entries\")\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   - Courses with bids: {summary['predictions_with_bids']}\")\n",
        "print(f\"   - Courses without bids: {summary['predictions_without_bids']}\")\n",
        "print(f\"   - Median bid range: {summary['median_bid_range']['min']:.0f} - {summary['median_bid_range']['max']:.0f}\")\n",
        "\n",
        "# Close database connection\n",
        "if connection:\n",
        "    connection.close()\n",
        "    print(\"\\nüîí Database connection closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bidly_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
