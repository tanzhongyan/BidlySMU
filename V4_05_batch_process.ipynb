{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1cfa91a",
      "metadata": {},
      "source": [
        "# **SMU Course Bidding Prediction Using CatBoost V4**\n",
        "\n",
        "<div style=\"background-color:#DFFFD6; padding:12px; border-radius:5px; border: 1px solid #228B22;\">\n",
        "   <h2 style=\"color:#006400;\">‚úÖ Looking to Implement This? ‚úÖ</h2>\n",
        "   <p>üöÄ **Get started quickly by using** <strong><a href=\"example_prediction.ipynb\">example_prediction.ipynb</a></strong>.</p> \n",
        "   <ul> \n",
        "      <li>üìå **Three pre-trained CatBoost models (`.cbm`) available for instant predictions.**</li>\n",
        "      <li>üîß Includes **step-by-step instructions** for making predictions with uncertainty quantification.</li>\n",
        "      <li>‚ö° Works **out-of-the-box**‚Äîjust load the models and start predicting!</li>\n",
        "   </ul>\n",
        "   <h3>üîó üìå Next Steps:</h3>\n",
        "   <p>üëâ <a href=\"example_prediction.ipynb\"><strong>Go to Example Prediction Notebook</strong></a></p>\n",
        "</div> \n",
        "<h2><span style=\"color:red\">NOTE: use at your own discretion.</span></h2>\n",
        "\n",
        "### **Changes in V4**\n",
        "- **Three-model architecture**: Added a classification model to predict whether a course will receive bids, complementing the existing median and min bid regression models\n",
        "- **Advanced uncertainty quantification**: Implemented entropy-based confidence scoring for classification and t-distribution-based uncertainty multipliers for regression models\n",
        "- **Enhanced feature engineering**: Incorporated day-of-week boolean flags (`has_mon`, `has_tue`, etc.) for better temporal pattern recognition\n",
        "- **Sophisticated safety factor system**: Custom t-distribution fitting to error data with percentile-based uncertainty multipliers, replacing simple percentage-based safety factors\n",
        "- **Comprehensive evaluation suite**: Added confidence interval coverage analysis, residual analysis with emphasis on under-predictions, and cross-model feature importance comparison\n",
        "\n",
        "### **Objective**\n",
        "This notebook predicts bidding outcomes for courses in the SMU bidding system using **three specialized CatBoost models**. Building on insights from **V1, V2, and V3**, this version introduces a comprehensive **multi-model approach** with advanced uncertainty quantification:\n",
        "\n",
        "1. **Classification Model**: Predicts whether a course will receive bids (optimized for high recall)\n",
        "2. **Median Bid Regression Model**: Predicts the median bid price with t-distribution-based confidence intervals\n",
        "3. **Min Bid Regression Model**: Predicts the minimum bid price with t-distribution-based confidence intervals\n",
        "\n",
        "### **Key Enhancements in V4**\n",
        "\n",
        "**Learning from V3:**\n",
        "   - V3 focused on two regression models for median and min bid prediction\n",
        "   - V4 adds a **classification component** to identify courses that will receive bidding activity\n",
        "   - Enhanced with **probabilistic predictions** and **confidence scoring**\n",
        "\n",
        "**New V4 Features:**\n",
        "   - **Entropy-based confidence scoring** for classification predictions with five confidence levels (Very Low to Very High)\n",
        "   - **T-distribution-based uncertainty multipliers** derived from validation error analysis for robust confidence estimation\n",
        "   - **Percentile-based safety factors** (1%-99%) with both empirical and theoretical multipliers for precise risk management\n",
        "   - **Comprehensive uncertainty analysis** including distribution fitting and coverage metrics\n",
        "\n",
        "### **Three-Model Architecture**\n",
        "\n",
        "| **Model Type** | **Purpose** | **Output** | **Uncertainty Measure** |\n",
        "|----------------|-------------|------------|--------------------------|\n",
        "| **Classification** | Predict bid courses | Probability + Confidence Level | Entropy-based confidence score |\n",
        "| **Median Bid Regression** | Predict median bid price | Price + Uncertainty Multipliers | T-distribution-based multipliers |\n",
        "| **Min Bid Regression** | Predict minimum bid price | Price + Uncertainty Multipliers | T-distribution-based multipliers |\n",
        "\n",
        "### **Updated Dataset Features**\n",
        "\n",
        "| **Feature Name** | **Type** | **Description** |\n",
        "|------------------|----------|--------------------|\n",
        "| **`subject_area`** | Categorical | Subject area (IS, ECON, etc.) |\n",
        "| **`catalogue_no`** | Categorical | Course number |\n",
        "| **`round`** | Categorical | Bidding round (1, 1A, 1B, 1C, 2, 2A) |\n",
        "| **`window`** | Numerical | Bidding window (1-5) |\n",
        "| **`before_process_vacancy`** | Numerical | Available spots before bidding |\n",
        "| **`acad_year_start`** | Numerical | Academic year start |\n",
        "| **`term`** | Categorical | Academic term (1, 2, 3A, 3B) |\n",
        "| **`start_time`** | Categorical | Class start time |\n",
        "| **`course_name`** | Categorical | Course name/description |\n",
        "| **`section`** | Categorical | Course section |\n",
        "| **`instructor`** | Categorical | Instructor name |\n",
        "| **`has_mon`** - **`has_sun`** | Boolean | Day-of-week indicators |\n",
        "| **üéØ Target Variables üéØ** | | **Model outputs** |\n",
        "| **`bids`** | Binary | Whether course receives bids |\n",
        "| **`target_median_bid`** | Numerical | Median bid price |\n",
        "| **`target_min_bid`** | Numerical | Minimum bid price |\n",
        "\n",
        "### **Advanced Uncertainty Quantification**\n",
        "\n",
        "**Classification Confidence:**\n",
        "- **Entropy-based scoring**: Measures prediction certainty using information entropy\n",
        "- **Five confidence levels**: Very Low, Low, Medium, High, Very High\n",
        "- **Probability outputs**: Separate probabilities for bid/non-bid outcomes\n",
        "\n",
        "**Regression Uncertainty Multipliers:**\n",
        "- **T-distribution fitting**: Models fitted to validation error distributions for accurate tail behavior\n",
        "- **Percentile-based multipliers**: 99 percentiles (1%-99%) with corresponding uncertainty multipliers\n",
        "- **Dual multiplier types**: Both empirical (data-driven) and theoretical (distribution-based) multipliers\n",
        "- **Usage formula**: `final_bid = predicted + uncertainty √ó multiplier`\n",
        "\n",
        "### **Enhanced Safety Factor System**\n",
        "\n",
        "**Traditional vs V4 Approach:**\n",
        "- **Old approach**: Fixed percentage multipliers (e.g., `bid = prediction √ó 1.7`)\n",
        "- **V4 approach**: Uncertainty-aware multipliers (e.g., `bid = prediction + uncertainty √ó 1.584`)\n",
        "\n",
        "**Distribution Analysis:**\n",
        "- **Error modeling**: T-distributions fitted to validation residuals\n",
        "- **Heavy tail handling**: Proper modeling of extreme prediction errors\n",
        "- **Confidence levels**: Precise percentile-based risk assessment (80%, 90%, 95%, 99%)\n",
        "\n",
        "**Usage Example (90% confidence):**\n",
        "```\n",
        "Median bid = median_predicted + median_uncertainty √ó 1.584\n",
        "Min bid = min_predicted + min_uncertainty √ó 1.533\n",
        "```\n",
        "\n",
        "### **Methodology**\n",
        "The notebook follows this enhanced structure:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Loading separate datasets for classification and regression tasks\n",
        "   - Feature standardization and categorical encoding\n",
        "   - Train-test splitting with consistent random seeds\n",
        "\n",
        "2. **Three-Model Training**:\n",
        "   - **Classification**: CatBoost with recall optimization for bid opportunity detection\n",
        "   - **Median Regression**: CatBoost with t-distribution uncertainty quantification\n",
        "   - **Min Regression**: CatBoost with distribution-based uncertainty modeling\n",
        "\n",
        "3. **Advanced Evaluation**:\n",
        "   - **Classification**: Recall (maximizing true positives for bid detection), confusion matrix, entropy-based confidence analysis\n",
        "   - **Regression**: MSE, MAE, R¬≤, distribution fitting, percentile-based uncertainty analysis\n",
        "   - **Cross-model feature importance comparison**\n",
        "\n",
        "4. **Comprehensive Visualization**:\n",
        "   - Confidence distribution plots and uncertainty analysis\n",
        "   - Error distribution modeling and safety factor derivation\n",
        "   - Feature importance rankings across all three models\n",
        "\n",
        "5. **Model Persistence and Reporting**:\n",
        "   - All models saved as `.cbm` files for deployment\n",
        "   - Detailed results exported to CSV format with updated schema\n",
        "   - Safety factor tables with percentile-based multipliers\n",
        "\n",
        "### **Key Metrics and Performance**\n",
        "\n",
        "**Classification Model:**\n",
        "- **Primary metric**: Recall (optimized for capturing all bidding opportunities - maximizing true positives)\n",
        "- **Confidence analysis**: Distribution of entropy-based confidence scores  \n",
        "- **Output**: Probabilities for bid/no-bid outcomes (`clf_has_bids_prob`), confidence levels, and entropy values\n",
        "\n",
        "**Regression Models:**\n",
        "- **Standard metrics**: MSE, MAE, R¬≤ for model accuracy\n",
        "- **Distribution analysis**: T-distribution fitting with degrees of freedom, location, and scale parameters\n",
        "- **Uncertainty metrics**: Percentile-based multipliers and coverage analysis\n",
        "- **Safety analysis**: Risk-calibrated predictions using distribution-based multipliers\n",
        "\n",
        "### **Classification Strategy - Maximizing Bidding Opportunities**\n",
        "\n",
        "**Recall-Optimized Approach:**\n",
        "- **Target**: Predict courses that will receive bids (positive class = 1)\n",
        "- **Primary Goal**: Maximize recall to capture all potential bidding opportunities\n",
        "- **Business Logic**: Missing a course that will receive bids (False Negative) is more costly than incorrectly predicting a course will receive bids (False Positive)\n",
        "- **Optimization**: Model trained to minimize missed bidding opportunities while maintaining reasonable precision\n",
        "\n",
        "### **Implementation Notes**\n",
        "To run this V4 notebook:\n",
        "- Install required packages: `pip install catboost pandas numpy matplotlib seaborn scikit-learn scipy`\n",
        "- Ensure you have the three required datasets:\n",
        "  - Classification training/test data\n",
        "  - Median bid regression training/test data\n",
        "  - Min bid regression training/test data\n",
        "- Models automatically save to `script_output_model_training/mode/` directory\n",
        "- Validation results required for safety factor calculation\n",
        "\n",
        "### **V4 Advantages**\n",
        "- **Comprehensive coverage**: Handles both bid opportunity detection and price prediction\n",
        "- **Risk-aware predictions**: T-distribution-based uncertainty modeling prevents dangerous under-bidding\n",
        "- **Confidence-calibrated**: Provides uncertainty measures for better decision-making with precise percentile-based risk assessment\n",
        "- **Feature-rich analysis**: Cross-model feature importance for strategic insights\n",
        "- **Production-ready**: All models saved with consistent interfaces for deployment\n",
        "- **Scientific rigor**: Distribution-based uncertainty quantification replacing ad-hoc safety factors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8520404",
      "metadata": {},
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359f5072",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import os\n",
        "import psycopg2\n",
        "from dotenv import load_dotenv\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import json\n",
        "import hashlib\n",
        "from typing import List\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add database configuration\n",
        "load_dotenv()\n",
        "db_config = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'database': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': int(os.getenv('DB_PORT', 5432)),\n",
        "    'gssencmode': 'disable'\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "output_dir = Path('script_output/predictions')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "cache_dir = Path('db_cache')\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CONFIGURATION\n",
        "# Define the academic term you are targeting for predictions.\n",
        "TARGET_AY_TERM = '2025-26_T1'\n",
        "\n",
        "# Central bidding schedule for each academic term.\n",
        "# The script uses this to correctly parse the 'bidding_window' strings.\n",
        "# Format: (results_datetime, \"Full Bidding Window Name\", \"Folder_Suffix\")\n",
        "BIDDING_SCHEDULES = {\n",
        "    '2025-26_T1': [\n",
        "        (datetime(2025, 7, 9, 14, 0), \"Round 1 Window 1\", \"R1W1\"),\n",
        "        (datetime(2025, 7, 11, 14, 0), \"Round 1A Window 1\", \"R1AW1\"),\n",
        "        (datetime(2025, 7, 14, 14, 0), \"Round 1A Window 2\", \"R1AW2\"),\n",
        "        (datetime(2025, 7, 16, 14, 0), \"Round 1A Window 3\", \"R1AW3\"),\n",
        "        (datetime(2025, 7, 18, 14, 0), \"Round 1B Window 1\", \"R1BW1\"),\n",
        "        (datetime(2025, 7, 21, 14, 0), \"Round 1B Window 2\", \"R1BW2\"),\n",
        "        (datetime(2025, 7, 30, 14, 0), \"Incoming Exchange Rnd 1C Win 1\", \"R1CW1\"),\n",
        "        (datetime(2025, 7, 31, 14, 0), \"Incoming Exchange Rnd 1C Win 2\", \"R1CW2\"),\n",
        "        (datetime(2025, 8, 1, 14, 0), \"Incoming Exchange Rnd 1C Win 3\", \"R1CW3\"),\n",
        "        (datetime(2025, 8, 11, 14, 0), \"Incoming Freshmen Rnd 1 Win 1\", \"R1FW1\"),\n",
        "        (datetime(2025, 8, 12, 14, 0), \"Incoming Freshmen Rnd 1 Win 2\", \"R1FW2\"),\n",
        "        (datetime(2025, 8, 13, 14, 0), \"Incoming Freshmen Rnd 1 Win 3\", \"R1FW3\"),\n",
        "        (datetime(2025, 8, 14, 14, 0), \"Incoming Freshmen Rnd 1 Win 4\", \"R1FW4\"),\n",
        "        (datetime(2025, 8, 20, 14, 0), \"Round 2 Window 1\", \"R2W1\"),\n",
        "        (datetime(2025, 8, 22, 14, 0), \"Round 2 Window 2\", \"R2W2\"),\n",
        "        (datetime(2025, 8, 25, 14, 0), \"Round 2 Window 3\", \"R2W3\"),\n",
        "        (datetime(2025, 8, 27, 14, 0), \"Round 2A Window 1\", \"R2AW1\"),\n",
        "        (datetime(2025, 8, 29, 14, 0), \"Round 2A Window 2\", \"R2AW2\"),\n",
        "        (datetime(2025, 9, 1, 14, 0), \"Round 2A Window 3\", \"R2AW3\"),\n",
        "    ]\n",
        "    # You can add schedules for other terms here, e.g., '2025-26_T2': [...]\n",
        "}\n",
        "\n",
        "# To enable automatic detection based on the current time, set MANUAL_ROUND to None.\n",
        "MANUAL_ROUND = None # Example: '1A'\n",
        "MANUAL_WINDOW = None    # Example: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309e5d9e",
      "metadata": {},
      "source": [
        "## **2. SMUBiddingTransformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ccf08c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SMUBiddingTransformer:\n",
        "    \"\"\"\n",
        "    A reusable transformer class for processing SMU course bidding data\n",
        "    optimized for CatBoost model.\n",
        "    \n",
        "    Uses categorical encoding for instructors and one-hot encoding for multi-valued days.\n",
        "    \n",
        "    Expected input columns:\n",
        "    - course_code: str (e.g. 'MGMT715', 'COR-COMM175')\n",
        "    - course_name: str\n",
        "    - acad_year_start: int\n",
        "    - term: str ('1', '2', '3A', '3B')\n",
        "    - start_time: str (e.g. '19:30', 'TBA') - preserved as categorical\n",
        "    - day_of_week: str (can be multivalued, e.g. 'Mon,Thu')\n",
        "    - before_process_vacancy: int\n",
        "    - bidding_window: str (e.g. 'Round 1 Window 1', 'Incoming Freshmen Rnd 1 Win 4')\n",
        "    - instructor: str (can be multivalued, e.g. 'JOHN DOE, JANE SMITH')\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the transformer for CatBoost optimization.\n",
        "        \n",
        "        Uses categorical encoding for instructors and one-hot encoding for days.\n",
        "        \"\"\"\n",
        "        # Fitted flags\n",
        "        self.is_fitted = False\n",
        "        \n",
        "        # Lists to track feature types for CatBoost\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "    def fit(self, df: pd.DataFrame) -> 'SMUBiddingTransformer':\n",
        "        \"\"\"\n",
        "        Fit the transformer on training data.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Training dataframe with all required columns\n",
        "        \"\"\"\n",
        "        # Validate required columns\n",
        "        required_cols = [\n",
        "            'course_code', 'course_name', 'acad_year_start', 'term',\n",
        "            'start_time', 'day_of_week', 'before_process_vacancy',\n",
        "            'bidding_window', 'instructor', 'section'\n",
        "        ]\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "        \n",
        "        print(f\"Fitting transformer on {len(df)} rows...\")\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "    \n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Transform the dataframe to CatBoost-ready format.\n",
        "        \"\"\"\n",
        "        # Try to load existing model if not fitted\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted before transform. Call fit() first.\")\n",
        "        \n",
        "        # Create a copy to avoid modifying original\n",
        "        df_transformed = df.copy()\n",
        "        \n",
        "        # Reset feature tracking\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "        # 1. Extract course components (categorical + numeric)\n",
        "        course_features = self._extract_course_features(df_transformed)\n",
        "        \n",
        "        # 2. Process bidding window (categorical + numeric)\n",
        "        round_window_features = self._extract_round_window(df_transformed)\n",
        "        \n",
        "        # 3. Basic features (preserve categorical nature) + instructor as categorical\n",
        "        basic_features = self._process_basic_features(df_transformed)\n",
        "        \n",
        "        # 4. Create day one-hot encoding\n",
        "        day_features = self._create_day_one_hot_encoding(df_transformed)\n",
        "        \n",
        "        # Combine all features - FIXED: Ensure proper concatenation\n",
        "        feature_dfs = [course_features, round_window_features, basic_features, day_features]\n",
        "        \n",
        "        # Filter out any empty DataFrames\n",
        "        feature_dfs = [df for df in feature_dfs if not df.empty]\n",
        "        \n",
        "        if not feature_dfs:\n",
        "            raise ValueError(\"No features were extracted\")\n",
        "        \n",
        "        # Concatenate all features\n",
        "        final_df = pd.concat(feature_dfs, axis=1)\n",
        "        \n",
        "        # Verify all expected features are present\n",
        "        expected_features = self.categorical_features + self.numeric_features\n",
        "        missing_features = [f for f in expected_features if f not in final_df.columns]\n",
        "        \n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing features in final dataframe: {missing_features}\")\n",
        "            print(f\"Available columns: {list(final_df.columns)}\")\n",
        "        \n",
        "        # Debug: Print feature summary\n",
        "        print(f\"Transformed data shape: {final_df.shape}\")\n",
        "        print(f\"Features included: {list(final_df.columns)[:10]}...\")  # Show first 10\n",
        "        \n",
        "        return final_df\n",
        "        \n",
        "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Fit the transformer and transform the data in one step.\"\"\"\n",
        "        self.fit(df)\n",
        "        return self.transform(df)\n",
        "    \n",
        "    def get_categorical_features(self) -> List[str]:\n",
        "        \"\"\"Get list of categorical feature names for CatBoost.\"\"\"\n",
        "        return self.categorical_features.copy()\n",
        "    \n",
        "    def get_numeric_features(self) -> List[str]:\n",
        "        \"\"\"Get list of numeric feature names.\"\"\"\n",
        "        return self.numeric_features.copy()\n",
        "    \n",
        "    def _extract_course_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract subject area and catalogue number from course code.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def split_course_code(code):\n",
        "            \"\"\"Split course code into subject area and catalogue number.\"\"\"\n",
        "            if pd.isna(code):\n",
        "                return None, None\n",
        "            \n",
        "            code = str(code).strip().upper()\n",
        "            \n",
        "            # Handle hyphenated codes like 'COR-COMM175'\n",
        "            if '-' in code:\n",
        "                parts = code.split('-')\n",
        "                if len(parts) >= 2:\n",
        "                    subject = '-'.join(parts[:-1])\n",
        "                    # Extract number from last part\n",
        "                    num_match = re.search(r'(\\d+)', parts[-1])\n",
        "                    if num_match:\n",
        "                        return subject, int(num_match.group(1))\n",
        "                    else:\n",
        "                        # Try extracting from full last part\n",
        "                        num_match = re.search(r'(\\d+)', code)\n",
        "                        if num_match:\n",
        "                            return subject, int(num_match.group(1))\n",
        "            \n",
        "            # Standard format like 'MGMT715'\n",
        "            match = re.match(r'([A-Z\\-]+)(\\d+)', code)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            return code, 0\n",
        "        \n",
        "        # Extract components\n",
        "        splits = df['course_code'].apply(split_course_code)\n",
        "        features['subject_area'] = splits.apply(lambda x: x[0] if x else None)\n",
        "        features['catalogue_no'] = splits.apply(lambda x: x[1] if x else 0)\n",
        "\n",
        "        # Debug: Verify extraction\n",
        "        print(f\"Extracted course features: {features.shape}\")\n",
        "        print(f\"Sample subject_area values: {features['subject_area'].head()}\")\n",
        "        print(f\"Sample catalogue_no values: {features['catalogue_no'].head()}\")\n",
        "\n",
        "        # subject_area and catalogue_no are categorical for CatBoost\n",
        "        self.categorical_features.extend(['subject_area', 'catalogue_no'])\n",
        "\n",
        "        return features\n",
        "    \n",
        "    def _extract_round_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract round and window from bidding_window string.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def parse_bidding_window(window_str):\n",
        "            \"\"\"Parse bidding window string into round and window number.\"\"\"\n",
        "            if pd.isna(window_str):\n",
        "                return None, None\n",
        "            \n",
        "            window_str = str(window_str).strip()\n",
        "            # Check for Incoming Freshmen FIRST (before other patterns)\n",
        "            if 'Incoming Freshmen' in window_str:\n",
        "                match = re.search(r'Rnd\\s+(\\d)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if match:\n",
        "                    # Add F suffix to distinguish from regular rounds\n",
        "                    return f\"{match.group(1)}F\", int(match.group(2))     \n",
        "            \n",
        "            # Pattern 1: Standard format\n",
        "            match = re.search(r'Round\\s+(\\d[A-C]?)\\s+Window\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 2: Abbreviated format\n",
        "            match = re.search(r'Rnd\\s+(\\d[A-C]?)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 3: Incoming Exchange format (keeps original round)\n",
        "            match = re.search(r'Incoming\\s+Exchange\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 4: Incoming Freshmen format (adds F suffix)\n",
        "            match = re.search(r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                original_round = match.group(1)\n",
        "                window_num = int(match.group(2))\n",
        "                # Map Incoming Freshmen Round 1 to Round 1F\n",
        "                if original_round == \"1\":\n",
        "                    round_str = \"1F\"\n",
        "                else:\n",
        "                    round_str = f\"{original_round}F\"\n",
        "                return round_str, window_num\n",
        "            \n",
        "            # Fallback patterns...\n",
        "            match = re.search(r'(\\d[A-C]?)', window_str)\n",
        "            if match:\n",
        "                win_match = re.search(r'Window\\s+(\\d)|Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if win_match:\n",
        "                    window_num = int(win_match.group(1) or win_match.group(2))\n",
        "                    return match.group(1), window_num\n",
        "                return match.group(1), 1\n",
        "            \n",
        "            return '1', 1\n",
        "        \n",
        "        # Extract round and window\n",
        "        parsed = df['bidding_window'].apply(parse_bidding_window)\n",
        "        features['round'] = parsed.apply(lambda x: x[0] if x else '1')\n",
        "        features['window'] = parsed.apply(lambda x: x[1] if x else 1)\n",
        "        \n",
        "        # Round as categorical (preserves ordering like 1, 1A, 1B, 2, 2A)\n",
        "        self.categorical_features.append('round')\n",
        "        \n",
        "        # Window as numeric\n",
        "        self.numeric_features.append('window')\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def _process_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process basic features, preserving categorical nature where beneficial.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Numeric features\n",
        "        features['before_process_vacancy'] = pd.to_numeric(\n",
        "            df['before_process_vacancy'], errors='coerce'\n",
        "        ).fillna(0)\n",
        "        features['acad_year_start'] = pd.to_numeric(\n",
        "            df['acad_year_start'], errors='coerce'\n",
        "        ).fillna(2025)\n",
        "        \n",
        "        self.numeric_features.extend(['before_process_vacancy', 'acad_year_start'])\n",
        "        \n",
        "        # Categorical features\n",
        "        features['term'] = df['term'].astype(str)\n",
        "        features['start_time'] = df['start_time'].astype(str)\n",
        "        features['course_name'] = df['course_name'].astype(str)\n",
        "        features['section'] = df['section'].astype(str)\n",
        "        \n",
        "        # Process instructor names (remove duplicates, handle comma-separated format)\n",
        "        features['instructor'] = df['instructor'].apply(self._process_instructor_names)\n",
        "\n",
        "        # Replace empty strings with None for proper CatBoost handling\n",
        "        features.loc[features['start_time'].isin(['', 'nan']), 'start_time'] = None\n",
        "        features.loc[features['course_name'].isin(['', 'nan']), 'course_name'] = None\n",
        "        features.loc[features['section'].isin(['', 'nan']), 'section'] = None\n",
        "        \n",
        "        self.categorical_features.extend(['term', 'start_time', 'course_name', 'section', 'instructor'])\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def _create_day_one_hot_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create one-hot encoding for days of the week.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Initialize all day columns as 0\n",
        "        day_columns = ['has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun']\n",
        "        for col in day_columns:\n",
        "            features[col] = 0\n",
        "        \n",
        "        # Day mapping\n",
        "        day_abbrev = {\n",
        "            'MONDAY': 'MON', 'TUESDAY': 'TUE', 'WEDNESDAY': 'WED',\n",
        "            'THURSDAY': 'THU', 'FRIDAY': 'FRI', 'SATURDAY': 'SAT', 'SUNDAY': 'SUN',\n",
        "            'MON': 'MON', 'TUE': 'TUE', 'WED': 'WED', 'THU': 'THU',\n",
        "            'FRI': 'FRI', 'SAT': 'SAT', 'SUN': 'SUN'\n",
        "        }\n",
        "        \n",
        "        day_to_column = {\n",
        "            'MON': 'has_mon', 'TUE': 'has_tue', 'WED': 'has_wed', 'THU': 'has_thu',\n",
        "            'FRI': 'has_fri', 'SAT': 'has_sat', 'SUN': 'has_sun'\n",
        "        }\n",
        "        \n",
        "        # Process each row's day_of_week\n",
        "        for idx, days_value in enumerate(df['day_of_week']):\n",
        "            if pd.isna(days_value) or str(days_value).strip() == '':\n",
        "                continue  # Leave all days as 0\n",
        "            \n",
        "            days_str = str(days_value).strip()\n",
        "            \n",
        "            # Handle JSON array format\n",
        "            if days_str.startswith('[') and days_str.endswith(']'):\n",
        "                try:\n",
        "                    import json\n",
        "                    days_list = json.loads(days_str)\n",
        "                    if isinstance(days_list, list):\n",
        "                        for day in days_list:\n",
        "                            day_upper = str(day).strip().upper()\n",
        "                            standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                            \n",
        "                            if standardized_day in day_to_column:\n",
        "                                features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try comma-separated format as fallback\n",
        "                    pass\n",
        "            else:\n",
        "                # Handle comma-separated format (legacy support)\n",
        "                for day in days_str.split(','):\n",
        "                    day_upper = day.strip().upper()\n",
        "                    standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                    \n",
        "                    if standardized_day in day_to_column:\n",
        "                        features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "        \n",
        "        # These are numeric binary features (0/1)\n",
        "        self.numeric_features.extend(day_columns)\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def get_feature_names(self) -> List[str]:\n",
        "        \"\"\"Get all feature names after transformation.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted to get feature names.\")\n",
        "        \n",
        "        return self.categorical_features + self.numeric_features\n",
        "    \n",
        "    def _process_instructor_names(self, instructor_input):\n",
        "        \"\"\"Process instructor names to ensure consistent JSON array format as categorical string.\"\"\"\n",
        "        # Handle list/array input\n",
        "        if isinstance(instructor_input, (list, np.ndarray)):\n",
        "            if len(instructor_input) == 0:\n",
        "                return None\n",
        "            # Convert list to string format for processing\n",
        "            instructor_str = ', '.join([str(inst).strip() for inst in instructor_input if pd.notna(inst) and str(inst).strip()])\n",
        "            if not instructor_str:\n",
        "                return None\n",
        "        else:\n",
        "            # Handle string input\n",
        "            if pd.isna(instructor_input) or str(instructor_input).strip() == '' or str(instructor_input).upper() == 'TBA':\n",
        "                return None\n",
        "            instructor_str = str(instructor_input).strip()\n",
        "        \n",
        "        # Check if it's already a JSON array\n",
        "        if instructor_str.startswith('[') and instructor_str.endswith(']'):\n",
        "            try:\n",
        "                import json\n",
        "                # Parse JSON array\n",
        "                instructors = json.loads(instructor_str)\n",
        "                if isinstance(instructors, list) and instructors:\n",
        "                    # Join back to string for processing\n",
        "                    instructor_str = ', '.join([str(inst).strip() for inst in instructors if pd.notna(inst) and str(inst).strip()])\n",
        "                    if not instructor_str:\n",
        "                        return None\n",
        "                else:\n",
        "                    return None\n",
        "            except json.JSONDecodeError:\n",
        "                # If JSON parsing fails, treat as regular string\n",
        "                pass\n",
        "        \n",
        "        # Load professor lookup mapping\n",
        "        professor_lookup = {}\n",
        "        lookup_path = Path(\"script_input/professor_lookup.csv\")\n",
        "        if lookup_path.exists():\n",
        "            lookup_df = pd.read_csv(lookup_path)\n",
        "            for _, row in lookup_df.iterrows():\n",
        "                if pd.notna(row.get('boss_name')) and pd.notna(row.get('afterclass_name')):\n",
        "                    professor_lookup[str(row['boss_name']).strip().upper()] = str(row['afterclass_name']).strip()\n",
        "        \n",
        "        # Step 1: Check if the entire name exists in professor_lookup (single professor)\n",
        "        prof_name_upper = instructor_str.upper()\n",
        "        if prof_name_upper in professor_lookup:\n",
        "            import json\n",
        "            return json.dumps([professor_lookup[prof_name_upper]])\n",
        "        \n",
        "        # Step 2: Check hardcoded multi-instructor combinations first\n",
        "        multi_instructor_combinations = {\n",
        "            \"ERIC YEE SHIN CHONG, MANDY THAM\": [\"ERIC YEE SHIN CHONG\", \"MANDY THAM\"],\n",
        "            \"ZHENG ZHICHAO, DANIEL, TAN KAR WAY\": [\"ZHENG ZHICHAO, DANIEL\", \"TAN KAR WAY\"],\n",
        "            \"KAM WAI WARREN BARTHOLOMEW CHIK, LANX GOH\": [\"KAM WAI WARREN BARTHOLOMEW CHIK\", \"LANX GOH\"],\n",
        "            \"ANDREW MIN HAN CHIN, DANIEL TAN\": [\"ANDREW MIN HAN CHIN\", \"DANIEL TAN\"],\n",
        "            \"PAUL GRIFFIN, TA NGUYEN BINH DUONG\": [\"PAUL GRIFFIN\", \"TA NGUYEN BINH DUONG\"],\n",
        "            \"ANDREW MIN HAN CHIN, JUNJI SUMITANI\": [\"ANDREW MIN HAN CHIN\", \"JUNJI SUMITANI\"],\n",
        "            \"DAVID GOMULYA, LIM CHON PHUNG, AJAY MAKHIJA\": [\"DAVID GOMULYA\", \"LIM CHON PHUNG\", \"AJAY MAKHIJA\"],\n",
        "            \"JACK HONG JIAJUN, ANG SER KENG\": [\"JACK HONG JIAJUN\", \"ANG SER KENG\"],\n",
        "            \"DAVID GOMULYA, DAVID LLEWELYN\": [\"DAVID GOMULYA\", \"DAVID LLEWELYN\"],\n",
        "            \"TERENCE FAN PING-CHING, JONATHAN TEE\": [\"TERENCE FAN PING-CHING\", \"JONATHAN TEE\"],\n",
        "            \"RONG WANG, CHENG QIANG, CHEN XIA, LIANDONG ZHANG, WANG JIWEI, YUE HENG\": [\"RONG WANG\", \"CHENG QIANG\", \"CHEN XIA\", \"LIANDONG ZHANG\", \"WANG JIWEI\", \"YUE HENG\"],\n",
        "            \"PASCALE CRAMA, ARNOUD DE MEYER\": [\"PASCALE CRAMA\", \"ARNOUD DE MEYER\"],\n",
        "            \"TERENCE FAN PING-CHING, WILSON TENG\": [\"TERENCE FAN PING-CHING\", \"WILSON TENG\"],\n",
        "            \"ANDREW MIN HAN CHIN, LI JIN\": [\"ANDREW MIN HAN CHIN\", \"LI JIN\"],\n",
        "            \"ONG, BENJAMIN JOSHUA, EUGENE TAN KHENG BOON\": [\"ONG, BENJAMIN JOSHUA\", \"EUGENE TAN KHENG BOON\"],\n",
        "            \"MANDY THAM, ERIC YEE SHIN CHONG\": [\"MANDY THAM\", \"ERIC YEE SHIN CHONG\"],\n",
        "            \"TERENCE FAN PING-CHING, RUTH CHIANG\": [\"TERENCE FAN PING-CHING\", \"RUTH CHIANG\"],\n",
        "            \"JARED POON JUN KEAT, CHAM YANWEI, DERRICK\": [\"JARED POON JUN KEAT\", \"CHAM YANWEI, DERRICK\"],\n",
        "            \"DAVID GOMULYA, SZE TIAM LIN\": [\"DAVID GOMULYA\", \"SZE TIAM LIN\"],\n",
        "            \"ANDREW MIN HAN CHIN, JAY WONG\": [\"ANDREW MIN HAN CHIN\", \"JAY WONG\"],\n",
        "            \"MARK CHONG YIEW KIM, VICTOR OCAMPO\": [\"MARK CHONG YIEW KIM\", \"VICTOR OCAMPO\"],\n",
        "            \"TSE, JUSTIN K, AIDAN WONG\": [\"TSE, JUSTIN K\", \"AIDAN WONG\"],\n",
        "            \"TANG HONG WEE, GERALD SEAH, MUHAMMED AMEER S/O MOHAMED NOOR, LAU MENG YAN\": [\"TANG HONG WEE\", \"GERALD SEAH\", \"MUHAMMED AMEER S/O MOHAMED NOOR\", \"LAU MENG YAN\"],\n",
        "            \"AURELIO GURREA MARTINEZ, LOH SONG-EN, SAMUEL\": [\"AURELIO GURREA MARTINEZ\", \"LOH SONG-EN, SAMUEL\"],\n",
        "            \"CHNG SHUQI, AMELIA CHUA, MUHAMMED AMEER S/O MOHAMED NOOR\": [\"CHNG SHUQI\", \"AMELIA CHUA\", \"MUHAMMED AMEER S/O MOHAMED NOOR\"]\n",
        "        }\n",
        "        \n",
        "        if instructor_str in multi_instructor_combinations:\n",
        "            split_names = multi_instructor_combinations[instructor_str]\n",
        "        else:\n",
        "            # Step 3: Progressive comma-based splitting with exact matching\n",
        "            # Split by comma first\n",
        "            comma_parts = [part.strip() for part in instructor_str.split(',') if part.strip()]\n",
        "            \n",
        "            # If only one part (no commas), treat as single professor\n",
        "            if len(comma_parts) <= 1:\n",
        "                if prof_name_upper in professor_lookup:\n",
        "                    import json\n",
        "                    return json.dumps([professor_lookup[prof_name_upper]])\n",
        "                else:\n",
        "                    words = instructor_str.strip().split()\n",
        "                    if len(words) >= 2:\n",
        "                        import json\n",
        "                        return json.dumps([instructor_str.strip()])\n",
        "                    else:\n",
        "                        return None\n",
        "            \n",
        "            # Get all boss_names from professor_lookup for matching\n",
        "            boss_names = set()\n",
        "            for boss_name in professor_lookup.keys():\n",
        "                if boss_name is not None and not pd.isna(boss_name):\n",
        "                    boss_name_str = str(boss_name).strip()\n",
        "                    if boss_name_str and boss_name_str.lower() != 'nan':\n",
        "                        boss_names.add(boss_name_str.upper())\n",
        "            \n",
        "            professors_found = []\n",
        "            i = 0\n",
        "            \n",
        "            while i < len(comma_parts):\n",
        "                current_candidate = comma_parts[i]\n",
        "                matched = False\n",
        "                \n",
        "                # Try progressive matching: add more comma parts until we find a match\n",
        "                for j in range(i + 1, len(comma_parts) + 1):\n",
        "                    candidate = ', '.join(comma_parts[i:j])\n",
        "                    candidate_upper = candidate.upper()\n",
        "                    \n",
        "                    # Check for exact match\n",
        "                    if candidate_upper in boss_names:\n",
        "                        professors_found.append(candidate)\n",
        "                        i = j  # Move past all used parts\n",
        "                        matched = True\n",
        "                        break\n",
        "                    \n",
        "                    # Check for partial word match (all words in candidate must be in some boss_name)\n",
        "                    candidate_words = set(candidate.replace(',', ' ').split())\n",
        "                    for boss_name in boss_names:\n",
        "                        boss_words = set(boss_name.replace(',', ' ').split())\n",
        "                        if candidate_words.issubset(boss_words) and len(candidate_words) >= 2:\n",
        "                            professors_found.append(candidate)\n",
        "                            i = j  # Move past all used parts\n",
        "                            matched = True\n",
        "                            break\n",
        "                    \n",
        "                    if matched:\n",
        "                        break\n",
        "                \n",
        "                # If no match found and we're at a single part, check if it's reasonable\n",
        "                if not matched:\n",
        "                    single_part = comma_parts[i]\n",
        "                    words_in_part = single_part.split()\n",
        "                    \n",
        "                    # Only accept if it has at least 2 words (avoid single word professors)\n",
        "                    if len(words_in_part) >= 2:\n",
        "                        professors_found.append(single_part)\n",
        "                        i += 1\n",
        "                    else:\n",
        "                        # Try to combine with next part if available\n",
        "                        if i + 1 < len(comma_parts):\n",
        "                            combined = f\"{single_part}, {comma_parts[i + 1]}\"\n",
        "                            professors_found.append(combined)\n",
        "                            i += 2  # Skip next part too\n",
        "                        else:\n",
        "                            # Skip single word professors entirely\n",
        "                            i += 1\n",
        "            \n",
        "            # Final validation: remove any single-word results\n",
        "            split_names = []\n",
        "            for prof in professors_found:\n",
        "                prof_words = prof.strip().replace(',', ' ').split()\n",
        "                if len(prof_words) >= 2:  # Only keep professors with at least 2 words\n",
        "                    split_names.append(prof)\n",
        "            \n",
        "            # If we couldn't split intelligently, fall back to treating as single professor\n",
        "            # but only if it has at least 2 words\n",
        "            if not split_names:\n",
        "                prof_words = instructor_str.strip().replace(',', ' ').split()\n",
        "                if len(prof_words) >= 2:\n",
        "                    split_names = [instructor_str]\n",
        "                else:\n",
        "                    return None\n",
        "        \n",
        "        # Map each split name to afterclass_name\n",
        "        mapped_names = []\n",
        "        for name in split_names:\n",
        "            name_upper = name.strip().upper()\n",
        "            if name_upper in professor_lookup:\n",
        "                mapped_names.append(professor_lookup[name_upper])\n",
        "            else:\n",
        "                # Keep original name if not found in lookup but has multiple words\n",
        "                words = name.strip().split()\n",
        "                if len(words) >= 2:\n",
        "                    mapped_names.append(name.strip())\n",
        "        \n",
        "        if mapped_names:\n",
        "            # Remove duplicates and sort\n",
        "            unique_mapped = []\n",
        "            seen = set()\n",
        "            for name in mapped_names:\n",
        "                if name not in seen:\n",
        "                    seen.add(name)\n",
        "                    unique_mapped.append(name)\n",
        "            unique_mapped.sort()\n",
        "            \n",
        "            import json\n",
        "            return json.dumps(unique_mapped)\n",
        "        \n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2ec324",
      "metadata": {},
      "source": [
        "## **3. Database Helper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4dac95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def connect_database():\n",
        "    \"\"\"Connect to PostgreSQL database\"\"\"\n",
        "    load_dotenv()\n",
        "    db_config = {\n",
        "        'host': os.getenv('DB_HOST'),\n",
        "        'database': os.getenv('DB_NAME'),\n",
        "        'user': os.getenv('DB_USER'),\n",
        "        'password': os.getenv('DB_PASSWORD'),\n",
        "        'port': int(os.getenv('DB_PORT', 5432)),\n",
        "        'gssencmode': 'disable'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        connection = psycopg2.connect(**db_config)\n",
        "        print(\"‚úÖ Database connection established\")\n",
        "        return connection\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Database connection failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_or_cache_data(connection, cache_dir):\n",
        "    \"\"\"Load data from cache or database\"\"\"\n",
        "    cache_files = {\n",
        "        'courses': cache_dir / 'courses_cache.pkl',\n",
        "        'classes': cache_dir / 'classes_cache.pkl',\n",
        "        'acad_terms': cache_dir / 'acad_terms_cache.pkl',\n",
        "        'professors': cache_dir / 'professors_cache.pkl'\n",
        "    }\n",
        "    \n",
        "    data_cache = {}\n",
        "    \n",
        "    # Try loading from cache first\n",
        "    if all(f.exists() for f in cache_files.values()):\n",
        "        print(\"‚úÖ Loading from cache...\")\n",
        "        for key, file in cache_files.items():\n",
        "            data_cache[key] = pd.read_pickle(file)\n",
        "    else:\n",
        "        print(\"üì• Downloading from database...\")\n",
        "        queries = {\n",
        "            'courses': \"SELECT * FROM courses\",\n",
        "            'classes': \"SELECT * FROM classes\",\n",
        "            'acad_terms': \"SELECT * FROM acad_term\",\n",
        "            'professors': \"SELECT * FROM professors\"\n",
        "        }\n",
        "        \n",
        "        for key, query in queries.items():\n",
        "            df = pd.read_sql_query(query, connection)\n",
        "            df.to_pickle(cache_files[key])\n",
        "            data_cache[key] = df\n",
        "    \n",
        "    return data_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815cf568",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Preparation Functions\n",
        "def prepare_prediction_data(raw_data_path='script_input/raw_data.xlsx', connection=None, db_cache=None):\n",
        "    \"\"\"Prepare data for prediction from raw_data.xlsx with time-based bidding window and database cache check\"\"\"\n",
        "    from datetime import datetime\n",
        "    import re\n",
        "\n",
        "    # a. Determine Target Bidding Window Name\n",
        "    active_window_name = None\n",
        "    mode = \"Automatic\"\n",
        "    if MANUAL_ROUND and MANUAL_WINDOW is not None:\n",
        "        mode = \"Manual\"\n",
        "        active_window_name = f\"Round {MANUAL_ROUND} Window {MANUAL_WINDOW}\"\n",
        "    else:\n",
        "        now = datetime.now()\n",
        "        # Find the first bidding window in the schedule whose closing time has not yet passed\n",
        "        for schedule_item in BIDDING_SCHEDULES.get(TARGET_AY_TERM, []):\n",
        "            if schedule_item[0] > now:\n",
        "                active_window_name = schedule_item[1]\n",
        "                break\n",
        "\n",
        "    if not active_window_name:\n",
        "        raise ValueError(\"Could not determine an active bidding window. Check BIDDING_SCHEDULES or manual override settings.\")\n",
        "\n",
        "    # b. Load and Filter Data\n",
        "    print(\"\\nüìÇ Loading raw data for filtering...\")\n",
        "    full_standalone_df = pd.read_excel(raw_data_path, sheet_name='standalone')\n",
        "    full_multiple_df = pd.read_excel(raw_data_path, sheet_name='multiple')\n",
        "\n",
        "    # Filter standalone\n",
        "    filtered_standalone_df = full_standalone_df[full_standalone_df['bidding_window'] == active_window_name].copy()\n",
        "\n",
        "    # c. Add a Diagnostic Print Statement\n",
        "    print(f\"‚úÖ Processing {mode} target: '{active_window_name}'. Found {len(filtered_standalone_df)} records.\")\n",
        "    \n",
        "    if filtered_standalone_df.empty:\n",
        "        print(\"‚ö†Ô∏è No records found for the target window. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame(), full_standalone_df, full_multiple_df, db_cache\n",
        "\n",
        "    # Extract keys and filter multiple\n",
        "    active_keys = filtered_standalone_df['record_key'].unique()\n",
        "    filtered_multiple_df = full_multiple_df[full_multiple_df['record_key'].isin(active_keys)].copy()\n",
        "    \n",
        "    # d. Process and Reset Index (using original logic on filtered data)\n",
        "    bidding_data = filtered_standalone_df[filtered_standalone_df['bidding_window'].notna() & filtered_standalone_df['total'].notna()].copy()\n",
        "    bidding_data['before_process_vacancy'] = bidding_data['total'] - bidding_data['current_enrolled']\n",
        "\n",
        "    # Define bidding schedule from the global configuration\n",
        "    bidding_schedule = BIDDING_SCHEDULES.get(TARGET_AY_TERM, [])\n",
        "    if not bidding_schedule:\n",
        "        print(f\"‚ö†Ô∏è Warning: No bidding schedule found for the target term '{TARGET_AY_TERM}'.\")\n",
        "\n",
        "    # This mapping is now just for a sanity check, as we already filtered\n",
        "    def map_bidding_window_to_round_window(bidding_window_str):\n",
        "        if not bidding_window_str or pd.isna(bidding_window_str): return None, None\n",
        "        text = str(bidding_window_str).strip()\n",
        "        match = re.search(r'Round\\s+([\\w\\d]+)\\s+Window\\s+(\\d+)', text, re.IGNORECASE)\n",
        "        if match: return match.group(1), int(match.group(2))\n",
        "        match = re.search(r'Rnd\\s+([\\w\\d]+)\\s+Win\\s+(\\d+)', text, re.IGNORECASE)\n",
        "        if match:\n",
        "            round_val, win_val = match.group(1), int(match.group(2))\n",
        "            if 'Freshmen' in text: return f\"{round_val}F\", win_val\n",
        "            return round_val, win_val\n",
        "        return None, None\n",
        "\n",
        "    bidding_data[['round', 'window']] = bidding_data['bidding_window'].apply(\n",
        "        lambda x: pd.Series(map_bidding_window_to_round_window(x))\n",
        "    )\n",
        "\n",
        "    if db_cache is None: db_cache = {}\n",
        "    if 'bid_prediction' not in db_cache and connection is not None:\n",
        "        try:\n",
        "            db_cache['bid_prediction'] = pd.read_sql_query(\"SELECT * FROM bid_prediction\", connection)\n",
        "        except Exception as e:\n",
        "            db_cache['bid_prediction'] = pd.DataFrame()\n",
        "\n",
        "    # Get instructor information from the filtered multiple sheet\n",
        "    instructor_map = {}\n",
        "    for record_key, group in filtered_multiple_df.groupby('record_key'):\n",
        "        professors = group['professor_name'].dropna().unique()\n",
        "        if len(professors) > 0:\n",
        "            instructor_map[record_key] = professors.tolist()\n",
        "    bidding_data['instructor'] = bidding_data['record_key'].map(lambda x: instructor_map.get(x, []))\n",
        "    \n",
        "    # Get day of week information from the filtered multiple sheet\n",
        "    day_map = {}\n",
        "    for record_key, group in filtered_multiple_df[filtered_multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        days = group['day_of_week'].dropna().unique()\n",
        "        if len(days) > 0: day_map[record_key] = ', '.join(days)\n",
        "    bidding_data['day_of_week'] = bidding_data['record_key'].map(lambda x: day_map.get(x, ''))\n",
        "    \n",
        "    # Get start time from the filtered multiple sheet\n",
        "    time_map = {}\n",
        "    for record_key, group in filtered_multiple_df[filtered_multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        times = group['start_time'].dropna()\n",
        "        if len(times) > 0: time_map[record_key] = times.iloc[0]\n",
        "    bidding_data['start_time'] = bidding_data['record_key'].map(lambda x: time_map.get(x, ''))\n",
        "\n",
        "    # Merging of new data files remains unchanged as it updates the general cache\n",
        "    new_courses_path = Path('script_output/verify/new_courses.csv')\n",
        "    if new_courses_path.exists():\n",
        "        new_courses_df = pd.read_csv(new_courses_path)\n",
        "        if 'courses' in db_cache:\n",
        "            db_cache['courses'] = pd.concat([db_cache['courses'], new_courses_df], ignore_index=True).drop_duplicates(subset=['id', 'code'])\n",
        "        else: db_cache['courses'] = new_courses_df\n",
        "\n",
        "    new_professors_path = Path('script_output/verify/new_professors.csv')\n",
        "    if new_professors_path.exists():\n",
        "        new_professors_df = pd.read_csv(new_professors_path)\n",
        "        if 'professors' in db_cache:\n",
        "            db_cache['professors'] = pd.concat([db_cache['professors'], new_professors_df], ignore_index=True).drop_duplicates(subset=['id'])\n",
        "        else: db_cache['professors'] = new_professors_df\n",
        "    \n",
        "    new_classes_path = Path('script_output/new_classes.csv')\n",
        "    if new_classes_path.exists():\n",
        "        new_classes_df = pd.read_csv(new_classes_path)\n",
        "        if 'classes' in db_cache:\n",
        "            acad_term_ids = bidding_data['acad_term_id'].unique()\n",
        "            db_classes_filtered = db_cache['classes'][db_cache['classes']['acad_term_id'].isin(acad_term_ids)]\n",
        "            combined_classes = pd.concat([db_classes_filtered, new_classes_df], ignore_index=True)\n",
        "            db_cache['classes'] = combined_classes.drop_duplicates(subset=['section', 'course_id', 'acad_term_id'])\n",
        "        else: db_cache['classes'] = new_classes_df\n",
        "\n",
        "    # Critical: Reset index before returning\n",
        "    bidding_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # e. Maintain Original Function Signature\n",
        "    return bidding_data, full_standalone_df, full_multiple_df, db_cache\n",
        "\n",
        "\n",
        "def map_classes_to_predictions(bidding_data, data_cache, connection):\n",
        "    \"\"\"Map predictions to class IDs - checks both database cache and new_classes.csv\"\"\"\n",
        "    courses_df = data_cache['courses']\n",
        "    classes_df = data_cache['classes']\n",
        "    \n",
        "    # Create course code to ID mapping from both sources\n",
        "    course_id_map = dict(zip(courses_df['code'], courses_df['id']))\n",
        "    \n",
        "    # Also check new_courses.csv for courses not in database yet\n",
        "    new_courses_paths = [\n",
        "        Path('script_output/new_courses.csv'),\n",
        "        Path('script_output/verify/new_courses.csv')\n",
        "    ]\n",
        "    \n",
        "    for path in new_courses_paths:\n",
        "        if path.exists():\n",
        "            try:\n",
        "                new_courses_df = pd.read_csv(path)\n",
        "                for _, row in new_courses_df.iterrows():\n",
        "                    if row['code'] not in course_id_map:\n",
        "                        course_id_map[row['code']] = row['id']\n",
        "                print(f\"üìö Added {len(new_courses_df)} courses from {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Could not load {path}: {e}\")\n",
        "    \n",
        "    # Combine classes from DB cache and new_classes.csv\n",
        "    all_classes = []\n",
        "    \n",
        "    # Add DB cache classes\n",
        "    if not classes_df.empty:\n",
        "        all_classes.extend(classes_df.to_dict('records'))\n",
        "    \n",
        "    # Load and add new_classes.csv\n",
        "    new_classes_path = Path('script_output/new_classes.csv')\n",
        "    if new_classes_path.exists():\n",
        "        try:\n",
        "            new_classes_df = pd.read_csv(new_classes_path)\n",
        "            all_classes.extend(new_classes_df.to_dict('records'))\n",
        "            print(f\"üìö Added {len(new_classes_df)} classes from new_classes.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not load new_classes.csv: {e}\")\n",
        "    \n",
        "    # Create lookup for efficient searching\n",
        "    class_lookup = defaultdict(list)\n",
        "    for class_rec in all_classes:\n",
        "        key = (class_rec['course_id'], str(class_rec['section']), class_rec['acad_term_id'])\n",
        "        class_lookup[key].append(class_rec)\n",
        "    \n",
        "    # Map each row to class IDs\n",
        "    class_mappings = []\n",
        "    unmapped_courses = set()\n",
        "    \n",
        "    for idx, row in bidding_data.iterrows():\n",
        "        course_code = row['course_code']\n",
        "        section = str(row['section'])\n",
        "        acad_term_id = row['acad_term_id']\n",
        "        record_key = row.get('record_key', '')\n",
        "        \n",
        "        # Get course ID\n",
        "        course_id = course_id_map.get(course_code)\n",
        "        if not course_id:\n",
        "            unmapped_courses.add(course_code)\n",
        "            continue\n",
        "        \n",
        "        # Look up classes using the combined lookup\n",
        "        lookup_key = (course_id, section, acad_term_id)\n",
        "        matching_classes = class_lookup.get(lookup_key, [])\n",
        "        \n",
        "        if matching_classes:\n",
        "            # Map to all matching classes (handles multi-professor)\n",
        "            for class_row in matching_classes:\n",
        "                mapping = {\n",
        "                    'prediction_idx': idx,\n",
        "                    'class_id': class_row['id'],\n",
        "                    'professor_id': class_row.get('professor_id'),\n",
        "                    'course_code': course_code,\n",
        "                    'section': section,\n",
        "                    'acad_term_id': acad_term_id,\n",
        "                    'record_key': record_key,\n",
        "                    'source': 'combined'\n",
        "                }\n",
        "                class_mappings.append(mapping)\n",
        "        else:\n",
        "            # Create a placeholder mapping\n",
        "            mapping = {\n",
        "                'prediction_idx': idx,\n",
        "                'class_id': f\"PENDING_{course_code}_{section}_{acad_term_id}\",\n",
        "                'professor_id': None,\n",
        "                'course_code': course_code,\n",
        "                'section': section,\n",
        "                'acad_term_id': acad_term_id,\n",
        "                'record_key': record_key,\n",
        "                'source': 'not_found'\n",
        "            }\n",
        "            class_mappings.append(mapping)\n",
        "    \n",
        "    # Create summary\n",
        "    mappings_df = pd.DataFrame(class_mappings)\n",
        "    \n",
        "    if not mappings_df.empty:\n",
        "        print(f\"\\nüìä Mapping Summary:\")\n",
        "        print(f\"   Total mappings: {len(mappings_df)}\")\n",
        "        print(f\"   Unique predictions mapped: {mappings_df['prediction_idx'].nunique()}\")\n",
        "        print(f\"   Classes per prediction: {len(mappings_df) / mappings_df['prediction_idx'].nunique():.2f}\")\n",
        "        \n",
        "        if unmapped_courses:\n",
        "            print(f\"\\n‚ö†Ô∏è Courses without IDs: {len(unmapped_courses)}\")\n",
        "            print(f\"   Sample: {list(unmapped_courses)[:5]}\")\n",
        "    \n",
        "    return mappings_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b904ae42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Load and Prepare Data\n",
        "print(\"=\"*60)\n",
        "print(\"DATA LOADING AND PREPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Connect to database\n",
        "connection = connect_database()\n",
        "if not connection:\n",
        "    raise Exception(\"Failed to connect to database\")\n",
        "\n",
        "# Load cache data\n",
        "data_cache = load_or_cache_data(connection, cache_dir)\n",
        "\n",
        "# Prepare prediction data\n",
        "bidding_data, standalone_df, multiple_df, data_cache = prepare_prediction_data(connection=connection, db_cache=data_cache)\n",
        "print(f\"üìä Prepared {len(bidding_data)} records for prediction\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample bidding data:\")\n",
        "print(bidding_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb7a7d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Transform Data (FIXED)\n",
        "def transform_bidding_data(bidding_data, output_dir):\n",
        "    print(\"=\"*60)\n",
        "    print(\"FEATURE TRANSFORMATION\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Initialize transformer\n",
        "    transformer = SMUBiddingTransformer()\n",
        "    transformer.fit(bidding_data)\n",
        "    \n",
        "    # Transform data\n",
        "    X_transformed = transformer.transform(bidding_data)\n",
        "    print(f\"‚úÖ Transformed data shape: {X_transformed.shape}\")\n",
        "    print(f\"üìã Features: {list(X_transformed.columns)[:10]}...\")\n",
        "    \n",
        "    # Ensure all categorical features have __NA__ for null values\n",
        "    categorical_features = transformer.get_categorical_features()\n",
        "    for col in categorical_features:\n",
        "        if col in X_transformed.columns:\n",
        "            # Convert to string type first\n",
        "            X_transformed[col] = X_transformed[col].astype(str)\n",
        "            # Replace 'nan' strings with a consistent placeholder\n",
        "            X_transformed[col] = X_transformed[col].replace('nan', '__NA__')\n",
        "            # Also handle any remaining NaN values\n",
        "            X_transformed[col] = X_transformed[col].fillna('__NA__')\n",
        "            # Handle empty strings\n",
        "            X_transformed[col] = X_transformed[col].replace('', '__NA__')\n",
        "    \n",
        "    # Save transformed data to CSV\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    transformed_output_path = output_dir / f'transformed_features_{timestamp}.csv'\n",
        "    \n",
        "    # Add the original identifiers to help with tracking\n",
        "    X_transformed_with_ids = X_transformed.copy()\n",
        "    X_transformed_with_ids['course_code'] = bidding_data['course_code'].values\n",
        "    X_transformed_with_ids['section'] = bidding_data['section'].values\n",
        "    X_transformed_with_ids['acad_term_id'] = bidding_data['acad_term_id'].values\n",
        "    X_transformed_with_ids['record_key'] = bidding_data['record_key'].values\n",
        "    \n",
        "    # Reorder columns to put identifiers first\n",
        "    id_cols = ['record_key', 'course_code', 'section', 'acad_term_id']\n",
        "    feature_cols = [col for col in X_transformed.columns if col not in id_cols]\n",
        "    X_transformed_with_ids = X_transformed_with_ids[id_cols + feature_cols]\n",
        "    \n",
        "    # Save to CSV\n",
        "    X_transformed_with_ids.to_csv(transformed_output_path, index=False)\n",
        "    print(f\"\\nüíæ Transformed features saved to: {transformed_output_path}\")\n",
        "    print(f\"   Total columns: {len(X_transformed_with_ids.columns)}\")\n",
        "    print(f\"   - Identifier columns: {len(id_cols)}\")\n",
        "    print(f\"   - Feature columns: {len(feature_cols)}\")\n",
        "    print(f\"   - Categorical features: {len(transformer.get_categorical_features())}\")\n",
        "    print(f\"   - Numeric features: {len(transformer.get_numeric_features())}\")\n",
        "    \n",
        "    # Also save a metadata file with feature information\n",
        "    metadata = {\n",
        "        'timestamp': timestamp,\n",
        "        'total_rows': len(X_transformed),\n",
        "        'total_features': len(feature_cols),\n",
        "        'categorical_features': transformer.get_categorical_features(),\n",
        "        'numeric_features': transformer.get_numeric_features(),\n",
        "        'identifier_columns': id_cols,\n",
        "        'transformation_date': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    metadata_path = output_dir / f'transformation_metadata_{timestamp}.json'\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"üìã Metadata saved to: {metadata_path}\")\n",
        "    \n",
        "    # Display sample of transformed data\n",
        "    print(\"\\nüîç Sample of transformed data:\")\n",
        "    print(X_transformed_with_ids.head())\n",
        "    \n",
        "    return X_transformed, transformer\n",
        "\n",
        "# Run Transformation\n",
        "X_transformed, transformer = transform_bidding_data(bidding_data, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4bf5880",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Map to Classes\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS MAPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Map to classes\n",
        "# Map to classes\n",
        "class_mappings = map_classes_to_predictions(bidding_data, data_cache, connection)\n",
        "print(f\"üîó Mapped to {len(class_mappings)} class instances\")\n",
        "\n",
        "# Display sample mappings\n",
        "print(\"\\nSample class mappings:\")\n",
        "print(class_mappings.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1fa3cc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Load Models and Generate Predictions (FIXED)\n",
        "def load_models_and_predict(X_transformed, bidding_data):\n",
        "    print(\"=\"*60)\n",
        "    print(\"MODEL PREDICTIONS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Load models\n",
        "    models = {\n",
        "        'classification': CatBoostClassifier(),\n",
        "        'median': CatBoostRegressor(),\n",
        "        'min': CatBoostRegressor()\n",
        "    }\n",
        "    \n",
        "    model_paths = {\n",
        "        'classification': 'script_output/models/classification/production_classification_model.cbm',\n",
        "        'median': 'script_output/models/regression_median/production_regression_median_model.cbm',\n",
        "        'min': 'script_output/models/regression_min/production_regression_min_model.cbm'\n",
        "    }\n",
        "    \n",
        "    # Load each model\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            model.load_model(model_paths[name])\n",
        "            print(f\"‚úÖ Loaded {name} model\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {name} model: {e}\")\n",
        "            return None\n",
        "    \n",
        "    # Verify data format matches model expectations\n",
        "    expected_features = [\n",
        "        'subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy',\n",
        "        'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor',\n",
        "        'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun'\n",
        "    ]\n",
        "    \n",
        "    # Create prediction dataset with only the features expected by models\n",
        "    prediction_data = X_transformed.copy()\n",
        "    \n",
        "    # Ensure all expected features are present\n",
        "    missing_features = set(expected_features) - set(prediction_data.columns)\n",
        "    if missing_features:\n",
        "        print(f\"‚ö†Ô∏è Warning: Missing features: {missing_features}\")\n",
        "    \n",
        "    # Select only the features that exist and are expected\n",
        "    available_features = [col for col in expected_features if col in prediction_data.columns]\n",
        "    prediction_data = prediction_data[available_features]\n",
        "    \n",
        "    print(f\"üìä Using {len(available_features)} features for prediction\")\n",
        "    print(f\"üîÆ Generating predictions for {len(prediction_data)} records...\")\n",
        "    \n",
        "    try:\n",
        "        # Classification predictions\n",
        "        clf_pred = models['classification'].predict(prediction_data)\n",
        "        clf_proba = models['classification'].predict_proba(prediction_data)\n",
        "        \n",
        "        # Regression predictions  \n",
        "        median_pred = models['median'].predict(prediction_data)\n",
        "        min_pred = models['min'].predict(prediction_data)\n",
        "        \n",
        "        print(f\"‚úÖ Generated predictions for {len(prediction_data)} records\")\n",
        "        \n",
        "        # Create results dataframe\n",
        "        results = {\n",
        "            'classification_prediction': clf_pred,\n",
        "            'classification_probabilities': clf_proba,\n",
        "            'median_prediction': median_pred,\n",
        "            'min_prediction': min_pred\n",
        "        }\n",
        "        \n",
        "        return results, models\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during prediction: {e}\")\n",
        "        print(f\"   Data shape: {prediction_data.shape}\")\n",
        "        print(f\"   Data types: {prediction_data.dtypes}\")\n",
        "        return None, models\n",
        "    \n",
        "# Load Models and Generate Predictions\n",
        "prediction_results, loaded_models = load_models_and_predict(X_transformed, bidding_data)\n",
        "if prediction_results:\n",
        "    clf_pred = prediction_results['classification_prediction']\n",
        "    clf_proba = prediction_results['classification_probabilities'] \n",
        "    median_pred = prediction_results['median_prediction']\n",
        "    min_pred = prediction_results['min_prediction']\n",
        "    \n",
        "    print(f\"\\nüìà Prediction Summary:\")\n",
        "    print(f\"   Classification predictions: {len(clf_pred)}\")\n",
        "    print(f\"   Median predictions range: {min(median_pred):.2f} - {max(median_pred):.2f}\")\n",
        "    print(f\"   Min predictions range: {min(min_pred):.2f} - {max(min_pred):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c3fd5b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Calculate Uncertainties and Confidence\n",
        "print(\"=\"*60)\n",
        "print(\"UNCERTAINTY QUANTIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def calculate_entropy_confidence(probabilities):\n",
        "    \"\"\"Calculate entropy-based confidence scores\"\"\"\n",
        "    epsilon = 1e-10\n",
        "    entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
        "    max_entropy = -np.log(1/probabilities.shape[1])\n",
        "    confidence_score = 1 - (entropy / max_entropy)\n",
        "    \n",
        "    confidence_levels = np.where(\n",
        "        confidence_score >= 0.9, 'Very High',\n",
        "        np.where(confidence_score >= 0.7, 'High',\n",
        "                np.where(confidence_score >= 0.5, 'Medium',\n",
        "                        np.where(confidence_score >= 0.3, 'Low', 'Very Low')))\n",
        "    )\n",
        "    return confidence_score, confidence_levels\n",
        "\n",
        "# Calculate classification confidence\n",
        "confidence_scores, confidence_levels = calculate_entropy_confidence(clf_proba)\n",
        "\n",
        "# Calculate regression uncertainties using virtual ensembles\n",
        "uncertainties = {}\n",
        "for model_name in ['median', 'min']:\n",
        "    model = loaded_models[model_name]\n",
        "    n_trees = model.tree_count_\n",
        "    n_subsets = 10\n",
        "    trees_per_subset = max(1, n_trees // n_subsets)\n",
        "    \n",
        "    subset_predictions = []\n",
        "    for i in range(n_subsets):\n",
        "        tree_start = i * trees_per_subset\n",
        "        tree_end = min((i + 1) * trees_per_subset, n_trees)\n",
        "        if tree_start < n_trees:\n",
        "            partial_pred = model.predict(X_transformed, \n",
        "                                       ntree_start=tree_start, \n",
        "                                       ntree_end=tree_end)\n",
        "            subset_predictions.append(partial_pred)\n",
        "    \n",
        "    uncertainties[model_name] = np.std(subset_predictions, axis=0)\n",
        "\n",
        "print(\"‚úÖ Calculated prediction uncertainties\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23065c79",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Apply Safety Factors\n",
        "print(\"=\"*60)\n",
        "print(\"SAFETY FACTOR APPLICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load safety factor tables\n",
        "median_sf_df = pd.read_csv('script_output/models/regression_median/median_bid_safety_factor_analysis.csv')\n",
        "min_sf_df = pd.read_csv('script_output/models/regression_min/min_bid_safety_factor_analysis.csv')\n",
        "\n",
        "# Find optimal safety factors (example: SF with TPR > 0.9)\n",
        "median_optimal_idx = median_sf_df[median_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "min_optimal_idx = min_sf_df[min_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "\n",
        "median_optimal_sf = median_sf_df.iloc[median_optimal_idx]['safety_factor']\n",
        "min_optimal_sf = min_sf_df.iloc[min_optimal_idx]['safety_factor']\n",
        "\n",
        "print(f\"üìä Optimal safety factors:\")\n",
        "print(f\"   Median: {median_optimal_sf:.2f}\")\n",
        "print(f\"   Min: {min_optimal_sf:.2f}\")\n",
        "\n",
        "# Apply safety factors\n",
        "median_recommended = median_pred * (1 + median_optimal_sf)\n",
        "min_recommended = min_pred * (1 + min_optimal_sf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd0ece3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Prepare Data and Create Output DataFrames (RESTRUCTURED)\n",
        "print(\"=\"*60)\n",
        "print(\"PREPARING DATA & CREATING OUTPUT TABLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- 1. Load and Combine Bid Window Data (ONCE) ---\n",
        "print(\"üîÑ Loading and combining all bid window data...\")\n",
        "combined_bid_windows_df = pd.DataFrame()\n",
        "\n",
        "# Load from database cache first\n",
        "# Assuming 'data_cache' is a dictionary holding cached dataframes\n",
        "if 'bid_windows' in data_cache and not data_cache['bid_windows'].empty:\n",
        "    combined_bid_windows_df = data_cache['bid_windows'].copy()\n",
        "    print(f\"‚úÖ Loaded {len(combined_bid_windows_df)} bid windows from database cache.\")\n",
        "else:\n",
        "    # As a fallback, try loading directly from the .pkl file if it exists\n",
        "    bid_window_cache_path = Path('db_cache/bid_window_cache.pkl')\n",
        "    if bid_window_cache_path.exists():\n",
        "        try:\n",
        "            combined_bid_windows_df = pd.read_pickle(bid_window_cache_path)\n",
        "            print(f\"‚úÖ Loaded {len(combined_bid_windows_df)} bid windows directly from cache file.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not read bid_window_cache.pkl: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No existing bid windows found in cache.\")\n",
        "\n",
        "# Load new bid windows from the CSV file generated in the previous step\n",
        "new_bid_window_path = Path('script_output/new_bid_window.csv')\n",
        "if new_bid_window_path.exists():\n",
        "    try:\n",
        "        new_bid_windows_df = pd.read_csv(new_bid_window_path)\n",
        "        if not new_bid_windows_df.empty:\n",
        "            print(f\"üìö Found {len(new_bid_windows_df)} new bid windows in '{new_bid_window_path}'.\")\n",
        "            # Combine the two dataframes\n",
        "            combined_df = pd.concat([combined_bid_windows_df, new_bid_windows_df], ignore_index=True)\n",
        "            # Remove duplicates, keeping the newest ones (from the CSV)\n",
        "            if not combined_df.empty:\n",
        "                combined_df.drop_duplicates(subset=['acad_term_id', 'round', 'window'], keep='last', inplace=True)\n",
        "            combined_bid_windows_df = combined_df\n",
        "            print(f\"‚úÖ Combined bid windows. Total unique windows: {len(combined_bid_windows_df)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not load or process new_bid_window.csv: {e}\")\n",
        "\n",
        "# Final check to ensure we have data to proceed\n",
        "if combined_bid_windows_df.empty:\n",
        "    print(\"‚ùå CRITICAL: No bid window data available from cache or CSV. Cannot proceed with predictions.\")\n",
        "else:\n",
        "    print(\"Columns in the final combined bid_windows_df:\", combined_bid_windows_df.columns)\n",
        "    \n",
        "# --- 2. Simplified Lookup Function ---\n",
        "def get_bid_window_id(row, all_bid_windows_df):\n",
        "    \"\"\"\n",
        "    Get bidWindowId from a pre-combined DataFrame of all bid windows.\n",
        "    This function is now much simpler and only performs the lookup.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure the combined DataFrame is not empty and has the required columns\n",
        "        if all_bid_windows_df.empty or not all(c in all_bid_windows_df.columns for c in ['acad_term_id', 'round', 'window', 'id']):\n",
        "            return f\"PENDING_{row.get('acad_term_id', 'NA')}_{row.get('round', 'NA')}_{row.get('window', 'NA')}\"\n",
        "\n",
        "        # Ensure input row has the necessary columns\n",
        "        term_id = str(row['acad_term_id'])\n",
        "        round_val = str(row['round'])\n",
        "        window_val = int(row['window'])\n",
        "\n",
        "        # Perform the lookup on the pre-combined DataFrame\n",
        "        matching_windows = all_bid_windows_df[\n",
        "            (all_bid_windows_df['acad_term_id'].astype(str) == term_id) &\n",
        "            (all_bid_windows_df['round'].astype(str) == round_val) &\n",
        "            (all_bid_windows_df['window'].astype(int) == window_val)\n",
        "        ]\n",
        "\n",
        "        if not matching_windows.empty:\n",
        "            return matching_windows.iloc[0]['id']\n",
        "        \n",
        "        # If no match, return a pending ID for later review\n",
        "        return f\"PENDING_{term_id}_{round_val}_{window_val}\"\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"‚ùå KeyError in get_bid_window_id: Missing column {e} in the input row. Row data: {row}\")\n",
        "        return \"ERROR_MISSING_DATA\"\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An unexpected error occurred in get_bid_window_id: {e}\")\n",
        "        return \"ERROR_UNEXPECTED\"\n",
        "\n",
        "# --- 3. Create BidPrediction entries using the simplified lookup ---\n",
        "bid_predictions = []\n",
        "missing_bid_windows = set()\n",
        "\n",
        "print(f\"\\nProcessing {len(bidding_data)} prediction rows...\")\n",
        "print(f\"Total class mappings: {len(class_mappings)}\")\n",
        "\n",
        "for idx in range(len(bidding_data)):\n",
        "    bidding_row = bidding_data.iloc[idx]\n",
        "    # Call the simplified function with the pre-loaded DataFrame\n",
        "    bid_window_id = get_bid_window_id(bidding_row, combined_bid_windows_df)\n",
        "    \n",
        "    if isinstance(bid_window_id, str) and bid_window_id.startswith(\"PENDING_\"):\n",
        "        missing_key = (bidding_row['acad_term_id'], bidding_row['round'], bidding_row['window'])\n",
        "        missing_bid_windows.add(missing_key)\n",
        "    \n",
        "    matching_mappings = class_mappings[class_mappings['prediction_idx'] == idx]\n",
        "    \n",
        "    if matching_mappings.empty:\n",
        "        # This warning is expected if a class couldn't be mapped earlier\n",
        "        continue\n",
        "    \n",
        "    for _, mapping in matching_mappings.iterrows():\n",
        "        if mapping['source'] == 'not_found' and str(mapping['class_id']).startswith('PENDING_'):\n",
        "            continue\n",
        "            \n",
        "        bid_prediction = {\n",
        "            'class_id': mapping['class_id'],\n",
        "            'bid_window_id': bid_window_id,\n",
        "            'model_version': 'v4.0',\n",
        "            'clf_has_bids_prob': float(clf_proba[idx, 1]),\n",
        "            'clf_confidence_score': float(confidence_scores[idx]),\n",
        "            'median_predicted': float(median_pred[idx]),\n",
        "            'median_uncertainty': float(uncertainties['median'][idx]),\n",
        "            'min_predicted': float(min_pred[idx]),\n",
        "            'min_uncertainty': float(uncertainties['min'][idx])\n",
        "        }\n",
        "        bid_predictions.append(bid_prediction)\n",
        "\n",
        "# --- 4. Final Reporting and DataFrame Creation ---\n",
        "if missing_bid_windows:\n",
        "    print(f\"\\n‚ö†Ô∏è Warning: {len(missing_bid_windows)} unique bid windows not found. These have been marked as 'PENDING'.\")\n",
        "    for term_id, round_val, window_val in sorted(missing_bid_windows):\n",
        "        print(f\"   - Term: {term_id}, Round: {round_val}, Window: {window_val}\")\n",
        "\n",
        "bid_predictions_df = pd.DataFrame(bid_predictions)\n",
        "print(f\"\\n‚úÖ Created {len(bid_predictions_df)} bid predictions\")\n",
        "print(f\"   - From {len(bidding_data)} unique predictions\")\n",
        "print(f\"   - Mapped to {len(class_mappings)} total classes (including multi-professor duplicates)\")\n",
        "\n",
        "# Create dataset + predictions CSV (with simplified columns)\n",
        "dataset_predictions = []\n",
        "for idx in range(len(X_transformed)):\n",
        "    row_features = X_transformed.iloc[idx].to_dict()\n",
        "    bidding_row = bidding_data.iloc[idx]\n",
        "    row_features['course_code'] = bidding_row['course_code']\n",
        "    row_features['section'] = bidding_row['section']\n",
        "    row_features['acad_term_id'] = bidding_row['acad_term_id']\n",
        "    \n",
        "    pred_row = {\n",
        "        **row_features,\n",
        "        'clf_has_bids_prob': float(clf_proba[idx, 1]),\n",
        "        'clf_confidence_score': float(confidence_scores[idx]),\n",
        "        'median_predicted': float(median_pred[idx]),\n",
        "        'median_uncertainty': float(uncertainties['median'][idx]),\n",
        "        'min_predicted': float(min_pred[idx]),\n",
        "        'min_uncertainty': float(uncertainties['min'][idx])\n",
        "    }\n",
        "    dataset_predictions.append(pred_row)\n",
        "\n",
        "dataset_predictions_df = pd.DataFrame(dataset_predictions)\n",
        "print(f\"‚úÖ Created {len(dataset_predictions_df)} dataset + prediction rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "258dcc86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Create Safety Factor Table (UPDATED - Distribution Analysis Approach)\n",
        "print(\"=\"*60)\n",
        "print(\"SAFETY FACTOR TABLE CREATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def fit_t_distribution(errors):\n",
        "    \"\"\"Fit a t-distribution to the error data and return parameters\"\"\"\n",
        "    # Remove any infinite or NaN values\n",
        "    clean_errors = errors[np.isfinite(errors)]\n",
        "    \n",
        "    # Fit t-distribution using maximum likelihood estimation\n",
        "    from scipy import stats\n",
        "    params = stats.t.fit(clean_errors)\n",
        "    df, loc, scale = params\n",
        "    \n",
        "    return df, loc, scale, params\n",
        "\n",
        "def calculate_percentile_multipliers(errors, df, loc, scale, prediction_type):\n",
        "    \"\"\"Calculate uncertainty multipliers for every percentile (1-99)\"\"\"\n",
        "    from scipy import stats\n",
        "    \n",
        "    print(f\"Calculating percentile multipliers for {prediction_type}...\")\n",
        "    \n",
        "    # Clean errors\n",
        "    clean_errors = errors[np.isfinite(errors)]\n",
        "    \n",
        "    # Calculate percentiles from 1% to 99%\n",
        "    percentiles = range(1, 100)\n",
        "    multiplier_results = []\n",
        "    \n",
        "    for percentile in percentiles:\n",
        "        # Calculate empirical percentile from actual error data\n",
        "        empirical_value = np.percentile(clean_errors, percentile)\n",
        "        \n",
        "        # Calculate theoretical t-distribution percentile\n",
        "        theoretical_percentile = percentile / 100.0\n",
        "        theoretical_value = stats.t.ppf(theoretical_percentile, df, loc, scale)\n",
        "        \n",
        "        # Calculate multipliers (how many standard deviations from mean)\n",
        "        if scale > 0:\n",
        "            empirical_multiplier = (empirical_value - loc) / scale\n",
        "            theoretical_multiplier = (theoretical_value - loc) / scale\n",
        "        else:\n",
        "            empirical_multiplier = 0\n",
        "            theoretical_multiplier = 0\n",
        "        \n",
        "        multiplier_results.append({\n",
        "            'prediction_type': prediction_type,\n",
        "            'beats_percentage': percentile,\n",
        "            'empirical_multiplier': empirical_multiplier,\n",
        "            'theoretical_multiplier': theoretical_multiplier,\n",
        "            'empirical_error_value': empirical_value,\n",
        "            'theoretical_error_value': theoretical_value\n",
        "        })\n",
        "    \n",
        "    return multiplier_results\n",
        "\n",
        "# Get unique acad_term_id from bidding data\n",
        "acad_term_id = bidding_data['acad_term_id'].iloc[0]  # Assuming all predictions are for same term\n",
        "\n",
        "print(f\"Creating safety factor table for academic term: {acad_term_id}\")\n",
        "\n",
        "# Load validation results to get error distributions\n",
        "print(\"Loading validation results for error analysis...\")\n",
        "\n",
        "try:\n",
        "    # Load median model results\n",
        "    median_dir = Path('script_output/models/regression_median')\n",
        "    min_dir = Path('script_output/models/regression_min')\n",
        "    \n",
        "    median_results = pd.read_csv(median_dir / \"regression_median_validation_results.csv\")\n",
        "    min_results = pd.read_csv(min_dir / \"regression_min_validation_results.csv\")\n",
        "    \n",
        "    print(f\"‚úÖ Loaded validation results: {len(median_results)} median, {len(min_results)} min samples\")\n",
        "    \n",
        "    # Extract residuals (errors = predicted - actual)\n",
        "    median_errors = median_results['residuals'].values\n",
        "    min_errors = min_results['residuals'].values\n",
        "    \n",
        "    print(f\"Median errors range: [{median_errors.min():.3f}, {median_errors.max():.3f}]\")\n",
        "    print(f\"Min errors range: [{min_errors.min():.3f}, {min_errors.max():.3f}]\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not load validation results: {e}\")\n",
        "    print(\"Creating placeholder safety factor table...\")\n",
        "    \n",
        "    # Create placeholder data if validation results not available\n",
        "    median_errors = np.random.normal(0, 10, 1000)  # Placeholder\n",
        "    min_errors = np.random.normal(0, 8, 1000)      # Placeholder\n",
        "\n",
        "# Fit t-distributions for both models\n",
        "print(\"\\nFitting t-distributions...\")\n",
        "\n",
        "# Median model distribution\n",
        "median_df, median_loc, median_scale, median_params = fit_t_distribution(median_errors)\n",
        "print(f\"Median model t-distribution: df={median_df:.4f}, loc={median_loc:.4f}, scale={median_scale:.4f}\")\n",
        "\n",
        "# Min model distribution  \n",
        "min_df, min_loc, min_scale, min_params = fit_t_distribution(min_errors)\n",
        "print(f\"Min model t-distribution: df={min_df:.4f}, loc={min_loc:.4f}, scale={min_scale:.4f}\")\n",
        "\n",
        "# Calculate multipliers for both models\n",
        "print(\"\\nCalculating uncertainty multipliers...\")\n",
        "\n",
        "median_multipliers = calculate_percentile_multipliers(\n",
        "    median_errors, median_df, median_loc, median_scale, \"median\"\n",
        ")\n",
        "\n",
        "min_multipliers = calculate_percentile_multipliers(\n",
        "    min_errors, min_df, min_loc, min_scale, \"min\"\n",
        ")\n",
        "\n",
        "# Combine all multipliers\n",
        "all_multipliers = median_multipliers + min_multipliers\n",
        "\n",
        "# Create safety factor table entries\n",
        "safety_factor_entries = []\n",
        "\n",
        "for multiplier_data in all_multipliers:\n",
        "    # Add empirical multiplier entry\n",
        "    empirical_entry = {\n",
        "        'acad_term_id': acad_term_id,\n",
        "        'prediction_type': multiplier_data['prediction_type'],\n",
        "        'beats_percentage': multiplier_data['beats_percentage'],\n",
        "        'multiplier': float(multiplier_data['empirical_multiplier']),\n",
        "        'multiplier_type': 'empirical'\n",
        "    }\n",
        "    safety_factor_entries.append(empirical_entry)\n",
        "    \n",
        "    # Add theoretical multiplier entry\n",
        "    theoretical_entry = {\n",
        "        'acad_term_id': acad_term_id,\n",
        "        'prediction_type': multiplier_data['prediction_type'],\n",
        "        'beats_percentage': multiplier_data['beats_percentage'],\n",
        "        'multiplier': float(multiplier_data['theoretical_multiplier']),\n",
        "        'multiplier_type': 'theoretical'\n",
        "    }\n",
        "    safety_factor_entries.append(theoretical_entry)\n",
        "\n",
        "safety_factor_df = pd.DataFrame(safety_factor_entries)\n",
        "\n",
        "print(f\"‚úÖ Created {len(safety_factor_df)} safety factor entries\")\n",
        "print(f\"   - {len(median_multipliers)} percentiles √ó 2 multiplier types √ó 2 prediction types\")\n",
        "print(f\"   - Academic term: {acad_term_id}\")\n",
        "\n",
        "# Display sample of key percentiles\n",
        "print(f\"\\nüìä Sample of key percentiles:\")\n",
        "key_percentiles = [80, 85, 90, 95, 99]\n",
        "\n",
        "for pred_type in ['median', 'min']:\n",
        "    print(f\"\\n{pred_type.title()} model multipliers:\")\n",
        "    for percentile in key_percentiles:\n",
        "        empirical_row = safety_factor_df[\n",
        "            (safety_factor_df['prediction_type'] == pred_type) & \n",
        "            (safety_factor_df['beats_percentage'] == percentile) &\n",
        "            (safety_factor_df['multiplier_type'] == 'empirical')\n",
        "        ]\n",
        "        theoretical_row = safety_factor_df[\n",
        "            (safety_factor_df['prediction_type'] == pred_type) & \n",
        "            (safety_factor_df['beats_percentage'] == percentile) &\n",
        "            (safety_factor_df['multiplier_type'] == 'theoretical')\n",
        "        ]\n",
        "        \n",
        "        if not empirical_row.empty and not theoretical_row.empty:\n",
        "            emp_mult = empirical_row['multiplier'].iloc[0]\n",
        "            theo_mult = theoretical_row['multiplier'].iloc[0]\n",
        "            print(f\"  {percentile:2d}%: empirical={emp_mult:6.3f}, theoretical={theo_mult:6.3f}\")\n",
        "\n",
        "print(f\"\\nüí° Usage example (90% confidence):\")\n",
        "median_90_emp = safety_factor_df[\n",
        "    (safety_factor_df['prediction_type'] == 'median') & \n",
        "    (safety_factor_df['beats_percentage'] == 90) &\n",
        "    (safety_factor_df['multiplier_type'] == 'empirical')\n",
        "]['multiplier'].iloc[0]\n",
        "\n",
        "min_90_emp = safety_factor_df[\n",
        "    (safety_factor_df['prediction_type'] == 'min') & \n",
        "    (safety_factor_df['beats_percentage'] == 90) &\n",
        "    (safety_factor_df['multiplier_type'] == 'empirical')\n",
        "]['multiplier'].iloc[0]\n",
        "\n",
        "print(f\"   Median bid = median_predicted + median_uncertainty √ó {median_90_emp:.3f}\")\n",
        "print(f\"   Min bid    = min_predicted + min_uncertainty √ó {min_90_emp:.3f}\")\n",
        "print(f\"   Example: 50 + 10√ó{median_90_emp:.3f} = {50 + 10*median_90_emp:.1f} (instead of 50√ó1.7 = 85)\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nüìà Distribution summary:\")\n",
        "print(f\"   Median model: df={median_df:.2f}, mean_error={median_loc:.2f}, scale={median_scale:.2f}\")\n",
        "print(f\"   Min model:    df={min_df:.2f}, mean_error={min_loc:.2f}, scale={min_scale:.2f}\")\n",
        "\n",
        "if median_df < 3:\n",
        "    print(f\"   ‚ö†Ô∏è Median model has very heavy tails (df < 3) - consider higher safety margins\")\n",
        "if min_df < 3:\n",
        "    print(f\"   ‚ö†Ô∏è Min model has very heavy tails (df < 3) - consider higher safety margins\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f598032",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Save Results (UPDATED)\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save all outputs with updated naming\n",
        "bid_predictions_df.to_csv(output_dir / f'bid_predictions_{timestamp}.csv', index=False)\n",
        "dataset_predictions_df.to_csv(output_dir / f'dataset_predictions_{timestamp}.csv', index=False)\n",
        "safety_factor_df.to_csv(output_dir / f'safety_factor_table_{timestamp}.csv', index=False)\n",
        "\n",
        "# Also save transformed features (already saved in cell 6, but ensure latest version)\n",
        "transformed_output_path = output_dir / f'transformed_features_{timestamp}.csv'\n",
        "\n",
        "# Create summary report with updated metrics\n",
        "summary = {\n",
        "    'timestamp': timestamp,\n",
        "    'total_predictions': len(dataset_predictions_df),\n",
        "    'total_bid_predictions': len(bid_predictions_df),  # Updated naming\n",
        "    'unique_classes': bid_predictions_df['class_id'].nunique() if len(bid_predictions_df) > 0 else 0,\n",
        "    'unique_courses': bidding_data['course_code'].nunique(),\n",
        "    'unique_terms': bidding_data['acad_term_id'].nunique(),\n",
        "    'predictions_with_bids': int(clf_pred.sum()),\n",
        "    'predictions_without_bids': int(len(clf_pred) - clf_pred.sum()),\n",
        "    'median_bid_range': {\n",
        "        'min': float(median_pred.min()),\n",
        "        'max': float(median_pred.max()),\n",
        "        'mean': float(median_pred.mean())\n",
        "    },\n",
        "    'min_bid_range': {\n",
        "        'min': float(min_pred.min()),\n",
        "        'max': float(min_pred.max()),\n",
        "        'mean': float(min_pred.mean())\n",
        "    },\n",
        "    'confidence_distribution': {\n",
        "        'very_low': int(sum(confidence_levels == 'Very Low')),\n",
        "        'low': int(sum(confidence_levels == 'Low')),\n",
        "        'medium': int(sum(confidence_levels == 'Medium')),\n",
        "        'high': int(sum(confidence_levels == 'High')),\n",
        "        'very_high': int(sum(confidence_levels == 'Very High'))\n",
        "    },\n",
        "    'safety_factors': {\n",
        "        'total_entries': len(safety_factor_df),\n",
        "        'percentiles_covered': len(safety_factor_df['beats_percentage'].unique()),\n",
        "        'multiplier_types': list(safety_factor_df['multiplier_type'].unique()),\n",
        "        'prediction_types': list(safety_factor_df['prediction_type'].unique())\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(output_dir / f'prediction_summary_{timestamp}.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Batch prediction completed!\")\n",
        "print(f\"üìÅ Results saved to {output_dir}\")\n",
        "print(f\"   - Bid predictions: {len(bid_predictions_df)} records\")  # Updated naming\n",
        "print(f\"   - Dataset predictions: {len(dataset_predictions_df)} records\")\n",
        "print(f\"   - Safety factors: {len(safety_factor_df)} entries\")\n",
        "print(f\"   - Transformed features: saved to {transformed_output_path}\")\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"   - Unique classes with predictions: {summary['unique_classes']}\")\n",
        "print(f\"   - Courses with bids: {summary['predictions_with_bids']}\")\n",
        "print(f\"   - Courses without bids: {summary['predictions_without_bids']}\")\n",
        "print(f\"   - Median bid range: {summary['median_bid_range']['min']:.0f} - {summary['median_bid_range']['max']:.0f}\")\n",
        "print(f\"   - Confidence distribution:\")\n",
        "for level, count in summary['confidence_distribution'].items():\n",
        "    print(f\"     {level}: {count}\")\n",
        "\n",
        "# Close database connection\n",
        "if connection:\n",
        "    connection.close()\n",
        "    print(\"\\nüîí Database connection closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bidlysmu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
