{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1cfa91a",
      "metadata": {},
      "source": [
        "# **SMU Course Bidding Prediction Using CatBoost V4**\n",
        "\n",
        "<div style=\"background-color:#DFFFD6; padding:12px; border-radius:5px; border: 1px solid #228B22;\">\n",
        "   <h2 style=\"color:#006400;\">✅ Looking to Implement This? ✅</h2>\n",
        "   <p>🚀 **Get started quickly by using** <strong><a href=\"example_prediction.ipynb\">example_prediction.ipynb</a></strong>.</p> \n",
        "   <ul> \n",
        "      <li>📌 **Three pre-trained CatBoost models (`.cbm`) available for instant predictions.**</li>\n",
        "      <li>🔧 Includes **step-by-step instructions** for making predictions with uncertainty quantification.</li>\n",
        "      <li>⚡ Works **out-of-the-box**—just load the models and start predicting!</li>\n",
        "   </ul>\n",
        "   <h3>🔗 📌 Next Steps:</h3>\n",
        "   <p>👉 <a href=\"example_prediction.ipynb\"><strong>Go to Example Prediction Notebook</strong></a></p>\n",
        "</div> \n",
        "<h2><span style=\"color:red\">NOTE: use at your own discretion.</span></h2>\n",
        "\n",
        "### **Changes in V4**\n",
        "- **Three-model architecture**: Added a classification model to predict whether a course will receive bids, complementing the existing median and min bid regression models\n",
        "- **Advanced uncertainty quantification**: Implemented entropy-based confidence scoring for classification and t-distribution-based uncertainty multipliers for regression models\n",
        "- **Enhanced feature engineering**: Incorporated day-of-week boolean flags (`has_mon`, `has_tue`, etc.) for better temporal pattern recognition\n",
        "- **Sophisticated safety factor system**: Custom t-distribution fitting to error data with percentile-based uncertainty multipliers, replacing simple percentage-based safety factors\n",
        "- **Comprehensive evaluation suite**: Added confidence interval coverage analysis, residual analysis with emphasis on under-predictions, and cross-model feature importance comparison\n",
        "\n",
        "### **Objective**\n",
        "This notebook predicts bidding outcomes for courses in the SMU bidding system using **three specialized CatBoost models**. Building on insights from **V1, V2, and V3**, this version introduces a comprehensive **multi-model approach** with advanced uncertainty quantification:\n",
        "\n",
        "1. **Classification Model**: Predicts whether a course will receive bids (optimized for high recall)\n",
        "2. **Median Bid Regression Model**: Predicts the median bid price with t-distribution-based confidence intervals\n",
        "3. **Min Bid Regression Model**: Predicts the minimum bid price with t-distribution-based confidence intervals\n",
        "\n",
        "### **Key Enhancements in V4**\n",
        "\n",
        "**Learning from V3:**\n",
        "   - V3 focused on two regression models for median and min bid prediction\n",
        "   - V4 adds a **classification component** to identify courses that will receive bidding activity\n",
        "   - Enhanced with **probabilistic predictions** and **confidence scoring**\n",
        "\n",
        "**New V4 Features:**\n",
        "   - **Entropy-based confidence scoring** for classification predictions with five confidence levels (Very Low to Very High)\n",
        "   - **T-distribution-based uncertainty multipliers** derived from validation error analysis for robust confidence estimation\n",
        "   - **Percentile-based safety factors** (1%-99%) with both empirical and theoretical multipliers for precise risk management\n",
        "   - **Comprehensive uncertainty analysis** including distribution fitting and coverage metrics\n",
        "\n",
        "### **Three-Model Architecture**\n",
        "\n",
        "| **Model Type** | **Purpose** | **Output** | **Uncertainty Measure** |\n",
        "|----------------|-------------|------------|--------------------------|\n",
        "| **Classification** | Predict bid courses | Probability + Confidence Level | Entropy-based confidence score |\n",
        "| **Median Bid Regression** | Predict median bid price | Price + Uncertainty Multipliers | T-distribution-based multipliers |\n",
        "| **Min Bid Regression** | Predict minimum bid price | Price + Uncertainty Multipliers | T-distribution-based multipliers |\n",
        "\n",
        "### **Updated Dataset Features**\n",
        "\n",
        "| **Feature Name** | **Type** | **Description** |\n",
        "|------------------|----------|--------------------|\n",
        "| **`subject_area`** | Categorical | Subject area (IS, ECON, etc.) |\n",
        "| **`catalogue_no`** | Categorical | Course number |\n",
        "| **`round`** | Categorical | Bidding round (1, 1A, 1B, 1C, 2, 2A) |\n",
        "| **`window`** | Numerical | Bidding window (1-5) |\n",
        "| **`before_process_vacancy`** | Numerical | Available spots before bidding |\n",
        "| **`acad_year_start`** | Numerical | Academic year start |\n",
        "| **`term`** | Categorical | Academic term (1, 2, 3A, 3B) |\n",
        "| **`start_time`** | Categorical | Class start time |\n",
        "| **`course_name`** | Categorical | Course name/description |\n",
        "| **`section`** | Categorical | Course section |\n",
        "| **`instructor`** | Categorical | Instructor name |\n",
        "| **`has_mon`** - **`has_sun`** | Boolean | Day-of-week indicators |\n",
        "| **🎯 Target Variables 🎯** | | **Model outputs** |\n",
        "| **`bids`** | Binary | Whether course receives bids |\n",
        "| **`target_median_bid`** | Numerical | Median bid price |\n",
        "| **`target_min_bid`** | Numerical | Minimum bid price |\n",
        "\n",
        "### **Advanced Uncertainty Quantification**\n",
        "\n",
        "**Classification Confidence:**\n",
        "- **Entropy-based scoring**: Measures prediction certainty using information entropy\n",
        "- **Five confidence levels**: Very Low, Low, Medium, High, Very High\n",
        "- **Probability outputs**: Separate probabilities for bid/non-bid outcomes\n",
        "\n",
        "**Regression Uncertainty Multipliers:**\n",
        "- **T-distribution fitting**: Models fitted to validation error distributions for accurate tail behavior\n",
        "- **Percentile-based multipliers**: 99 percentiles (1%-99%) with corresponding uncertainty multipliers\n",
        "- **Dual multiplier types**: Both empirical (data-driven) and theoretical (distribution-based) multipliers\n",
        "- **Usage formula**: `final_bid = predicted + uncertainty × multiplier`\n",
        "\n",
        "### **Enhanced Safety Factor System**\n",
        "\n",
        "**Traditional vs V4 Approach:**\n",
        "- **Old approach**: Fixed percentage multipliers (e.g., `bid = prediction × 1.7`)\n",
        "- **V4 approach**: Uncertainty-aware multipliers (e.g., `bid = prediction + uncertainty × 1.584`)\n",
        "\n",
        "**Distribution Analysis:**\n",
        "- **Error modeling**: T-distributions fitted to validation residuals\n",
        "- **Heavy tail handling**: Proper modeling of extreme prediction errors\n",
        "- **Confidence levels**: Precise percentile-based risk assessment (80%, 90%, 95%, 99%)\n",
        "\n",
        "**Usage Example (90% confidence):**\n",
        "```\n",
        "Median bid = median_predicted + median_uncertainty × 1.584\n",
        "Min bid = min_predicted + min_uncertainty × 1.533\n",
        "```\n",
        "\n",
        "### **Methodology**\n",
        "The notebook follows this enhanced structure:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Loading separate datasets for classification and regression tasks\n",
        "   - Feature standardization and categorical encoding\n",
        "   - Train-test splitting with consistent random seeds\n",
        "\n",
        "2. **Three-Model Training**:\n",
        "   - **Classification**: CatBoost with recall optimization for bid opportunity detection\n",
        "   - **Median Regression**: CatBoost with t-distribution uncertainty quantification\n",
        "   - **Min Regression**: CatBoost with distribution-based uncertainty modeling\n",
        "\n",
        "3. **Advanced Evaluation**:\n",
        "   - **Classification**: Recall (maximizing true positives for bid detection), confusion matrix, entropy-based confidence analysis\n",
        "   - **Regression**: MSE, MAE, R², distribution fitting, percentile-based uncertainty analysis\n",
        "   - **Cross-model feature importance comparison**\n",
        "\n",
        "4. **Comprehensive Visualization**:\n",
        "   - Confidence distribution plots and uncertainty analysis\n",
        "   - Error distribution modeling and safety factor derivation\n",
        "   - Feature importance rankings across all three models\n",
        "\n",
        "5. **Model Persistence and Reporting**:\n",
        "   - All models saved as `.cbm` files for deployment\n",
        "   - Detailed results exported to CSV format with updated schema\n",
        "   - Safety factor tables with percentile-based multipliers\n",
        "\n",
        "### **Key Metrics and Performance**\n",
        "\n",
        "**Classification Model:**\n",
        "- **Primary metric**: Recall (optimized for capturing all bidding opportunities - maximizing true positives)\n",
        "- **Confidence analysis**: Distribution of entropy-based confidence scores  \n",
        "- **Output**: Probabilities for bid/no-bid outcomes (`clf_has_bids_prob`), confidence levels, and entropy values\n",
        "\n",
        "**Regression Models:**\n",
        "- **Standard metrics**: MSE, MAE, R² for model accuracy\n",
        "- **Distribution analysis**: T-distribution fitting with degrees of freedom, location, and scale parameters\n",
        "- **Uncertainty metrics**: Percentile-based multipliers and coverage analysis\n",
        "- **Safety analysis**: Risk-calibrated predictions using distribution-based multipliers\n",
        "\n",
        "### **Classification Strategy - Maximizing Bidding Opportunities**\n",
        "\n",
        "**Recall-Optimized Approach:**\n",
        "- **Target**: Predict courses that will receive bids (positive class = 1)\n",
        "- **Primary Goal**: Maximize recall to capture all potential bidding opportunities\n",
        "- **Business Logic**: Missing a course that will receive bids (False Negative) is more costly than incorrectly predicting a course will receive bids (False Positive)\n",
        "- **Optimization**: Model trained to minimize missed bidding opportunities while maintaining reasonable precision\n",
        "\n",
        "### **Implementation Notes**\n",
        "To run this V4 notebook:\n",
        "- Install required packages: `pip install catboost pandas numpy matplotlib seaborn scikit-learn scipy`\n",
        "- Ensure you have the three required datasets:\n",
        "  - Classification training/test data\n",
        "  - Median bid regression training/test data\n",
        "  - Min bid regression training/test data\n",
        "- Models automatically save to `script_output_model_training/mode/` directory\n",
        "- Validation results required for safety factor calculation\n",
        "\n",
        "### **V4 Advantages**\n",
        "- **Comprehensive coverage**: Handles both bid opportunity detection and price prediction\n",
        "- **Risk-aware predictions**: T-distribution-based uncertainty modeling prevents dangerous under-bidding\n",
        "- **Confidence-calibrated**: Provides uncertainty measures for better decision-making with precise percentile-based risk assessment\n",
        "- **Feature-rich analysis**: Cross-model feature importance for strategic insights\n",
        "- **Production-ready**: All models saved with consistent interfaces for deployment\n",
        "- **Scientific rigor**: Distribution-based uncertainty quantification replacing ad-hoc safety factors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8520404",
      "metadata": {},
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359f5072",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import os\n",
        "import psycopg2\n",
        "from dotenv import load_dotenv\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import json\n",
        "import hashlib\n",
        "from typing import List\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add database configuration\n",
        "load_dotenv()\n",
        "db_config = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'database': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': int(os.getenv('DB_PORT', 5432)),\n",
        "    'gssencmode': 'disable'\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "output_dir = Path('script_output/predictions')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "cache_dir = Path('db_cache')\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CONFIGURATION\n",
        "# Define the academic term you are targeting for predictions.\n",
        "TARGET_AY_TERM = '2025-26_T1'\n",
        "\n",
        "# Define the specific bidding round and window you want to target.\n",
        "# Set to None to let the script auto-detect the current phase based on the schedule.\n",
        "TARGET_ROUND = None   # e.g., '1A', '2', etc.\n",
        "TARGET_WINDOW = None  # e.g., 1, 2, 3, etc.\n",
        "\n",
        "# Central bidding schedule for each academic term.\n",
        "# The script uses this to correctly parse the 'bidding_window' strings.\n",
        "# Format: (results_datetime, \"Full Bidding Window Name\", \"Folder_Suffix\")\n",
        "BIDDING_SCHEDULES = {\n",
        "    '2025-26_T1': [\n",
        "        (datetime(2025, 7, 9, 14, 0), \"Round 1 Window 1\", \"R1W1\"),\n",
        "        (datetime(2025, 7, 11, 14, 0), \"Round 1A Window 1\", \"R1AW1\"),\n",
        "        (datetime(2025, 7, 14, 14, 0), \"Round 1A Window 2\", \"R1AW2\"),\n",
        "        (datetime(2025, 7, 16, 14, 0), \"Round 1A Window 3\", \"R1AW3\"),\n",
        "        (datetime(2025, 7, 18, 14, 0), \"Round 1B Window 1\", \"R1BW1\"),\n",
        "        (datetime(2025, 7, 21, 14, 0), \"Round 1B Window 2\", \"R1BW2\"),\n",
        "        (datetime(2025, 7, 30, 14, 0), \"Incoming Exchange Rnd 1C Win 1\", \"R1CW1\"),\n",
        "        (datetime(2025, 7, 31, 14, 0), \"Incoming Exchange Rnd 1C Win 2\", \"R1CW2\"),\n",
        "        (datetime(2025, 8, 1, 14, 0), \"Incoming Exchange Rnd 1C Win 3\", \"R1CW3\"),\n",
        "        (datetime(2025, 8, 11, 14, 0), \"Incoming Freshmen Rnd 1 Win 1\", \"R1FW1\"),\n",
        "        (datetime(2025, 8, 12, 14, 0), \"Incoming Freshmen Rnd 1 Win 2\", \"R1FW2\"),\n",
        "        (datetime(2025, 8, 13, 14, 0), \"Incoming Freshmen Rnd 1 Win 3\", \"R1FW3\"),\n",
        "        (datetime(2025, 8, 14, 14, 0), \"Incoming Freshmen Rnd 1 Win 4\", \"R1FW4\"),\n",
        "        (datetime(2025, 8, 20, 14, 0), \"Round 2 Window 1\", \"R2W1\"),\n",
        "        (datetime(2025, 8, 22, 14, 0), \"Round 2 Window 2\", \"R2W2\"),\n",
        "        (datetime(2025, 8, 25, 14, 0), \"Round 2 Window 3\", \"R2W3\"),\n",
        "        (datetime(2025, 8, 27, 14, 0), \"Round 2A Window 1\", \"R2AW1\"),\n",
        "        (datetime(2025, 8, 29, 14, 0), \"Round 2A Window 2\", \"R2AW2\"),\n",
        "        (datetime(2025, 9, 1, 14, 0), \"Round 2A Window 3\", \"R2AW3\"),\n",
        "    ]\n",
        "    # You can add schedules for other terms here, e.g., '2025-26_T2': [...]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309e5d9e",
      "metadata": {},
      "source": [
        "## **2. SMUBiddingTransformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ccf08c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SMUBiddingTransformer:\n",
        "    \"\"\"\n",
        "    A reusable transformer class for processing SMU course bidding data\n",
        "    optimized for CatBoost model.\n",
        "    \n",
        "    Uses categorical encoding for instructors and one-hot encoding for multi-valued days.\n",
        "    \n",
        "    Expected input columns:\n",
        "    - course_code: str (e.g. 'MGMT715', 'COR-COMM175')\n",
        "    - course_name: str\n",
        "    - acad_year_start: int\n",
        "    - term: str ('1', '2', '3A', '3B')\n",
        "    - start_time: str (e.g. '19:30', 'TBA') - preserved as categorical\n",
        "    - day_of_week: str (can be multivalued, e.g. 'Mon,Thu')\n",
        "    - before_process_vacancy: int\n",
        "    - bidding_window: str (e.g. 'Round 1 Window 1', 'Incoming Freshmen Rnd 1 Win 4')\n",
        "    - instructor: str (can be multivalued, e.g. 'JOHN DOE, JANE SMITH')\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the transformer for CatBoost optimization.\n",
        "        \n",
        "        Uses categorical encoding for instructors and one-hot encoding for days.\n",
        "        \"\"\"\n",
        "        # Fitted flags\n",
        "        self.is_fitted = False\n",
        "        \n",
        "        # Lists to track feature types for CatBoost\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "    def fit(self, df: pd.DataFrame) -> 'SMUBiddingTransformer':\n",
        "        \"\"\"\n",
        "        Fit the transformer on training data.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Training dataframe with all required columns\n",
        "        \"\"\"\n",
        "        # Validate required columns\n",
        "        required_cols = [\n",
        "            'course_code', 'course_name', 'acad_year_start', 'term',\n",
        "            'start_time', 'day_of_week', 'before_process_vacancy',\n",
        "            'bidding_window', 'instructor', 'section'\n",
        "        ]\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "        \n",
        "        print(f\"Fitting transformer on {len(df)} rows...\")\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "    \n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Transform the dataframe to CatBoost-ready format.\n",
        "        \"\"\"\n",
        "        # Try to load existing model if not fitted\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted before transform. Call fit() first.\")\n",
        "        \n",
        "        # Create a copy to avoid modifying original\n",
        "        df_transformed = df.copy()\n",
        "        \n",
        "        # Reset feature tracking\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "        # 1. Extract course components (categorical + numeric)\n",
        "        course_features = self._extract_course_features(df_transformed)\n",
        "        \n",
        "        # 2. Process bidding window (categorical + numeric)\n",
        "        round_window_features = self._extract_round_window(df_transformed)\n",
        "        \n",
        "        # 3. Basic features (preserve categorical nature) + instructor as categorical\n",
        "        basic_features = self._process_basic_features(df_transformed)\n",
        "        \n",
        "        # 4. Create day one-hot encoding\n",
        "        day_features = self._create_day_one_hot_encoding(df_transformed)\n",
        "        \n",
        "        # Combine all features - FIXED: Ensure proper concatenation\n",
        "        feature_dfs = [course_features, round_window_features, basic_features, day_features]\n",
        "        \n",
        "        # Filter out any empty DataFrames\n",
        "        feature_dfs = [df for df in feature_dfs if not df.empty]\n",
        "        \n",
        "        if not feature_dfs:\n",
        "            raise ValueError(\"No features were extracted\")\n",
        "        \n",
        "        # Concatenate all features\n",
        "        final_df = pd.concat(feature_dfs, axis=1)\n",
        "        \n",
        "        # Verify all expected features are present\n",
        "        expected_features = self.categorical_features + self.numeric_features\n",
        "        missing_features = [f for f in expected_features if f not in final_df.columns]\n",
        "        \n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing features in final dataframe: {missing_features}\")\n",
        "            print(f\"Available columns: {list(final_df.columns)}\")\n",
        "        \n",
        "        # Debug: Print feature summary\n",
        "        print(f\"Transformed data shape: {final_df.shape}\")\n",
        "        print(f\"Features included: {list(final_df.columns)[:10]}...\")  # Show first 10\n",
        "        \n",
        "        return final_df\n",
        "        \n",
        "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Fit the transformer and transform the data in one step.\"\"\"\n",
        "        self.fit(df)\n",
        "        return self.transform(df)\n",
        "    \n",
        "    def get_categorical_features(self) -> List[str]:\n",
        "        \"\"\"Get list of categorical feature names for CatBoost.\"\"\"\n",
        "        return self.categorical_features.copy()\n",
        "    \n",
        "    def get_numeric_features(self) -> List[str]:\n",
        "        \"\"\"Get list of numeric feature names.\"\"\"\n",
        "        return self.numeric_features.copy()\n",
        "    \n",
        "    def _extract_course_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract subject area and catalogue number from course code.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def split_course_code(code):\n",
        "            \"\"\"Split course code into subject area and catalogue number.\"\"\"\n",
        "            if pd.isna(code):\n",
        "                return None, None\n",
        "            \n",
        "            code = str(code).strip().upper()\n",
        "            \n",
        "            # Handle hyphenated codes like 'COR-COMM175'\n",
        "            if '-' in code:\n",
        "                parts = code.split('-')\n",
        "                if len(parts) >= 2:\n",
        "                    subject = '-'.join(parts[:-1])\n",
        "                    # Extract number from last part\n",
        "                    num_match = re.search(r'(\\d+)', parts[-1])\n",
        "                    if num_match:\n",
        "                        return subject, int(num_match.group(1))\n",
        "                    else:\n",
        "                        # Try extracting from full last part\n",
        "                        num_match = re.search(r'(\\d+)', code)\n",
        "                        if num_match:\n",
        "                            return subject, int(num_match.group(1))\n",
        "            \n",
        "            # Standard format like 'MGMT715'\n",
        "            match = re.match(r'([A-Z\\-]+)(\\d+)', code)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            return code, 0\n",
        "        \n",
        "        # Extract components\n",
        "        splits = df['course_code'].apply(split_course_code)\n",
        "        features['subject_area'] = splits.apply(lambda x: x[0] if x else None)\n",
        "        features['catalogue_no'] = splits.apply(lambda x: x[1] if x else 0)\n",
        "\n",
        "        # Debug: Verify extraction\n",
        "        print(f\"Extracted course features: {features.shape}\")\n",
        "        print(f\"Sample subject_area values: {features['subject_area'].head()}\")\n",
        "        print(f\"Sample catalogue_no values: {features['catalogue_no'].head()}\")\n",
        "\n",
        "        # subject_area and catalogue_no are categorical for CatBoost\n",
        "        self.categorical_features.extend(['subject_area', 'catalogue_no'])\n",
        "\n",
        "        return features\n",
        "    \n",
        "    def _extract_round_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract round and window from bidding_window string.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def parse_bidding_window(window_str):\n",
        "            \"\"\"Parse bidding window string into round and window number.\"\"\"\n",
        "            if pd.isna(window_str):\n",
        "                return None, None\n",
        "            \n",
        "            window_str = str(window_str).strip()\n",
        "            # Check for Incoming Freshmen FIRST (before other patterns)\n",
        "            if 'Incoming Freshmen' in window_str:\n",
        "                match = re.search(r'Rnd\\s+(\\d)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if match:\n",
        "                    # Add F suffix to distinguish from regular rounds\n",
        "                    return f\"{match.group(1)}F\", int(match.group(2))     \n",
        "            \n",
        "            # Pattern 1: Standard format\n",
        "            match = re.search(r'Round\\s+(\\d[A-C]?)\\s+Window\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 2: Abbreviated format\n",
        "            match = re.search(r'Rnd\\s+(\\d[A-C]?)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 3: Incoming Exchange format (keeps original round)\n",
        "            match = re.search(r'Incoming\\s+Exchange\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 4: Incoming Freshmen format (adds F suffix)\n",
        "            match = re.search(r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                original_round = match.group(1)\n",
        "                window_num = int(match.group(2))\n",
        "                # Map Incoming Freshmen Round 1 to Round 1F\n",
        "                if original_round == \"1\":\n",
        "                    round_str = \"1F\"\n",
        "                else:\n",
        "                    round_str = f\"{original_round}F\"\n",
        "                return round_str, window_num\n",
        "            \n",
        "            # Fallback patterns...\n",
        "            match = re.search(r'(\\d[A-C]?)', window_str)\n",
        "            if match:\n",
        "                win_match = re.search(r'Window\\s+(\\d)|Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if win_match:\n",
        "                    window_num = int(win_match.group(1) or win_match.group(2))\n",
        "                    return match.group(1), window_num\n",
        "                return match.group(1), 1\n",
        "            \n",
        "            return '1', 1\n",
        "        \n",
        "        # Extract round and window\n",
        "        parsed = df['bidding_window'].apply(parse_bidding_window)\n",
        "        features['round'] = parsed.apply(lambda x: x[0] if x else '1')\n",
        "        features['window'] = parsed.apply(lambda x: x[1] if x else 1)\n",
        "        \n",
        "        # Round as categorical (preserves ordering like 1, 1A, 1B, 2, 2A)\n",
        "        self.categorical_features.append('round')\n",
        "        \n",
        "        # Window as numeric\n",
        "        self.numeric_features.append('window')\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def _process_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process basic features, preserving categorical nature where beneficial.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Numeric features\n",
        "        features['before_process_vacancy'] = pd.to_numeric(\n",
        "            df['before_process_vacancy'], errors='coerce'\n",
        "        ).fillna(0)\n",
        "        features['acad_year_start'] = pd.to_numeric(\n",
        "            df['acad_year_start'], errors='coerce'\n",
        "        ).fillna(2025)\n",
        "        \n",
        "        self.numeric_features.extend(['before_process_vacancy', 'acad_year_start'])\n",
        "        \n",
        "        # Categorical features\n",
        "        features['term'] = df['term'].astype(str)\n",
        "        features['start_time'] = df['start_time'].astype(str)\n",
        "        features['course_name'] = df['course_name'].astype(str)\n",
        "        features['section'] = df['section'].astype(str)\n",
        "        \n",
        "        # Process instructor names (remove duplicates, handle comma-separated format)\n",
        "        features['instructor'] = df['instructor'].apply(self._process_instructor_names)\n",
        "\n",
        "        # Replace empty strings with None for proper CatBoost handling\n",
        "        features.loc[features['start_time'].isin(['', 'nan']), 'start_time'] = None\n",
        "        features.loc[features['course_name'].isin(['', 'nan']), 'course_name'] = None\n",
        "        features.loc[features['section'].isin(['', 'nan']), 'section'] = None\n",
        "        \n",
        "        self.categorical_features.extend(['term', 'start_time', 'course_name', 'section', 'instructor'])\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def _create_day_one_hot_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create one-hot encoding for days of the week.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Initialize all day columns as 0\n",
        "        day_columns = ['has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun']\n",
        "        for col in day_columns:\n",
        "            features[col] = 0\n",
        "        \n",
        "        # Day mapping\n",
        "        day_abbrev = {\n",
        "            'MONDAY': 'MON', 'TUESDAY': 'TUE', 'WEDNESDAY': 'WED',\n",
        "            'THURSDAY': 'THU', 'FRIDAY': 'FRI', 'SATURDAY': 'SAT', 'SUNDAY': 'SUN',\n",
        "            'MON': 'MON', 'TUE': 'TUE', 'WED': 'WED', 'THU': 'THU',\n",
        "            'FRI': 'FRI', 'SAT': 'SAT', 'SUN': 'SUN'\n",
        "        }\n",
        "        \n",
        "        day_to_column = {\n",
        "            'MON': 'has_mon', 'TUE': 'has_tue', 'WED': 'has_wed', 'THU': 'has_thu',\n",
        "            'FRI': 'has_fri', 'SAT': 'has_sat', 'SUN': 'has_sun'\n",
        "        }\n",
        "        \n",
        "        # Process each row's day_of_week\n",
        "        for idx, days_value in enumerate(df['day_of_week']):\n",
        "            if pd.isna(days_value) or str(days_value).strip() == '':\n",
        "                continue  # Leave all days as 0\n",
        "            \n",
        "            days_str = str(days_value).strip()\n",
        "            \n",
        "            # Handle JSON array format\n",
        "            if days_str.startswith('[') and days_str.endswith(']'):\n",
        "                try:\n",
        "                    import json\n",
        "                    days_list = json.loads(days_str)\n",
        "                    if isinstance(days_list, list):\n",
        "                        for day in days_list:\n",
        "                            day_upper = str(day).strip().upper()\n",
        "                            standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                            \n",
        "                            if standardized_day in day_to_column:\n",
        "                                features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try comma-separated format as fallback\n",
        "                    pass\n",
        "            else:\n",
        "                # Handle comma-separated format (legacy support)\n",
        "                for day in days_str.split(','):\n",
        "                    day_upper = day.strip().upper()\n",
        "                    standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                    \n",
        "                    if standardized_day in day_to_column:\n",
        "                        features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "        \n",
        "        # These are numeric binary features (0/1)\n",
        "        self.numeric_features.extend(day_columns)\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def get_feature_names(self) -> List[str]:\n",
        "        \"\"\"Get all feature names after transformation.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted to get feature names.\")\n",
        "        \n",
        "        return self.categorical_features + self.numeric_features\n",
        "    \n",
        "    def _process_instructor_names(self, instructor_input):\n",
        "        \"\"\"Process instructor names to ensure consistent JSON array format as categorical string.\"\"\"\n",
        "        # Handle list/array input\n",
        "        if isinstance(instructor_input, (list, np.ndarray)):\n",
        "            if len(instructor_input) == 0:\n",
        "                return None\n",
        "            # Convert list to string format for processing\n",
        "            instructor_str = ', '.join([str(inst).strip() for inst in instructor_input if pd.notna(inst) and str(inst).strip()])\n",
        "            if not instructor_str:\n",
        "                return None\n",
        "        else:\n",
        "            # Handle string input\n",
        "            if pd.isna(instructor_input) or str(instructor_input).strip() == '' or str(instructor_input).upper() == 'TBA':\n",
        "                return None\n",
        "            instructor_str = str(instructor_input).strip()\n",
        "\n",
        "        # Load professor lookup mapping\n",
        "        professor_lookup = {}\n",
        "        lookup_path = Path(\"script_input/professor_lookup.csv\")\n",
        "        if lookup_path.exists():\n",
        "            lookup_df = pd.read_csv(lookup_path)\n",
        "            for _, row in lookup_df.iterrows():\n",
        "                if pd.notna(row.get('boss_name')) and pd.notna(row.get('afterclass_name')):\n",
        "                    professor_lookup[str(row['boss_name']).strip().upper()] = str(row['afterclass_name']).strip()\n",
        "        \n",
        "        # Step 1: Check if the entire string is already a known professor.\n",
        "        # This handles names that include commas, like \"LEE, MICHELLE PUI YEE\".\n",
        "        if instructor_str.upper() in professor_lookup:\n",
        "            import json\n",
        "            return json.dumps([professor_lookup[instructor_str.upper()]])\n",
        "\n",
        "        # If there are no commas, it's treated as a single professor.\n",
        "        if ',' not in instructor_str:\n",
        "            mapped_name = professor_lookup.get(instructor_str.upper(), instructor_str)\n",
        "            import json\n",
        "            return json.dumps([mapped_name])\n",
        "\n",
        "        # Step 2: Use greedy, longest-match-first approach for comma-separated names.\n",
        "        parts = [p.strip() for p in instructor_str.split(',') if p.strip()]\n",
        "        found_professors = []\n",
        "        i = 0\n",
        "        while i < len(parts):\n",
        "            match_found = False\n",
        "            # Start from the longest possible combination of remaining parts.\n",
        "            for j in range(len(parts), i, -1):\n",
        "                candidate = ', '.join(parts[i:j])\n",
        "                # Check if this candidate is a known professor.\n",
        "                if candidate.upper() in professor_lookup:\n",
        "                    found_professors.append(candidate)\n",
        "                    i = j  # Move the pointer past the consumed parts.\n",
        "                    match_found = True\n",
        "                    break\n",
        "            \n",
        "            # If no match was found, treat the current part as an unknown entity.\n",
        "            if not match_found:\n",
        "                unknown_part = parts[i]\n",
        "                # Append an unknown single-word part to the previously found professor.\n",
        "                if found_professors and len(unknown_part.split()) == 1:\n",
        "                    found_professors[-1] = f\"{found_professors[-1]}, {unknown_part}\"\n",
        "                else:\n",
        "                    # Otherwise, treat it as its own (potentially new) professor.\n",
        "                    found_professors.append(unknown_part)\n",
        "                i += 1\n",
        "        \n",
        "        split_names = found_professors\n",
        "        \n",
        "        # Map each split name to afterclass_name\n",
        "        mapped_names = []\n",
        "        for name in split_names:\n",
        "            name_upper = name.strip().upper()\n",
        "            if name_upper in professor_lookup:\n",
        "                mapped_names.append(professor_lookup[name_upper])\n",
        "            else:\n",
        "                # Keep original name if not found in lookup but has multiple words\n",
        "                words = name.strip().split()\n",
        "                if len(words) >= 2:\n",
        "                    mapped_names.append(name.strip())\n",
        "        \n",
        "        if mapped_names:\n",
        "            # Remove duplicates and sort\n",
        "            unique_mapped = sorted(list(set(mapped_names)))\n",
        "            \n",
        "            import json\n",
        "            return json.dumps(unique_mapped)\n",
        "        \n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2ec324",
      "metadata": {},
      "source": [
        "## **3. Database Helper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4dac95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def connect_database():\n",
        "    \"\"\"Connect to PostgreSQL database\"\"\"\n",
        "    load_dotenv()\n",
        "    db_config = {\n",
        "        'host': os.getenv('DB_HOST'),\n",
        "        'database': os.getenv('DB_NAME'),\n",
        "        'user': os.getenv('DB_USER'),\n",
        "        'password': os.getenv('DB_PASSWORD'),\n",
        "        'port': int(os.getenv('DB_PORT', 5432)),\n",
        "        'gssencmode': 'disable'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        connection = psycopg2.connect(**db_config)\n",
        "        print(\"✅ Database connection established\")\n",
        "        return connection\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Database connection failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_or_cache_data(connection, cache_dir):\n",
        "    \"\"\"Load data from cache or database\"\"\"\n",
        "    cache_files = {\n",
        "        'courses': cache_dir / 'courses_cache.pkl',\n",
        "        'classes': cache_dir / 'classes_cache.pkl',\n",
        "        'acad_terms': cache_dir / 'acad_terms_cache.pkl',\n",
        "        'professors': cache_dir / 'professors_cache.pkl',\n",
        "        'bid_windows': cache_dir / 'bid_window_cache.pkl',\n",
        "        'bid_prediction': cache_dir / 'bid_prediction_cache.pkl',\n",
        "    }\n",
        "    \n",
        "    data_cache = {}\n",
        "    \n",
        "    # Try loading from cache first\n",
        "    if all(f.exists() for f in cache_files.values()):\n",
        "        print(\"✅ Loading from cache...\")\n",
        "        for key, file in cache_files.items():\n",
        "            data_cache[key] = pd.read_pickle(file)\n",
        "    else:\n",
        "        print(\"📥 Downloading from database...\")\n",
        "        queries = {\n",
        "            'courses': \"SELECT * FROM courses\",\n",
        "            'classes': \"SELECT * FROM classes\",\n",
        "            'acad_terms': \"SELECT * FROM acad_term\",\n",
        "            'professors': \"SELECT * FROM professors\",\n",
        "            'bid_windows': \"SELECT * FROM bid_window\",\n",
        "            'bid_prediction': \"SELECT * FROM bid_prediction\",\n",
        "        }\n",
        "        \n",
        "        for key, query in queries.items():\n",
        "            df = pd.read_sql_query(query, connection)\n",
        "            df.to_pickle(cache_files[key])\n",
        "            data_cache[key] = df\n",
        "    \n",
        "    return data_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815cf568",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Preparation Functions\n",
        "def prepare_prediction_data(raw_data_path='script_input/raw_data.xlsx', connection=None, db_cache=None):\n",
        "    \"\"\"Prepare data for prediction from raw_data.xlsx with time-based bidding window and database cache check\"\"\"\n",
        "    from datetime import datetime\n",
        "    import re\n",
        "\n",
        "    # a. Determine Target Bidding Window Name\n",
        "    active_window_name = None\n",
        "    mode = \"Automatic\"\n",
        "    if TARGET_ROUND and TARGET_WINDOW is not None:\n",
        "        mode = \"Manual\"\n",
        "        active_window_name = f\"Round {TARGET_ROUND} Window {TARGET_WINDOW}\"\n",
        "    else:\n",
        "        now = datetime.now()\n",
        "        # Find the first bidding window in the schedule whose closing time has not yet passed\n",
        "        for schedule_item in BIDDING_SCHEDULES.get(TARGET_AY_TERM, []):\n",
        "            if schedule_item[0] > now:\n",
        "                active_window_name = schedule_item[1]\n",
        "                break\n",
        "\n",
        "    if not active_window_name:\n",
        "        raise ValueError(\"Could not determine an active bidding window. Check BIDDING_SCHEDULES or manual override settings.\")\n",
        "\n",
        "    # b. Load and Filter Data\n",
        "    print(\"\\n📂 Loading raw data for filtering...\")\n",
        "    full_standalone_df = pd.read_excel(raw_data_path, sheet_name='standalone')\n",
        "    full_multiple_df = pd.read_excel(raw_data_path, sheet_name='multiple')\n",
        "\n",
        "    # Filter standalone\n",
        "    filtered_standalone_df = full_standalone_df[full_standalone_df['bidding_window'] == active_window_name].copy()\n",
        "\n",
        "    # c. Add a Diagnostic Print Statement\n",
        "    print(f\"✅ Processing {mode} target: '{active_window_name}'. Found {len(filtered_standalone_df)} records.\")\n",
        "    \n",
        "    if filtered_standalone_df.empty:\n",
        "        print(\"⚠️ No records found for the target window. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame(), full_standalone_df, full_multiple_df, db_cache\n",
        "\n",
        "    # Extract keys and filter multiple\n",
        "    active_keys = filtered_standalone_df['record_key'].unique()\n",
        "    filtered_multiple_df = full_multiple_df[full_multiple_df['record_key'].isin(active_keys)].copy()\n",
        "    \n",
        "    # d. Process and Reset Index (using original logic on filtered data)\n",
        "    bidding_data = filtered_standalone_df[filtered_standalone_df['bidding_window'].notna() & filtered_standalone_df['total'].notna()].copy()\n",
        "    bidding_data['before_process_vacancy'] = bidding_data['total'] - bidding_data['current_enrolled']\n",
        "\n",
        "    # Define bidding schedule from the global configuration\n",
        "    bidding_schedule = BIDDING_SCHEDULES.get(TARGET_AY_TERM, [])\n",
        "    if not bidding_schedule:\n",
        "        print(f\"⚠️ Warning: No bidding schedule found for the target term '{TARGET_AY_TERM}'.\")\n",
        "\n",
        "    # This mapping is now just for a sanity check, as we already filtered\n",
        "    def map_bidding_window_to_round_window(bidding_window_str):\n",
        "        if not bidding_window_str or pd.isna(bidding_window_str): return None, None\n",
        "        text = str(bidding_window_str).strip()\n",
        "        match = re.search(r'Round\\s+([\\w\\d]+)\\s+Window\\s+(\\d+)', text, re.IGNORECASE)\n",
        "        if match: return match.group(1), int(match.group(2))\n",
        "        match = re.search(r'Rnd\\s+([\\w\\d]+)\\s+Win\\s+(\\d+)', text, re.IGNORECASE)\n",
        "        if match:\n",
        "            round_val, win_val = match.group(1), int(match.group(2))\n",
        "            if 'Freshmen' in text: return f\"{round_val}F\", win_val\n",
        "            return round_val, win_val\n",
        "        return None, None\n",
        "\n",
        "    bidding_data[['round', 'window']] = bidding_data['bidding_window'].apply(\n",
        "        lambda x: pd.Series(map_bidding_window_to_round_window(x))\n",
        "    )\n",
        "\n",
        "    if db_cache is None: db_cache = {}\n",
        "    if 'bid_prediction' not in db_cache and connection is not None:\n",
        "        try:\n",
        "            db_cache['bid_prediction'] = pd.read_sql_query(\"SELECT * FROM bid_prediction\", connection)\n",
        "        except Exception as e:\n",
        "            db_cache['bid_prediction'] = pd.DataFrame()\n",
        "\n",
        "    # Get instructor information from the filtered multiple sheet\n",
        "    instructor_map = {}\n",
        "    for record_key, group in filtered_multiple_df.groupby('record_key'):\n",
        "        professors = group['professor_name'].dropna().unique()\n",
        "        if len(professors) > 0:\n",
        "            instructor_map[record_key] = professors.tolist()\n",
        "    bidding_data['instructor'] = bidding_data['record_key'].map(lambda x: instructor_map.get(x, []))\n",
        "    \n",
        "    # Get day of week information from the filtered multiple sheet\n",
        "    day_map = {}\n",
        "    for record_key, group in filtered_multiple_df[filtered_multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        days = group['day_of_week'].dropna().unique()\n",
        "        if len(days) > 0: day_map[record_key] = ', '.join(days)\n",
        "    bidding_data['day_of_week'] = bidding_data['record_key'].map(lambda x: day_map.get(x, ''))\n",
        "    \n",
        "    # Get start time from the filtered multiple sheet\n",
        "    time_map = {}\n",
        "    for record_key, group in filtered_multiple_df[filtered_multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        times = group['start_time'].dropna()\n",
        "        if len(times) > 0: time_map[record_key] = times.iloc[0]\n",
        "    bidding_data['start_time'] = bidding_data['record_key'].map(lambda x: time_map.get(x, ''))\n",
        "\n",
        "    # Merging of new data files remains unchanged as it updates the general cache\n",
        "    new_courses_path = Path('script_output/verify/new_courses.csv')\n",
        "    if new_courses_path.exists():\n",
        "        new_courses_df = pd.read_csv(new_courses_path)\n",
        "        if 'courses' in db_cache:\n",
        "            db_cache['courses'] = pd.concat([db_cache['courses'], new_courses_df], ignore_index=True).drop_duplicates(subset=['id', 'code'])\n",
        "        else: db_cache['courses'] = new_courses_df\n",
        "\n",
        "    new_professors_path = Path('script_output/verify/new_professors.csv')\n",
        "    if new_professors_path.exists():\n",
        "        new_professors_df = pd.read_csv(new_professors_path)\n",
        "        if 'professors' in db_cache:\n",
        "            db_cache['professors'] = pd.concat([db_cache['professors'], new_professors_df], ignore_index=True).drop_duplicates(subset=['id'])\n",
        "        else: db_cache['professors'] = new_professors_df\n",
        "    \n",
        "    new_classes_path = Path('script_output/new_classes.csv')\n",
        "    if new_classes_path.exists():\n",
        "        new_classes_df = pd.read_csv(new_classes_path)\n",
        "        if 'classes' in db_cache:\n",
        "            acad_term_ids = bidding_data['acad_term_id'].unique()\n",
        "            db_classes_filtered = db_cache['classes'][db_cache['classes']['acad_term_id'].isin(acad_term_ids)]\n",
        "            combined_classes = pd.concat([db_classes_filtered, new_classes_df], ignore_index=True)\n",
        "            # FIX: Deduplicate by class ID only, not by course+section+term (which removes multi-professor classes)\n",
        "            db_cache['classes'] = combined_classes.drop_duplicates(subset=['id'])\n",
        "        else: db_cache['classes'] = new_classes_df\n",
        "\n",
        "    # Critical: Reset index before returning\n",
        "    bidding_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # e. Maintain Original Function Signature\n",
        "    return bidding_data, full_standalone_df, full_multiple_df, db_cache\n",
        "\n",
        "\n",
        "def map_classes_to_predictions(bidding_data, data_cache, connection):\n",
        "    \"\"\"Map predictions to class IDs - checks both database cache and new_classes.csv\"\"\"\n",
        "    courses_df = data_cache['courses']\n",
        "    classes_df = data_cache['classes']\n",
        "    \n",
        "    # Create course code to ID mapping from both sources\n",
        "    course_id_map = dict(zip(courses_df['code'], courses_df['id']))\n",
        "    \n",
        "    # Also check new_courses.csv for courses not in database yet\n",
        "    new_courses_paths = [\n",
        "        Path('script_output/new_courses.csv'),\n",
        "        Path('script_output/verify/new_courses.csv')\n",
        "    ]\n",
        "    \n",
        "    for path in new_courses_paths:\n",
        "        if path.exists():\n",
        "            try:\n",
        "                new_courses_df = pd.read_csv(path)\n",
        "                for _, row in new_courses_df.iterrows():\n",
        "                    if row['code'] not in course_id_map:\n",
        "                        course_id_map[row['code']] = row['id']\n",
        "                print(f\"📚 Added {len(new_courses_df)} courses from {path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Could not load {path}: {e}\")\n",
        "    \n",
        "    # Combine classes from DB cache and new_classes.csv\n",
        "    all_classes = []\n",
        "    \n",
        "    # Add DB cache classes\n",
        "    if not classes_df.empty:\n",
        "        all_classes.extend(classes_df.to_dict('records'))\n",
        "    \n",
        "    # Load and add new_classes.csv\n",
        "    new_classes_path = Path('script_output/new_classes.csv')\n",
        "    if new_classes_path.exists():\n",
        "        try:\n",
        "            new_classes_df = pd.read_csv(new_classes_path)\n",
        "            all_classes.extend(new_classes_df.to_dict('records'))\n",
        "            print(f\"📚 Added {len(new_classes_df)} classes from new_classes.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not load new_classes.csv: {e}\")\n",
        "    \n",
        "    # Deduplicate classes by id to avoid duplicate mappings\n",
        "    seen_class_ids = set()\n",
        "    unique_classes = []\n",
        "    for class_rec in all_classes:\n",
        "        class_id = class_rec['id']\n",
        "        if class_id not in seen_class_ids:\n",
        "            seen_class_ids.add(class_id)\n",
        "            unique_classes.append(class_rec)\n",
        "    \n",
        "    print(f\"📚 Total classes after deduplication: {len(unique_classes)} (removed {len(all_classes) - len(unique_classes)} duplicates)\")\n",
        "    \n",
        "    # Create lookup for efficient searching\n",
        "    class_lookup = defaultdict(list)\n",
        "    for class_rec in unique_classes:\n",
        "        key = (class_rec['course_id'], str(class_rec['section']), class_rec['acad_term_id'])\n",
        "        class_lookup[key].append(class_rec)\n",
        "    \n",
        "    # Map each row to class IDs\n",
        "    class_mappings = []\n",
        "    unmapped_courses = set()\n",
        "    \n",
        "    for idx, row in bidding_data.iterrows():\n",
        "        course_code = row['course_code']\n",
        "        section = str(row['section'])\n",
        "        acad_term_id = row['acad_term_id']\n",
        "        record_key = row.get('record_key', '')\n",
        "        \n",
        "        # Get course ID\n",
        "        course_id = course_id_map.get(course_code)\n",
        "        if not course_id:\n",
        "            unmapped_courses.add(course_code)\n",
        "            continue\n",
        "        \n",
        "        # Look up classes using the combined lookup\n",
        "        lookup_key = (course_id, section, acad_term_id)\n",
        "        matching_classes = class_lookup.get(lookup_key, [])\n",
        "        \n",
        "        if matching_classes:\n",
        "            # Map to all matching classes (handles multi-professor automatically)\n",
        "            for class_row in matching_classes:\n",
        "                mapping = {\n",
        "                    'prediction_idx': idx,\n",
        "                    'class_id': class_row['id'],\n",
        "                    'professor_id': class_row.get('professor_id'),\n",
        "                    'course_code': course_code,\n",
        "                    'section': section,\n",
        "                    'acad_term_id': acad_term_id,\n",
        "                    'record_key': record_key,\n",
        "                    'source': 'combined'\n",
        "                }\n",
        "                class_mappings.append(mapping)\n",
        "        else:\n",
        "            # Create a placeholder mapping\n",
        "            mapping = {\n",
        "                'prediction_idx': idx,\n",
        "                'class_id': f\"PENDING_{course_code}_{section}_{acad_term_id}\",\n",
        "                'professor_id': None,\n",
        "                'course_code': course_code,\n",
        "                'section': section,\n",
        "                'acad_term_id': acad_term_id,\n",
        "                'record_key': record_key,\n",
        "                'source': 'not_found'\n",
        "            }\n",
        "            class_mappings.append(mapping)\n",
        "    \n",
        "    # Create summary\n",
        "    mappings_df = pd.DataFrame(class_mappings)\n",
        "    \n",
        "    if not mappings_df.empty:\n",
        "        print(f\"\\n📊 Mapping Summary:\")\n",
        "        print(f\"   Total mappings: {len(mappings_df)}\")\n",
        "        print(f\"   Unique predictions mapped: {mappings_df['prediction_idx'].nunique()}\")\n",
        "        print(f\"   Classes per prediction: {len(mappings_df) / mappings_df['prediction_idx'].nunique():.2f}\")\n",
        "        \n",
        "        if unmapped_courses:\n",
        "            print(f\"\\n⚠️ Courses without IDs: {len(unmapped_courses)}\")\n",
        "            print(f\"   Sample: {list(unmapped_courses)[:5]}\")\n",
        "    \n",
        "    return mappings_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b904ae42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Setup and Configuration for Catch-Up Processing\n",
        "print(\"=\"*60)\n",
        "print(\"CATCH-UP PROCESSING SETUP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Connect to database\n",
        "connection = connect_database()\n",
        "if not connection:\n",
        "    raise Exception(\"Failed to connect to database\")\n",
        "\n",
        "# Load cache data once\n",
        "data_cache = load_or_cache_data(connection, cache_dir)\n",
        "\n",
        "# Determine current time and processing range\n",
        "from datetime import datetime\n",
        "current_time = datetime.now()\n",
        "print(f\"📅 Current time: {current_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Get the full bidding schedule for the target term\n",
        "bidding_schedule = BIDDING_SCHEDULES.get(TARGET_AY_TERM, [])\n",
        "if not bidding_schedule:\n",
        "    raise ValueError(f\"No bidding schedule found for term '{TARGET_AY_TERM}'\")\n",
        "\n",
        "# Find the current live window (first window whose closing time hasn't passed)\n",
        "current_live_window = None\n",
        "for schedule_item in bidding_schedule:\n",
        "    if schedule_item[0] > current_time:\n",
        "        current_live_window = schedule_item[1]  # Full window name\n",
        "        break\n",
        "\n",
        "if not current_live_window:\n",
        "    # If all scheduled windows have passed, use the last one\n",
        "    current_live_window = bidding_schedule[-1][1]\n",
        "    print(f\"⚠️ All scheduled windows have passed. Using last window: {current_live_window}\")\n",
        "else:\n",
        "    print(f\"🎯 Current live window identified: {current_live_window}\")\n",
        "\n",
        "# Create processing range (all windows from start up to current live window)\n",
        "processing_range = []\n",
        "for schedule_item in bidding_schedule:\n",
        "    processing_range.append(schedule_item[1])  # Full window name\n",
        "    if schedule_item[1] == current_live_window:\n",
        "        break\n",
        "\n",
        "print(f\"📋 Processing range determined: {len(processing_range)} windows\")\n",
        "for i, window in enumerate(processing_range):\n",
        "    print(f\"   {i+1:2d}. {window}\")\n",
        "\n",
        "# Load existing predictions to determine what's already been processed\n",
        "existing_predictions_df = pd.DataFrame()\n",
        "\n",
        "# Try loading from database cache first\n",
        "if 'bid_prediction' in data_cache and not data_cache['bid_prediction'].empty:\n",
        "    existing_predictions_df = data_cache['bid_prediction'].copy()\n",
        "    print(f\"✅ Loaded {len(existing_predictions_df)} existing predictions from database cache\")\n",
        "elif connection:\n",
        "    try:\n",
        "        existing_predictions_df = pd.read_sql_query(\"SELECT * FROM bid_prediction\", connection)\n",
        "        print(f\"✅ Loaded {len(existing_predictions_df)} existing predictions from database\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not load existing predictions from database: {e}\")\n",
        "\n",
        "# Create set of processed window IDs for fast lookup\n",
        "processed_window_ids = set()\n",
        "if not existing_predictions_df.empty:\n",
        "    processed_window_ids = set(existing_predictions_df['bid_window_id'].unique())\n",
        "    print(f\"🔍 Found predictions for {len(processed_window_ids)} unique bid windows\")\n",
        "\n",
        "# Load raw data once (unfiltered)\n",
        "print(f\"\\n📂 Loading raw data for processing...\")\n",
        "raw_data_path = 'script_input/raw_data.xlsx'\n",
        "full_standalone_df = pd.read_excel(raw_data_path, sheet_name='standalone')\n",
        "full_multiple_df = pd.read_excel(raw_data_path, sheet_name='multiple')\n",
        "print(f\"✅ Loaded raw data: {len(full_standalone_df)} standalone, {len(full_multiple_df)} multiple records\")\n",
        "\n",
        "# Pre-load models once\n",
        "print(f\"\\n🤖 Pre-loading models...\")\n",
        "models = {\n",
        "    'classification': CatBoostClassifier(),\n",
        "    'median': CatBoostRegressor(),\n",
        "    'min': CatBoostRegressor()\n",
        "}\n",
        "\n",
        "model_paths = {\n",
        "    'classification': 'script_output/models/classification/production_classification_model.cbm',\n",
        "    'median': 'script_output/models/regression_median/production_regression_median_model.cbm',\n",
        "    'min': 'script_output/models/regression_min/production_regression_min_model.cbm'\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        model.load_model(model_paths[name])\n",
        "        print(f\"✅ Pre-loaded {name} model\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error pre-loading {name} model: {e}\")\n",
        "        raise\n",
        "\n",
        "# Load safety factor data\n",
        "median_sf_df = pd.read_csv('script_output/models/regression_median/median_bid_safety_factor_analysis.csv')\n",
        "min_sf_df = pd.read_csv('script_output/models/regression_min/min_bid_safety_factor_analysis.csv')\n",
        "\n",
        "# Find optimal safety factors\n",
        "median_optimal_idx = median_sf_df[median_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "min_optimal_idx = min_sf_df[min_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "median_optimal_sf = median_sf_df.iloc[median_optimal_idx]['safety_factor']\n",
        "min_optimal_sf = min_sf_df.iloc[min_optimal_idx]['safety_factor']\n",
        "\n",
        "print(f\"📊 Optimal safety factors loaded: median={median_optimal_sf:.2f}, min={min_optimal_sf:.2f}\")\n",
        "\n",
        "# Combine bid window data for lookups\n",
        "combined_bid_windows_df = pd.DataFrame()\n",
        "if 'bid_windows' in data_cache and not data_cache['bid_windows'].empty:\n",
        "    combined_bid_windows_df = data_cache['bid_windows'].copy()\n",
        "\n",
        "# Load new bid windows if available\n",
        "new_bid_window_path = Path('script_output/new_bid_window.csv')\n",
        "if new_bid_window_path.exists():\n",
        "    try:\n",
        "        new_bid_windows_df = pd.read_csv(new_bid_window_path)\n",
        "        combined_bid_windows_df = pd.concat([combined_bid_windows_df, new_bid_windows_df], ignore_index=True)\n",
        "        combined_bid_windows_df.drop_duplicates(subset=['acad_term_id', 'round', 'window'], keep='last', inplace=True)\n",
        "        print(f\"✅ Combined bid windows data: {len(combined_bid_windows_df)} total windows\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not load new_bid_window.csv: {e}\")\n",
        "\n",
        "print(f\"\\n🚀 Setup complete. Ready to process {len(processing_range)} windows.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b3a843",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Main Processing Loop for Catch-Up\n",
        "print(\"=\"*60)\n",
        "print(\"CHRONOLOGICAL PROCESSING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize master collections\n",
        "all_bid_predictions = []\n",
        "all_dataset_predictions = []\n",
        "all_safety_factors = []\n",
        "processed_windows = []\n",
        "skipped_windows = []\n",
        "\n",
        "# Initialize master collections for transformed data\n",
        "all_transformed_data = []\n",
        "all_metadata_records = []\n",
        "\n",
        "# Helper function to parse window name and get bid_window_id\n",
        "def get_bid_window_id_for_window(window_name, all_bid_windows_df, target_term):\n",
        "    \"\"\"Parse window name and lookup bid_window_id - FIXED VERSION with format conversion\"\"\"\n",
        "    import re\n",
        "    \n",
        "    # CONVERSION FUNCTION: Convert '2025-26_T1' to 'AY202526T1' format\n",
        "    def convert_target_term_format(target_term):\n",
        "        \"\"\"Convert TARGET_AY_TERM format to database format\"\"\"\n",
        "        # Handle format like '2025-26_T1' -> 'AY202526T1'\n",
        "        match = re.match(r'(\\d{4})-(\\d{2})_T(\\d+)', target_term)\n",
        "        if match:\n",
        "            start_year = match.group(1)  # 2025\n",
        "            end_year_suffix = match.group(2)  # 26\n",
        "            term_num = match.group(3)  # 1\n",
        "            \n",
        "            # Construct database format: AY + start_year + end_year_suffix + T + term_num\n",
        "            db_format = f\"AY{start_year}{end_year_suffix}T{term_num}\"\n",
        "            return db_format\n",
        "        \n",
        "        # If already in correct format or unknown format, return as-is\n",
        "        return target_term\n",
        "    \n",
        "    # Parse window name to extract round and window number\n",
        "    def parse_window_name(window_str):\n",
        "        # Handle Incoming Freshmen\n",
        "        if 'Incoming Freshmen' in window_str:\n",
        "            match = re.search(r'Rnd\\s+(\\d)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return f\"{match.group(1)}F\", int(match.group(2))\n",
        "        \n",
        "        # Handle Incoming Exchange\n",
        "        if 'Incoming Exchange' in window_str:\n",
        "            match = re.search(r'Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "        \n",
        "        # Standard format: \"Round 1A Window 2\"\n",
        "        match = re.search(r'Round\\s+(\\d[A-C]?)\\s+Window\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1), int(match.group(2))\n",
        "        \n",
        "        # Abbreviated format: \"Rnd 1A Win 2\"\n",
        "        match = re.search(r'Rnd\\s+(\\d[A-C]?)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1), int(match.group(2))\n",
        "        \n",
        "        return None, None\n",
        "    \n",
        "    round_val, window_num = parse_window_name(window_name)\n",
        "    if not round_val or not window_num:\n",
        "        print(f\"❌ Could not parse window name: {window_name}\")\n",
        "        return f\"PENDING_{window_name.replace(' ', '_')}\"\n",
        "    \n",
        "    # CRITICAL FIX: Convert the target_term format\n",
        "    target_term_id = convert_target_term_format(target_term)\n",
        "    \n",
        "    print(f\"🔍 Looking for: acad_term_id='{target_term_id}', round='{round_val}', window={window_num}\")\n",
        "    print(f\"   (Converted from TARGET_AY_TERM: '{target_term}' -> '{target_term_id}')\")\n",
        "    \n",
        "    # Look up in bid windows\n",
        "    if all_bid_windows_df.empty:\n",
        "        print(f\"❌ bid_windows_df is empty\")\n",
        "        return f\"PENDING_{target_term_id}_{round_val}_{window_num}\"\n",
        "    \n",
        "    # Debug: Show what we're matching against\n",
        "    matching_candidates = all_bid_windows_df[\n",
        "        all_bid_windows_df['acad_term_id'] == target_term_id\n",
        "    ]\n",
        "    print(f\"📋 Found {len(matching_candidates)} windows for term {target_term_id}\")\n",
        "    \n",
        "    matching_windows = all_bid_windows_df[\n",
        "        (all_bid_windows_df['acad_term_id'] == target_term_id) &\n",
        "        (all_bid_windows_df['round'].astype(str) == str(round_val)) &\n",
        "        (all_bid_windows_df['window'].astype(int) == window_num)\n",
        "    ]\n",
        "    \n",
        "    if not matching_windows.empty:\n",
        "        found_id = matching_windows.iloc[0]['id']\n",
        "        print(f\"✅ Found bid_window_id: {found_id}\")\n",
        "        return found_id\n",
        "    else:\n",
        "        print(f\"❌ No match found for round='{round_val}', window={window_num}\")\n",
        "        # Show available rounds/windows for debugging\n",
        "        if not matching_candidates.empty:\n",
        "            available = matching_candidates[['round', 'window', 'id']].head(10).values.tolist()\n",
        "            print(f\"   Available: {available}\")\n",
        "        return f\"PENDING_{target_term_id}_{round_val}_{window_num}\"\n",
        "\n",
        "# Start the chronological processing loop\n",
        "for window_idx, window_name in enumerate(processing_range):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"PROCESSING WINDOW {window_idx + 1}/{len(processing_range)}: {window_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Get bid_window_id for this window\n",
        "    bid_window_id = get_bid_window_id_for_window(window_name, combined_bid_windows_df, TARGET_AY_TERM)\n",
        "    \n",
        "    # Critical check: Skip if already processed\n",
        "    if bid_window_id in processed_window_ids:\n",
        "        print(f\"✅ SKIPPING '{window_name}' - predictions already exist (bid_window_id: {bid_window_id})\")\n",
        "        skipped_windows.append(window_name)\n",
        "        continue\n",
        "    \n",
        "    print(f\"🔄 PROCESSING '{window_name}' - no existing predictions found\")\n",
        "    \n",
        "    # Filter data for current window\n",
        "    current_bidding_data = full_standalone_df[\n",
        "        full_standalone_df['bidding_window'] == window_name\n",
        "    ].copy()\n",
        "    \n",
        "    if current_bidding_data.empty:\n",
        "        print(f\"⚠️ No data found for window '{window_name}'. Skipping.\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"📊 Found {len(current_bidding_data)} records for this window\")\n",
        "    \n",
        "    # Data enrichment (replicate original logic)\n",
        "    current_bidding_data = current_bidding_data[\n",
        "        current_bidding_data['bidding_window'].notna() & current_bidding_data['total'].notna()\n",
        "    ].copy()\n",
        "    current_bidding_data['before_process_vacancy'] = current_bidding_data['total'] - current_bidding_data['current_enrolled']\n",
        "    \n",
        "    # Get instructor information from multiple sheet\n",
        "    instructor_map = {}\n",
        "    window_keys = current_bidding_data['record_key'].unique()\n",
        "    filtered_multiple_df = full_multiple_df[full_multiple_df['record_key'].isin(window_keys)].copy()\n",
        "    \n",
        "    for record_key, group in filtered_multiple_df.groupby('record_key'):\n",
        "        professors = group['professor_name'].dropna().unique()\n",
        "        if len(professors) > 0:\n",
        "            instructor_map[record_key] = professors.tolist()\n",
        "    current_bidding_data['instructor'] = current_bidding_data['record_key'].map(lambda x: instructor_map.get(x, []))\n",
        "    \n",
        "    # Get day of week and start time\n",
        "    day_map = {}\n",
        "    time_map = {}\n",
        "    for record_key, group in filtered_multiple_df[filtered_multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        days = group['day_of_week'].dropna().unique()\n",
        "        if len(days) > 0: \n",
        "            day_map[record_key] = ', '.join(days)\n",
        "        \n",
        "        times = group['start_time'].dropna()\n",
        "        if len(times) > 0: \n",
        "            time_map[record_key] = times.iloc[0]\n",
        "    \n",
        "    current_bidding_data['day_of_week'] = current_bidding_data['record_key'].map(lambda x: day_map.get(x, ''))\n",
        "    current_bidding_data['start_time'] = current_bidding_data['record_key'].map(lambda x: time_map.get(x, ''))\n",
        "    \n",
        "    # Reset index for processing\n",
        "    current_bidding_data.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    # Transform features using a new transformer instance\n",
        "    print(f\"🔧 Transforming features...\")\n",
        "    transformer = SMUBiddingTransformer()\n",
        "    X_transformed = transformer.fit_transform(current_bidding_data)\n",
        "    print(f\"✅ Transformed {len(X_transformed)} records with {X_transformed.shape[1]} features\")\n",
        "    \n",
        "    # === CONSOLIDATION LOGIC: COLLECT DATA, DON'T SAVE ===\n",
        "    # Add identifiers and bidding window context to the transformed data\n",
        "    X_transformed_with_ids = X_transformed.copy()\n",
        "    X_transformed_with_ids['bidding_window'] = window_name # Add window name for context\n",
        "    X_transformed_with_ids['course_code'] = current_bidding_data['course_code']\n",
        "    X_transformed_with_ids['section'] = current_bidding_data['section']\n",
        "    X_transformed_with_ids['acad_term_id'] = current_bidding_data['acad_term_id']\n",
        "    X_transformed_with_ids['record_key'] = current_bidding_data['record_key']\n",
        "    all_transformed_data.append(X_transformed_with_ids)\n",
        "\n",
        "    # Collect metadata for this window\n",
        "    feature_cols = [col for col in X_transformed.columns if col not in ['course_code', 'section', 'acad_term_id', 'record_key']]\n",
        "    metadata_record = {\n",
        "        'bidding_window': window_name,\n",
        "        'total_rows': len(X_transformed),\n",
        "        'total_features': len(feature_cols),\n",
        "        'categorical_features': transformer.get_categorical_features(),\n",
        "        'numeric_features': transformer.get_numeric_features(),\n",
        "        'transformation_timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "    all_metadata_records.append(metadata_record)\n",
        "    print(f\"✅ Collected transformed data and metadata for '{window_name}'\")\n",
        "\n",
        "    # Map to classes\n",
        "    print(f\"🔗 Mapping to classes...\")\n",
        "    class_mappings = map_classes_to_predictions(current_bidding_data, data_cache, connection)\n",
        "    print(f\"✅ Created {len(class_mappings)} class mappings\")\n",
        "    \n",
        "    # Generate predictions using pre-loaded models - FIXED VERSION\n",
        "    print(f\"🤖 Generating predictions...\")\n",
        "    \n",
        "    # Prepare prediction data - FIXED to handle None values\n",
        "    available_features = [col for col in X_transformed.columns if col in [\n",
        "        'subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy',\n",
        "        'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor',\n",
        "        'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun'\n",
        "    ]]\n",
        "    prediction_data = X_transformed[available_features].copy()\n",
        "    \n",
        "    # CRITICAL FIX: Handle None values before prediction\n",
        "    print(f\"🔍 Checking for problematic values before prediction...\")\n",
        "    \n",
        "    # Check for None values and replace them\n",
        "    for col in prediction_data.columns:\n",
        "        none_count = prediction_data[col].isnull().sum()\n",
        "        none_values = prediction_data[col].apply(lambda x: x is None).sum()\n",
        "        \n",
        "        if none_count > 0 or none_values > 0:\n",
        "            print(f\"⚠️ Found {none_count} nulls + {none_values} None values in column '{col}'\")\n",
        "            \n",
        "            # Replace None/null values based on column type\n",
        "            if prediction_data[col].dtype == 'object':\n",
        "                prediction_data[col] = prediction_data[col].fillna('Unknown')\n",
        "                prediction_data[col] = prediction_data[col].apply(lambda x: 'Unknown' if x is None else x)\n",
        "            else:\n",
        "                prediction_data[col] = prediction_data[col].fillna(0)\n",
        "                prediction_data[col] = prediction_data[col].apply(lambda x: 0 if x is None else x)\n",
        "            \n",
        "            print(f\"✅ Fixed {col}\")\n",
        "    \n",
        "    # Convert any remaining object columns that should be numeric\n",
        "    for col in prediction_data.columns:\n",
        "        if prediction_data[col].dtype == 'object':\n",
        "            # Try to convert to numeric if possible\n",
        "            try:\n",
        "                numeric_version = pd.to_numeric(prediction_data[col], errors='coerce')\n",
        "                if not numeric_version.isnull().all():\n",
        "                    prediction_data[col] = numeric_version.fillna(0)\n",
        "                    print(f\"✅ Converted {col} to numeric\")\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    # Final check\n",
        "    print(f\"📊 Final prediction data shape: {prediction_data.shape}\")\n",
        "    print(f\"📊 Data types: {prediction_data.dtypes.to_dict()}\")\n",
        "    \n",
        "    try:\n",
        "        # Classification predictions\n",
        "        clf_pred = models['classification'].predict(prediction_data)\n",
        "        clf_proba = models['classification'].predict_proba(prediction_data)\n",
        "        \n",
        "        # Regression predictions  \n",
        "        median_pred = models['median'].predict(prediction_data)\n",
        "        min_pred = models['min'].predict(prediction_data)\n",
        "        \n",
        "        print(f\"✅ Generated predictions: {len(clf_pred)} classification, {len(median_pred)} median, {len(min_pred)} min\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during prediction for {window_name}: {e}\")\n",
        "        print(f\"🔍 Prediction data sample:\")\n",
        "        print(prediction_data.head())\n",
        "        print(f\"🔍 Prediction data info:\")\n",
        "        print(prediction_data.info())\n",
        "        continue\n",
        "    \n",
        "    # Calculate uncertainties and confidence\n",
        "    print(f\"📊 Calculating uncertainties...\")\n",
        "    \n",
        "    # Classification confidence\n",
        "    def calculate_entropy_confidence(probabilities):\n",
        "        epsilon = 1e-10\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
        "        max_entropy = -np.log(1/probabilities.shape[1])\n",
        "        confidence_score = 1 - (entropy / max_entropy)\n",
        "        return confidence_score\n",
        "    \n",
        "    confidence_scores = calculate_entropy_confidence(clf_proba)\n",
        "    \n",
        "    # Regression uncertainties\n",
        "    uncertainties = {}\n",
        "    for model_name in ['median', 'min']:\n",
        "        model = models[model_name]\n",
        "        n_trees = model.tree_count_\n",
        "        n_subsets = 10\n",
        "        trees_per_subset = max(1, n_trees // n_subsets)\n",
        "        \n",
        "        subset_predictions = []\n",
        "        for i in range(n_subsets):\n",
        "            tree_start = i * trees_per_subset\n",
        "            tree_end = min((i + 1) * trees_per_subset, n_trees)\n",
        "            if tree_start < n_trees:\n",
        "                partial_pred = model.predict(prediction_data, \n",
        "                                           ntree_start=tree_start, \n",
        "                                           ntree_end=tree_end)\n",
        "                subset_predictions.append(partial_pred)\n",
        "        \n",
        "        uncertainties[model_name] = np.std(subset_predictions, axis=0)\n",
        "    \n",
        "    # Create bid predictions for this window\n",
        "    window_bid_predictions = []\n",
        "    for idx in range(len(current_bidding_data)):\n",
        "        matching_mappings = class_mappings[class_mappings['prediction_idx'] == idx]\n",
        "        \n",
        "        for _, mapping in matching_mappings.iterrows():\n",
        "            if mapping['source'] == 'not_found' and str(mapping['class_id']).startswith('PENDING_'):\n",
        "                continue\n",
        "                \n",
        "            bid_prediction = {\n",
        "                'class_id': mapping['class_id'],\n",
        "                'bid_window_id': bid_window_id,\n",
        "                'model_version': 'v4.0',\n",
        "                'clf_has_bids_prob': float(clf_proba[idx, 1]),\n",
        "                'clf_confidence_score': float(confidence_scores[idx]),\n",
        "                'median_predicted': float(median_pred[idx]),\n",
        "                'median_uncertainty': float(uncertainties['median'][idx]),\n",
        "                'min_predicted': float(min_pred[idx]),\n",
        "                'min_uncertainty': float(uncertainties['min'][idx])\n",
        "            }\n",
        "            window_bid_predictions.append(bid_prediction)\n",
        "    \n",
        "    # Create dataset predictions for this window\n",
        "    window_dataset_predictions = []\n",
        "    for idx in range(len(X_transformed)):\n",
        "        row_features = X_transformed.iloc[idx].to_dict()\n",
        "        bidding_row = current_bidding_data.iloc[idx]\n",
        "        row_features['course_code'] = bidding_row['course_code']\n",
        "        row_features['section'] = bidding_row['section']\n",
        "        row_features['acad_term_id'] = bidding_row['acad_term_id']\n",
        "        row_features['bidding_window'] = window_name\n",
        "        \n",
        "        pred_row = {\n",
        "            **row_features,\n",
        "            'clf_has_bids_prob': float(clf_proba[idx, 1]),\n",
        "            'clf_confidence_score': float(confidence_scores[idx]),\n",
        "            'median_predicted': float(median_pred[idx]),\n",
        "            'median_uncertainty': float(uncertainties['median'][idx]),\n",
        "            'min_predicted': float(min_pred[idx]),\n",
        "            'min_uncertainty': float(uncertainties['min'][idx])\n",
        "        }\n",
        "        window_dataset_predictions.append(pred_row)\n",
        "    \n",
        "    # Add to master collections\n",
        "    all_bid_predictions.extend(window_bid_predictions)\n",
        "    all_dataset_predictions.extend(window_dataset_predictions)\n",
        "    processed_windows.append(window_name)\n",
        "    \n",
        "    print(f\"✅ Window '{window_name}' completed:\")\n",
        "    print(f\"   - Bid predictions: {len(window_bid_predictions)}\")\n",
        "    print(f\"   - Dataset predictions: {len(window_dataset_predictions)}\")\n",
        "\n",
        "# === CONSOLIDATION LOGIC: SAVE ALL COLLECTED DATA AT ONCE ===\n",
        "# After the loop, consolidate and save the collected data\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CONSOLIDATING AND SAVING TRANSFORMED DATA\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "if all_transformed_data:\n",
        "    # Combine all transformed dataframes into one\n",
        "    final_transformed_df = pd.concat(all_transformed_data, ignore_index=True)\n",
        "\n",
        "    # Reorder columns to put identifiers first\n",
        "    id_cols = ['record_key', 'course_code', 'section', 'acad_term_id', 'bidding_window']\n",
        "    feature_cols = [col for col in final_transformed_df.columns if col not in id_cols]\n",
        "    final_transformed_df = final_transformed_df[id_cols + feature_cols]\n",
        "\n",
        "    # Save to a single CSV\n",
        "    transformed_output_path = output_dir / f'transformed_features_{timestamp}.csv'\n",
        "    final_transformed_df.to_csv(transformed_output_path, index=False)\n",
        "    print(f\"💾 Consolidated transformed features saved to: {transformed_output_path}\")\n",
        "    print(f\"   - Total records: {len(final_transformed_df)}\")\n",
        "else:\n",
        "    print(\"⚠️ No transformed data was generated to save.\")\n",
        "\n",
        "if all_metadata_records:\n",
        "    # Save all metadata records to a single JSON file\n",
        "    metadata_output_path = output_dir / f'transformation_metadata_{timestamp}.json'\n",
        "    with open(metadata_output_path, 'w') as f:\n",
        "        json.dump(all_metadata_records, f, indent=2)\n",
        "    print(f\"📋 Consolidated metadata saved to: {metadata_output_path}\")\n",
        "    print(f\"   - Total windows processed: {len(all_metadata_records)}\")\n",
        "else:\n",
        "    print(\"⚠️ No metadata was generated to save.\")\n",
        "\n",
        "# Create final DataFrames\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"FINALIZING RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "bid_predictions_df = pd.DataFrame(all_bid_predictions)\n",
        "dataset_predictions_df = pd.DataFrame(all_dataset_predictions)\n",
        "\n",
        "# Create safety factor table (comprehensive 1-99 percentiles)\n",
        "if processed_windows and not dataset_predictions_df.empty:\n",
        "    acad_term_id = dataset_predictions_df['acad_term_id'].iloc[0]\n",
        "    \n",
        "    print(f\"Creating comprehensive safety factor table for academic term: {acad_term_id}\")\n",
        "    \n",
        "    # Load validation results for error analysis\n",
        "    try:\n",
        "        from scipy import stats\n",
        "        \n",
        "        median_dir = Path('script_output/models/regression_median')\n",
        "        min_dir = Path('script_output/models/regression_min')\n",
        "        \n",
        "        median_results = pd.read_csv(median_dir / \"regression_median_validation_results.csv\")\n",
        "        min_results = pd.read_csv(min_dir / \"regression_min_validation_results.csv\")\n",
        "        \n",
        "        median_errors = median_results['residuals'].values\n",
        "        min_errors = min_results['residuals'].values\n",
        "        \n",
        "        print(f\"✅ Loaded validation results: {len(median_results)} median, {len(min_results)} min samples\")\n",
        "        \n",
        "        # Fit t-distributions for both models\n",
        "        def fit_t_distribution(errors):\n",
        "            \"\"\"Fit a t-distribution to the error data and return parameters\"\"\"\n",
        "            clean_errors = errors[np.isfinite(errors)]\n",
        "            params = stats.t.fit(clean_errors)\n",
        "            df, loc, scale = params\n",
        "            return df, loc, scale, params\n",
        "        \n",
        "        def calculate_percentile_multipliers(errors, df, loc, scale, prediction_type):\n",
        "            \"\"\"Calculate uncertainty multipliers for every percentile (1-99)\"\"\"\n",
        "            print(f\"Calculating percentile multipliers for {prediction_type}...\")\n",
        "            \n",
        "            clean_errors = errors[np.isfinite(errors)]\n",
        "            \n",
        "            # Calculate percentiles from 1% to 99% (ORIGINAL RANGE)\n",
        "            percentiles = range(1, 100)\n",
        "            multiplier_results = []\n",
        "            \n",
        "            for percentile in percentiles:\n",
        "                # Calculate empirical percentile from actual error data\n",
        "                empirical_value = np.percentile(clean_errors, percentile)\n",
        "                \n",
        "                # Calculate theoretical t-distribution percentile\n",
        "                theoretical_percentile = percentile / 100.0\n",
        "                theoretical_value = stats.t.ppf(theoretical_percentile, df, loc, scale)\n",
        "                \n",
        "                # Calculate multipliers (how many standard deviations from mean)\n",
        "                if scale > 0:\n",
        "                    empirical_multiplier = (empirical_value - loc) / scale\n",
        "                    theoretical_multiplier = (theoretical_value - loc) / scale\n",
        "                else:\n",
        "                    empirical_multiplier = 0\n",
        "                    theoretical_multiplier = 0\n",
        "                \n",
        "                multiplier_results.append({\n",
        "                    'prediction_type': prediction_type,\n",
        "                    'beats_percentage': percentile,\n",
        "                    'empirical_multiplier': empirical_multiplier,\n",
        "                    'theoretical_multiplier': theoretical_multiplier,\n",
        "                    'empirical_error_value': empirical_value,\n",
        "                    'theoretical_error_value': theoretical_value\n",
        "                })\n",
        "            \n",
        "            return multiplier_results\n",
        "        \n",
        "        # Fit t-distributions\n",
        "        print(\"\\nFitting t-distributions...\")\n",
        "        median_df, median_loc, median_scale, median_params = fit_t_distribution(median_errors)\n",
        "        min_df, min_loc, min_scale, min_params = fit_t_distribution(min_errors)\n",
        "        \n",
        "        print(f\"Median model t-distribution: df={median_df:.4f}, loc={median_loc:.4f}, scale={median_scale:.4f}\")\n",
        "        print(f\"Min model t-distribution: df={min_df:.4f}, loc={min_loc:.4f}, scale={min_scale:.4f}\")\n",
        "        \n",
        "        # Calculate multipliers for both models (1-99 percentiles)\n",
        "        print(\"\\nCalculating uncertainty multipliers for all percentiles 1-99...\")\n",
        "        median_multipliers = calculate_percentile_multipliers(\n",
        "            median_errors, median_df, median_loc, median_scale, \"median\"\n",
        "        )\n",
        "        min_multipliers = calculate_percentile_multipliers(\n",
        "            min_errors, min_df, min_loc, min_scale, \"min\"\n",
        "        )\n",
        "        \n",
        "        # Combine all multipliers\n",
        "        all_multipliers = median_multipliers + min_multipliers\n",
        "        \n",
        "        # Create safety factor table entries (BOTH empirical and theoretical)\n",
        "        safety_factor_entries = []\n",
        "        \n",
        "        for multiplier_data in all_multipliers:\n",
        "            # Add empirical multiplier entry\n",
        "            empirical_entry = {\n",
        "                'acad_term_id': acad_term_id,\n",
        "                'prediction_type': multiplier_data['prediction_type'],\n",
        "                'beats_percentage': multiplier_data['beats_percentage'],\n",
        "                'multiplier': float(multiplier_data['empirical_multiplier']),\n",
        "                'multiplier_type': 'empirical'\n",
        "            }\n",
        "            safety_factor_entries.append(empirical_entry)\n",
        "            \n",
        "            # Add theoretical multiplier entry\n",
        "            theoretical_entry = {\n",
        "                'acad_term_id': acad_term_id,\n",
        "                'prediction_type': multiplier_data['prediction_type'],\n",
        "                'beats_percentage': multiplier_data['beats_percentage'],\n",
        "                'multiplier': float(multiplier_data['theoretical_multiplier']),\n",
        "                'multiplier_type': 'theoretical'\n",
        "            }\n",
        "            safety_factor_entries.append(theoretical_entry)\n",
        "        \n",
        "        safety_factor_df = pd.DataFrame(safety_factor_entries)\n",
        "        \n",
        "        print(f\"✅ Created {len(safety_factor_df)} safety factor entries\")\n",
        "        print(f\"   - 99 percentiles × 2 multiplier types × 2 prediction types = {99*2*2} entries\")\n",
        "        print(f\"   - Academic term: {acad_term_id}\")\n",
        "        \n",
        "        # Display sample of key percentiles for verification\n",
        "        print(f\"\\n📊 Sample of key percentiles:\")\n",
        "        key_percentiles = [80, 85, 90, 95, 99]\n",
        "        \n",
        "        for pred_type in ['median', 'min']:\n",
        "            print(f\"\\n{pred_type.title()} model multipliers:\")\n",
        "            for percentile in key_percentiles:\n",
        "                empirical_row = safety_factor_df[\n",
        "                    (safety_factor_df['prediction_type'] == pred_type) & \n",
        "                    (safety_factor_df['beats_percentage'] == percentile) &\n",
        "                    (safety_factor_df['multiplier_type'] == 'empirical')\n",
        "                ]\n",
        "                theoretical_row = safety_factor_df[\n",
        "                    (safety_factor_df['prediction_type'] == pred_type) & \n",
        "                    (safety_factor_df['beats_percentage'] == percentile) &\n",
        "                    (safety_factor_df['multiplier_type'] == 'theoretical')\n",
        "                ]\n",
        "                \n",
        "                if not empirical_row.empty and not theoretical_row.empty:\n",
        "                    emp_mult = empirical_row['multiplier'].iloc[0]\n",
        "                    theo_mult = theoretical_row['multiplier'].iloc[0]\n",
        "                    print(f\"  {percentile:2d}%: empirical={emp_mult:6.3f}, theoretical={theo_mult:6.3f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not create safety factor table: {e}\")\n",
        "        safety_factor_df = pd.DataFrame()\n",
        "else:\n",
        "    safety_factor_df = pd.DataFrame()\n",
        "\n",
        "# Save results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "if not bid_predictions_df.empty:\n",
        "    bid_predictions_df.to_csv(output_dir / f'bid_predictions{timestamp}.csv', index=False)\n",
        "    print(f\"💾 Saved {len(bid_predictions_df)} bid predictions\")\n",
        "\n",
        "if not dataset_predictions_df.empty:\n",
        "    dataset_predictions_df.to_csv(output_dir / f'dataset_predictions{timestamp}.csv', index=False)\n",
        "    print(f\"💾 Saved {len(dataset_predictions_df)} dataset predictions\")\n",
        "\n",
        "if not safety_factor_df.empty:\n",
        "    safety_factor_df.to_csv(output_dir / f'safety_factor_table{timestamp}.csv', index=False)\n",
        "    print(f\"💾 Saved {len(safety_factor_df)} safety factor entries\")\n",
        "\n",
        "# Create comprehensive summary\n",
        "summary = {\n",
        "    'timestamp': timestamp,\n",
        "    'processing_mode': 'catch_up',\n",
        "    'target_term': TARGET_AY_TERM,\n",
        "    'total_windows_in_range': len(processing_range),\n",
        "    'windows_processed': len(processed_windows),\n",
        "    'windows_skipped': len(skipped_windows),\n",
        "    'processed_windows': processed_windows,\n",
        "    'skipped_windows': skipped_windows,\n",
        "    'total_bid_predictions': len(bid_predictions_df) if not bid_predictions_df.empty else 0,\n",
        "    'total_dataset_predictions': len(dataset_predictions_df) if not dataset_predictions_df.empty else 0,\n",
        "    'unique_classes_predicted': bid_predictions_df['class_id'].nunique() if not bid_predictions_df.empty else 0,\n",
        "    'safety_factor_entries': len(safety_factor_df) if not safety_factor_df.empty else 0\n",
        "}\n",
        "\n",
        "with open(output_dir / f'prediction_summary_{timestamp}.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# Final report\n",
        "print(f\"\\n🎉 CATCH-UP PROCESSING COMPLETED!\")\n",
        "print(f\"📊 Summary:\")\n",
        "print(f\"   - Windows in range: {summary['total_windows_in_range']}\")\n",
        "print(f\"   - Windows processed: {summary['windows_processed']}\")\n",
        "print(f\"   - Windows skipped: {summary['windows_skipped']}\")\n",
        "print(f\"   - Total bid predictions: {summary['total_bid_predictions']}\")\n",
        "print(f\"   - Unique classes: {summary['unique_classes_predicted']}\")\n",
        "\n",
        "if processed_windows:\n",
        "    print(f\"\\n✅ Processed windows:\")\n",
        "    for window in processed_windows:\n",
        "        print(f\"   - {window}\")\n",
        "\n",
        "if skipped_windows:\n",
        "    print(f\"\\n⏩ Skipped windows (already had predictions):\")\n",
        "    for window in skipped_windows:\n",
        "        print(f\"   - {window}\")\n",
        "\n",
        "# Close database connection\n",
        "if connection:\n",
        "    connection.close()\n",
        "    print(f\"\\n🔒 Database connection closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bidly_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
