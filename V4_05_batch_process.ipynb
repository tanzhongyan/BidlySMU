{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1cfa91a",
      "metadata": {},
      "source": [
        "# **SMU Course Bidding Prediction Using CatBoost V4**\n",
        "\n",
        "<div style=\"background-color:#DFFFD6; padding:12px; border-radius:5px; border: 1px solid #228B22;\">\n",
        "   <h2 style=\"color:#006400;\">âœ… Looking to Implement This? âœ…</h2>\n",
        "   <p>ðŸš€ **Get started quickly by using** <strong><a href=\"example_prediction.ipynb\">example_prediction.ipynb</a></strong>.</p> \n",
        "   <ul> \n",
        "      <li>ðŸ“Œ **Three pre-trained CatBoost models (`.cbm`) available for instant predictions.**</li>\n",
        "      <li>ðŸ”§ Includes **step-by-step instructions** for making predictions with uncertainty quantification.</li>\n",
        "      <li>âš¡ Works **out-of-the-box**â€”just load the models and start predicting!</li>\n",
        "   </ul>\n",
        "   <h3>ðŸ”— ðŸ“Œ Next Steps:</h3>\n",
        "   <p>ðŸ‘‰ <a href=\"example_prediction.ipynb\"><strong>Go to Example Prediction Notebook</strong></a></p>\n",
        "</div> \n",
        "<h2><span style=\"color:red\">NOTE: use at your own discretion.</span></h2>\n",
        "\n",
        "### **Changes in V4**\n",
        "- **Three-model architecture**: Added a classification model to predict whether a course will receive bids, complementing the existing median and min bid regression models\n",
        "- **Advanced uncertainty quantification**: Implemented entropy-based confidence scoring for classification and bootstrap-based confidence intervals for regression models\n",
        "- **Enhanced feature engineering**: Incorporated day-of-week boolean flags (`has_mon`, `has_tue`, etc.) for better temporal pattern recognition\n",
        "- **Asymmetric loss function**: Custom loss that penalizes under-predictions more heavily than over-predictions, crucial for bidding strategy\n",
        "- **Comprehensive evaluation suite**: Added confidence interval coverage analysis, residual analysis with emphasis on under-predictions, and cross-model feature importance comparison\n",
        "\n",
        "### **Objective**\n",
        "This notebook predicts bidding outcomes for courses in the SMU bidding system using **three specialized CatBoost models**. Building on insights from **V1, V2, and V3**, this version introduces a comprehensive **multi-model approach** with advanced uncertainty quantification:\n",
        "\n",
        "1. **Classification Model**: Predicts whether a course will receive bids (optimized for high recall)\n",
        "2. **Median Bid Regression Model**: Predicts the median bid price with confidence intervals\n",
        "3. **Min Bid Regression Model**: Predicts the minimum bid price with confidence intervals\n",
        "\n",
        "### **Key Enhancements in V4**\n",
        "\n",
        "**Learning from V3:**\n",
        "   - V3 focused on two regression models for median and min bid prediction\n",
        "   - V4 adds a **classification component** to identify courses that will receive bidding activity\n",
        "   - Enhanced with **probabilistic predictions** and **confidence scoring**\n",
        "\n",
        "**New V4 Features:**\n",
        "   - **Entropy-based confidence scoring** for classification predictions with five confidence levels (Very Low to Very High)\n",
        "   - **Bootstrap sampling** (100 iterations) for robust confidence interval estimation\n",
        "   - **Asymmetric loss function** (Î±=2.0) that heavily penalizes dangerous under-predictions\n",
        "   - **Comprehensive uncertainty analysis** including interval width and coverage metrics\n",
        "\n",
        "### **Three-Model Architecture**\n",
        "\n",
        "| **Model Type** | **Purpose** | **Output** | **Uncertainty Measure** |\n",
        "|----------------|-------------|------------|-------------------------|\n",
        "| **Classification** | Predict bid courses | Probability + Confidence Level | Entropy-based confidence score |\n",
        "| **Median Bid Regression** | Predict median bid price | Price + 95% CI | Bootstrap confidence intervals |\n",
        "| **Min Bid Regression** | Predict minimum bid price | Price + 95% CI | Bootstrap confidence intervals |\n",
        "\n",
        "### **Updated Dataset Features**\n",
        "\n",
        "| **Feature Name** | **Type** | **Description** |\n",
        "|------------------|----------|-----------------|\n",
        "| **`subject_area`** | Categorical | Subject area (IS, ECON, etc.) |\n",
        "| **`catalogue_no`** | Categorical | Course number |\n",
        "| **`round`** | Categorical | Bidding round (1, 1A, 1B, 1C, 2, 2A) |\n",
        "| **`window`** | Numerical | Bidding window (1-5) |\n",
        "| **`before_process_vacancy`** | Numerical | Available spots before bidding |\n",
        "| **`acad_year_start`** | Numerical | Academic year start |\n",
        "| **`term`** | Categorical | Academic term (1, 2, 3A, 3B) |\n",
        "| **`start_time`** | Categorical | Class start time |\n",
        "| **`course_name`** | Categorical | Course name/description |\n",
        "| **`section`** | Categorical | Course section |\n",
        "| **`instructor`** | Categorical | Instructor name |\n",
        "| **`has_mon`** - **`has_sun`** | Boolean | Day-of-week indicators |\n",
        "| **ðŸŽ¯ Target Variables ðŸŽ¯** | | **Model outputs** |\n",
        "| **`bids`** | Binary | Whether course receives bids |\n",
        "| **`target_median_bid`** | Numerical | Median bid price |\n",
        "| **`target_min_bid`** | Numerical | Minimum bid price |\n",
        "\n",
        "### **Advanced Uncertainty Quantification**\n",
        "\n",
        "**Classification Confidence:**\n",
        "- **Entropy-based scoring**: Measures prediction certainty using information entropy\n",
        "- **Five confidence levels**: Very Low, Low, Medium, High, Very High\n",
        "- **Probability outputs**: Separate probabilities for bid/non-bid outcomes\n",
        "\n",
        "**Regression Confidence Intervals:**\n",
        "- **Bootstrap sampling**: 100 model iterations for robust uncertainty estimation\n",
        "- **95% confidence intervals**: Upper and lower bounds for each prediction\n",
        "- **Interval width analysis**: Wider intervals indicate higher uncertainty\n",
        "\n",
        "### **Methodology**\n",
        "The notebook follows this enhanced structure:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Loading separate datasets for classification and regression tasks\n",
        "   - Feature standardization and categorical encoding\n",
        "   - Train-test splitting with consistent random seeds\n",
        "\n",
        "2. **Three-Model Training**:\n",
        "   - **Classification**: CatBoost with recall optimization for bid opportunity detection\n",
        "   - **Median Regression**: CatBoost with bootstrap uncertainty quantification\n",
        "   - **Min Regression**: CatBoost with asymmetric loss for under-prediction penalties\n",
        "\n",
        "3. **Advanced Evaluation**:\n",
        "   - **Classification**: Recall (maximizing true positives for bid detection), confusion matrix, entropy-based confidence analysis\n",
        "   - **Regression**: MSE, MAE, RÂ², asymmetric MSE, confidence interval coverage\n",
        "   - **Cross-model feature importance comparison**\n",
        "\n",
        "4. **Comprehensive Visualization**:\n",
        "   - Confidence distribution plots and uncertainty analysis\n",
        "   - Residual analysis with under-prediction emphasis\n",
        "   - Feature importance rankings across all three models\n",
        "\n",
        "5. **Model Persistence and Reporting**:\n",
        "   - All models saved as `.cbm` files for deployment\n",
        "   - Detailed results exported to CSV format\n",
        "   - Comprehensive summary report generation\n",
        "\n",
        "### **Key Metrics and Performance**\n",
        "\n",
        "**Classification Model:**\n",
        "- **Primary metric**: Recall (optimized for capturing all bidding opportunities - maximizing true positives)\n",
        "- **Confidence analysis**: Distribution of entropy-based confidence scores  \n",
        "- **Output**: Probabilities for bid/no-bid outcomes, confidence levels, and entropy values\n",
        "\n",
        "**Regression Models:**\n",
        "- **Standard metrics**: MSE, MAE, RÂ² for model accuracy\n",
        "- **Asymmetric MSE**: Custom metric penalizing under-predictions (Î±=2.0)\n",
        "- **Uncertainty metrics**: Mean confidence interval width and coverage percentage\n",
        "- **Safety analysis**: Percentage of dangerous under-predictions\n",
        "\n",
        "### **Classification Strategy - Maximizing Bidding Opportunities**\n",
        "\n",
        "**Recall-Optimized Approach:**\n",
        "- **Target**: Predict courses that will receive bids (positive class = 1)\n",
        "- **Primary Goal**: Maximize recall to capture all potential bidding opportunities\n",
        "- **Business Logic**: Missing a course that will receive bids (False Negative) is more costly than incorrectly predicting a course will receive bids (False Positive)\n",
        "- **Optimization**: Model trained to minimize missed bidding opportunities while maintaining reasonable precision\n",
        "\n",
        "### **Implementation Notes**\n",
        "To run this V4 notebook:\n",
        "- Install required packages: `pip install catboost pandas numpy matplotlib seaborn scikit-learn scipy`\n",
        "- Ensure you have the three required datasets:\n",
        "  - Classification training/test data\n",
        "  - Median bid regression training/test data\n",
        "  - Min bid regression training/test data\n",
        "- Models automatically save to `script_output_model_training/mode/` directory\n",
        "\n",
        "### **V4 Advantages**\n",
        "- **Comprehensive coverage**: Handles both bid opportunity detection and price prediction\n",
        "- **Risk-aware predictions**: Asymmetric loss prevents dangerous under-bidding\n",
        "- **Confidence-calibrated**: Provides uncertainty measures for better decision-making\n",
        "- **Feature-rich analysis**: Cross-model feature importance for strategic insights\n",
        "- **Production-ready**: All models saved with consistent interfaces for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8520404",
      "metadata": {},
      "source": [
        "## **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359f5072",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import os\n",
        "import psycopg2\n",
        "from dotenv import load_dotenv\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import json\n",
        "import hashlib\n",
        "from typing import List\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add database configuration\n",
        "load_dotenv()\n",
        "db_config = {\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'database': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'port': int(os.getenv('DB_PORT', 5432)),\n",
        "    'gssencmode': 'disable'\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "output_dir = Path('script_output/predictions')\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "cache_dir = Path('db_cache')\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309e5d9e",
      "metadata": {},
      "source": [
        "## **2. SMUBiddingTransformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ccf08c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SMUBiddingTransformer:\n",
        "    \"\"\"\n",
        "    A reusable transformer class for processing SMU course bidding data\n",
        "    optimized for CatBoost model.\n",
        "    \n",
        "    Uses categorical encoding for instructors and one-hot encoding for multi-valued days.\n",
        "    \n",
        "    Expected input columns:\n",
        "    - course_code: str (e.g. 'MGMT715', 'COR-COMM175')\n",
        "    - course_name: str\n",
        "    - acad_year_start: int\n",
        "    - term: str ('1', '2', '3A', '3B')\n",
        "    - start_time: str (e.g. '19:30', 'TBA') - preserved as categorical\n",
        "    - day_of_week: str (can be multivalued, e.g. 'Mon,Thu')\n",
        "    - before_process_vacancy: int\n",
        "    - bidding_window: str (e.g. 'Round 1 Window 1', 'Incoming Freshmen Rnd 1 Win 4')\n",
        "    - instructor: str (can be multivalued, e.g. 'JOHN DOE, JANE SMITH')\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the transformer for CatBoost optimization.\n",
        "        \n",
        "        Uses categorical encoding for instructors and one-hot encoding for days.\n",
        "        \"\"\"\n",
        "        # Fitted flags\n",
        "        self.is_fitted = False\n",
        "        \n",
        "        # Lists to track feature types for CatBoost\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "    def fit(self, df: pd.DataFrame) -> 'SMUBiddingTransformer':\n",
        "        \"\"\"\n",
        "        Fit the transformer on training data.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            Training dataframe with all required columns\n",
        "        \"\"\"\n",
        "        # Validate required columns\n",
        "        required_cols = [\n",
        "            'course_code', 'course_name', 'acad_year_start', 'term',\n",
        "            'start_time', 'day_of_week', 'before_process_vacancy',\n",
        "            'bidding_window', 'instructor', 'section'\n",
        "        ]\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "        \n",
        "        print(f\"Fitting transformer on {len(df)} rows...\")\n",
        "        \n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "    \n",
        "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Transform the dataframe to CatBoost-ready format.\n",
        "        \"\"\"\n",
        "        # Try to load existing model if not fitted\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted before transform. Call fit() first.\")\n",
        "        \n",
        "        # Create a copy to avoid modifying original\n",
        "        df_transformed = df.copy()\n",
        "        \n",
        "        # Reset feature tracking\n",
        "        self.categorical_features = []\n",
        "        self.numeric_features = []\n",
        "        \n",
        "        # 1. Extract course components (categorical + numeric)\n",
        "        course_features = self._extract_course_features(df_transformed)\n",
        "        \n",
        "        # 2. Process bidding window (categorical + numeric)\n",
        "        round_window_features = self._extract_round_window(df_transformed)\n",
        "        \n",
        "        # 3. Basic features (preserve categorical nature) + instructor as categorical\n",
        "        basic_features = self._process_basic_features(df_transformed)\n",
        "        \n",
        "        # 4. Create day one-hot encoding\n",
        "        day_features = self._create_day_one_hot_encoding(df_transformed)\n",
        "        \n",
        "        # Combine all features - FIXED: Ensure proper concatenation\n",
        "        feature_dfs = [course_features, round_window_features, basic_features, day_features]\n",
        "        \n",
        "        # Filter out any empty DataFrames\n",
        "        feature_dfs = [df for df in feature_dfs if not df.empty]\n",
        "        \n",
        "        if not feature_dfs:\n",
        "            raise ValueError(\"No features were extracted\")\n",
        "        \n",
        "        # Concatenate all features\n",
        "        final_df = pd.concat(feature_dfs, axis=1)\n",
        "        \n",
        "        # Verify all expected features are present\n",
        "        expected_features = self.categorical_features + self.numeric_features\n",
        "        missing_features = [f for f in expected_features if f not in final_df.columns]\n",
        "        \n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing features in final dataframe: {missing_features}\")\n",
        "            print(f\"Available columns: {list(final_df.columns)}\")\n",
        "        \n",
        "        # Debug: Print feature summary\n",
        "        print(f\"Transformed data shape: {final_df.shape}\")\n",
        "        print(f\"Features included: {list(final_df.columns)[:10]}...\")  # Show first 10\n",
        "        \n",
        "        return final_df\n",
        "        \n",
        "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Fit the transformer and transform the data in one step.\"\"\"\n",
        "        self.fit(df)\n",
        "        return self.transform(df)\n",
        "    \n",
        "    def get_categorical_features(self) -> List[str]:\n",
        "        \"\"\"Get list of categorical feature names for CatBoost.\"\"\"\n",
        "        return self.categorical_features.copy()\n",
        "    \n",
        "    def get_numeric_features(self) -> List[str]:\n",
        "        \"\"\"Get list of numeric feature names.\"\"\"\n",
        "        return self.numeric_features.copy()\n",
        "    \n",
        "    def _extract_course_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract subject area and catalogue number from course code.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def split_course_code(code):\n",
        "            \"\"\"Split course code into subject area and catalogue number.\"\"\"\n",
        "            if pd.isna(code):\n",
        "                return None, None\n",
        "            \n",
        "            code = str(code).strip().upper()\n",
        "            \n",
        "            # Handle hyphenated codes like 'COR-COMM175'\n",
        "            if '-' in code:\n",
        "                parts = code.split('-')\n",
        "                if len(parts) >= 2:\n",
        "                    subject = '-'.join(parts[:-1])\n",
        "                    # Extract number from last part\n",
        "                    num_match = re.search(r'(\\d+)', parts[-1])\n",
        "                    if num_match:\n",
        "                        return subject, int(num_match.group(1))\n",
        "                    else:\n",
        "                        # Try extracting from full last part\n",
        "                        num_match = re.search(r'(\\d+)', code)\n",
        "                        if num_match:\n",
        "                            return subject, int(num_match.group(1))\n",
        "            \n",
        "            # Standard format like 'MGMT715'\n",
        "            match = re.match(r'([A-Z\\-]+)(\\d+)', code)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            return code, 0\n",
        "        \n",
        "        # Extract components\n",
        "        splits = df['course_code'].apply(split_course_code)\n",
        "        features['subject_area'] = splits.apply(lambda x: x[0] if x else None)\n",
        "        features['catalogue_no'] = splits.apply(lambda x: x[1] if x else 0)\n",
        "\n",
        "        # Debug: Verify extraction\n",
        "        print(f\"Extracted course features: {features.shape}\")\n",
        "        print(f\"Sample subject_area values: {features['subject_area'].head()}\")\n",
        "        print(f\"Sample catalogue_no values: {features['catalogue_no'].head()}\")\n",
        "\n",
        "        # subject_area and catalogue_no are categorical for CatBoost\n",
        "        self.categorical_features.extend(['subject_area', 'catalogue_no'])\n",
        "\n",
        "        return features\n",
        "    \n",
        "    def _extract_round_window(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Extract round and window from bidding_window string.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        def parse_bidding_window(window_str):\n",
        "            \"\"\"Parse bidding window string into round and window number.\"\"\"\n",
        "            if pd.isna(window_str):\n",
        "                return None, None\n",
        "            \n",
        "            window_str = str(window_str).strip()\n",
        "            # Check for Incoming Freshmen FIRST (before other patterns)\n",
        "            if 'Incoming Freshmen' in window_str:\n",
        "                match = re.search(r'Rnd\\s+(\\d)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if match:\n",
        "                    # Add F suffix to distinguish from regular rounds\n",
        "                    return f\"{match.group(1)}F\", int(match.group(2))     \n",
        "            \n",
        "            # Pattern 1: Standard format\n",
        "            match = re.search(r'Round\\s+(\\d[A-C]?)\\s+Window\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 2: Abbreviated format\n",
        "            match = re.search(r'Rnd\\s+(\\d[A-C]?)\\s+Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 3: Incoming Exchange format (keeps original round)\n",
        "            match = re.search(r'Incoming\\s+Exchange\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1), int(match.group(2))\n",
        "            \n",
        "            # Pattern 4: Incoming Freshmen format (adds F suffix)\n",
        "            match = re.search(r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', window_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                original_round = match.group(1)\n",
        "                window_num = int(match.group(2))\n",
        "                # Map Incoming Freshmen Round 1 to Round 1F\n",
        "                if original_round == \"1\":\n",
        "                    round_str = \"1F\"\n",
        "                else:\n",
        "                    round_str = f\"{original_round}F\"\n",
        "                return round_str, window_num\n",
        "            \n",
        "            # Fallback patterns...\n",
        "            match = re.search(r'(\\d[A-C]?)', window_str)\n",
        "            if match:\n",
        "                win_match = re.search(r'Window\\s+(\\d)|Win\\s+(\\d)', window_str, re.IGNORECASE)\n",
        "                if win_match:\n",
        "                    window_num = int(win_match.group(1) or win_match.group(2))\n",
        "                    return match.group(1), window_num\n",
        "                return match.group(1), 1\n",
        "            \n",
        "            return '1', 1\n",
        "        \n",
        "        # Extract round and window\n",
        "        parsed = df['bidding_window'].apply(parse_bidding_window)\n",
        "        features['round'] = parsed.apply(lambda x: x[0] if x else '1')\n",
        "        features['window'] = parsed.apply(lambda x: x[1] if x else 1)\n",
        "        \n",
        "        # Round as categorical (preserves ordering like 1, 1A, 1B, 2, 2A)\n",
        "        self.categorical_features.append('round')\n",
        "        \n",
        "        # Window as numeric\n",
        "        self.numeric_features.append('window')\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def _process_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Process basic features, preserving categorical nature where beneficial.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Numeric features\n",
        "        features['before_process_vacancy'] = pd.to_numeric(\n",
        "            df['before_process_vacancy'], errors='coerce'\n",
        "        ).fillna(0)\n",
        "        features['acad_year_start'] = pd.to_numeric(\n",
        "            df['acad_year_start'], errors='coerce'\n",
        "        ).fillna(2025)\n",
        "        \n",
        "        self.numeric_features.extend(['before_process_vacancy', 'acad_year_start'])\n",
        "        \n",
        "        # Categorical features\n",
        "        features['term'] = df['term'].astype(str)\n",
        "        features['start_time'] = df['start_time'].astype(str)\n",
        "        features['course_name'] = df['course_name'].astype(str)\n",
        "        features['section'] = df['section'].astype(str)\n",
        "        \n",
        "        # Process instructor names (remove duplicates, handle comma-separated format)\n",
        "        features['instructor'] = df['instructor'].apply(self._process_instructor_names)\n",
        "\n",
        "        # Replace empty strings with None for proper CatBoost handling\n",
        "        features.loc[features['start_time'].isin(['', 'nan']), 'start_time'] = None\n",
        "        features.loc[features['course_name'].isin(['', 'nan']), 'course_name'] = None\n",
        "        features.loc[features['section'].isin(['', 'nan']), 'section'] = None\n",
        "        \n",
        "        self.categorical_features.extend(['term', 'start_time', 'course_name', 'section', 'instructor'])\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def _create_day_one_hot_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create one-hot encoding for days of the week.\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        \n",
        "        # Initialize all day columns as 0\n",
        "        day_columns = ['has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun']\n",
        "        for col in day_columns:\n",
        "            features[col] = 0\n",
        "        \n",
        "        # Day mapping\n",
        "        day_abbrev = {\n",
        "            'MONDAY': 'MON', 'TUESDAY': 'TUE', 'WEDNESDAY': 'WED',\n",
        "            'THURSDAY': 'THU', 'FRIDAY': 'FRI', 'SATURDAY': 'SAT', 'SUNDAY': 'SUN',\n",
        "            'MON': 'MON', 'TUE': 'TUE', 'WED': 'WED', 'THU': 'THU',\n",
        "            'FRI': 'FRI', 'SAT': 'SAT', 'SUN': 'SUN'\n",
        "        }\n",
        "        \n",
        "        day_to_column = {\n",
        "            'MON': 'has_mon', 'TUE': 'has_tue', 'WED': 'has_wed', 'THU': 'has_thu',\n",
        "            'FRI': 'has_fri', 'SAT': 'has_sat', 'SUN': 'has_sun'\n",
        "        }\n",
        "        \n",
        "        # Process each row's day_of_week\n",
        "        for idx, days_value in enumerate(df['day_of_week']):\n",
        "            if pd.isna(days_value) or str(days_value).strip() == '':\n",
        "                continue  # Leave all days as 0\n",
        "            \n",
        "            days_str = str(days_value).strip()\n",
        "            \n",
        "            # Handle JSON array format\n",
        "            if days_str.startswith('[') and days_str.endswith(']'):\n",
        "                try:\n",
        "                    import json\n",
        "                    days_list = json.loads(days_str)\n",
        "                    if isinstance(days_list, list):\n",
        "                        for day in days_list:\n",
        "                            day_upper = str(day).strip().upper()\n",
        "                            standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                            \n",
        "                            if standardized_day in day_to_column:\n",
        "                                features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try comma-separated format as fallback\n",
        "                    pass\n",
        "            else:\n",
        "                # Handle comma-separated format (legacy support)\n",
        "                for day in days_str.split(','):\n",
        "                    day_upper = day.strip().upper()\n",
        "                    standardized_day = day_abbrev.get(day_upper, day_upper)\n",
        "                    \n",
        "                    if standardized_day in day_to_column:\n",
        "                        features.loc[df.index[idx], day_to_column[standardized_day]] = 1\n",
        "        \n",
        "        # These are numeric binary features (0/1)\n",
        "        self.numeric_features.extend(day_columns)\n",
        "        \n",
        "        return features\n",
        "\n",
        "    def get_feature_names(self) -> List[str]:\n",
        "        \"\"\"Get all feature names after transformation.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer must be fitted to get feature names.\")\n",
        "        \n",
        "        return self.categorical_features + self.numeric_features\n",
        "    \n",
        "    def _process_instructor_names(self, instructor_input):\n",
        "        \"\"\"Process instructor names to ensure consistent JSON array format as categorical string.\"\"\"\n",
        "        # Handle list/array input\n",
        "        if isinstance(instructor_input, (list, np.ndarray)):\n",
        "            if len(instructor_input) == 0:\n",
        "                return None\n",
        "            # Convert list to string format for processing\n",
        "            instructor_str = ', '.join([str(inst).strip() for inst in instructor_input if pd.notna(inst) and str(inst).strip()])\n",
        "            if not instructor_str:\n",
        "                return None\n",
        "        else:\n",
        "            # Handle string input\n",
        "            if pd.isna(instructor_input) or str(instructor_input).strip() == '' or str(instructor_input).upper() == 'TBA':\n",
        "                return None\n",
        "            instructor_str = str(instructor_input).strip()\n",
        "        \n",
        "        # Check if it's already a JSON array\n",
        "        if instructor_str.startswith('[') and instructor_str.endswith(']'):\n",
        "            try:\n",
        "                import json\n",
        "                # Parse JSON array\n",
        "                instructors = json.loads(instructor_str)\n",
        "                if isinstance(instructors, list) and instructors:\n",
        "                    # Join back to string for processing\n",
        "                    instructor_str = ', '.join([str(inst).strip() for inst in instructors if pd.notna(inst) and str(inst).strip()])\n",
        "                    if not instructor_str:\n",
        "                        return None\n",
        "                else:\n",
        "                    return None\n",
        "            except json.JSONDecodeError:\n",
        "                # If JSON parsing fails, treat as regular string\n",
        "                pass\n",
        "        \n",
        "        # Load professor lookup mapping\n",
        "        professor_lookup = {}\n",
        "        lookup_path = Path(\"script_input/professor_lookup.csv\")\n",
        "        if lookup_path.exists():\n",
        "            lookup_df = pd.read_csv(lookup_path)\n",
        "            for _, row in lookup_df.iterrows():\n",
        "                if pd.notna(row.get('boss_name')) and pd.notna(row.get('afterclass_name')):\n",
        "                    professor_lookup[str(row['boss_name']).strip().upper()] = str(row['afterclass_name']).strip()\n",
        "        \n",
        "        # Step 1: Check if the entire name exists in professor_lookup (single professor)\n",
        "        prof_name_upper = instructor_str.upper()\n",
        "        if prof_name_upper in professor_lookup:\n",
        "            import json\n",
        "            return json.dumps([professor_lookup[prof_name_upper]])\n",
        "        \n",
        "        # Step 2: Check hardcoded multi-instructor combinations first\n",
        "        multi_instructor_combinations = {\n",
        "            \"ERIC YEE SHIN CHONG, MANDY THAM\": [\"ERIC YEE SHIN CHONG\", \"MANDY THAM\"],\n",
        "            \"ZHENG ZHICHAO, DANIEL, TAN KAR WAY\": [\"ZHENG ZHICHAO, DANIEL\", \"TAN KAR WAY\"],\n",
        "            \"KAM WAI WARREN BARTHOLOMEW CHIK, LANX GOH\": [\"KAM WAI WARREN BARTHOLOMEW CHIK\", \"LANX GOH\"],\n",
        "            \"ANDREW MIN HAN CHIN, DANIEL TAN\": [\"ANDREW MIN HAN CHIN\", \"DANIEL TAN\"],\n",
        "            \"PAUL GRIFFIN, TA NGUYEN BINH DUONG\": [\"PAUL GRIFFIN\", \"TA NGUYEN BINH DUONG\"],\n",
        "            \"ANDREW MIN HAN CHIN, JUNJI SUMITANI\": [\"ANDREW MIN HAN CHIN\", \"JUNJI SUMITANI\"],\n",
        "            \"DAVID GOMULYA, LIM CHON PHUNG, AJAY MAKHIJA\": [\"DAVID GOMULYA\", \"LIM CHON PHUNG\", \"AJAY MAKHIJA\"],\n",
        "            \"JACK HONG JIAJUN, ANG SER KENG\": [\"JACK HONG JIAJUN\", \"ANG SER KENG\"],\n",
        "            \"DAVID GOMULYA, DAVID LLEWELYN\": [\"DAVID GOMULYA\", \"DAVID LLEWELYN\"],\n",
        "            \"TERENCE FAN PING-CHING, JONATHAN TEE\": [\"TERENCE FAN PING-CHING\", \"JONATHAN TEE\"],\n",
        "            \"RONG WANG, CHENG QIANG, CHEN XIA, LIANDONG ZHANG, WANG JIWEI, YUE HENG\": [\"RONG WANG\", \"CHENG QIANG\", \"CHEN XIA\", \"LIANDONG ZHANG\", \"WANG JIWEI\", \"YUE HENG\"],\n",
        "            \"PASCALE CRAMA, ARNOUD DE MEYER\": [\"PASCALE CRAMA\", \"ARNOUD DE MEYER\"],\n",
        "            \"TERENCE FAN PING-CHING, WILSON TENG\": [\"TERENCE FAN PING-CHING\", \"WILSON TENG\"],\n",
        "            \"ANDREW MIN HAN CHIN, LI JIN\": [\"ANDREW MIN HAN CHIN\", \"LI JIN\"],\n",
        "            \"ONG, BENJAMIN JOSHUA, EUGENE TAN KHENG BOON\": [\"ONG, BENJAMIN JOSHUA\", \"EUGENE TAN KHENG BOON\"],\n",
        "            \"MANDY THAM, ERIC YEE SHIN CHONG\": [\"MANDY THAM\", \"ERIC YEE SHIN CHONG\"],\n",
        "            \"TERENCE FAN PING-CHING, RUTH CHIANG\": [\"TERENCE FAN PING-CHING\", \"RUTH CHIANG\"],\n",
        "            \"JARED POON JUN KEAT, CHAM YANWEI, DERRICK\": [\"JARED POON JUN KEAT\", \"CHAM YANWEI, DERRICK\"],\n",
        "            \"DAVID GOMULYA, SZE TIAM LIN\": [\"DAVID GOMULYA\", \"SZE TIAM LIN\"],\n",
        "            \"ANDREW MIN HAN CHIN, JAY WONG\": [\"ANDREW MIN HAN CHIN\", \"JAY WONG\"],\n",
        "            \"MARK CHONG YIEW KIM, VICTOR OCAMPO\": [\"MARK CHONG YIEW KIM\", \"VICTOR OCAMPO\"],\n",
        "            \"TSE, JUSTIN K, AIDAN WONG\": [\"TSE, JUSTIN K\", \"AIDAN WONG\"],\n",
        "            \"TANG HONG WEE, GERALD SEAH, MUHAMMED AMEER S/O MOHAMED NOOR, LAU MENG YAN\": [\"TANG HONG WEE\", \"GERALD SEAH\", \"MUHAMMED AMEER S/O MOHAMED NOOR\", \"LAU MENG YAN\"],\n",
        "            \"AURELIO GURREA MARTINEZ, LOH SONG-EN, SAMUEL\": [\"AURELIO GURREA MARTINEZ\", \"LOH SONG-EN, SAMUEL\"],\n",
        "            \"CHNG SHUQI, AMELIA CHUA, MUHAMMED AMEER S/O MOHAMED NOOR\": [\"CHNG SHUQI\", \"AMELIA CHUA\", \"MUHAMMED AMEER S/O MOHAMED NOOR\"]\n",
        "        }\n",
        "        \n",
        "        if instructor_str in multi_instructor_combinations:\n",
        "            split_names = multi_instructor_combinations[instructor_str]\n",
        "        else:\n",
        "            # Step 3: Progressive comma-based splitting with exact matching\n",
        "            # Split by comma first\n",
        "            comma_parts = [part.strip() for part in instructor_str.split(',') if part.strip()]\n",
        "            \n",
        "            # If only one part (no commas), treat as single professor\n",
        "            if len(comma_parts) <= 1:\n",
        "                if prof_name_upper in professor_lookup:\n",
        "                    import json\n",
        "                    return json.dumps([professor_lookup[prof_name_upper]])\n",
        "                else:\n",
        "                    words = instructor_str.strip().split()\n",
        "                    if len(words) >= 2:\n",
        "                        import json\n",
        "                        return json.dumps([instructor_str.strip()])\n",
        "                    else:\n",
        "                        return None\n",
        "            \n",
        "            # Get all boss_names from professor_lookup for matching\n",
        "            boss_names = set()\n",
        "            for boss_name in professor_lookup.keys():\n",
        "                if boss_name is not None and not pd.isna(boss_name):\n",
        "                    boss_name_str = str(boss_name).strip()\n",
        "                    if boss_name_str and boss_name_str.lower() != 'nan':\n",
        "                        boss_names.add(boss_name_str.upper())\n",
        "            \n",
        "            professors_found = []\n",
        "            i = 0\n",
        "            \n",
        "            while i < len(comma_parts):\n",
        "                current_candidate = comma_parts[i]\n",
        "                matched = False\n",
        "                \n",
        "                # Try progressive matching: add more comma parts until we find a match\n",
        "                for j in range(i + 1, len(comma_parts) + 1):\n",
        "                    candidate = ', '.join(comma_parts[i:j])\n",
        "                    candidate_upper = candidate.upper()\n",
        "                    \n",
        "                    # Check for exact match\n",
        "                    if candidate_upper in boss_names:\n",
        "                        professors_found.append(candidate)\n",
        "                        i = j  # Move past all used parts\n",
        "                        matched = True\n",
        "                        break\n",
        "                    \n",
        "                    # Check for partial word match (all words in candidate must be in some boss_name)\n",
        "                    candidate_words = set(candidate.replace(',', ' ').split())\n",
        "                    for boss_name in boss_names:\n",
        "                        boss_words = set(boss_name.replace(',', ' ').split())\n",
        "                        if candidate_words.issubset(boss_words) and len(candidate_words) >= 2:\n",
        "                            professors_found.append(candidate)\n",
        "                            i = j  # Move past all used parts\n",
        "                            matched = True\n",
        "                            break\n",
        "                    \n",
        "                    if matched:\n",
        "                        break\n",
        "                \n",
        "                # If no match found and we're at a single part, check if it's reasonable\n",
        "                if not matched:\n",
        "                    single_part = comma_parts[i]\n",
        "                    words_in_part = single_part.split()\n",
        "                    \n",
        "                    # Only accept if it has at least 2 words (avoid single word professors)\n",
        "                    if len(words_in_part) >= 2:\n",
        "                        professors_found.append(single_part)\n",
        "                        i += 1\n",
        "                    else:\n",
        "                        # Try to combine with next part if available\n",
        "                        if i + 1 < len(comma_parts):\n",
        "                            combined = f\"{single_part}, {comma_parts[i + 1]}\"\n",
        "                            professors_found.append(combined)\n",
        "                            i += 2  # Skip next part too\n",
        "                        else:\n",
        "                            # Skip single word professors entirely\n",
        "                            i += 1\n",
        "            \n",
        "            # Final validation: remove any single-word results\n",
        "            split_names = []\n",
        "            for prof in professors_found:\n",
        "                prof_words = prof.strip().replace(',', ' ').split()\n",
        "                if len(prof_words) >= 2:  # Only keep professors with at least 2 words\n",
        "                    split_names.append(prof)\n",
        "            \n",
        "            # If we couldn't split intelligently, fall back to treating as single professor\n",
        "            # but only if it has at least 2 words\n",
        "            if not split_names:\n",
        "                prof_words = instructor_str.strip().replace(',', ' ').split()\n",
        "                if len(prof_words) >= 2:\n",
        "                    split_names = [instructor_str]\n",
        "                else:\n",
        "                    return None\n",
        "        \n",
        "        # Map each split name to afterclass_name\n",
        "        mapped_names = []\n",
        "        for name in split_names:\n",
        "            name_upper = name.strip().upper()\n",
        "            if name_upper in professor_lookup:\n",
        "                mapped_names.append(professor_lookup[name_upper])\n",
        "            else:\n",
        "                # Keep original name if not found in lookup but has multiple words\n",
        "                words = name.strip().split()\n",
        "                if len(words) >= 2:\n",
        "                    mapped_names.append(name.strip())\n",
        "        \n",
        "        if mapped_names:\n",
        "            # Remove duplicates and sort\n",
        "            unique_mapped = []\n",
        "            seen = set()\n",
        "            for name in mapped_names:\n",
        "                if name not in seen:\n",
        "                    seen.add(name)\n",
        "                    unique_mapped.append(name)\n",
        "            unique_mapped.sort()\n",
        "            \n",
        "            import json\n",
        "            return json.dumps(unique_mapped)\n",
        "        \n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc2ec324",
      "metadata": {},
      "source": [
        "## **3. Database Helper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4dac95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def connect_database():\n",
        "    \"\"\"Connect to PostgreSQL database\"\"\"\n",
        "    load_dotenv()\n",
        "    db_config = {\n",
        "        'host': os.getenv('DB_HOST'),\n",
        "        'database': os.getenv('DB_NAME'),\n",
        "        'user': os.getenv('DB_USER'),\n",
        "        'password': os.getenv('DB_PASSWORD'),\n",
        "        'port': int(os.getenv('DB_PORT', 5432)),\n",
        "        'gssencmode': 'disable'\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        connection = psycopg2.connect(**db_config)\n",
        "        print(\"âœ… Database connection established\")\n",
        "        return connection\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Database connection failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_or_cache_data(connection, cache_dir):\n",
        "    \"\"\"Load data from cache or database\"\"\"\n",
        "    cache_files = {\n",
        "        'courses': cache_dir / 'courses_cache.pkl',\n",
        "        'classes': cache_dir / 'classes_cache.pkl',\n",
        "        'acad_terms': cache_dir / 'acad_terms_cache.pkl',\n",
        "        'professors': cache_dir / 'professors_cache.pkl'\n",
        "    }\n",
        "    \n",
        "    data_cache = {}\n",
        "    \n",
        "    # Try loading from cache first\n",
        "    if all(f.exists() for f in cache_files.values()):\n",
        "        print(\"âœ… Loading from cache...\")\n",
        "        for key, file in cache_files.items():\n",
        "            data_cache[key] = pd.read_pickle(file)\n",
        "    else:\n",
        "        print(\"ðŸ“¥ Downloading from database...\")\n",
        "        queries = {\n",
        "            'courses': \"SELECT * FROM courses\",\n",
        "            'classes': \"SELECT * FROM classes\",\n",
        "            'acad_terms': \"SELECT * FROM acad_term\",\n",
        "            'professors': \"SELECT * FROM professors\"\n",
        "        }\n",
        "        \n",
        "        for key, query in queries.items():\n",
        "            df = pd.read_sql_query(query, connection)\n",
        "            df.to_pickle(cache_files[key])\n",
        "            data_cache[key] = df\n",
        "    \n",
        "    return data_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "815cf568",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Data Preparation Functions\n",
        "def prepare_prediction_data(raw_data_path='script_input/raw_data.xlsx'):\n",
        "    \"\"\"Prepare data for prediction from raw_data.xlsx\"\"\"\n",
        "    print(\"ðŸ“‚ Loading raw data...\")\n",
        "    \n",
        "    # Load sheets\n",
        "    standalone_df = pd.read_excel(raw_data_path, sheet_name='standalone')\n",
        "    multiple_df = pd.read_excel(raw_data_path, sheet_name='multiple')\n",
        "    \n",
        "    # Filter for classes with bidding data\n",
        "    bidding_data = standalone_df[\n",
        "        standalone_df['bidding_window'].notna() & \n",
        "        standalone_df['total'].notna()\n",
        "    ].copy()\n",
        "    \n",
        "    # Calculate before_process_vacancy\n",
        "    bidding_data['before_process_vacancy'] = bidding_data['total'] - bidding_data['current_enrolled']\n",
        "    \n",
        "    # Extract round and window from bidding_window\n",
        "    def parse_bidding_window(window_str):\n",
        "        if pd.isna(window_str):\n",
        "            return None, None\n",
        "        \n",
        "        import re\n",
        "        # Handle various formats\n",
        "        patterns = [\n",
        "            (r'Round\\s+(\\w+)\\s+Window\\s+(\\d+)', lambda m: (m.group(1), int(m.group(2)))),\n",
        "            (r'Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', lambda m: (m.group(1), int(m.group(2)))),\n",
        "            (r'Incoming\\s+Freshmen\\s+Rnd\\s+(\\w+)\\s+Win\\s+(\\d+)', lambda m: (f\"{m.group(1)}F\", int(m.group(2))))\n",
        "        ]\n",
        "        \n",
        "        for pattern, extractor in patterns:\n",
        "            match = re.search(pattern, str(window_str), re.IGNORECASE)\n",
        "            if match:\n",
        "                return extractor(match)\n",
        "        return '1', 1\n",
        "    \n",
        "    bidding_data[['round', 'window']] = bidding_data['bidding_window'].apply(\n",
        "        lambda x: pd.Series(parse_bidding_window(x))\n",
        "    )\n",
        "    \n",
        "    # Get instructor information from multiple sheet\n",
        "    instructor_map = {}\n",
        "    for record_key, group in multiple_df.groupby('record_key'):\n",
        "        professors = group['professor_name'].dropna().unique()\n",
        "        if len(professors) > 0:\n",
        "            instructor_map[record_key] = professors.tolist()\n",
        "    \n",
        "    # Map instructors to bidding data\n",
        "    bidding_data['instructor'] = bidding_data['record_key'].map(\n",
        "        lambda x: instructor_map.get(x, [])\n",
        "    )\n",
        "    \n",
        "    # Get day of week information\n",
        "    day_map = {}\n",
        "    for record_key, group in multiple_df[multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        days = group['day_of_week'].dropna().unique()\n",
        "        if len(days) > 0:\n",
        "            day_map[record_key] = ', '.join(days)\n",
        "    \n",
        "    bidding_data['day_of_week'] = bidding_data['record_key'].map(\n",
        "        lambda x: day_map.get(x, '')\n",
        "    )\n",
        "    \n",
        "    # Get start time\n",
        "    time_map = {}\n",
        "    for record_key, group in multiple_df[multiple_df['type'] == 'CLASS'].groupby('record_key'):\n",
        "        times = group['start_time'].dropna()\n",
        "        if len(times) > 0:\n",
        "            time_map[record_key] = times.iloc[0]\n",
        "    \n",
        "    bidding_data['start_time'] = bidding_data['record_key'].map(\n",
        "        lambda x: time_map.get(x, '')\n",
        "    )\n",
        "    \n",
        "    return bidding_data, standalone_df, multiple_df\n",
        "\n",
        "def map_classes_to_predictions(bidding_data, data_cache):\n",
        "    \"\"\"Map predictions to class IDs - checks both database cache and new_classes.csv\"\"\"\n",
        "    courses_df = data_cache['courses']\n",
        "    classes_df = data_cache['classes']\n",
        "    \n",
        "    # Create course code to ID mapping from both sources\n",
        "    course_id_map = dict(zip(courses_df['code'], courses_df['id']))\n",
        "    \n",
        "    # Also check new_courses.csv for courses not in database yet\n",
        "    new_courses_path = Path('script_output/new_courses.csv')\n",
        "    if new_courses_path.exists():\n",
        "        try:\n",
        "            new_courses_df = pd.read_csv(new_courses_path)\n",
        "            for _, row in new_courses_df.iterrows():\n",
        "                if row['code'] not in course_id_map:\n",
        "                    course_id_map[row['code']] = row['id']\n",
        "            print(f\"ðŸ“š Added {len(new_courses_df)} courses from new_courses.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not load new_courses.csv: {e}\")\n",
        "    \n",
        "    # Also check verify folder for new courses\n",
        "    verify_courses_path = Path('script_output/verify/new_courses.csv')\n",
        "    if verify_courses_path.exists():\n",
        "        try:\n",
        "            verify_courses_df = pd.read_csv(verify_courses_path)\n",
        "            for _, row in verify_courses_df.iterrows():\n",
        "                if row['code'] not in course_id_map:\n",
        "                    course_id_map[row['code']] = row['id']\n",
        "            print(f\"ðŸ“š Added {len(verify_courses_df)} courses from verify/new_courses.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not load verify/new_courses.csv: {e}\")\n",
        "    \n",
        "    # Load new_classes.csv to find classes not in database yet\n",
        "    new_classes_df = None\n",
        "    new_classes_path = Path('script_output/new_classes.csv')\n",
        "    if new_classes_path.exists():\n",
        "        try:\n",
        "            new_classes_df = pd.read_csv(new_classes_path)\n",
        "            print(f\"ðŸ“š Loaded {len(new_classes_df)} classes from new_classes.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not load new_classes.csv: {e}\")\n",
        "    \n",
        "    # Map each row to class IDs\n",
        "    class_mappings = []\n",
        "    unmapped_courses = set()\n",
        "    \n",
        "    for idx, row in bidding_data.iterrows():\n",
        "        course_code = row['course_code']\n",
        "        section = str(row['section'])\n",
        "        acad_term_id = row['acad_term_id']\n",
        "        record_key = row.get('record_key', '')\n",
        "        \n",
        "        # Get course ID\n",
        "        course_id = course_id_map.get(course_code)\n",
        "        if not course_id:\n",
        "            unmapped_courses.add(course_code)\n",
        "            continue\n",
        "        \n",
        "        found_in_db = False\n",
        "        found_in_new = False\n",
        "        \n",
        "        # First, try to find in database cache\n",
        "        matching_classes = classes_df[\n",
        "            (classes_df['course_id'] == course_id) & \n",
        "            (classes_df['section'] == section) & \n",
        "            (classes_df['acad_term_id'] == acad_term_id)\n",
        "        ]\n",
        "        \n",
        "        if not matching_classes.empty:\n",
        "            found_in_db = True\n",
        "            for _, class_row in matching_classes.iterrows():\n",
        "                mapping = {\n",
        "                    'prediction_idx': idx,\n",
        "                    'class_id': class_row['id'],\n",
        "                    'professor_id': class_row.get('professor_id'),\n",
        "                    'course_code': course_code,\n",
        "                    'section': section,\n",
        "                    'acad_term_id': acad_term_id,\n",
        "                    'record_key': record_key,\n",
        "                    'source': 'database'\n",
        "                }\n",
        "                class_mappings.append(mapping)\n",
        "        \n",
        "        # If not found in database, check new_classes.csv\n",
        "        if not found_in_db and new_classes_df is not None:\n",
        "            # Try matching by course_id, section, and acad_term_id\n",
        "            new_matching = new_classes_df[\n",
        "                (new_classes_df['course_id'] == course_id) & \n",
        "                (new_classes_df['section'] == section) & \n",
        "                (new_classes_df['acad_term_id'] == acad_term_id)\n",
        "            ]\n",
        "            \n",
        "            if not new_matching.empty:\n",
        "                found_in_new = True\n",
        "                for _, class_row in new_matching.iterrows():\n",
        "                    mapping = {\n",
        "                        'prediction_idx': idx,\n",
        "                        'class_id': class_row['id'],\n",
        "                        'professor_id': class_row.get('professor_id'),\n",
        "                        'course_code': course_code,\n",
        "                        'section': section,\n",
        "                        'acad_term_id': acad_term_id,\n",
        "                        'record_key': record_key,\n",
        "                        'source': 'new_classes'\n",
        "                    }\n",
        "                    class_mappings.append(mapping)\n",
        "        \n",
        "        # If still not found anywhere\n",
        "        if not found_in_db and not found_in_new:\n",
        "            # Create a placeholder mapping\n",
        "            mapping = {\n",
        "                'prediction_idx': idx,\n",
        "                'class_id': f\"PENDING_{course_code}_{section}_{acad_term_id}\",\n",
        "                'professor_id': None,\n",
        "                'course_code': course_code,\n",
        "                'section': section,\n",
        "                'acad_term_id': acad_term_id,\n",
        "                'record_key': record_key,\n",
        "                'source': 'not_found'\n",
        "            }\n",
        "            class_mappings.append(mapping)\n",
        "    \n",
        "    # Create summary\n",
        "    mappings_df = pd.DataFrame(class_mappings)\n",
        "    \n",
        "    if not mappings_df.empty:\n",
        "        print(f\"\\nðŸ“Š Mapping Summary:\")\n",
        "        print(f\"   Total mappings: {len(mappings_df)}\")\n",
        "        print(f\"   Unique predictions mapped: {mappings_df['prediction_idx'].nunique()}\")\n",
        "        source_counts = mappings_df['source'].value_counts()\n",
        "        for source, count in source_counts.items():\n",
        "            print(f\"   From {source}: {count}\")\n",
        "        \n",
        "        if unmapped_courses:\n",
        "            print(f\"\\nâš ï¸ Courses without IDs: {len(unmapped_courses)}\")\n",
        "            print(f\"   Sample: {list(unmapped_courses)[:5]}\")\n",
        "    \n",
        "    return mappings_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b904ae42",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Load and Prepare Data\n",
        "print(\"=\"*60)\n",
        "print(\"DATA LOADING AND PREPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Connect to database\n",
        "connection = connect_database()\n",
        "if not connection:\n",
        "    raise Exception(\"Failed to connect to database\")\n",
        "\n",
        "# Load cache data\n",
        "data_cache = load_or_cache_data(connection, cache_dir)\n",
        "\n",
        "# Prepare prediction data\n",
        "bidding_data, standalone_df, multiple_df = prepare_prediction_data()\n",
        "print(f\"ðŸ“Š Prepared {len(bidding_data)} records for prediction\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nSample bidding data:\")\n",
        "print(bidding_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb7a7d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Transform Data (FIXED)\n",
        "def transform_bidding_data(bidding_data, output_dir):\n",
        "    print(\"=\"*60)\n",
        "    print(\"FEATURE TRANSFORMATION\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Initialize transformer\n",
        "    transformer = SMUBiddingTransformer()\n",
        "    transformer.fit(bidding_data)\n",
        "    \n",
        "    # Transform data\n",
        "    X_transformed = transformer.transform(bidding_data)\n",
        "    print(f\"âœ… Transformed data shape: {X_transformed.shape}\")\n",
        "    print(f\"ðŸ“‹ Features: {list(X_transformed.columns)[:10]}...\")\n",
        "    \n",
        "    # Ensure all categorical features have __NA__ for null values\n",
        "    categorical_features = transformer.get_categorical_features()\n",
        "    for col in categorical_features:\n",
        "        if col in X_transformed.columns:\n",
        "            # Convert to string type first\n",
        "            X_transformed[col] = X_transformed[col].astype(str)\n",
        "            # Replace 'nan' strings with a consistent placeholder\n",
        "            X_transformed[col] = X_transformed[col].replace('nan', '__NA__')\n",
        "            # Also handle any remaining NaN values\n",
        "            X_transformed[col] = X_transformed[col].fillna('__NA__')\n",
        "            # Handle empty strings\n",
        "            X_transformed[col] = X_transformed[col].replace('', '__NA__')\n",
        "    \n",
        "    # Save transformed data to CSV\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    transformed_output_path = output_dir / f'transformed_features_{timestamp}.csv'\n",
        "    \n",
        "    # Add the original identifiers to help with tracking\n",
        "    X_transformed_with_ids = X_transformed.copy()\n",
        "    X_transformed_with_ids['course_code'] = bidding_data['course_code'].values\n",
        "    X_transformed_with_ids['section'] = bidding_data['section'].values\n",
        "    X_transformed_with_ids['acad_term_id'] = bidding_data['acad_term_id'].values\n",
        "    X_transformed_with_ids['record_key'] = bidding_data['record_key'].values\n",
        "    \n",
        "    # Reorder columns to put identifiers first\n",
        "    id_cols = ['record_key', 'course_code', 'section', 'acad_term_id']\n",
        "    feature_cols = [col for col in X_transformed.columns if col not in id_cols]\n",
        "    X_transformed_with_ids = X_transformed_with_ids[id_cols + feature_cols]\n",
        "    \n",
        "    # Save to CSV\n",
        "    X_transformed_with_ids.to_csv(transformed_output_path, index=False)\n",
        "    print(f\"\\nðŸ’¾ Transformed features saved to: {transformed_output_path}\")\n",
        "    print(f\"   Total columns: {len(X_transformed_with_ids.columns)}\")\n",
        "    print(f\"   - Identifier columns: {len(id_cols)}\")\n",
        "    print(f\"   - Feature columns: {len(feature_cols)}\")\n",
        "    print(f\"   - Categorical features: {len(transformer.get_categorical_features())}\")\n",
        "    print(f\"   - Numeric features: {len(transformer.get_numeric_features())}\")\n",
        "    \n",
        "    # Also save a metadata file with feature information\n",
        "    metadata = {\n",
        "        'timestamp': timestamp,\n",
        "        'total_rows': len(X_transformed),\n",
        "        'total_features': len(feature_cols),\n",
        "        'categorical_features': transformer.get_categorical_features(),\n",
        "        'numeric_features': transformer.get_numeric_features(),\n",
        "        'identifier_columns': id_cols,\n",
        "        'transformation_date': datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    metadata_path = output_dir / f'transformation_metadata_{timestamp}.json'\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"ðŸ“‹ Metadata saved to: {metadata_path}\")\n",
        "    \n",
        "    # Display sample of transformed data\n",
        "    print(\"\\nðŸ” Sample of transformed data:\")\n",
        "    print(X_transformed_with_ids.head())\n",
        "    \n",
        "    return X_transformed, transformer\n",
        "\n",
        "# Run Transformation\n",
        "X_transformed, transformer = transform_bidding_data(bidding_data, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4bf5880",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Map to Classes\n",
        "print(\"=\"*60)\n",
        "print(\"CLASS MAPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Map to classes\n",
        "class_mappings = map_classes_to_predictions(bidding_data, data_cache)\n",
        "print(f\"ðŸ”— Mapped to {len(class_mappings)} class instances\")\n",
        "\n",
        "# Display sample mappings\n",
        "print(\"\\nSample class mappings:\")\n",
        "print(class_mappings.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1fa3cc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Load Models and Generate Predictions (FIXED)\n",
        "def load_models_and_predict(X_transformed, bidding_data):\n",
        "    print(\"=\"*60)\n",
        "    print(\"MODEL PREDICTIONS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Load models\n",
        "    models = {\n",
        "        'classification': CatBoostClassifier(),\n",
        "        'median': CatBoostRegressor(),\n",
        "        'min': CatBoostRegressor()\n",
        "    }\n",
        "    \n",
        "    model_paths = {\n",
        "        'classification': 'script_output/models/classification/production_classification_model.cbm',\n",
        "        'median': 'script_output/models/regression_median/production_regression_median_model.cbm',\n",
        "        'min': 'script_output/models/regression_min/production_regression_min_model.cbm'\n",
        "    }\n",
        "    \n",
        "    # Load each model\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            model.load_model(model_paths[name])\n",
        "            print(f\"âœ… Loaded {name} model\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading {name} model: {e}\")\n",
        "            return None\n",
        "    \n",
        "    # Verify data format matches model expectations\n",
        "    expected_features = [\n",
        "        'subject_area', 'catalogue_no', 'round', 'window', 'before_process_vacancy',\n",
        "        'acad_year_start', 'term', 'start_time', 'course_name', 'section', 'instructor',\n",
        "        'has_mon', 'has_tue', 'has_wed', 'has_thu', 'has_fri', 'has_sat', 'has_sun'\n",
        "    ]\n",
        "    \n",
        "    # Create prediction dataset with only the features expected by models\n",
        "    prediction_data = X_transformed.copy()\n",
        "    \n",
        "    # Ensure all expected features are present\n",
        "    missing_features = set(expected_features) - set(prediction_data.columns)\n",
        "    if missing_features:\n",
        "        print(f\"âš ï¸ Warning: Missing features: {missing_features}\")\n",
        "    \n",
        "    # Select only the features that exist and are expected\n",
        "    available_features = [col for col in expected_features if col in prediction_data.columns]\n",
        "    prediction_data = prediction_data[available_features]\n",
        "    \n",
        "    print(f\"ðŸ“Š Using {len(available_features)} features for prediction\")\n",
        "    print(f\"ðŸ”® Generating predictions for {len(prediction_data)} records...\")\n",
        "    \n",
        "    try:\n",
        "        # Classification predictions\n",
        "        clf_pred = models['classification'].predict(prediction_data)\n",
        "        clf_proba = models['classification'].predict_proba(prediction_data)\n",
        "        \n",
        "        # Regression predictions  \n",
        "        median_pred = models['median'].predict(prediction_data)\n",
        "        min_pred = models['min'].predict(prediction_data)\n",
        "        \n",
        "        print(f\"âœ… Generated predictions for {len(prediction_data)} records\")\n",
        "        \n",
        "        # Create results dataframe\n",
        "        results = {\n",
        "            'classification_prediction': clf_pred,\n",
        "            'classification_probabilities': clf_proba,\n",
        "            'median_prediction': median_pred,\n",
        "            'min_prediction': min_pred\n",
        "        }\n",
        "        \n",
        "        return results, models\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during prediction: {e}\")\n",
        "        print(f\"   Data shape: {prediction_data.shape}\")\n",
        "        print(f\"   Data types: {prediction_data.dtypes}\")\n",
        "        return None, models\n",
        "    \n",
        "# Load Models and Generate Predictions\n",
        "prediction_results, loaded_models = load_models_and_predict(X_transformed, bidding_data)\n",
        "if prediction_results:\n",
        "    clf_pred = prediction_results['classification_prediction']\n",
        "    clf_proba = prediction_results['classification_probabilities'] \n",
        "    median_pred = prediction_results['median_prediction']\n",
        "    min_pred = prediction_results['min_prediction']\n",
        "    \n",
        "    print(f\"\\nðŸ“ˆ Prediction Summary:\")\n",
        "    print(f\"   Classification predictions: {len(clf_pred)}\")\n",
        "    print(f\"   Median predictions range: {min(median_pred):.2f} - {max(median_pred):.2f}\")\n",
        "    print(f\"   Min predictions range: {min(min_pred):.2f} - {max(min_pred):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c3fd5b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Calculate Uncertainties and Confidence\n",
        "print(\"=\"*60)\n",
        "print(\"UNCERTAINTY QUANTIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def calculate_entropy_confidence(probabilities):\n",
        "    \"\"\"Calculate entropy-based confidence scores\"\"\"\n",
        "    epsilon = 1e-10\n",
        "    entropy = -np.sum(probabilities * np.log(probabilities + epsilon), axis=1)\n",
        "    max_entropy = -np.log(1/probabilities.shape[1])\n",
        "    confidence_score = 1 - (entropy / max_entropy)\n",
        "    \n",
        "    confidence_levels = np.where(\n",
        "        confidence_score >= 0.9, 'Very High',\n",
        "        np.where(confidence_score >= 0.7, 'High',\n",
        "                np.where(confidence_score >= 0.5, 'Medium',\n",
        "                        np.where(confidence_score >= 0.3, 'Low', 'Very Low')))\n",
        "    )\n",
        "    return confidence_score, confidence_levels\n",
        "\n",
        "# Calculate classification confidence\n",
        "confidence_scores, confidence_levels = calculate_entropy_confidence(clf_proba)\n",
        "\n",
        "# Calculate regression uncertainties using virtual ensembles\n",
        "uncertainties = {}\n",
        "for model_name in ['median', 'min']:\n",
        "    model = loaded_models[model_name]\n",
        "    n_trees = model.tree_count_\n",
        "    n_subsets = 10\n",
        "    trees_per_subset = max(1, n_trees // n_subsets)\n",
        "    \n",
        "    subset_predictions = []\n",
        "    for i in range(n_subsets):\n",
        "        tree_start = i * trees_per_subset\n",
        "        tree_end = min((i + 1) * trees_per_subset, n_trees)\n",
        "        if tree_start < n_trees:\n",
        "            partial_pred = model.predict(X_transformed, \n",
        "                                       ntree_start=tree_start, \n",
        "                                       ntree_end=tree_end)\n",
        "            subset_predictions.append(partial_pred)\n",
        "    \n",
        "    uncertainties[model_name] = np.std(subset_predictions, axis=0)\n",
        "\n",
        "print(\"âœ… Calculated prediction uncertainties\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23065c79",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Apply Safety Factors\n",
        "print(\"=\"*60)\n",
        "print(\"SAFETY FACTOR APPLICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load safety factor tables\n",
        "median_sf_df = pd.read_csv('script_output/models/regression_median/median_bid_safety_factor_analysis.csv')\n",
        "min_sf_df = pd.read_csv('script_output/models/regression_min/min_bid_safety_factor_analysis.csv')\n",
        "\n",
        "# Find optimal safety factors (example: SF with TPR > 0.9)\n",
        "median_optimal_idx = median_sf_df[median_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "min_optimal_idx = min_sf_df[min_sf_df['tpr'] > 0.9]['safety_factor'].idxmin()\n",
        "\n",
        "median_optimal_sf = median_sf_df.iloc[median_optimal_idx]['safety_factor']\n",
        "min_optimal_sf = min_sf_df.iloc[min_optimal_idx]['safety_factor']\n",
        "\n",
        "print(f\"ðŸ“Š Optimal safety factors:\")\n",
        "print(f\"   Median: {median_optimal_sf:.2f}\")\n",
        "print(f\"   Min: {min_optimal_sf:.2f}\")\n",
        "\n",
        "# Apply safety factors\n",
        "median_recommended = median_pred * (1 + median_optimal_sf)\n",
        "min_recommended = min_pred * (1 + min_optimal_sf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd0ece3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Create Output DataFrames\n",
        "print(\"=\"*60)\n",
        "print(\"CREATING OUTPUT TABLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get bidWindowId for each prediction\n",
        "def get_bid_window_id(row, data_cache, connection):\n",
        "    \"\"\"Get bidWindowId from acadTermId, round, and window\"\"\"\n",
        "    # Always try to load new_bid_window.csv first\n",
        "    new_bid_window_path = Path('script_output/new_bid_window.csv')\n",
        "    \n",
        "    # Initialize bid_windows_df\n",
        "    if 'bid_windows' not in data_cache:\n",
        "        # Load from database if not cached\n",
        "        try:\n",
        "            bid_windows_df = pd.read_sql_query(\"SELECT * FROM bid_window\", connection)\n",
        "        except:\n",
        "            bid_windows_df = pd.DataFrame()\n",
        "        data_cache['bid_windows'] = bid_windows_df\n",
        "    else:\n",
        "        bid_windows_df = data_cache['bid_windows'].copy()\n",
        "    \n",
        "    # Load and append new_bid_window.csv every time to ensure latest data\n",
        "    if new_bid_window_path.exists():\n",
        "        try:\n",
        "            new_bid_windows_df = pd.read_csv(new_bid_window_path)\n",
        "            if not new_bid_windows_df.empty:\n",
        "                # Combine with existing bid windows, prioritizing new ones\n",
        "                combined_df = pd.concat([bid_windows_df, new_bid_windows_df], ignore_index=True)\n",
        "                # Remove duplicates, keeping last occurrence (from new file)\n",
        "                combined_df = combined_df.drop_duplicates(\n",
        "                    subset=['acad_term_id', 'round', 'window'], \n",
        "                    keep='last'\n",
        "                )\n",
        "                bid_windows_df = combined_df\n",
        "                print(f\"ðŸ“š Loaded {len(new_bid_windows_df)} bid windows from new_bid_window.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not load new_bid_window.csv: {e}\")\n",
        "    \n",
        "    # Debug: Print what we're trying to match\n",
        "    print(f\"ðŸ” Looking for: acad_term_id='{row['acad_term_id']}', round='{row['round']}', window={row['window']}\")\n",
        "    \n",
        "    # Try exact match with more flexible matching\n",
        "    matching_windows = bid_windows_df[\n",
        "        (bid_windows_df['acad_term_id'].astype(str) == str(row['acad_term_id'])) &\n",
        "        (bid_windows_df['round'].astype(str) == str(row['round'])) &\n",
        "        (bid_windows_df['window'].astype(int) == int(row['window']))\n",
        "    ]\n",
        "    \n",
        "    if not matching_windows.empty:\n",
        "        print(f\"âœ… Found matching bid window: {matching_windows.iloc[0]['id']}\")\n",
        "        return matching_windows.iloc[0]['id']\n",
        "    \n",
        "    # Debug: Show available bid windows for this term\n",
        "    term_windows = bid_windows_df[bid_windows_df['acad_term_id'].astype(str) == str(row['acad_term_id'])]\n",
        "    if not term_windows.empty:\n",
        "        print(f\"Available windows for term {row['acad_term_id']}:\")\n",
        "        for _, window in term_windows.iterrows():\n",
        "            print(f\"  - round='{window['round']}', window={window['window']}, id={window['id']}\")\n",
        "    \n",
        "    # If no match, create a pending ID\n",
        "    pending_id = f\"PENDING_{row['acad_term_id']}_{row['round']}_{row['window']}\"\n",
        "    print(f\"âš ï¸ No match found, using: {pending_id}\")\n",
        "    return pending_id\n",
        "\n",
        "# Create PredictionResult entries linked to classId and bidWindowId\n",
        "prediction_results = []\n",
        "missing_bid_windows = set()\n",
        "\n",
        "print(f\"Processing {len(bidding_data)} prediction rows...\")\n",
        "print(f\"Total class mappings: {len(class_mappings)}\")\n",
        "\n",
        "for idx in range(len(bidding_data)):\n",
        "    bidding_row = bidding_data.iloc[idx]\n",
        "    bid_window_id = get_bid_window_id(bidding_row, data_cache, connection)\n",
        "    \n",
        "    if isinstance(bid_window_id, str) and bid_window_id.startswith(\"PENDING_\"):\n",
        "        missing_key = (bidding_row['acad_term_id'], bidding_row['round'], bidding_row['window'])\n",
        "        missing_bid_windows.add(missing_key)\n",
        "    \n",
        "    # Get all class IDs for this prediction (handles multi-professor classes)\n",
        "    matching_mappings = class_mappings[class_mappings['prediction_idx'] == idx]\n",
        "    \n",
        "    if matching_mappings.empty:\n",
        "        print(f\"Warning: No class mappings found for prediction idx {idx}\")\n",
        "        continue\n",
        "    \n",
        "    for _, mapping in matching_mappings.iterrows():\n",
        "        # Skip if class not found\n",
        "        if mapping['source'] == 'not_found' and str(mapping['class_id']).startswith('PENDING_'):\n",
        "            continue\n",
        "            \n",
        "        # Determine action based on confidence and prediction\n",
        "        clf_predicted_bool = bool(clf_pred[idx])\n",
        "        confidence_score = float(confidence_scores[idx])\n",
        "        \n",
        "        if clf_predicted_bool and confidence_score >= 0.7:\n",
        "            action = \"bid more aggressively due to high confidence of bids\"\n",
        "        elif clf_predicted_bool and confidence_score < 0.7:\n",
        "            action = \"bid carefully due to low confidence of bid\"\n",
        "        elif not clf_predicted_bool and confidence_score >= 0.7:\n",
        "            action = \"bid less aggressively due to high confidence of non bids\"\n",
        "        else:\n",
        "            action = \"bid carefully due to low confidence of non bid\"\n",
        "        \n",
        "        # Calculate risk-based recommendations\n",
        "        median_pred_val = float(median_pred[idx])\n",
        "        min_pred_val = float(min_pred[idx])\n",
        "        \n",
        "        recommendations = {\n",
        "            \"action\": action,\n",
        "            \"high-risk\": {\n",
        "                \"median_predicted\": median_pred_val,\n",
        "                \"min_predicted\": min_pred_val,\n",
        "                \"safety_factor_median\": 0,\n",
        "                \"safety_factor_min\": 0\n",
        "            },\n",
        "            \"medium-risk\": {\n",
        "                \"median_predicted\": median_pred_val * 1.3,\n",
        "                \"min_predicted\": min_pred_val * 1.3,\n",
        "                \"safety_factor_median\": 0.3,\n",
        "                \"safety_factor_min\": 0.3\n",
        "            },\n",
        "            \"low-risk\": {\n",
        "                \"median_predicted\": median_pred_val * 1.7,\n",
        "                \"min_predicted\": min_pred_val * 1.7,\n",
        "                \"safety_factor_median\": 0.7,\n",
        "                \"safety_factor_min\": 0.7\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        prediction_result = {\n",
        "            'class_id': mapping['class_id'],\n",
        "            'bid_window_id': bid_window_id,\n",
        "            'model_version': 'v4.0',\n",
        "            'clf_predicted': clf_predicted_bool,\n",
        "            'clf_prob_no_bid': float(clf_proba[idx, 0]),\n",
        "            'clf_prob_bid': float(clf_proba[idx, 1]),\n",
        "            'clf_confidence_score': confidence_score,\n",
        "            'clf_confidence_level': confidence_levels[idx],\n",
        "            'median_predicted': median_pred_val,\n",
        "            'median_lower_95ci': float(median_pred[idx] - 1.96 * uncertainties['median'][idx]),\n",
        "            'median_upper_95ci': float(median_pred[idx] + 1.96 * uncertainties['median'][idx]),\n",
        "            'median_uncertainty': float(uncertainties['median'][idx]),\n",
        "            'min_predicted': min_pred_val,\n",
        "            'min_lower_95ci': float(min_pred[idx] - 1.96 * uncertainties['min'][idx]),\n",
        "            'min_upper_95ci': float(min_pred[idx] + 1.96 * uncertainties['min'][idx]),\n",
        "            'min_uncertainty': float(uncertainties['min'][idx]),\n",
        "            'recommendations': json.dumps(recommendations)\n",
        "        }\n",
        "        prediction_results.append(prediction_result)\n",
        "\n",
        "# Report missing bid windows\n",
        "if missing_bid_windows:\n",
        "    print(f\"\\nâš ï¸ Warning: {len(missing_bid_windows)} unique bid windows not found in database:\")\n",
        "    for term_id, round_val, window_val in sorted(missing_bid_windows):\n",
        "        print(f\"   - Term: {term_id}, Round: {round_val}, Window: {window_val}\")\n",
        "\n",
        "prediction_results_df = pd.DataFrame(prediction_results)\n",
        "print(f\"\\nâœ… Created {len(prediction_results_df)} prediction results\")\n",
        "print(f\"   - From {len(bidding_data)} unique predictions\")\n",
        "print(f\"   - Mapped to {len(class_mappings)} total classes (including multi-professor duplicates)\")\n",
        "\n",
        "# Create dataset + predictions CSV\n",
        "dataset_predictions = []\n",
        "\n",
        "for idx in range(len(X_transformed)):\n",
        "    # Get transformed features\n",
        "    row_features = X_transformed.iloc[idx].to_dict()\n",
        "    \n",
        "    # Add original identifiers from bidding_data\n",
        "    bidding_row = bidding_data.iloc[idx]\n",
        "    row_features['course_code'] = bidding_row['course_code']\n",
        "    row_features['section'] = bidding_row['section']\n",
        "    row_features['acad_term_id'] = bidding_row['acad_term_id']\n",
        "    \n",
        "    # Add prediction results\n",
        "    pred_row = {\n",
        "        **row_features,\n",
        "        'clf_predicted': bool(clf_pred[idx]),\n",
        "        'clf_prob_no_bid': float(clf_proba[idx, 0]),\n",
        "        'clf_prob_bid': float(clf_proba[idx, 1]),\n",
        "        'clf_confidence_score': float(confidence_scores[idx]),\n",
        "        'clf_confidence_level': confidence_levels[idx],\n",
        "        'median_predicted': float(median_pred[idx]),\n",
        "        'median_lower_95ci': float(median_pred[idx] - 1.96 * uncertainties['median'][idx]),\n",
        "        'median_upper_95ci': float(median_pred[idx] + 1.96 * uncertainties['median'][idx]),\n",
        "        'median_uncertainty': float(uncertainties['median'][idx]),\n",
        "        'min_predicted': float(min_pred[idx]),\n",
        "        'min_lower_95ci': float(min_pred[idx] - 1.96 * uncertainties['min'][idx]),\n",
        "        'min_upper_95ci': float(min_pred[idx] + 1.96 * uncertainties['min'][idx]),\n",
        "        'min_uncertainty': float(uncertainties['min'][idx]),\n",
        "        'recommendations': json.dumps(recommendations)\n",
        "    }\n",
        "    dataset_predictions.append(pred_row)\n",
        "\n",
        "dataset_predictions_df = pd.DataFrame(dataset_predictions)\n",
        "print(f\"âœ… Created {len(dataset_predictions_df)} dataset + prediction rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "258dcc86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Create Safety Factor Table\n",
        "print(\"=\"*60)\n",
        "print(\"SAFETY FACTOR TABLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get unique acad_term_id from bidding data\n",
        "acad_term_id = bidding_data['acad_term_id'].iloc[0]  # Assuming all predictions are for same term\n",
        "\n",
        "safety_factor_entries = []\n",
        "\n",
        "# Process median safety factors\n",
        "for _, row in median_sf_df.iterrows():\n",
        "    entry = {\n",
        "        'acad_term_id': acad_term_id,\n",
        "        'prediction_type': 'median',\n",
        "        'safety_factor': float(row['safety_factor']),\n",
        "        'tpr': float(row['tpr']),\n",
        "        'mean_loss': float(row['mean_loss']),\n",
        "        'under_prediction_rate': None,\n",
        "        'mae': float(row['mae']),\n",
        "        'mse': float(row['mse']),\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    safety_factor_entries.append(entry)\n",
        "\n",
        "# Process min safety factors\n",
        "for _, row in min_sf_df.iterrows():\n",
        "    entry = {\n",
        "        'acad_term_id': acad_term_id,\n",
        "        'prediction_type': 'min',\n",
        "        'safety_factor': float(row['safety_factor']),\n",
        "        'tpr': float(row['tpr']),\n",
        "        'mean_loss': float(row['mean_loss']),\n",
        "        'under_prediction_rate': float(row.get('under_prediction_rate', 0)),\n",
        "        'mae': float(row['mae']),\n",
        "        'mse': float(row['mse']),\n",
        "        'created_at': datetime.now().isoformat()\n",
        "    }\n",
        "    safety_factor_entries.append(entry)\n",
        "\n",
        "safety_factor_df = pd.DataFrame(safety_factor_entries)\n",
        "print(f\"âœ… Created {len(safety_factor_df)} safety factor entries for term {acad_term_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f598032",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Save Results\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Save all outputs\n",
        "prediction_results_df.to_csv(output_dir / f'prediction_results_{timestamp}.csv', index=False)\n",
        "dataset_predictions_df.to_csv(output_dir / f'dataset_predictions_{timestamp}.csv', index=False)\n",
        "safety_factor_df.to_csv(output_dir / f'safety_factor_table_{timestamp}.csv', index=False)\n",
        "\n",
        "# Also save transformed features (already saved in cell 6, but ensure latest version)\n",
        "transformed_output_path = output_dir / f'transformed_features_{timestamp}.csv'\n",
        "\n",
        "# Create summary report\n",
        "summary = {\n",
        "    'timestamp': timestamp,\n",
        "    'total_predictions': len(dataset_predictions_df),\n",
        "    'total_prediction_results': len(prediction_results_df),\n",
        "    'unique_classes': prediction_results_df['class_id'].nunique() if len(prediction_results_df) > 0 else 0,\n",
        "    'unique_courses': bidding_data['course_code'].nunique(),\n",
        "    'unique_terms': bidding_data['acad_term_id'].nunique(),\n",
        "    'predictions_with_bids': int(clf_pred.sum()),\n",
        "    'predictions_without_bids': int(len(clf_pred) - clf_pred.sum()),\n",
        "    'median_bid_range': {\n",
        "        'min': float(median_pred.min()),\n",
        "        'max': float(median_pred.max()),\n",
        "        'mean': float(median_pred.mean())\n",
        "    },\n",
        "    'min_bid_range': {\n",
        "        'min': float(min_pred.min()),\n",
        "        'max': float(min_pred.max()),\n",
        "        'mean': float(min_pred.mean())\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(output_dir / f'prediction_summary_{timestamp}.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… Batch prediction completed!\")\n",
        "print(f\"ðŸ“ Results saved to {output_dir}\")\n",
        "print(f\"   - Prediction results: {len(prediction_results_df)} records\")\n",
        "print(f\"   - Dataset predictions: {len(dataset_predictions_df)} records\")\n",
        "print(f\"   - Safety factors: {len(safety_factor_df)} entries\")\n",
        "print(f\"   - Transformed features: saved to {transformed_output_path}\")\n",
        "print(f\"\\nðŸ“Š Summary:\")\n",
        "print(f\"   - Unique classes with predictions: {summary['unique_classes']}\")\n",
        "print(f\"   - Courses with bids: {summary['predictions_with_bids']}\")\n",
        "print(f\"   - Courses without bids: {summary['predictions_without_bids']}\")\n",
        "print(f\"   - Median bid range: {summary['median_bid_range']['min']:.0f} - {summary['median_bid_range']['max']:.0f}\")\n",
        "\n",
        "# Close database connection\n",
        "if connection:\n",
        "    connection.close()\n",
        "    print(\"\\nðŸ”’ Database connection closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bidly_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
